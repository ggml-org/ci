### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.12 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.45 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.28 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.71 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.00 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.95 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.38 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.05 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.94 sec*proc (29 tests)

Total Test time (real) = 165.95 sec

real	2m45.970s
user	4m39.959s
sys	0m5.691s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.45 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.27 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.82 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.32 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.52 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.36 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.44 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.30 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.92 sec*proc (29 tests)

Total Test time (real) =  48.93 sec

real	0m48.946s
user	0m54.595s
sys	0m5.202s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.249 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.719 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.729 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.731 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.731 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.732 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.735 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.736 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.737 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.737 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.738 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.741 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.742 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.742 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.743 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.743 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.744 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.745 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.072 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.074 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.075 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.075 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.076 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.076 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.034.077 I llama_model_loader: - type  f32:  124 tensors
0.00.034.077 I llama_model_loader: - type  f16:   73 tensors
0.00.034.078 I print_info: file format = GGUF V3 (latest)
0.00.034.079 I print_info: file type   = F16
0.00.034.080 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.038.604 I load: special tokens cache size = 5
0.00.040.902 I load: token to piece cache size = 0.2032 MB
0.00.040.928 I print_info: arch             = bert
0.00.040.929 I print_info: vocab_only       = 0
0.00.040.930 I print_info: n_ctx_train      = 512
0.00.040.930 I print_info: n_embd           = 384
0.00.040.930 I print_info: n_layer          = 12
0.00.040.933 I print_info: n_head           = 12
0.00.040.934 I print_info: n_head_kv        = 12
0.00.040.934 I print_info: n_rot            = 32
0.00.040.941 I print_info: n_swa            = 0
0.00.040.941 I print_info: n_embd_head_k    = 32
0.00.040.941 I print_info: n_embd_head_v    = 32
0.00.040.942 I print_info: n_gqa            = 1
0.00.040.943 I print_info: n_embd_k_gqa     = 384
0.00.040.944 I print_info: n_embd_v_gqa     = 384
0.00.040.945 I print_info: f_norm_eps       = 1.0e-12
0.00.040.945 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.946 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.946 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.946 I print_info: f_logit_scale    = 0.0e+00
0.00.040.947 I print_info: n_ff             = 1536
0.00.040.947 I print_info: n_expert         = 0
0.00.040.947 I print_info: n_expert_used    = 0
0.00.040.948 I print_info: causal attn      = 0
0.00.040.948 I print_info: pooling type     = 2
0.00.040.948 I print_info: rope type        = 2
0.00.040.950 I print_info: rope scaling     = linear
0.00.040.951 I print_info: freq_base_train  = 10000.0
0.00.040.951 I print_info: freq_scale_train = 1
0.00.040.951 I print_info: n_ctx_orig_yarn  = 512
0.00.040.952 I print_info: rope_finetuned   = unknown
0.00.040.952 I print_info: ssm_d_conv       = 0
0.00.040.952 I print_info: ssm_d_inner      = 0
0.00.040.952 I print_info: ssm_d_state      = 0
0.00.040.952 I print_info: ssm_dt_rank      = 0
0.00.040.952 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.953 I print_info: model type       = 33M
0.00.040.953 I print_info: model params     = 33.21 M
0.00.040.953 I print_info: general.name     = Bge Small
0.00.040.954 I print_info: vocab type       = WPM
0.00.040.954 I print_info: n_vocab          = 30522
0.00.040.955 I print_info: n_merges         = 0
0.00.040.955 I print_info: BOS token        = 101 '[CLS]'
0.00.040.955 I print_info: UNK token        = 100 '[UNK]'
0.00.040.955 I print_info: SEP token        = 102 '[SEP]'
0.00.040.956 I print_info: PAD token        = 0 '[PAD]'
0.00.040.956 I print_info: MASK token       = 103 '[MASK]'
0.00.040.956 I print_info: LF token         = 0 '[PAD]'
0.00.040.956 I print_info: max token length = 21
0.00.040.957 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.044.058 I load_tensors: offloading 12 repeating layers to GPU
0.00.044.059 I load_tensors: offloading output layer to GPU
0.00.044.060 I load_tensors: offloaded 13/13 layers to GPU
0.00.044.085 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.087 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.387 I llama_init_from_model: n_seq_max     = 1
0.00.044.388 I llama_init_from_model: n_ctx         = 512
0.00.044.388 I llama_init_from_model: n_ctx_per_seq = 512
0.00.044.388 I llama_init_from_model: n_batch       = 2048
0.00.044.389 I llama_init_from_model: n_ubatch      = 2048
0.00.044.389 I llama_init_from_model: flash_attn    = 0
0.00.044.390 I llama_init_from_model: freq_base     = 10000.0
0.00.044.390 I llama_init_from_model: freq_scale    = 1
0.00.044.391 I ggml_metal_init: allocating
0.00.044.396 I ggml_metal_init: found device: Apple M4
0.00.044.401 I ggml_metal_init: picking default device: Apple M4
0.00.045.072 I ggml_metal_init: using embedded metal library
0.00.049.277 I ggml_metal_init: GPU name:   Apple M4
0.00.049.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.280 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.281 I ggml_metal_init: simdgroup reduction   = true
0.00.049.281 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.281 I ggml_metal_init: has residency sets    = true
0.00.049.281 I ggml_metal_init: has bfloat            = true
0.00.049.281 I ggml_metal_init: use bfloat            = true
0.00.049.282 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.639 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.062.303 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.062.305 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.062.307 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.063.508 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.063.509 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.063.509 I llama_init_from_model: graph nodes  = 429
0.00.063.510 I llama_init_from_model: graph splits = 2
0.00.063.511 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.063.511 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.069.227 I 
0.00.069.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.069.950 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.096 I llama_perf_context_print:        load time =      47.03 ms
0.00.075.097 I llama_perf_context_print: prompt eval time =       4.98 ms /     9 tokens (    0.55 ms per token,  1806.14 tokens per second)
0.00.075.098 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.098 I llama_perf_context_print:       total time =       5.87 ms /    10 tokens
0.00.075.266 I ggml_metal_free: deallocating

real	0m0.287s
user	0m0.052s
sys	0m0.039s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.422 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.184 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.190 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.190 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.191 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.191 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.192 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.192 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.193 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.193 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.193 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.195 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.196 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.196 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.197 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.197 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.197 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.612 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.205 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.206 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.206 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.206 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.207 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.207 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.207 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.208 I llama_model_loader: - type  f32:  124 tensors
0.00.015.208 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.208 I print_info: file format = GGUF V3 (latest)
0.00.015.209 I print_info: file type   = Q8_0
0.00.015.210 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.585 I load: special tokens cache size = 5
0.00.018.932 I load: token to piece cache size = 0.2032 MB
0.00.018.941 I print_info: arch             = bert
0.00.018.942 I print_info: vocab_only       = 0
0.00.018.942 I print_info: n_ctx_train      = 512
0.00.018.942 I print_info: n_embd           = 384
0.00.018.942 I print_info: n_layer          = 12
0.00.018.945 I print_info: n_head           = 12
0.00.018.946 I print_info: n_head_kv        = 12
0.00.018.946 I print_info: n_rot            = 32
0.00.018.946 I print_info: n_swa            = 0
0.00.018.946 I print_info: n_embd_head_k    = 32
0.00.018.946 I print_info: n_embd_head_v    = 32
0.00.018.947 I print_info: n_gqa            = 1
0.00.018.947 I print_info: n_embd_k_gqa     = 384
0.00.018.948 I print_info: n_embd_v_gqa     = 384
0.00.018.948 I print_info: f_norm_eps       = 1.0e-12
0.00.018.949 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.949 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.951 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.951 I print_info: f_logit_scale    = 0.0e+00
0.00.018.951 I print_info: n_ff             = 1536
0.00.018.952 I print_info: n_expert         = 0
0.00.018.952 I print_info: n_expert_used    = 0
0.00.018.952 I print_info: causal attn      = 0
0.00.018.952 I print_info: pooling type     = 2
0.00.018.952 I print_info: rope type        = 2
0.00.018.952 I print_info: rope scaling     = linear
0.00.018.953 I print_info: freq_base_train  = 10000.0
0.00.018.955 I print_info: freq_scale_train = 1
0.00.018.955 I print_info: n_ctx_orig_yarn  = 512
0.00.018.955 I print_info: rope_finetuned   = unknown
0.00.018.955 I print_info: ssm_d_conv       = 0
0.00.018.955 I print_info: ssm_d_inner      = 0
0.00.018.955 I print_info: ssm_d_state      = 0
0.00.018.955 I print_info: ssm_dt_rank      = 0
0.00.018.955 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.956 I print_info: model type       = 33M
0.00.018.956 I print_info: model params     = 33.21 M
0.00.018.956 I print_info: general.name     = Bge Small
0.00.018.957 I print_info: vocab type       = WPM
0.00.018.958 I print_info: n_vocab          = 30522
0.00.018.958 I print_info: n_merges         = 0
0.00.018.958 I print_info: BOS token        = 101 '[CLS]'
0.00.018.958 I print_info: UNK token        = 100 '[UNK]'
0.00.018.958 I print_info: SEP token        = 102 '[SEP]'
0.00.018.959 I print_info: PAD token        = 0 '[PAD]'
0.00.018.959 I print_info: MASK token       = 103 '[MASK]'
0.00.018.959 I print_info: LF token         = 0 '[PAD]'
0.00.018.959 I print_info: max token length = 21
0.00.018.959 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.605 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.606 I load_tensors: offloading output layer to GPU
0.00.020.606 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.612 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.613 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.795 I llama_init_from_model: n_seq_max     = 1
0.00.020.796 I llama_init_from_model: n_ctx         = 512
0.00.020.796 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.796 I llama_init_from_model: n_batch       = 2048
0.00.020.796 I llama_init_from_model: n_ubatch      = 2048
0.00.020.796 I llama_init_from_model: flash_attn    = 0
0.00.020.797 I llama_init_from_model: freq_base     = 10000.0
0.00.020.797 I llama_init_from_model: freq_scale    = 1
0.00.020.798 I ggml_metal_init: allocating
0.00.020.802 I ggml_metal_init: found device: Apple M4
0.00.020.805 I ggml_metal_init: picking default device: Apple M4
0.00.021.263 I ggml_metal_init: using embedded metal library
0.00.023.875 I ggml_metal_init: GPU name:   Apple M4
0.00.023.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.878 I ggml_metal_init: simdgroup reduction   = true
0.00.023.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.878 I ggml_metal_init: has residency sets    = true
0.00.023.878 I ggml_metal_init: has bfloat            = true
0.00.023.878 I ggml_metal_init: use bfloat            = true
0.00.023.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.879 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.236 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.865 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.867 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.869 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.854 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.855 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.856 I llama_init_from_model: graph nodes  = 429
0.00.034.856 I llama_init_from_model: graph splits = 2
0.00.034.857 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.934 I 
0.00.038.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.497 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.858 I llama_perf_context_print:        load time =      29.51 ms
0.00.043.859 I llama_perf_context_print: prompt eval time =       4.25 ms /     9 tokens (    0.47 ms per token,  2118.15 tokens per second)
0.00.043.860 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.860 I llama_perf_context_print:       total time =       4.92 ms /    10 tokens
0.00.044.075 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.144 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.369 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.975 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.980 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.022.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.981 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.022.983 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.022.984 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.022.984 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.022.985 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.022.985 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.022.986 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.022.986 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.022.989 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.989 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.989 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.022.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.990 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.026.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.027.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.030.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.030.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.030.403 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.030.404 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.030.404 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.030.404 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.405 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.030.405 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.030.405 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.030.406 I llama_model_loader: - type  f32:   40 tensors
0.00.030.406 I llama_model_loader: - type  f16:   30 tensors
0.00.030.407 I print_info: file format = GGUF V3 (latest)
0.00.030.407 I print_info: file type   = F16
0.00.030.409 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.033.054 W load: empty token at index 5
0.00.036.556 W load: model vocab missing newline token, using special_pad_id instead
0.00.037.696 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.037.729 I load: special tokens cache size = 5
0.00.292.722 I load: token to piece cache size = 1.5060 MB
0.00.292.762 I print_info: arch             = jina-bert-v2
0.00.292.763 I print_info: vocab_only       = 0
0.00.292.763 I print_info: n_ctx_train      = 8192
0.00.292.764 I print_info: n_embd           = 384
0.00.292.764 I print_info: n_layer          = 4
0.00.292.769 I print_info: n_head           = 12
0.00.292.769 I print_info: n_head_kv        = 12
0.00.292.769 I print_info: n_rot            = 32
0.00.292.769 I print_info: n_swa            = 0
0.00.292.770 I print_info: n_embd_head_k    = 32
0.00.292.770 I print_info: n_embd_head_v    = 32
0.00.292.770 I print_info: n_gqa            = 1
0.00.292.771 I print_info: n_embd_k_gqa     = 384
0.00.292.771 I print_info: n_embd_v_gqa     = 384
0.00.292.772 I print_info: f_norm_eps       = 1.0e-12
0.00.292.772 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.292.772 I print_info: f_clamp_kqv      = 0.0e+00
0.00.292.773 I print_info: f_max_alibi_bias = 8.0e+00
0.00.292.773 I print_info: f_logit_scale    = 0.0e+00
0.00.292.773 I print_info: n_ff             = 1536
0.00.292.773 I print_info: n_expert         = 0
0.00.292.774 I print_info: n_expert_used    = 0
0.00.292.774 I print_info: causal attn      = 0
0.00.292.774 I print_info: pooling type     = -1
0.00.292.774 I print_info: rope type        = -1
0.00.292.774 I print_info: rope scaling     = linear
0.00.292.775 I print_info: freq_base_train  = 10000.0
0.00.292.775 I print_info: freq_scale_train = 1
0.00.292.775 I print_info: n_ctx_orig_yarn  = 8192
0.00.292.775 I print_info: rope_finetuned   = unknown
0.00.292.775 I print_info: ssm_d_conv       = 0
0.00.292.775 I print_info: ssm_d_inner      = 0
0.00.292.776 I print_info: ssm_d_state      = 0
0.00.292.776 I print_info: ssm_dt_rank      = 0
0.00.292.776 I print_info: ssm_dt_b_c_rms   = 0
0.00.292.776 I print_info: model type       = 33M
0.00.292.776 I print_info: model params     = 32.90 M
0.00.292.777 I print_info: general.name     = Jina Bert Implementation
0.00.292.777 I print_info: vocab type       = BPE
0.00.292.777 I print_info: n_vocab          = 61056
0.00.292.778 I print_info: n_merges         = 39382
0.00.292.778 I print_info: BOS token        = 0 '<s>'
0.00.292.778 I print_info: EOS token        = 2 '</s>'
0.00.292.778 I print_info: UNK token        = 3 '<unk>'
0.00.292.778 I print_info: SEP token        = 2 '</s>'
0.00.292.779 I print_info: PAD token        = 1 '<pad>'
0.00.292.779 I print_info: MASK token       = 4 '<mask>'
0.00.292.779 I print_info: EOG token        = 2 '</s>'
0.00.292.779 I print_info: max token length = 45
0.00.292.780 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.294.169 I load_tensors: offloading 4 repeating layers to GPU
0.00.294.170 I load_tensors: offloading output layer to GPU
0.00.294.171 I load_tensors: offloaded 5/5 layers to GPU
0.00.294.191 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.294.192 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.294.470 I llama_init_from_model: n_seq_max     = 1
0.00.294.471 I llama_init_from_model: n_ctx         = 8192
0.00.294.471 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.294.471 I llama_init_from_model: n_batch       = 2048
0.00.294.471 I llama_init_from_model: n_ubatch      = 2048
0.00.294.471 I llama_init_from_model: flash_attn    = 0
0.00.294.472 I llama_init_from_model: freq_base     = 10000.0
0.00.294.472 I llama_init_from_model: freq_scale    = 1
0.00.294.473 I ggml_metal_init: allocating
0.00.294.477 I ggml_metal_init: found device: Apple M4
0.00.294.480 I ggml_metal_init: picking default device: Apple M4
0.00.294.984 I ggml_metal_init: using embedded metal library
0.00.297.632 I ggml_metal_init: GPU name:   Apple M4
0.00.297.633 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.297.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.297.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.297.634 I ggml_metal_init: simdgroup reduction   = true
0.00.297.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.297.635 I ggml_metal_init: has residency sets    = true
0.00.297.635 I ggml_metal_init: has bfloat            = true
0.00.297.635 I ggml_metal_init: use bfloat            = true
0.00.297.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.297.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.308.295 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.311.590 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.311.593 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.311.597 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.319.253 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.319.256 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.319.256 I llama_init_from_model: graph nodes  = 154
0.00.319.256 I llama_init_from_model: graph splits = 2
0.00.319.258 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.319.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.338.245 I 
0.00.338.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.338.641 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.338.642 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.338.647 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.338.647 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.338.653 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.338.653 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.339.218 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.343.060 I llama_perf_context_print:        load time =     320.87 ms
0.00.343.062 I llama_perf_context_print: prompt eval time =       3.83 ms /    62 tokens (    0.06 ms per token, 16192.22 tokens per second)
0.00.343.062 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.343.063 I llama_perf_context_print:       total time =       4.82 ms /    63 tokens
0.00.343.343 I ggml_metal_free: deallocating

real	0m1.123s
user	0m0.305s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.367 I main: llama backend init
0.00.000.373 I main: load the model and apply lora adapter, if any
0.00.049.492 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.062.314 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.336 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.359 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.360 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.360 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.071.580 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.073.595 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.080.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.080.774 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.080.775 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.080.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.080.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.080.777 I llama_model_loader: - type  f32:  194 tensors
0.00.080.777 I llama_model_loader: - type  f16:   98 tensors
0.00.080.779 I print_info: file format = GGUF V3 (latest)
0.00.080.782 I print_info: file type   = all F32 (guessed)
0.00.080.784 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.023 I load: special tokens cache size = 25
0.00.102.839 I load: token to piece cache size = 0.2984 MB
0.00.102.862 I print_info: arch             = gptneox
0.00.102.863 I print_info: vocab_only       = 0
0.00.102.863 I print_info: n_ctx_train      = 2048
0.00.102.864 I print_info: n_embd           = 2048
0.00.102.864 I print_info: n_layer          = 24
0.00.102.867 I print_info: n_head           = 16
0.00.102.868 I print_info: n_head_kv        = 16
0.00.102.868 I print_info: n_rot            = 32
0.00.102.868 I print_info: n_swa            = 0
0.00.102.869 I print_info: n_embd_head_k    = 128
0.00.102.869 I print_info: n_embd_head_v    = 128
0.00.102.870 I print_info: n_gqa            = 1
0.00.102.871 I print_info: n_embd_k_gqa     = 2048
0.00.102.871 I print_info: n_embd_v_gqa     = 2048
0.00.102.872 I print_info: f_norm_eps       = 1.0e-05
0.00.102.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.874 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.874 I print_info: f_logit_scale    = 0.0e+00
0.00.102.876 I print_info: n_ff             = 8192
0.00.102.876 I print_info: n_expert         = 0
0.00.102.876 I print_info: n_expert_used    = 0
0.00.102.876 I print_info: causal attn      = 1
0.00.102.876 I print_info: pooling type     = 0
0.00.102.878 I print_info: rope type        = 2
0.00.102.878 I print_info: rope scaling     = linear
0.00.102.879 I print_info: freq_base_train  = 10000.0
0.00.102.879 I print_info: freq_scale_train = 1
0.00.102.879 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.880 I print_info: rope_finetuned   = unknown
0.00.102.880 I print_info: ssm_d_conv       = 0
0.00.102.880 I print_info: ssm_d_inner      = 0
0.00.102.880 I print_info: ssm_d_state      = 0
0.00.102.880 I print_info: ssm_dt_rank      = 0
0.00.102.880 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.880 I print_info: model type       = 1.4B
0.00.102.881 I print_info: model params     = 1.41 B
0.00.102.881 I print_info: general.name     = 1.4B
0.00.102.882 I print_info: vocab type       = BPE
0.00.102.882 I print_info: n_vocab          = 50304
0.00.102.882 I print_info: n_merges         = 50009
0.00.102.882 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.882 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.883 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.887 I print_info: LF token         = 187 ''
0.00.102.887 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.887 I print_info: max token length = 1024
0.00.102.888 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.152.139 I load_tensors: offloading 24 repeating layers to GPU
0.00.152.142 I load_tensors: offloading output layer to GPU
0.00.152.142 I load_tensors: offloaded 25/25 layers to GPU
0.00.152.170 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.152.171 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.152.828 I llama_init_from_model: n_seq_max     = 1
0.00.152.829 I llama_init_from_model: n_ctx         = 2048
0.00.152.829 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.152.829 I llama_init_from_model: n_batch       = 2048
0.00.152.829 I llama_init_from_model: n_ubatch      = 512
0.00.152.830 I llama_init_from_model: flash_attn    = 0
0.00.152.830 I llama_init_from_model: freq_base     = 10000.0
0.00.152.830 I llama_init_from_model: freq_scale    = 1
0.00.152.833 I ggml_metal_init: allocating
0.00.152.932 I ggml_metal_init: found device: Apple M4
0.00.152.939 I ggml_metal_init: picking default device: Apple M4
0.00.153.564 I ggml_metal_init: using embedded metal library
0.00.417.546 I ggml_metal_init: GPU name:   Apple M4
0.00.417.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.417.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.417.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.417.566 I ggml_metal_init: simdgroup reduction   = true
0.00.417.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.417.567 I ggml_metal_init: has residency sets    = true
0.00.417.567 I ggml_metal_init: has bfloat            = true
0.00.417.568 I ggml_metal_init: use bfloat            = true
0.00.417.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.417.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.456.142 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.493.040 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.493.050 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.493.073 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.497.484 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.497.487 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.497.488 I llama_init_from_model: graph nodes  = 967
0.00.497.488 I llama_init_from_model: graph splits = 2
0.00.497.493 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.497.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.497.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.375 I main: llama threadpool init, n_threads = 4
0.00.564.437 I 
0.00.564.466 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.467 I 
0.00.564.651 I sampler seed: 1234
0.00.564.655 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.564.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.564.690 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.564.691 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.401.590 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.02.401.591 I llama_perf_context_print:        load time =     513.56 ms
0.02.401.591 I llama_perf_context_print: prompt eval time =      54.70 ms /     7 tokens (    7.81 ms per token,   127.96 tokens per second)
0.02.401.592 I llama_perf_context_print:        eval time =    1779.24 ms /    63 runs   (   28.24 ms per token,    35.41 tokens per second)
0.02.401.592 I llama_perf_context_print:       total time =    1838.52 ms /    70 tokens
0.02.401.793 I ggml_metal_free: deallocating

real	0m2.739s
user	0m0.145s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.507 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.237 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.189 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.199 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.203 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.445 I llama_model_loader: - type  f32:  194 tensors
0.00.053.446 I llama_model_loader: - type  f16:   98 tensors
0.00.053.447 I print_info: file format = GGUF V3 (latest)
0.00.053.448 I print_info: file type   = all F32 (guessed)
0.00.053.449 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.704 I load: special tokens cache size = 25
0.00.072.532 I load: token to piece cache size = 0.2984 MB
0.00.072.554 I print_info: arch             = gptneox
0.00.072.555 I print_info: vocab_only       = 0
0.00.072.555 I print_info: n_ctx_train      = 2048
0.00.072.555 I print_info: n_embd           = 2048
0.00.072.555 I print_info: n_layer          = 24
0.00.072.560 I print_info: n_head           = 16
0.00.072.560 I print_info: n_head_kv        = 16
0.00.072.560 I print_info: n_rot            = 32
0.00.072.561 I print_info: n_swa            = 0
0.00.072.561 I print_info: n_embd_head_k    = 128
0.00.072.561 I print_info: n_embd_head_v    = 128
0.00.072.562 I print_info: n_gqa            = 1
0.00.072.562 I print_info: n_embd_k_gqa     = 2048
0.00.072.563 I print_info: n_embd_v_gqa     = 2048
0.00.072.564 I print_info: f_norm_eps       = 1.0e-05
0.00.072.564 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.564 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.568 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.568 I print_info: f_logit_scale    = 0.0e+00
0.00.072.568 I print_info: n_ff             = 8192
0.00.072.569 I print_info: n_expert         = 0
0.00.072.569 I print_info: n_expert_used    = 0
0.00.072.569 I print_info: causal attn      = 1
0.00.072.569 I print_info: pooling type     = 0
0.00.072.569 I print_info: rope type        = 2
0.00.072.569 I print_info: rope scaling     = linear
0.00.072.570 I print_info: freq_base_train  = 10000.0
0.00.072.570 I print_info: freq_scale_train = 1
0.00.072.570 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.570 I print_info: rope_finetuned   = unknown
0.00.072.571 I print_info: ssm_d_conv       = 0
0.00.072.571 I print_info: ssm_d_inner      = 0
0.00.072.571 I print_info: ssm_d_state      = 0
0.00.072.572 I print_info: ssm_dt_rank      = 0
0.00.072.573 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.573 I print_info: model type       = 1.4B
0.00.072.573 I print_info: model params     = 1.41 B
0.00.072.573 I print_info: general.name     = 1.4B
0.00.072.574 I print_info: vocab type       = BPE
0.00.072.574 I print_info: n_vocab          = 50304
0.00.072.574 I print_info: n_merges         = 50009
0.00.072.575 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.575 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.575 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.575 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.575 I print_info: LF token         = 187 ''
0.00.072.576 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.576 I print_info: max token length = 1024
0.00.072.576 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.502.507 I load_tensors: offloading 24 repeating layers to GPU
0.01.502.513 I load_tensors: offloading output layer to GPU
0.01.502.513 I load_tensors: offloaded 25/25 layers to GPU
0.01.502.546 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.502.548 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.503.540 I llama_init_from_model: n_seq_max     = 1
0.01.503.541 I llama_init_from_model: n_ctx         = 128
0.01.503.542 I llama_init_from_model: n_ctx_per_seq = 128
0.01.503.542 I llama_init_from_model: n_batch       = 128
0.01.503.542 I llama_init_from_model: n_ubatch      = 128
0.01.503.542 I llama_init_from_model: flash_attn    = 0
0.01.503.543 I llama_init_from_model: freq_base     = 10000.0
0.01.503.543 I llama_init_from_model: freq_scale    = 1
0.01.503.544 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.503.545 I ggml_metal_init: allocating
0.01.503.600 I ggml_metal_init: found device: Apple M4
0.01.503.607 I ggml_metal_init: picking default device: Apple M4
0.01.504.589 I ggml_metal_init: using embedded metal library
0.01.508.524 I ggml_metal_init: GPU name:   Apple M4
0.01.508.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.508.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.508.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.508.527 I ggml_metal_init: simdgroup reduction   = true
0.01.508.528 I ggml_metal_init: simdgroup matrix mul. = true
0.01.508.528 I ggml_metal_init: has residency sets    = true
0.01.508.528 I ggml_metal_init: has bfloat            = true
0.01.508.528 I ggml_metal_init: use bfloat            = true
0.01.508.529 I ggml_metal_init: hasUnifiedMemory      = true
0.01.508.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.519.996 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.521.692 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.521.694 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.521.710 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.523.364 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.523.366 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.523.366 I llama_init_from_model: graph nodes  = 967
0.01.523.366 I llama_init_from_model: graph splits = 2
0.01.523.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.523.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.558.461 I 
0.01.558.502 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.558.506 I perplexity: tokenizing the input ..
0.01.563.897 I perplexity: tokenization took 5.389 ms
0.01.563.907 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.682.268 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.683.735 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.683.765 I llama_perf_context_print:        load time =    1534.22 ms
0.01.683.766 I llama_perf_context_print: prompt eval time =     118.05 ms /   128 tokens (    0.92 ms per token,  1084.26 tokens per second)
0.01.683.767 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.683.767 I llama_perf_context_print:       total time =     125.31 ms /   129 tokens
0.01.684.081 I ggml_metal_free: deallocating

real	0m1.893s
user	0m0.093s
sys	0m0.245s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.754 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.755 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.756 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.757 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.763 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.966 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.967 I llama_model_loader: - type  f32:  194 tensors
0.00.040.968 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.968 I print_info: file format = GGUF V3 (latest)
0.00.040.969 I print_info: file type   = Q8_0
0.00.040.970 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.050.434 I load: special tokens cache size = 25
0.00.058.252 I load: token to piece cache size = 0.2984 MB
0.00.058.269 I print_info: arch             = gptneox
0.00.058.270 I print_info: vocab_only       = 0
0.00.058.270 I print_info: n_ctx_train      = 2048
0.00.058.271 I print_info: n_embd           = 2048
0.00.058.271 I print_info: n_layer          = 24
0.00.058.275 I print_info: n_head           = 16
0.00.058.276 I print_info: n_head_kv        = 16
0.00.058.276 I print_info: n_rot            = 32
0.00.058.276 I print_info: n_swa            = 0
0.00.058.276 I print_info: n_embd_head_k    = 128
0.00.058.276 I print_info: n_embd_head_v    = 128
0.00.058.277 I print_info: n_gqa            = 1
0.00.058.278 I print_info: n_embd_k_gqa     = 2048
0.00.058.279 I print_info: n_embd_v_gqa     = 2048
0.00.058.280 I print_info: f_norm_eps       = 1.0e-05
0.00.058.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.281 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.281 I print_info: f_logit_scale    = 0.0e+00
0.00.058.282 I print_info: n_ff             = 8192
0.00.058.282 I print_info: n_expert         = 0
0.00.058.282 I print_info: n_expert_used    = 0
0.00.058.284 I print_info: causal attn      = 1
0.00.058.284 I print_info: pooling type     = 0
0.00.058.284 I print_info: rope type        = 2
0.00.058.284 I print_info: rope scaling     = linear
0.00.058.285 I print_info: freq_base_train  = 10000.0
0.00.058.285 I print_info: freq_scale_train = 1
0.00.058.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.288 I print_info: rope_finetuned   = unknown
0.00.058.288 I print_info: ssm_d_conv       = 0
0.00.058.288 I print_info: ssm_d_inner      = 0
0.00.058.288 I print_info: ssm_d_state      = 0
0.00.058.288 I print_info: ssm_dt_rank      = 0
0.00.058.288 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.289 I print_info: model type       = 1.4B
0.00.058.289 I print_info: model params     = 1.41 B
0.00.058.289 I print_info: general.name     = 1.4B
0.00.058.292 I print_info: vocab type       = BPE
0.00.058.292 I print_info: n_vocab          = 50304
0.00.058.292 I print_info: n_merges         = 50009
0.00.058.292 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.293 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.293 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.293 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.293 I print_info: LF token         = 187 ''
0.00.058.294 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.294 I print_info: max token length = 1024
0.00.058.294 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.176.784 I load_tensors: offloading 24 repeating layers to GPU
0.01.176.789 I load_tensors: offloading output layer to GPU
0.01.176.790 I load_tensors: offloaded 25/25 layers to GPU
0.01.176.814 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.176.815 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.178.009 I llama_init_from_model: n_seq_max     = 1
0.01.178.010 I llama_init_from_model: n_ctx         = 2048
0.01.178.011 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.178.011 I llama_init_from_model: n_batch       = 2048
0.01.178.011 I llama_init_from_model: n_ubatch      = 512
0.01.178.012 I llama_init_from_model: flash_attn    = 0
0.01.178.012 I llama_init_from_model: freq_base     = 10000.0
0.01.178.013 I llama_init_from_model: freq_scale    = 1
0.01.178.014 I ggml_metal_init: allocating
0.01.178.024 I ggml_metal_init: found device: Apple M4
0.01.178.031 I ggml_metal_init: picking default device: Apple M4
0.01.179.078 I ggml_metal_init: using embedded metal library
0.01.184.485 I ggml_metal_init: GPU name:   Apple M4
0.01.184.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.184.490 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.184.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.184.491 I ggml_metal_init: simdgroup reduction   = true
0.01.184.491 I ggml_metal_init: simdgroup matrix mul. = true
0.01.184.492 I ggml_metal_init: has residency sets    = true
0.01.184.492 I ggml_metal_init: has bfloat            = true
0.01.184.492 I ggml_metal_init: use bfloat            = true
0.01.184.493 I ggml_metal_init: hasUnifiedMemory      = true
0.01.184.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.200.663 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.258.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.258.059 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.258.080 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.262.573 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.262.575 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.262.575 I llama_init_from_model: graph nodes  = 967
0.01.262.575 I llama_init_from_model: graph splits = 2
0.01.262.581 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.262.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.262.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.319.380 I main: llama threadpool init, n_threads = 4
0.01.319.426 I 
0.01.319.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.319.446 I 
0.01.319.597 I sampler seed: 1234
0.01.319.602 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.319.616 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.319.617 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.319.617 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.403.539 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.02.403.540 I llama_perf_context_print:        load time =    1307.84 ms
0.02.403.541 I llama_perf_context_print: prompt eval time =      49.01 ms /     7 tokens (    7.00 ms per token,   142.82 tokens per second)
0.02.403.542 I llama_perf_context_print:        eval time =    1032.01 ms /    63 runs   (   16.38 ms per token,    61.05 tokens per second)
0.02.403.544 I llama_perf_context_print:       total time =    1084.87 ms /    70 tokens
0.02.403.803 I ggml_metal_free: deallocating

real	0m2.423s
user	0m0.111s
sys	0m0.294s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.583 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.811 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.324 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.325 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.326 I llama_model_loader: - type  f32:  194 tensors
0.00.033.326 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.327 I print_info: file format = GGUF V3 (latest)
0.00.033.327 I print_info: file type   = Q8_0
0.00.033.328 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.559 I load: special tokens cache size = 25
0.00.048.218 I load: token to piece cache size = 0.2984 MB
0.00.048.231 I print_info: arch             = gptneox
0.00.048.232 I print_info: vocab_only       = 0
0.00.048.232 I print_info: n_ctx_train      = 2048
0.00.048.232 I print_info: n_embd           = 2048
0.00.048.233 I print_info: n_layer          = 24
0.00.048.237 I print_info: n_head           = 16
0.00.048.238 I print_info: n_head_kv        = 16
0.00.048.238 I print_info: n_rot            = 32
0.00.048.238 I print_info: n_swa            = 0
0.00.048.238 I print_info: n_embd_head_k    = 128
0.00.048.238 I print_info: n_embd_head_v    = 128
0.00.048.239 I print_info: n_gqa            = 1
0.00.048.240 I print_info: n_embd_k_gqa     = 2048
0.00.048.240 I print_info: n_embd_v_gqa     = 2048
0.00.048.241 I print_info: f_norm_eps       = 1.0e-05
0.00.048.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.242 I print_info: f_logit_scale    = 0.0e+00
0.00.048.242 I print_info: n_ff             = 8192
0.00.048.242 I print_info: n_expert         = 0
0.00.048.243 I print_info: n_expert_used    = 0
0.00.048.243 I print_info: causal attn      = 1
0.00.048.243 I print_info: pooling type     = 0
0.00.048.243 I print_info: rope type        = 2
0.00.048.243 I print_info: rope scaling     = linear
0.00.048.244 I print_info: freq_base_train  = 10000.0
0.00.048.244 I print_info: freq_scale_train = 1
0.00.048.244 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.244 I print_info: rope_finetuned   = unknown
0.00.048.244 I print_info: ssm_d_conv       = 0
0.00.048.245 I print_info: ssm_d_inner      = 0
0.00.048.245 I print_info: ssm_d_state      = 0
0.00.048.245 I print_info: ssm_dt_rank      = 0
0.00.048.245 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.245 I print_info: model type       = 1.4B
0.00.048.245 I print_info: model params     = 1.41 B
0.00.048.247 I print_info: general.name     = 1.4B
0.00.048.247 I print_info: vocab type       = BPE
0.00.048.248 I print_info: n_vocab          = 50304
0.00.048.248 I print_info: n_merges         = 50009
0.00.048.248 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.248 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.249 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.249 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.249 I print_info: LF token         = 187 ''
0.00.048.250 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.250 I print_info: max token length = 1024
0.00.048.250 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.015.020 I load_tensors: offloading 24 repeating layers to GPU
0.01.015.027 I load_tensors: offloading output layer to GPU
0.01.015.027 I load_tensors: offloaded 25/25 layers to GPU
0.01.015.057 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.015.060 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.016.577 I llama_init_from_model: n_seq_max     = 1
0.01.016.579 I llama_init_from_model: n_ctx         = 128
0.01.016.579 I llama_init_from_model: n_ctx_per_seq = 128
0.01.016.580 I llama_init_from_model: n_batch       = 128
0.01.016.580 I llama_init_from_model: n_ubatch      = 128
0.01.016.580 I llama_init_from_model: flash_attn    = 0
0.01.016.582 I llama_init_from_model: freq_base     = 10000.0
0.01.016.582 I llama_init_from_model: freq_scale    = 1
0.01.016.583 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.016.584 I ggml_metal_init: allocating
0.01.016.644 I ggml_metal_init: found device: Apple M4
0.01.016.653 I ggml_metal_init: picking default device: Apple M4
0.01.017.823 I ggml_metal_init: using embedded metal library
0.01.023.500 I ggml_metal_init: GPU name:   Apple M4
0.01.023.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.023.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.023.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.023.505 I ggml_metal_init: simdgroup reduction   = true
0.01.023.505 I ggml_metal_init: simdgroup matrix mul. = true
0.01.023.505 I ggml_metal_init: has residency sets    = true
0.01.023.506 I ggml_metal_init: has bfloat            = true
0.01.023.506 I ggml_metal_init: use bfloat            = true
0.01.023.507 I ggml_metal_init: hasUnifiedMemory      = true
0.01.023.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.039.336 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.042.582 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.042.591 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.042.622 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.045.591 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.045.593 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.045.593 I llama_init_from_model: graph nodes  = 967
0.01.045.594 I llama_init_from_model: graph splits = 2
0.01.045.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.045.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.075.194 I 
0.01.075.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.075.305 I perplexity: tokenizing the input ..
0.01.082.211 I perplexity: tokenization took 6.904 ms
0.01.082.216 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.219.698 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.221.048 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.221.076 I llama_perf_context_print:        load time =    1065.60 ms
0.01.221.078 I llama_perf_context_print: prompt eval time =     137.22 ms /   128 tokens (    1.07 ms per token,   932.84 tokens per second)
0.01.221.080 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.221.080 I llama_perf_context_print:       total time =     145.89 ms /   129 tokens
0.01.221.478 I ggml_metal_free: deallocating

real	0m1.242s
user	0m0.077s
sys	0m0.186s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.018.409 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.077 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.082 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.084 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.090 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.090 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.091 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.092 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.092 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.092 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.093 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.095 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.098 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.098 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.099 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.175 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.176 I llama_model_loader: - type  f32:  194 tensors
0.00.045.176 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.177 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.177 I print_info: file format = GGUF V3 (latest)
0.00.045.178 I print_info: file type   = Q4_0
0.00.045.178 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.559 I load: special tokens cache size = 25
0.00.061.906 I load: token to piece cache size = 0.2984 MB
0.00.061.922 I print_info: arch             = gptneox
0.00.061.923 I print_info: vocab_only       = 0
0.00.061.924 I print_info: n_ctx_train      = 2048
0.00.061.924 I print_info: n_embd           = 2048
0.00.061.924 I print_info: n_layer          = 24
0.00.061.929 I print_info: n_head           = 16
0.00.061.930 I print_info: n_head_kv        = 16
0.00.061.930 I print_info: n_rot            = 32
0.00.061.930 I print_info: n_swa            = 0
0.00.061.930 I print_info: n_embd_head_k    = 128
0.00.061.931 I print_info: n_embd_head_v    = 128
0.00.061.931 I print_info: n_gqa            = 1
0.00.061.932 I print_info: n_embd_k_gqa     = 2048
0.00.061.933 I print_info: n_embd_v_gqa     = 2048
0.00.061.933 I print_info: f_norm_eps       = 1.0e-05
0.00.061.934 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.934 I print_info: f_logit_scale    = 0.0e+00
0.00.061.935 I print_info: n_ff             = 8192
0.00.061.935 I print_info: n_expert         = 0
0.00.061.935 I print_info: n_expert_used    = 0
0.00.061.936 I print_info: causal attn      = 1
0.00.061.936 I print_info: pooling type     = 0
0.00.061.940 I print_info: rope type        = 2
0.00.061.940 I print_info: rope scaling     = linear
0.00.061.940 I print_info: freq_base_train  = 10000.0
0.00.061.941 I print_info: freq_scale_train = 1
0.00.061.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.942 I print_info: rope_finetuned   = unknown
0.00.061.942 I print_info: ssm_d_conv       = 0
0.00.061.943 I print_info: ssm_d_inner      = 0
0.00.061.947 I print_info: ssm_d_state      = 0
0.00.061.947 I print_info: ssm_dt_rank      = 0
0.00.061.947 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.948 I print_info: model type       = 1.4B
0.00.061.948 I print_info: model params     = 1.41 B
0.00.061.948 I print_info: general.name     = 1.4B
0.00.061.949 I print_info: vocab type       = BPE
0.00.061.949 I print_info: n_vocab          = 50304
0.00.061.949 I print_info: n_merges         = 50009
0.00.061.950 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.950 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.950 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.950 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.950 I print_info: LF token         = 187 ''
0.00.061.951 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.951 I print_info: max token length = 1024
0.00.061.952 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.658.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.295 I load_tensors: offloading output layer to GPU
0.00.658.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.658.326 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.658.328 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.659.762 I llama_init_from_model: n_seq_max     = 1
0.00.659.766 I llama_init_from_model: n_ctx         = 2048
0.00.659.767 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.659.768 I llama_init_from_model: n_batch       = 2048
0.00.659.768 I llama_init_from_model: n_ubatch      = 512
0.00.659.768 I llama_init_from_model: flash_attn    = 0
0.00.659.771 I llama_init_from_model: freq_base     = 10000.0
0.00.659.772 I llama_init_from_model: freq_scale    = 1
0.00.659.774 I ggml_metal_init: allocating
0.00.659.847 I ggml_metal_init: found device: Apple M4
0.00.659.865 I ggml_metal_init: picking default device: Apple M4
0.00.661.535 I ggml_metal_init: using embedded metal library
0.00.667.447 I ggml_metal_init: GPU name:   Apple M4
0.00.667.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.465 I ggml_metal_init: simdgroup reduction   = true
0.00.667.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.466 I ggml_metal_init: has residency sets    = true
0.00.667.466 I ggml_metal_init: has bfloat            = true
0.00.667.466 I ggml_metal_init: use bfloat            = true
0.00.667.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.152 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.748.256 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.748.262 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.748.286 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.752.998 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.753.000 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.753.000 I llama_init_from_model: graph nodes  = 967
0.00.753.001 I llama_init_from_model: graph splits = 2
0.00.753.007 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.753.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.753.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.440 I main: llama threadpool init, n_threads = 4
0.00.810.480 I 
0.00.810.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.500 I 
0.00.810.648 I sampler seed: 1234
0.00.810.652 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.694 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.694 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.492.088 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47779.27 tokens per second)
0.01.492.089 I llama_perf_context_print:        load time =     791.32 ms
0.01.492.090 I llama_perf_context_print: prompt eval time =      49.51 ms /     7 tokens (    7.07 ms per token,   141.40 tokens per second)
0.01.492.091 I llama_perf_context_print:        eval time =     629.35 ms /    63 runs   (    9.99 ms per token,   100.10 tokens per second)
0.01.492.091 I llama_perf_context_print:       total time =     682.36 ms /    70 tokens
0.01.492.343 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.114s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.965 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.966 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.713 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.463 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.465 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.466 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.466 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.467 I llama_model_loader: - type  f32:  194 tensors
0.00.034.467 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.468 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.468 I print_info: file format = GGUF V3 (latest)
0.00.034.469 I print_info: file type   = Q4_0
0.00.034.470 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.636 I load: special tokens cache size = 25
0.00.049.258 I load: token to piece cache size = 0.2984 MB
0.00.049.274 I print_info: arch             = gptneox
0.00.049.275 I print_info: vocab_only       = 0
0.00.049.275 I print_info: n_ctx_train      = 2048
0.00.049.276 I print_info: n_embd           = 2048
0.00.049.276 I print_info: n_layer          = 24
0.00.049.279 I print_info: n_head           = 16
0.00.049.280 I print_info: n_head_kv        = 16
0.00.049.280 I print_info: n_rot            = 32
0.00.049.280 I print_info: n_swa            = 0
0.00.049.283 I print_info: n_embd_head_k    = 128
0.00.049.283 I print_info: n_embd_head_v    = 128
0.00.049.284 I print_info: n_gqa            = 1
0.00.049.284 I print_info: n_embd_k_gqa     = 2048
0.00.049.285 I print_info: n_embd_v_gqa     = 2048
0.00.049.286 I print_info: f_norm_eps       = 1.0e-05
0.00.049.286 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.287 I print_info: f_logit_scale    = 0.0e+00
0.00.049.287 I print_info: n_ff             = 8192
0.00.049.287 I print_info: n_expert         = 0
0.00.049.287 I print_info: n_expert_used    = 0
0.00.049.288 I print_info: causal attn      = 1
0.00.049.288 I print_info: pooling type     = 0
0.00.049.288 I print_info: rope type        = 2
0.00.049.288 I print_info: rope scaling     = linear
0.00.049.288 I print_info: freq_base_train  = 10000.0
0.00.049.289 I print_info: freq_scale_train = 1
0.00.049.289 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.289 I print_info: rope_finetuned   = unknown
0.00.049.289 I print_info: ssm_d_conv       = 0
0.00.049.289 I print_info: ssm_d_inner      = 0
0.00.049.289 I print_info: ssm_d_state      = 0
0.00.049.289 I print_info: ssm_dt_rank      = 0
0.00.049.289 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.290 I print_info: model type       = 1.4B
0.00.049.290 I print_info: model params     = 1.41 B
0.00.049.290 I print_info: general.name     = 1.4B
0.00.049.291 I print_info: vocab type       = BPE
0.00.049.291 I print_info: n_vocab          = 50304
0.00.049.291 I print_info: n_merges         = 50009
0.00.049.291 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.291 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.291 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.292 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.292 I print_info: LF token         = 187 ''
0.00.049.292 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.292 I print_info: max token length = 1024
0.00.049.293 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.705 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.720 I load_tensors: offloading output layer to GPU
0.00.636.721 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.756 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.636.758 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.638.654 I llama_init_from_model: n_seq_max     = 1
0.00.638.657 I llama_init_from_model: n_ctx         = 128
0.00.638.657 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.658 I llama_init_from_model: n_batch       = 128
0.00.638.658 I llama_init_from_model: n_ubatch      = 128
0.00.638.658 I llama_init_from_model: flash_attn    = 0
0.00.638.661 I llama_init_from_model: freq_base     = 10000.0
0.00.638.661 I llama_init_from_model: freq_scale    = 1
0.00.638.662 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.667 I ggml_metal_init: allocating
0.00.638.795 I ggml_metal_init: found device: Apple M4
0.00.638.810 I ggml_metal_init: picking default device: Apple M4
0.00.640.480 I ggml_metal_init: using embedded metal library
0.00.647.424 I ggml_metal_init: GPU name:   Apple M4
0.00.647.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.431 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.432 I ggml_metal_init: simdgroup reduction   = true
0.00.647.433 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.433 I ggml_metal_init: has residency sets    = true
0.00.647.433 I ggml_metal_init: has bfloat            = true
0.00.647.433 I ggml_metal_init: use bfloat            = true
0.00.647.434 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.023 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.626 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.669.630 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.669.673 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.817 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.819 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.819 I llama_init_from_model: graph nodes  = 967
0.00.672.819 I llama_init_from_model: graph splits = 2
0.00.672.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.997 I 
0.00.697.079 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.085 I perplexity: tokenizing the input ..
0.00.704.974 I perplexity: tokenization took 7.885 ms
0.00.704.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.588 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.829.932 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.829.954 I llama_perf_context_print:        load time =     681.23 ms
0.00.829.955 I llama_perf_context_print: prompt eval time =     122.67 ms /   128 tokens (    0.96 ms per token,  1043.41 tokens per second)
0.00.829.956 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.957 I llama_perf_context_print:       total time =     132.96 ms /   129 tokens
0.00.830.380 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.082s
sys	0m0.130s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.201 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.127 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.563 I llama_model_loader: - type  f32:  194 tensors
0.00.025.564 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.565 I print_info: file format = GGUF V3 (latest)
0.00.025.565 I print_info: file type   = Q4_1
0.00.025.566 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.826 I load: special tokens cache size = 25
0.00.040.265 I load: token to piece cache size = 0.2984 MB
0.00.040.283 I print_info: arch             = gptneox
0.00.040.284 I print_info: vocab_only       = 0
0.00.040.284 I print_info: n_ctx_train      = 2048
0.00.040.284 I print_info: n_embd           = 2048
0.00.040.284 I print_info: n_layer          = 24
0.00.040.289 I print_info: n_head           = 16
0.00.040.289 I print_info: n_head_kv        = 16
0.00.040.289 I print_info: n_rot            = 32
0.00.040.289 I print_info: n_swa            = 0
0.00.040.290 I print_info: n_embd_head_k    = 128
0.00.040.290 I print_info: n_embd_head_v    = 128
0.00.040.290 I print_info: n_gqa            = 1
0.00.040.291 I print_info: n_embd_k_gqa     = 2048
0.00.040.291 I print_info: n_embd_v_gqa     = 2048
0.00.040.292 I print_info: f_norm_eps       = 1.0e-05
0.00.040.292 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.293 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.296 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.298 I print_info: f_logit_scale    = 0.0e+00
0.00.040.298 I print_info: n_ff             = 8192
0.00.040.298 I print_info: n_expert         = 0
0.00.040.298 I print_info: n_expert_used    = 0
0.00.040.299 I print_info: causal attn      = 1
0.00.040.299 I print_info: pooling type     = 0
0.00.040.299 I print_info: rope type        = 2
0.00.040.301 I print_info: rope scaling     = linear
0.00.040.301 I print_info: freq_base_train  = 10000.0
0.00.040.301 I print_info: freq_scale_train = 1
0.00.040.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.302 I print_info: rope_finetuned   = unknown
0.00.040.302 I print_info: ssm_d_conv       = 0
0.00.040.302 I print_info: ssm_d_inner      = 0
0.00.040.302 I print_info: ssm_d_state      = 0
0.00.040.302 I print_info: ssm_dt_rank      = 0
0.00.040.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.302 I print_info: model type       = 1.4B
0.00.040.303 I print_info: model params     = 1.41 B
0.00.040.303 I print_info: general.name     = 1.4B
0.00.040.303 I print_info: vocab type       = BPE
0.00.040.304 I print_info: n_vocab          = 50304
0.00.040.304 I print_info: n_merges         = 50009
0.00.040.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.305 I print_info: LF token         = 187 ''
0.00.040.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.305 I print_info: max token length = 1024
0.00.040.306 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.840 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.851 I load_tensors: offloading output layer to GPU
0.00.647.852 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.882 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.647.883 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.649.571 I llama_init_from_model: n_seq_max     = 1
0.00.649.583 I llama_init_from_model: n_ctx         = 2048
0.00.649.584 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.584 I llama_init_from_model: n_batch       = 2048
0.00.649.585 I llama_init_from_model: n_ubatch      = 512
0.00.649.585 I llama_init_from_model: flash_attn    = 0
0.00.649.586 I llama_init_from_model: freq_base     = 10000.0
0.00.649.587 I llama_init_from_model: freq_scale    = 1
0.00.649.589 I ggml_metal_init: allocating
0.00.649.648 I ggml_metal_init: found device: Apple M4
0.00.649.665 I ggml_metal_init: picking default device: Apple M4
0.00.651.245 I ggml_metal_init: using embedded metal library
0.00.656.355 I ggml_metal_init: GPU name:   Apple M4
0.00.656.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.362 I ggml_metal_init: simdgroup reduction   = true
0.00.656.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.362 I ggml_metal_init: has residency sets    = true
0.00.656.362 I ggml_metal_init: has bfloat            = true
0.00.656.362 I ggml_metal_init: use bfloat            = true
0.00.656.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.789 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.463 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.703.469 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.703.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.608 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.707.610 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.707.610 I llama_init_from_model: graph nodes  = 967
0.00.707.611 I llama_init_from_model: graph splits = 2
0.00.707.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.736 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.737 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.874 I main: llama threadpool init, n_threads = 4
0.00.766.922 I 
0.00.766.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.943 I 
0.00.767.132 I sampler seed: 1234
0.00.767.137 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.152 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.154 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.154 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.497.090 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.497.091 I llama_perf_context_print:        load time =     756.93 ms
0.01.497.092 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.81 tokens per second)
0.01.497.093 I llama_perf_context_print:        eval time =     678.55 ms /    63 runs   (   10.77 ms per token,    92.85 tokens per second)
0.01.497.093 I llama_perf_context_print:       total time =     730.96 ms /    70 tokens
0.01.497.407 I ggml_metal_free: deallocating

real	0m1.515s
user	0m0.104s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.975 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.041 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.049 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.484 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.484 I llama_model_loader: - type  f32:  194 tensors
0.00.025.485 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.485 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.486 I print_info: file format = GGUF V3 (latest)
0.00.025.486 I print_info: file type   = Q4_1
0.00.025.488 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.817 I load: special tokens cache size = 25
0.00.040.374 I load: token to piece cache size = 0.2984 MB
0.00.040.391 I print_info: arch             = gptneox
0.00.040.392 I print_info: vocab_only       = 0
0.00.040.392 I print_info: n_ctx_train      = 2048
0.00.040.393 I print_info: n_embd           = 2048
0.00.040.393 I print_info: n_layer          = 24
0.00.040.397 I print_info: n_head           = 16
0.00.040.397 I print_info: n_head_kv        = 16
0.00.040.397 I print_info: n_rot            = 32
0.00.040.398 I print_info: n_swa            = 0
0.00.040.398 I print_info: n_embd_head_k    = 128
0.00.040.398 I print_info: n_embd_head_v    = 128
0.00.040.398 I print_info: n_gqa            = 1
0.00.040.399 I print_info: n_embd_k_gqa     = 2048
0.00.040.402 I print_info: n_embd_v_gqa     = 2048
0.00.040.403 I print_info: f_norm_eps       = 1.0e-05
0.00.040.403 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.403 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.405 I print_info: f_logit_scale    = 0.0e+00
0.00.040.406 I print_info: n_ff             = 8192
0.00.040.406 I print_info: n_expert         = 0
0.00.040.406 I print_info: n_expert_used    = 0
0.00.040.406 I print_info: causal attn      = 1
0.00.040.406 I print_info: pooling type     = 0
0.00.040.406 I print_info: rope type        = 2
0.00.040.407 I print_info: rope scaling     = linear
0.00.040.407 I print_info: freq_base_train  = 10000.0
0.00.040.407 I print_info: freq_scale_train = 1
0.00.040.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.408 I print_info: rope_finetuned   = unknown
0.00.040.408 I print_info: ssm_d_conv       = 0
0.00.040.408 I print_info: ssm_d_inner      = 0
0.00.040.408 I print_info: ssm_d_state      = 0
0.00.040.408 I print_info: ssm_dt_rank      = 0
0.00.040.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.408 I print_info: model type       = 1.4B
0.00.040.409 I print_info: model params     = 1.41 B
0.00.040.409 I print_info: general.name     = 1.4B
0.00.040.409 I print_info: vocab type       = BPE
0.00.040.410 I print_info: n_vocab          = 50304
0.00.040.410 I print_info: n_merges         = 50009
0.00.040.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.410 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.410 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.411 I print_info: LF token         = 187 ''
0.00.040.411 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.411 I print_info: max token length = 1024
0.00.040.411 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.448 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.457 I load_tensors: offloading output layer to GPU
0.00.634.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.487 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.634.489 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.635.831 I llama_init_from_model: n_seq_max     = 1
0.00.635.834 I llama_init_from_model: n_ctx         = 128
0.00.635.834 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.835 I llama_init_from_model: n_batch       = 128
0.00.635.835 I llama_init_from_model: n_ubatch      = 128
0.00.635.835 I llama_init_from_model: flash_attn    = 0
0.00.635.837 I llama_init_from_model: freq_base     = 10000.0
0.00.635.838 I llama_init_from_model: freq_scale    = 1
0.00.635.838 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.840 I ggml_metal_init: allocating
0.00.635.906 I ggml_metal_init: found device: Apple M4
0.00.635.920 I ggml_metal_init: picking default device: Apple M4
0.00.637.470 I ggml_metal_init: using embedded metal library
0.00.644.153 I ggml_metal_init: GPU name:   Apple M4
0.00.644.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.167 I ggml_metal_init: simdgroup reduction   = true
0.00.644.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.167 I ggml_metal_init: has residency sets    = true
0.00.644.168 I ggml_metal_init: has bfloat            = true
0.00.644.168 I ggml_metal_init: use bfloat            = true
0.00.644.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.692 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.241 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.667.245 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.667.294 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.534 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.670.535 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.670.536 I llama_init_from_model: graph nodes  = 967
0.00.670.536 I llama_init_from_model: graph splits = 2
0.00.670.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.670.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.276 I 
0.00.696.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.369 I perplexity: tokenizing the input ..
0.00.703.802 I perplexity: tokenization took 7.431 ms
0.00.703.813 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.371 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.841.740 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.841.764 I llama_perf_context_print:        load time =     687.29 ms
0.00.841.765 I llama_perf_context_print: prompt eval time =     135.69 ms /   128 tokens (    1.06 ms per token,   943.31 tokens per second)
0.00.841.766 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.766 I llama_perf_context_print:       total time =     145.49 ms /   129 tokens
0.00.842.147 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.081s
sys	0m0.120s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.255 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.640 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.653 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.656 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.657 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.225 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.226 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.227 I llama_model_loader: - type  f32:  194 tensors
0.00.026.227 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.227 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.228 I print_info: file format = GGUF V3 (latest)
0.00.026.229 I print_info: file type   = Q5_0
0.00.026.230 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.659 I load: special tokens cache size = 25
0.00.040.785 I load: token to piece cache size = 0.2984 MB
0.00.040.803 I print_info: arch             = gptneox
0.00.040.804 I print_info: vocab_only       = 0
0.00.040.804 I print_info: n_ctx_train      = 2048
0.00.040.804 I print_info: n_embd           = 2048
0.00.040.804 I print_info: n_layer          = 24
0.00.040.808 I print_info: n_head           = 16
0.00.040.810 I print_info: n_head_kv        = 16
0.00.040.810 I print_info: n_rot            = 32
0.00.040.811 I print_info: n_swa            = 0
0.00.040.811 I print_info: n_embd_head_k    = 128
0.00.040.811 I print_info: n_embd_head_v    = 128
0.00.040.811 I print_info: n_gqa            = 1
0.00.040.812 I print_info: n_embd_k_gqa     = 2048
0.00.040.812 I print_info: n_embd_v_gqa     = 2048
0.00.040.813 I print_info: f_norm_eps       = 1.0e-05
0.00.040.813 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.814 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.814 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.814 I print_info: f_logit_scale    = 0.0e+00
0.00.040.815 I print_info: n_ff             = 8192
0.00.040.815 I print_info: n_expert         = 0
0.00.040.815 I print_info: n_expert_used    = 0
0.00.040.816 I print_info: causal attn      = 1
0.00.040.816 I print_info: pooling type     = 0
0.00.040.816 I print_info: rope type        = 2
0.00.040.816 I print_info: rope scaling     = linear
0.00.040.816 I print_info: freq_base_train  = 10000.0
0.00.040.817 I print_info: freq_scale_train = 1
0.00.040.817 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.819 I print_info: rope_finetuned   = unknown
0.00.040.819 I print_info: ssm_d_conv       = 0
0.00.040.819 I print_info: ssm_d_inner      = 0
0.00.040.819 I print_info: ssm_d_state      = 0
0.00.040.819 I print_info: ssm_dt_rank      = 0
0.00.040.820 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.820 I print_info: model type       = 1.4B
0.00.040.820 I print_info: model params     = 1.41 B
0.00.040.826 I print_info: general.name     = 1.4B
0.00.040.828 I print_info: vocab type       = BPE
0.00.040.829 I print_info: n_vocab          = 50304
0.00.040.829 I print_info: n_merges         = 50009
0.00.040.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.829 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.829 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.830 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.830 I print_info: LF token         = 187 ''
0.00.040.833 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.833 I print_info: max token length = 1024
0.00.040.834 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.417 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.433 I load_tensors: offloading output layer to GPU
0.00.647.434 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.495 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.647.497 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.648.632 I llama_init_from_model: n_seq_max     = 1
0.00.648.638 I llama_init_from_model: n_ctx         = 2048
0.00.648.639 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.648.640 I llama_init_from_model: n_batch       = 2048
0.00.648.640 I llama_init_from_model: n_ubatch      = 512
0.00.648.640 I llama_init_from_model: flash_attn    = 0
0.00.648.642 I llama_init_from_model: freq_base     = 10000.0
0.00.648.643 I llama_init_from_model: freq_scale    = 1
0.00.648.645 I ggml_metal_init: allocating
0.00.648.722 I ggml_metal_init: found device: Apple M4
0.00.648.739 I ggml_metal_init: picking default device: Apple M4
0.00.650.570 I ggml_metal_init: using embedded metal library
0.00.657.216 I ggml_metal_init: GPU name:   Apple M4
0.00.657.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.223 I ggml_metal_init: simdgroup reduction   = true
0.00.657.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.223 I ggml_metal_init: has residency sets    = true
0.00.657.224 I ggml_metal_init: has bfloat            = true
0.00.657.224 I ggml_metal_init: use bfloat            = true
0.00.657.225 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.814 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.181 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.188 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.211 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.484 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.486 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.486 I llama_init_from_model: graph nodes  = 967
0.00.741.486 I llama_init_from_model: graph splits = 2
0.00.741.493 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.980 I main: llama threadpool init, n_threads = 4
0.00.803.025 I 
0.00.803.045 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.045 I 
0.00.803.218 I sampler seed: 1234
0.00.803.222 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.253 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.254 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.254 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.595.908 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.595.908 I llama_perf_context_print:        load time =     793.00 ms
0.01.595.909 I llama_perf_context_print: prompt eval time =      53.25 ms /     7 tokens (    7.61 ms per token,   131.47 tokens per second)
0.01.595.910 I llama_perf_context_print:        eval time =     736.53 ms /    63 runs   (   11.69 ms per token,    85.54 tokens per second)
0.01.595.910 I llama_perf_context_print:       total time =     793.65 ms /    70 tokens
0.01.596.182 I ggml_metal_free: deallocating

real	0m1.618s
user	0m0.112s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.001 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.811 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.822 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.823 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.824 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.824 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.232 I llama_model_loader: - type  f32:  194 tensors
0.00.029.232 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.233 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.233 I print_info: file format = GGUF V3 (latest)
0.00.029.234 I print_info: file type   = Q5_0
0.00.029.235 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.442 I load: special tokens cache size = 25
0.00.043.933 I load: token to piece cache size = 0.2984 MB
0.00.043.950 I print_info: arch             = gptneox
0.00.043.951 I print_info: vocab_only       = 0
0.00.043.951 I print_info: n_ctx_train      = 2048
0.00.043.951 I print_info: n_embd           = 2048
0.00.043.951 I print_info: n_layer          = 24
0.00.043.955 I print_info: n_head           = 16
0.00.043.956 I print_info: n_head_kv        = 16
0.00.043.956 I print_info: n_rot            = 32
0.00.043.956 I print_info: n_swa            = 0
0.00.043.957 I print_info: n_embd_head_k    = 128
0.00.043.957 I print_info: n_embd_head_v    = 128
0.00.043.957 I print_info: n_gqa            = 1
0.00.043.958 I print_info: n_embd_k_gqa     = 2048
0.00.043.958 I print_info: n_embd_v_gqa     = 2048
0.00.043.959 I print_info: f_norm_eps       = 1.0e-05
0.00.043.959 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.959 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.961 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.961 I print_info: f_logit_scale    = 0.0e+00
0.00.043.962 I print_info: n_ff             = 8192
0.00.043.962 I print_info: n_expert         = 0
0.00.043.962 I print_info: n_expert_used    = 0
0.00.043.962 I print_info: causal attn      = 1
0.00.043.962 I print_info: pooling type     = 0
0.00.043.962 I print_info: rope type        = 2
0.00.043.962 I print_info: rope scaling     = linear
0.00.043.965 I print_info: freq_base_train  = 10000.0
0.00.043.965 I print_info: freq_scale_train = 1
0.00.043.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.965 I print_info: rope_finetuned   = unknown
0.00.043.965 I print_info: ssm_d_conv       = 0
0.00.043.966 I print_info: ssm_d_inner      = 0
0.00.043.966 I print_info: ssm_d_state      = 0
0.00.043.966 I print_info: ssm_dt_rank      = 0
0.00.043.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.966 I print_info: model type       = 1.4B
0.00.043.966 I print_info: model params     = 1.41 B
0.00.043.967 I print_info: general.name     = 1.4B
0.00.043.967 I print_info: vocab type       = BPE
0.00.043.967 I print_info: n_vocab          = 50304
0.00.043.968 I print_info: n_merges         = 50009
0.00.043.968 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.968 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.968 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.968 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.969 I print_info: LF token         = 187 ''
0.00.043.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.969 I print_info: max token length = 1024
0.00.043.969 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.712.464 I load_tensors: offloading 24 repeating layers to GPU
0.00.712.477 I load_tensors: offloading output layer to GPU
0.00.712.478 I load_tensors: offloaded 25/25 layers to GPU
0.00.712.506 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.712.508 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.713.966 I llama_init_from_model: n_seq_max     = 1
0.00.713.971 I llama_init_from_model: n_ctx         = 128
0.00.713.971 I llama_init_from_model: n_ctx_per_seq = 128
0.00.713.972 I llama_init_from_model: n_batch       = 128
0.00.713.972 I llama_init_from_model: n_ubatch      = 128
0.00.713.973 I llama_init_from_model: flash_attn    = 0
0.00.713.974 I llama_init_from_model: freq_base     = 10000.0
0.00.713.975 I llama_init_from_model: freq_scale    = 1
0.00.713.975 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.713.988 I ggml_metal_init: allocating
0.00.714.070 I ggml_metal_init: found device: Apple M4
0.00.714.084 I ggml_metal_init: picking default device: Apple M4
0.00.715.540 I ggml_metal_init: using embedded metal library
0.00.721.727 I ggml_metal_init: GPU name:   Apple M4
0.00.721.734 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.737 I ggml_metal_init: simdgroup reduction   = true
0.00.721.737 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.737 I ggml_metal_init: has residency sets    = true
0.00.721.738 I ggml_metal_init: has bfloat            = true
0.00.721.738 I ggml_metal_init: use bfloat            = true
0.00.721.739 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.998 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.522 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.744.529 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.744.580 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.907 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.747.909 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.747.910 I llama_init_from_model: graph nodes  = 967
0.00.747.910 I llama_init_from_model: graph splits = 2
0.00.747.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.747.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.044 I 
0.00.777.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.140 I perplexity: tokenizing the input ..
0.00.784.347 I perplexity: tokenization took 7.202 ms
0.00.784.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.920.264 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.921.595 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.921.613 I llama_perf_context_print:        load time =     768.03 ms
0.00.921.614 I llama_perf_context_print: prompt eval time =     134.95 ms /   128 tokens (    1.05 ms per token,   948.48 tokens per second)
0.00.921.615 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.921.615 I llama_perf_context_print:       total time =     144.57 ms /   129 tokens
0.00.922.002 I ggml_metal_free: deallocating

real	0m0.936s
user	0m0.080s
sys	0m0.133s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.010 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.015 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.027 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.027 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.723 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.413 I llama_model_loader: - type  f32:  194 tensors
0.00.027.414 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.414 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.414 I print_info: file format = GGUF V3 (latest)
0.00.027.415 I print_info: file type   = Q5_1
0.00.027.416 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.132 I load: special tokens cache size = 25
0.00.041.346 I load: token to piece cache size = 0.2984 MB
0.00.041.361 I print_info: arch             = gptneox
0.00.041.362 I print_info: vocab_only       = 0
0.00.041.362 I print_info: n_ctx_train      = 2048
0.00.041.362 I print_info: n_embd           = 2048
0.00.041.362 I print_info: n_layer          = 24
0.00.041.365 I print_info: n_head           = 16
0.00.041.366 I print_info: n_head_kv        = 16
0.00.041.366 I print_info: n_rot            = 32
0.00.041.366 I print_info: n_swa            = 0
0.00.041.366 I print_info: n_embd_head_k    = 128
0.00.041.366 I print_info: n_embd_head_v    = 128
0.00.041.370 I print_info: n_gqa            = 1
0.00.041.371 I print_info: n_embd_k_gqa     = 2048
0.00.041.371 I print_info: n_embd_v_gqa     = 2048
0.00.041.372 I print_info: f_norm_eps       = 1.0e-05
0.00.041.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.374 I print_info: f_logit_scale    = 0.0e+00
0.00.041.375 I print_info: n_ff             = 8192
0.00.041.375 I print_info: n_expert         = 0
0.00.041.375 I print_info: n_expert_used    = 0
0.00.041.375 I print_info: causal attn      = 1
0.00.041.375 I print_info: pooling type     = 0
0.00.041.375 I print_info: rope type        = 2
0.00.041.375 I print_info: rope scaling     = linear
0.00.041.376 I print_info: freq_base_train  = 10000.0
0.00.041.376 I print_info: freq_scale_train = 1
0.00.041.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.376 I print_info: rope_finetuned   = unknown
0.00.041.376 I print_info: ssm_d_conv       = 0
0.00.041.376 I print_info: ssm_d_inner      = 0
0.00.041.376 I print_info: ssm_d_state      = 0
0.00.041.377 I print_info: ssm_dt_rank      = 0
0.00.041.380 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.381 I print_info: model type       = 1.4B
0.00.041.381 I print_info: model params     = 1.41 B
0.00.041.382 I print_info: general.name     = 1.4B
0.00.041.383 I print_info: vocab type       = BPE
0.00.041.383 I print_info: n_vocab          = 50304
0.00.041.383 I print_info: n_merges         = 50009
0.00.041.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.383 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.384 I print_info: LF token         = 187 ''
0.00.041.384 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.384 I print_info: max token length = 1024
0.00.041.386 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.632 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.645 I load_tensors: offloading output layer to GPU
0.00.604.647 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.678 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.680 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.606.402 I llama_init_from_model: n_seq_max     = 1
0.00.606.405 I llama_init_from_model: n_ctx         = 2048
0.00.606.405 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.405 I llama_init_from_model: n_batch       = 2048
0.00.606.406 I llama_init_from_model: n_ubatch      = 512
0.00.606.406 I llama_init_from_model: flash_attn    = 0
0.00.606.407 I llama_init_from_model: freq_base     = 10000.0
0.00.606.408 I llama_init_from_model: freq_scale    = 1
0.00.606.409 I ggml_metal_init: allocating
0.00.606.429 I ggml_metal_init: found device: Apple M4
0.00.606.436 I ggml_metal_init: picking default device: Apple M4
0.00.607.730 I ggml_metal_init: using embedded metal library
0.00.614.053 I ggml_metal_init: GPU name:   Apple M4
0.00.614.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.058 I ggml_metal_init: simdgroup reduction   = true
0.00.614.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.059 I ggml_metal_init: has residency sets    = true
0.00.614.059 I ggml_metal_init: has bfloat            = true
0.00.614.059 I ggml_metal_init: use bfloat            = true
0.00.614.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.063 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.677 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.682.684 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.682.706 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.055 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.058 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.059 I llama_init_from_model: graph nodes  = 967
0.00.688.059 I llama_init_from_model: graph splits = 2
0.00.688.064 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.904 I main: llama threadpool init, n_threads = 4
0.00.748.959 I 
0.00.748.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.982 I 
0.00.749.137 I sampler seed: 1234
0.00.749.141 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.156 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.157 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.157 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.597.234 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.597.235 I llama_perf_context_print:        load time =     736.51 ms
0.01.597.237 I llama_perf_context_print: prompt eval time =      50.88 ms /     7 tokens (    7.27 ms per token,   137.57 tokens per second)
0.01.597.238 I llama_perf_context_print:        eval time =     794.28 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.597.239 I llama_perf_context_print:       total time =     849.07 ms /    70 tokens
0.01.597.452 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.107s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.290 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.165 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.167 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.167 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.168 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.168 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.168 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.169 I llama_model_loader: - type  f32:  194 tensors
0.00.033.169 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.170 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.170 I print_info: file format = GGUF V3 (latest)
0.00.033.170 I print_info: file type   = Q5_1
0.00.033.176 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.413 I load: special tokens cache size = 25
0.00.047.630 I load: token to piece cache size = 0.2984 MB
0.00.047.647 I print_info: arch             = gptneox
0.00.047.648 I print_info: vocab_only       = 0
0.00.047.648 I print_info: n_ctx_train      = 2048
0.00.047.648 I print_info: n_embd           = 2048
0.00.047.649 I print_info: n_layer          = 24
0.00.047.652 I print_info: n_head           = 16
0.00.047.653 I print_info: n_head_kv        = 16
0.00.047.653 I print_info: n_rot            = 32
0.00.047.653 I print_info: n_swa            = 0
0.00.047.653 I print_info: n_embd_head_k    = 128
0.00.047.653 I print_info: n_embd_head_v    = 128
0.00.047.654 I print_info: n_gqa            = 1
0.00.047.655 I print_info: n_embd_k_gqa     = 2048
0.00.047.655 I print_info: n_embd_v_gqa     = 2048
0.00.047.656 I print_info: f_norm_eps       = 1.0e-05
0.00.047.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.658 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.658 I print_info: f_logit_scale    = 0.0e+00
0.00.047.659 I print_info: n_ff             = 8192
0.00.047.659 I print_info: n_expert         = 0
0.00.047.659 I print_info: n_expert_used    = 0
0.00.047.659 I print_info: causal attn      = 1
0.00.047.659 I print_info: pooling type     = 0
0.00.047.659 I print_info: rope type        = 2
0.00.047.660 I print_info: rope scaling     = linear
0.00.047.661 I print_info: freq_base_train  = 10000.0
0.00.047.661 I print_info: freq_scale_train = 1
0.00.047.661 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.661 I print_info: rope_finetuned   = unknown
0.00.047.661 I print_info: ssm_d_conv       = 0
0.00.047.661 I print_info: ssm_d_inner      = 0
0.00.047.662 I print_info: ssm_d_state      = 0
0.00.047.662 I print_info: ssm_dt_rank      = 0
0.00.047.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.662 I print_info: model type       = 1.4B
0.00.047.663 I print_info: model params     = 1.41 B
0.00.047.663 I print_info: general.name     = 1.4B
0.00.047.663 I print_info: vocab type       = BPE
0.00.047.663 I print_info: n_vocab          = 50304
0.00.047.664 I print_info: n_merges         = 50009
0.00.047.664 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.664 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.664 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.665 I print_info: LF token         = 187 ''
0.00.047.665 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.665 I print_info: max token length = 1024
0.00.047.666 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.423 I load_tensors: offloading output layer to GPU
0.00.683.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.459 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.683.460 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.685.240 I llama_init_from_model: n_seq_max     = 1
0.00.685.243 I llama_init_from_model: n_ctx         = 128
0.00.685.243 I llama_init_from_model: n_ctx_per_seq = 128
0.00.685.244 I llama_init_from_model: n_batch       = 128
0.00.685.245 I llama_init_from_model: n_ubatch      = 128
0.00.685.245 I llama_init_from_model: flash_attn    = 0
0.00.685.247 I llama_init_from_model: freq_base     = 10000.0
0.00.685.247 I llama_init_from_model: freq_scale    = 1
0.00.685.248 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.685.251 I ggml_metal_init: allocating
0.00.685.332 I ggml_metal_init: found device: Apple M4
0.00.685.346 I ggml_metal_init: picking default device: Apple M4
0.00.686.953 I ggml_metal_init: using embedded metal library
0.00.693.747 I ggml_metal_init: GPU name:   Apple M4
0.00.693.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.758 I ggml_metal_init: simdgroup reduction   = true
0.00.693.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.758 I ggml_metal_init: has residency sets    = true
0.00.693.758 I ggml_metal_init: has bfloat            = true
0.00.693.759 I ggml_metal_init: use bfloat            = true
0.00.693.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.768 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.447 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.899 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.714.903 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.714.928 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.304 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.718.306 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.718.306 I llama_init_from_model: graph nodes  = 967
0.00.718.306 I llama_init_from_model: graph splits = 2
0.00.718.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.718.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.185 I 
0.00.745.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.281 I perplexity: tokenizing the input ..
0.00.753.143 I perplexity: tokenization took 7.858 ms
0.00.753.151 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.216 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.890.567 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.890.597 I llama_perf_context_print:        load time =     729.89 ms
0.00.890.600 I llama_perf_context_print: prompt eval time =     135.09 ms /   128 tokens (    1.06 ms per token,   947.52 tokens per second)
0.00.890.602 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.602 I llama_perf_context_print:       total time =     145.41 ms /   129 tokens
0.00.890.973 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.081s
sys	0m0.145s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.429 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.430 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.430 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.432 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.783 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.785 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.786 I llama_model_loader: - type  f32:  194 tensors
0.00.023.786 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.787 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.787 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.787 I print_info: file format = GGUF V3 (latest)
0.00.023.788 I print_info: file type   = Q2_K - Medium
0.00.023.788 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.869 I load: special tokens cache size = 25
0.00.038.410 I load: token to piece cache size = 0.2984 MB
0.00.038.424 I print_info: arch             = gptneox
0.00.038.425 I print_info: vocab_only       = 0
0.00.038.425 I print_info: n_ctx_train      = 2048
0.00.038.426 I print_info: n_embd           = 2048
0.00.038.426 I print_info: n_layer          = 24
0.00.038.429 I print_info: n_head           = 16
0.00.038.429 I print_info: n_head_kv        = 16
0.00.038.430 I print_info: n_rot            = 32
0.00.038.430 I print_info: n_swa            = 0
0.00.038.432 I print_info: n_embd_head_k    = 128
0.00.038.432 I print_info: n_embd_head_v    = 128
0.00.038.433 I print_info: n_gqa            = 1
0.00.038.434 I print_info: n_embd_k_gqa     = 2048
0.00.038.434 I print_info: n_embd_v_gqa     = 2048
0.00.038.435 I print_info: f_norm_eps       = 1.0e-05
0.00.038.435 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.435 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.436 I print_info: f_logit_scale    = 0.0e+00
0.00.038.436 I print_info: n_ff             = 8192
0.00.038.440 I print_info: n_expert         = 0
0.00.038.440 I print_info: n_expert_used    = 0
0.00.038.441 I print_info: causal attn      = 1
0.00.038.441 I print_info: pooling type     = 0
0.00.038.441 I print_info: rope type        = 2
0.00.038.441 I print_info: rope scaling     = linear
0.00.038.442 I print_info: freq_base_train  = 10000.0
0.00.038.442 I print_info: freq_scale_train = 1
0.00.038.442 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.443 I print_info: rope_finetuned   = unknown
0.00.038.443 I print_info: ssm_d_conv       = 0
0.00.038.444 I print_info: ssm_d_inner      = 0
0.00.038.444 I print_info: ssm_d_state      = 0
0.00.038.444 I print_info: ssm_dt_rank      = 0
0.00.038.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.444 I print_info: model type       = 1.4B
0.00.038.444 I print_info: model params     = 1.41 B
0.00.038.446 I print_info: general.name     = 1.4B
0.00.038.446 I print_info: vocab type       = BPE
0.00.038.446 I print_info: n_vocab          = 50304
0.00.038.448 I print_info: n_merges         = 50009
0.00.038.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: LF token         = 187 ''
0.00.038.449 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.449 I print_info: max token length = 1024
0.00.038.449 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.337.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.337.187 I load_tensors: offloading output layer to GPU
0.00.337.188 I load_tensors: offloaded 25/25 layers to GPU
0.00.337.223 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.337.224 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.338.838 I llama_init_from_model: n_seq_max     = 1
0.00.338.841 I llama_init_from_model: n_ctx         = 2048
0.00.338.842 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.338.842 I llama_init_from_model: n_batch       = 2048
0.00.338.843 I llama_init_from_model: n_ubatch      = 512
0.00.338.843 I llama_init_from_model: flash_attn    = 0
0.00.338.845 I llama_init_from_model: freq_base     = 10000.0
0.00.338.846 I llama_init_from_model: freq_scale    = 1
0.00.338.848 I ggml_metal_init: allocating
0.00.338.938 I ggml_metal_init: found device: Apple M4
0.00.338.951 I ggml_metal_init: picking default device: Apple M4
0.00.340.545 I ggml_metal_init: using embedded metal library
0.00.346.239 I ggml_metal_init: GPU name:   Apple M4
0.00.346.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.254 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.255 I ggml_metal_init: simdgroup reduction   = true
0.00.346.255 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.255 I ggml_metal_init: has residency sets    = true
0.00.346.255 I ggml_metal_init: has bfloat            = true
0.00.346.256 I ggml_metal_init: use bfloat            = true
0.00.346.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.282 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.994 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.428.918 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.428.924 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.428.958 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.433.626 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.433.628 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.433.629 I llama_init_from_model: graph nodes  = 967
0.00.433.629 I llama_init_from_model: graph splits = 2
0.00.433.636 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.433.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.433.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.657 I main: llama threadpool init, n_threads = 4
0.00.491.708 I 
0.00.491.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.730 I 
0.00.491.910 I sampler seed: 1234
0.00.491.914 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.491.930 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.491.932 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.491.932 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.161.236 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.161.238 I llama_perf_context_print:        load time =     482.06 ms
0.01.161.238 I llama_perf_context_print: prompt eval time =      35.54 ms /     7 tokens (    5.08 ms per token,   196.96 tokens per second)
0.01.161.239 I llama_perf_context_print:        eval time =     630.94 ms /    63 runs   (   10.01 ms per token,    99.85 tokens per second)
0.01.161.239 I llama_perf_context_print:       total time =     670.29 ms /    70 tokens
0.01.161.482 I ggml_metal_free: deallocating

real	0m1.179s
user	0m0.113s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.818 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.818 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.818 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.819 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.819 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.820 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.821 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.825 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.029.194 I llama_model_loader: - type  f32:  194 tensors
0.00.029.195 I llama_model_loader: - type q2_K:   49 tensors
0.00.029.195 I llama_model_loader: - type q3_K:   48 tensors
0.00.029.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.196 I print_info: file format = GGUF V3 (latest)
0.00.029.196 I print_info: file type   = Q2_K - Medium
0.00.029.197 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.037.476 I load: special tokens cache size = 25
0.00.043.817 I load: token to piece cache size = 0.2984 MB
0.00.043.834 I print_info: arch             = gptneox
0.00.043.835 I print_info: vocab_only       = 0
0.00.043.835 I print_info: n_ctx_train      = 2048
0.00.043.836 I print_info: n_embd           = 2048
0.00.043.836 I print_info: n_layer          = 24
0.00.043.840 I print_info: n_head           = 16
0.00.043.840 I print_info: n_head_kv        = 16
0.00.043.841 I print_info: n_rot            = 32
0.00.043.841 I print_info: n_swa            = 0
0.00.043.841 I print_info: n_embd_head_k    = 128
0.00.043.841 I print_info: n_embd_head_v    = 128
0.00.043.841 I print_info: n_gqa            = 1
0.00.043.842 I print_info: n_embd_k_gqa     = 2048
0.00.043.843 I print_info: n_embd_v_gqa     = 2048
0.00.043.843 I print_info: f_norm_eps       = 1.0e-05
0.00.043.844 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.844 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.844 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.847 I print_info: f_logit_scale    = 0.0e+00
0.00.043.848 I print_info: n_ff             = 8192
0.00.043.848 I print_info: n_expert         = 0
0.00.043.848 I print_info: n_expert_used    = 0
0.00.043.848 I print_info: causal attn      = 1
0.00.043.848 I print_info: pooling type     = 0
0.00.043.848 I print_info: rope type        = 2
0.00.043.848 I print_info: rope scaling     = linear
0.00.043.849 I print_info: freq_base_train  = 10000.0
0.00.043.850 I print_info: freq_scale_train = 1
0.00.043.851 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.851 I print_info: rope_finetuned   = unknown
0.00.043.851 I print_info: ssm_d_conv       = 0
0.00.043.851 I print_info: ssm_d_inner      = 0
0.00.043.851 I print_info: ssm_d_state      = 0
0.00.043.851 I print_info: ssm_dt_rank      = 0
0.00.043.851 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.852 I print_info: model type       = 1.4B
0.00.043.852 I print_info: model params     = 1.41 B
0.00.043.852 I print_info: general.name     = 1.4B
0.00.043.853 I print_info: vocab type       = BPE
0.00.043.854 I print_info: n_vocab          = 50304
0.00.043.854 I print_info: n_merges         = 50009
0.00.043.854 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.855 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.855 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.855 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.855 I print_info: LF token         = 187 ''
0.00.043.855 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.856 I print_info: max token length = 1024
0.00.043.858 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.368.908 I load_tensors: offloading 24 repeating layers to GPU
0.00.368.918 I load_tensors: offloading output layer to GPU
0.00.368.918 I load_tensors: offloaded 25/25 layers to GPU
0.00.368.947 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.368.949 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.370.523 I llama_init_from_model: n_seq_max     = 1
0.00.370.527 I llama_init_from_model: n_ctx         = 128
0.00.370.527 I llama_init_from_model: n_ctx_per_seq = 128
0.00.370.528 I llama_init_from_model: n_batch       = 128
0.00.370.528 I llama_init_from_model: n_ubatch      = 128
0.00.370.528 I llama_init_from_model: flash_attn    = 0
0.00.370.530 I llama_init_from_model: freq_base     = 10000.0
0.00.370.531 I llama_init_from_model: freq_scale    = 1
0.00.370.531 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.370.534 I ggml_metal_init: allocating
0.00.370.606 I ggml_metal_init: found device: Apple M4
0.00.370.622 I ggml_metal_init: picking default device: Apple M4
0.00.372.065 I ggml_metal_init: using embedded metal library
0.00.377.454 I ggml_metal_init: GPU name:   Apple M4
0.00.377.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.377.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.377.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.377.474 I ggml_metal_init: simdgroup reduction   = true
0.00.377.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.377.474 I ggml_metal_init: has residency sets    = true
0.00.377.475 I ggml_metal_init: has bfloat            = true
0.00.377.475 I ggml_metal_init: use bfloat            = true
0.00.377.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.377.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.400.402 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.404.164 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.404.169 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.404.199 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.407.586 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.407.587 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.407.588 I llama_init_from_model: graph nodes  = 967
0.00.407.589 I llama_init_from_model: graph splits = 2
0.00.407.592 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.407.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.181 I 
0.00.439.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.280 I perplexity: tokenizing the input ..
0.00.445.754 I perplexity: tokenization took 6.47 ms
0.00.445.760 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.591.775 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.593.289 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.593.317 I llama_perf_context_print:        load time =     430.26 ms
0.00.593.319 I llama_perf_context_print: prompt eval time =     145.13 ms /   128 tokens (    1.13 ms per token,   881.94 tokens per second)
0.00.593.319 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.320 I llama_perf_context_print:       total time =     154.14 ms /   129 tokens
0.00.593.707 I ggml_metal_free: deallocating

real	0m0.617s
user	0m0.081s
sys	0m0.085s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.730 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.078 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.084 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.087 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.088 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.089 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.627 I llama_model_loader: - type  f32:  194 tensors
0.00.024.627 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.627 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.627 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.627 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.628 I print_info: file format = GGUF V3 (latest)
0.00.024.628 I print_info: file type   = Q3_K - Medium
0.00.024.629 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.399 I load: special tokens cache size = 25
0.00.038.793 I load: token to piece cache size = 0.2984 MB
0.00.038.807 I print_info: arch             = gptneox
0.00.038.808 I print_info: vocab_only       = 0
0.00.038.809 I print_info: n_ctx_train      = 2048
0.00.038.809 I print_info: n_embd           = 2048
0.00.038.809 I print_info: n_layer          = 24
0.00.038.812 I print_info: n_head           = 16
0.00.038.813 I print_info: n_head_kv        = 16
0.00.038.813 I print_info: n_rot            = 32
0.00.038.813 I print_info: n_swa            = 0
0.00.038.813 I print_info: n_embd_head_k    = 128
0.00.038.813 I print_info: n_embd_head_v    = 128
0.00.038.814 I print_info: n_gqa            = 1
0.00.038.815 I print_info: n_embd_k_gqa     = 2048
0.00.038.815 I print_info: n_embd_v_gqa     = 2048
0.00.038.816 I print_info: f_norm_eps       = 1.0e-05
0.00.038.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.817 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.819 I print_info: f_logit_scale    = 0.0e+00
0.00.038.819 I print_info: n_ff             = 8192
0.00.038.819 I print_info: n_expert         = 0
0.00.038.819 I print_info: n_expert_used    = 0
0.00.038.821 I print_info: causal attn      = 1
0.00.038.821 I print_info: pooling type     = 0
0.00.038.821 I print_info: rope type        = 2
0.00.038.821 I print_info: rope scaling     = linear
0.00.038.822 I print_info: freq_base_train  = 10000.0
0.00.038.822 I print_info: freq_scale_train = 1
0.00.038.822 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.822 I print_info: rope_finetuned   = unknown
0.00.038.822 I print_info: ssm_d_conv       = 0
0.00.038.823 I print_info: ssm_d_inner      = 0
0.00.038.823 I print_info: ssm_d_state      = 0
0.00.038.823 I print_info: ssm_dt_rank      = 0
0.00.038.823 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.823 I print_info: model type       = 1.4B
0.00.038.823 I print_info: model params     = 1.41 B
0.00.038.823 I print_info: general.name     = 1.4B
0.00.038.827 I print_info: vocab type       = BPE
0.00.038.828 I print_info: n_vocab          = 50304
0.00.038.828 I print_info: n_merges         = 50009
0.00.038.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.829 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.829 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.829 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.829 I print_info: LF token         = 187 ''
0.00.038.830 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.831 I print_info: max token length = 1024
0.00.038.831 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.459.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.459.598 I load_tensors: offloading output layer to GPU
0.00.459.598 I load_tensors: offloaded 25/25 layers to GPU
0.00.459.631 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.459.632 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.461.261 I llama_init_from_model: n_seq_max     = 1
0.00.461.264 I llama_init_from_model: n_ctx         = 2048
0.00.461.265 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.461.265 I llama_init_from_model: n_batch       = 2048
0.00.461.266 I llama_init_from_model: n_ubatch      = 512
0.00.461.266 I llama_init_from_model: flash_attn    = 0
0.00.461.268 I llama_init_from_model: freq_base     = 10000.0
0.00.461.268 I llama_init_from_model: freq_scale    = 1
0.00.461.270 I ggml_metal_init: allocating
0.00.461.353 I ggml_metal_init: found device: Apple M4
0.00.461.366 I ggml_metal_init: picking default device: Apple M4
0.00.462.962 I ggml_metal_init: using embedded metal library
0.00.468.679 I ggml_metal_init: GPU name:   Apple M4
0.00.468.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.468.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.468.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.468.685 I ggml_metal_init: simdgroup reduction   = true
0.00.468.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.468.686 I ggml_metal_init: has residency sets    = true
0.00.468.686 I ggml_metal_init: has bfloat            = true
0.00.468.686 I ggml_metal_init: use bfloat            = true
0.00.468.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.468.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.488.366 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.551.465 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.551.472 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.551.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.555.982 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.555.984 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.555.985 I llama_init_from_model: graph nodes  = 967
0.00.555.985 I llama_init_from_model: graph splits = 2
0.00.555.991 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.556.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.556.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.009 I main: llama threadpool init, n_threads = 4
0.00.610.069 I 
0.00.610.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.094 I 
0.00.610.244 I sampler seed: 1234
0.00.610.249 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.610.264 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.610.266 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.610.266 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.125 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48696.84 tokens per second)
0.01.342.125 I llama_perf_context_print:        load time =     600.55 ms
0.01.342.126 I llama_perf_context_print: prompt eval time =      40.17 ms /     7 tokens (    5.74 ms per token,   174.25 tokens per second)
0.01.342.128 I llama_perf_context_print:        eval time =     689.06 ms /    63 runs   (   10.94 ms per token,    91.43 tokens per second)
0.01.342.128 I llama_perf_context_print:       total time =     732.85 ms /    70 tokens
0.01.342.365 I ggml_metal_free: deallocating

real	0m1.361s
user	0m0.109s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.072 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.271 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.278 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.285 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.285 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.290 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.125 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.881 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.882 I llama_model_loader: - type  f32:  194 tensors
0.00.024.883 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.883 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.883 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.884 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.884 I print_info: file format = GGUF V3 (latest)
0.00.024.885 I print_info: file type   = Q3_K - Medium
0.00.024.886 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.093 I load: special tokens cache size = 25
0.00.039.571 I load: token to piece cache size = 0.2984 MB
0.00.039.589 I print_info: arch             = gptneox
0.00.039.589 I print_info: vocab_only       = 0
0.00.039.590 I print_info: n_ctx_train      = 2048
0.00.039.590 I print_info: n_embd           = 2048
0.00.039.590 I print_info: n_layer          = 24
0.00.039.594 I print_info: n_head           = 16
0.00.039.594 I print_info: n_head_kv        = 16
0.00.039.596 I print_info: n_rot            = 32
0.00.039.596 I print_info: n_swa            = 0
0.00.039.596 I print_info: n_embd_head_k    = 128
0.00.039.596 I print_info: n_embd_head_v    = 128
0.00.039.598 I print_info: n_gqa            = 1
0.00.039.599 I print_info: n_embd_k_gqa     = 2048
0.00.039.600 I print_info: n_embd_v_gqa     = 2048
0.00.039.601 I print_info: f_norm_eps       = 1.0e-05
0.00.039.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.602 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.602 I print_info: f_logit_scale    = 0.0e+00
0.00.039.603 I print_info: n_ff             = 8192
0.00.039.603 I print_info: n_expert         = 0
0.00.039.603 I print_info: n_expert_used    = 0
0.00.039.603 I print_info: causal attn      = 1
0.00.039.603 I print_info: pooling type     = 0
0.00.039.603 I print_info: rope type        = 2
0.00.039.604 I print_info: rope scaling     = linear
0.00.039.604 I print_info: freq_base_train  = 10000.0
0.00.039.604 I print_info: freq_scale_train = 1
0.00.039.604 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.605 I print_info: rope_finetuned   = unknown
0.00.039.605 I print_info: ssm_d_conv       = 0
0.00.039.605 I print_info: ssm_d_inner      = 0
0.00.039.605 I print_info: ssm_d_state      = 0
0.00.039.611 I print_info: ssm_dt_rank      = 0
0.00.039.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.613 I print_info: model type       = 1.4B
0.00.039.615 I print_info: model params     = 1.41 B
0.00.039.615 I print_info: general.name     = 1.4B
0.00.039.616 I print_info: vocab type       = BPE
0.00.039.616 I print_info: n_vocab          = 50304
0.00.039.616 I print_info: n_merges         = 50009
0.00.039.616 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.616 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: LF token         = 187 ''
0.00.039.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: max token length = 1024
0.00.039.618 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.450.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.450.918 I load_tensors: offloading output layer to GPU
0.00.450.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.450.954 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.450.956 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.452.595 I llama_init_from_model: n_seq_max     = 1
0.00.452.599 I llama_init_from_model: n_ctx         = 128
0.00.452.599 I llama_init_from_model: n_ctx_per_seq = 128
0.00.452.600 I llama_init_from_model: n_batch       = 128
0.00.452.601 I llama_init_from_model: n_ubatch      = 128
0.00.452.601 I llama_init_from_model: flash_attn    = 0
0.00.452.604 I llama_init_from_model: freq_base     = 10000.0
0.00.452.604 I llama_init_from_model: freq_scale    = 1
0.00.452.605 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.452.607 I ggml_metal_init: allocating
0.00.452.685 I ggml_metal_init: found device: Apple M4
0.00.452.700 I ggml_metal_init: picking default device: Apple M4
0.00.454.251 I ggml_metal_init: using embedded metal library
0.00.460.665 I ggml_metal_init: GPU name:   Apple M4
0.00.460.673 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.674 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.674 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.675 I ggml_metal_init: simdgroup reduction   = true
0.00.460.675 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.676 I ggml_metal_init: has residency sets    = true
0.00.460.676 I ggml_metal_init: has bfloat            = true
0.00.460.676 I ggml_metal_init: use bfloat            = true
0.00.460.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.480.038 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.693 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.483.701 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.483.753 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.487.040 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.487.042 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.487.042 I llama_init_from_model: graph nodes  = 967
0.00.487.043 I llama_init_from_model: graph splits = 2
0.00.487.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.487.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.254 I 
0.00.516.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.357 I perplexity: tokenizing the input ..
0.00.523.072 I perplexity: tokenization took 6.711 ms
0.00.523.080 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.719 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.056 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.081 I llama_perf_context_print:        load time =     507.17 ms
0.00.665.082 I llama_perf_context_print: prompt eval time =     140.14 ms /   128 tokens (    1.09 ms per token,   913.38 tokens per second)
0.00.665.082 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.083 I llama_perf_context_print:       total time =     148.83 ms /   129 tokens
0.00.665.509 I ggml_metal_free: deallocating

real	0m0.680s
user	0m0.079s
sys	0m0.119s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.280 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.974 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.655 I llama_model_loader: - type  f32:  194 tensors
0.00.025.655 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.655 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.655 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.656 I print_info: file format = GGUF V3 (latest)
0.00.025.656 I print_info: file type   = Q4_K - Medium
0.00.025.657 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.379 I load: special tokens cache size = 25
0.00.039.568 I load: token to piece cache size = 0.2984 MB
0.00.039.582 I print_info: arch             = gptneox
0.00.039.583 I print_info: vocab_only       = 0
0.00.039.583 I print_info: n_ctx_train      = 2048
0.00.039.583 I print_info: n_embd           = 2048
0.00.039.583 I print_info: n_layer          = 24
0.00.039.586 I print_info: n_head           = 16
0.00.039.587 I print_info: n_head_kv        = 16
0.00.039.587 I print_info: n_rot            = 32
0.00.039.588 I print_info: n_swa            = 0
0.00.039.588 I print_info: n_embd_head_k    = 128
0.00.039.588 I print_info: n_embd_head_v    = 128
0.00.039.589 I print_info: n_gqa            = 1
0.00.039.590 I print_info: n_embd_k_gqa     = 2048
0.00.039.590 I print_info: n_embd_v_gqa     = 2048
0.00.039.592 I print_info: f_norm_eps       = 1.0e-05
0.00.039.592 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.592 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.592 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.593 I print_info: f_logit_scale    = 0.0e+00
0.00.039.593 I print_info: n_ff             = 8192
0.00.039.595 I print_info: n_expert         = 0
0.00.039.595 I print_info: n_expert_used    = 0
0.00.039.595 I print_info: causal attn      = 1
0.00.039.595 I print_info: pooling type     = 0
0.00.039.595 I print_info: rope type        = 2
0.00.039.595 I print_info: rope scaling     = linear
0.00.039.596 I print_info: freq_base_train  = 10000.0
0.00.039.597 I print_info: freq_scale_train = 1
0.00.039.597 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.597 I print_info: rope_finetuned   = unknown
0.00.039.598 I print_info: ssm_d_conv       = 0
0.00.039.598 I print_info: ssm_d_inner      = 0
0.00.039.598 I print_info: ssm_d_state      = 0
0.00.039.598 I print_info: ssm_dt_rank      = 0
0.00.039.598 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.598 I print_info: model type       = 1.4B
0.00.039.598 I print_info: model params     = 1.41 B
0.00.039.599 I print_info: general.name     = 1.4B
0.00.039.599 I print_info: vocab type       = BPE
0.00.039.599 I print_info: n_vocab          = 50304
0.00.039.599 I print_info: n_merges         = 50009
0.00.039.600 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: LF token         = 187 ''
0.00.039.600 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.601 I print_info: max token length = 1024
0.00.039.604 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.517.740 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.753 I load_tensors: offloading output layer to GPU
0.00.517.754 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.786 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.788 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.519.260 I llama_init_from_model: n_seq_max     = 1
0.00.519.263 I llama_init_from_model: n_ctx         = 2048
0.00.519.263 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.264 I llama_init_from_model: n_batch       = 2048
0.00.519.264 I llama_init_from_model: n_ubatch      = 512
0.00.519.265 I llama_init_from_model: flash_attn    = 0
0.00.519.267 I llama_init_from_model: freq_base     = 10000.0
0.00.519.267 I llama_init_from_model: freq_scale    = 1
0.00.519.271 I ggml_metal_init: allocating
0.00.519.327 I ggml_metal_init: found device: Apple M4
0.00.519.340 I ggml_metal_init: picking default device: Apple M4
0.00.520.848 I ggml_metal_init: using embedded metal library
0.00.526.331 I ggml_metal_init: GPU name:   Apple M4
0.00.526.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.338 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.339 I ggml_metal_init: simdgroup reduction   = true
0.00.526.339 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.339 I ggml_metal_init: has residency sets    = true
0.00.526.340 I ggml_metal_init: has bfloat            = true
0.00.526.340 I ggml_metal_init: use bfloat            = true
0.00.526.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.346 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.603.746 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.603.754 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.603.779 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.296 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.608.297 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.608.298 I llama_init_from_model: graph nodes  = 967
0.00.608.298 I llama_init_from_model: graph splits = 2
0.00.608.303 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.608.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.608.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.363 I main: llama threadpool init, n_threads = 4
0.00.657.414 I 
0.00.657.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.438 I 
0.00.657.553 I sampler seed: 1234
0.00.657.558 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.657.571 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.657.573 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.657.573 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.417.376 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51749.27 tokens per second)
0.01.417.376 I llama_perf_context_print:        load time =     646.74 ms
0.01.417.377 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.23 tokens per second)
0.01.417.378 I llama_perf_context_print:        eval time =     709.70 ms /    63 runs   (   11.27 ms per token,    88.77 tokens per second)
0.01.417.379 I llama_perf_context_print:       total time =     760.73 ms /    70 tokens
0.01.417.629 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.109s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.272 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.291 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.295 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.296 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.714 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.717 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.718 I llama_model_loader: - type  f32:  194 tensors
0.00.025.718 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.718 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.719 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.719 I print_info: file format = GGUF V3 (latest)
0.00.025.721 I print_info: file type   = Q4_K - Medium
0.00.025.723 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.756 I load: special tokens cache size = 25
0.00.039.741 I load: token to piece cache size = 0.2984 MB
0.00.039.757 I print_info: arch             = gptneox
0.00.039.758 I print_info: vocab_only       = 0
0.00.039.758 I print_info: n_ctx_train      = 2048
0.00.039.758 I print_info: n_embd           = 2048
0.00.039.758 I print_info: n_layer          = 24
0.00.039.763 I print_info: n_head           = 16
0.00.039.763 I print_info: n_head_kv        = 16
0.00.039.763 I print_info: n_rot            = 32
0.00.039.764 I print_info: n_swa            = 0
0.00.039.764 I print_info: n_embd_head_k    = 128
0.00.039.764 I print_info: n_embd_head_v    = 128
0.00.039.764 I print_info: n_gqa            = 1
0.00.039.765 I print_info: n_embd_k_gqa     = 2048
0.00.039.766 I print_info: n_embd_v_gqa     = 2048
0.00.039.767 I print_info: f_norm_eps       = 1.0e-05
0.00.039.767 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.767 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.768 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.768 I print_info: f_logit_scale    = 0.0e+00
0.00.039.768 I print_info: n_ff             = 8192
0.00.039.771 I print_info: n_expert         = 0
0.00.039.771 I print_info: n_expert_used    = 0
0.00.039.771 I print_info: causal attn      = 1
0.00.039.771 I print_info: pooling type     = 0
0.00.039.771 I print_info: rope type        = 2
0.00.039.771 I print_info: rope scaling     = linear
0.00.039.772 I print_info: freq_base_train  = 10000.0
0.00.039.772 I print_info: freq_scale_train = 1
0.00.039.772 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.772 I print_info: rope_finetuned   = unknown
0.00.039.772 I print_info: ssm_d_conv       = 0
0.00.039.772 I print_info: ssm_d_inner      = 0
0.00.039.772 I print_info: ssm_d_state      = 0
0.00.039.773 I print_info: ssm_dt_rank      = 0
0.00.039.774 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.774 I print_info: model type       = 1.4B
0.00.039.775 I print_info: model params     = 1.41 B
0.00.039.775 I print_info: general.name     = 1.4B
0.00.039.775 I print_info: vocab type       = BPE
0.00.039.776 I print_info: n_vocab          = 50304
0.00.039.776 I print_info: n_merges         = 50009
0.00.039.776 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.776 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: LF token         = 187 ''
0.00.039.778 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: max token length = 1024
0.00.039.778 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.513.482 I load_tensors: offloading 24 repeating layers to GPU
0.00.513.497 I load_tensors: offloading output layer to GPU
0.00.513.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.513.533 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.513.534 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.066 I llama_init_from_model: n_seq_max     = 1
0.00.515.069 I llama_init_from_model: n_ctx         = 128
0.00.515.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.515.070 I llama_init_from_model: n_batch       = 128
0.00.515.071 I llama_init_from_model: n_ubatch      = 128
0.00.515.071 I llama_init_from_model: flash_attn    = 0
0.00.515.074 I llama_init_from_model: freq_base     = 10000.0
0.00.515.074 I llama_init_from_model: freq_scale    = 1
0.00.515.075 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.515.077 I ggml_metal_init: allocating
0.00.515.210 I ggml_metal_init: found device: Apple M4
0.00.515.232 I ggml_metal_init: picking default device: Apple M4
0.00.517.079 I ggml_metal_init: using embedded metal library
0.00.523.933 I ggml_metal_init: GPU name:   Apple M4
0.00.523.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.940 I ggml_metal_init: simdgroup reduction   = true
0.00.523.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.940 I ggml_metal_init: has residency sets    = true
0.00.523.940 I ggml_metal_init: has bfloat            = true
0.00.523.941 I ggml_metal_init: use bfloat            = true
0.00.523.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.202 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.545.793 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.545.797 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.545.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.013 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.549.015 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.549.016 I llama_init_from_model: graph nodes  = 967
0.00.549.016 I llama_init_from_model: graph splits = 2
0.00.549.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.549.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.041 I 
0.00.580.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.142 I perplexity: tokenizing the input ..
0.00.586.987 I perplexity: tokenization took 6.844 ms
0.00.586.993 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.523 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.856 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.880 I llama_perf_context_print:        load time =     569.76 ms
0.00.726.881 I llama_perf_context_print: prompt eval time =     138.30 ms /   128 tokens (    1.08 ms per token,   925.52 tokens per second)
0.00.726.881 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.882 I llama_perf_context_print:       total time =     146.84 ms /   129 tokens
0.00.727.257 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.078s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.354 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.773 I llama_model_loader: - type  f32:  194 tensors
0.00.023.773 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.773 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.774 I print_info: file format = GGUF V3 (latest)
0.00.023.774 I print_info: file type   = Q5_K - Medium
0.00.023.775 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.478 I load: special tokens cache size = 25
0.00.037.769 I load: token to piece cache size = 0.2984 MB
0.00.037.783 I print_info: arch             = gptneox
0.00.037.784 I print_info: vocab_only       = 0
0.00.037.784 I print_info: n_ctx_train      = 2048
0.00.037.785 I print_info: n_embd           = 2048
0.00.037.785 I print_info: n_layer          = 24
0.00.037.787 I print_info: n_head           = 16
0.00.037.788 I print_info: n_head_kv        = 16
0.00.037.788 I print_info: n_rot            = 32
0.00.037.788 I print_info: n_swa            = 0
0.00.037.789 I print_info: n_embd_head_k    = 128
0.00.037.789 I print_info: n_embd_head_v    = 128
0.00.037.789 I print_info: n_gqa            = 1
0.00.037.790 I print_info: n_embd_k_gqa     = 2048
0.00.037.791 I print_info: n_embd_v_gqa     = 2048
0.00.037.791 I print_info: f_norm_eps       = 1.0e-05
0.00.037.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.792 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.792 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.792 I print_info: f_logit_scale    = 0.0e+00
0.00.037.794 I print_info: n_ff             = 8192
0.00.037.794 I print_info: n_expert         = 0
0.00.037.794 I print_info: n_expert_used    = 0
0.00.037.795 I print_info: causal attn      = 1
0.00.037.795 I print_info: pooling type     = 0
0.00.037.795 I print_info: rope type        = 2
0.00.037.795 I print_info: rope scaling     = linear
0.00.037.795 I print_info: freq_base_train  = 10000.0
0.00.037.796 I print_info: freq_scale_train = 1
0.00.037.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.796 I print_info: rope_finetuned   = unknown
0.00.037.796 I print_info: ssm_d_conv       = 0
0.00.037.798 I print_info: ssm_d_inner      = 0
0.00.037.798 I print_info: ssm_d_state      = 0
0.00.037.798 I print_info: ssm_dt_rank      = 0
0.00.037.798 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.798 I print_info: model type       = 1.4B
0.00.037.798 I print_info: model params     = 1.41 B
0.00.037.799 I print_info: general.name     = 1.4B
0.00.037.799 I print_info: vocab type       = BPE
0.00.037.799 I print_info: n_vocab          = 50304
0.00.037.799 I print_info: n_merges         = 50009
0.00.037.800 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.800 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.800 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.800 I print_info: LF token         = 187 ''
0.00.037.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.801 I print_info: max token length = 1024
0.00.037.801 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.036 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.051 I load_tensors: offloading output layer to GPU
0.00.599.052 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.089 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.091 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.600.769 I llama_init_from_model: n_seq_max     = 1
0.00.600.772 I llama_init_from_model: n_ctx         = 2048
0.00.600.772 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.600.772 I llama_init_from_model: n_batch       = 2048
0.00.600.773 I llama_init_from_model: n_ubatch      = 512
0.00.600.774 I llama_init_from_model: flash_attn    = 0
0.00.600.775 I llama_init_from_model: freq_base     = 10000.0
0.00.600.775 I llama_init_from_model: freq_scale    = 1
0.00.600.777 I ggml_metal_init: allocating
0.00.600.822 I ggml_metal_init: found device: Apple M4
0.00.600.834 I ggml_metal_init: picking default device: Apple M4
0.00.602.192 I ggml_metal_init: using embedded metal library
0.00.608.460 I ggml_metal_init: GPU name:   Apple M4
0.00.608.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.469 I ggml_metal_init: simdgroup reduction   = true
0.00.608.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.469 I ggml_metal_init: has residency sets    = true
0.00.608.470 I ggml_metal_init: has bfloat            = true
0.00.608.470 I ggml_metal_init: use bfloat            = true
0.00.608.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.491 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.678.498 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.531 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.215 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.683.217 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.683.217 I llama_init_from_model: graph nodes  = 967
0.00.683.218 I llama_init_from_model: graph splits = 2
0.00.683.223 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.683.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.683.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.194 I main: llama threadpool init, n_threads = 4
0.00.747.246 I 
0.00.747.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.290 I 
0.00.747.474 I sampler seed: 1234
0.00.747.479 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.494 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.494 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.597.700 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.01.597.700 I llama_perf_context_print:        load time =     737.69 ms
0.01.597.701 I llama_perf_context_print: prompt eval time =      52.58 ms /     7 tokens (    7.51 ms per token,   133.14 tokens per second)
0.01.597.702 I llama_perf_context_print:        eval time =     794.82 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.597.702 I llama_perf_context_print:       total time =     851.22 ms /    70 tokens
0.01.598.008 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.202 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.835 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.617 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.332 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.332 I llama_model_loader: - type  f32:  194 tensors
0.00.024.333 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.333 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.334 I print_info: file format = GGUF V3 (latest)
0.00.024.334 I print_info: file type   = Q5_K - Medium
0.00.024.335 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.600 I load: special tokens cache size = 25
0.00.039.088 I load: token to piece cache size = 0.2984 MB
0.00.039.106 I print_info: arch             = gptneox
0.00.039.107 I print_info: vocab_only       = 0
0.00.039.107 I print_info: n_ctx_train      = 2048
0.00.039.107 I print_info: n_embd           = 2048
0.00.039.107 I print_info: n_layer          = 24
0.00.039.111 I print_info: n_head           = 16
0.00.039.112 I print_info: n_head_kv        = 16
0.00.039.112 I print_info: n_rot            = 32
0.00.039.112 I print_info: n_swa            = 0
0.00.039.112 I print_info: n_embd_head_k    = 128
0.00.039.113 I print_info: n_embd_head_v    = 128
0.00.039.113 I print_info: n_gqa            = 1
0.00.039.114 I print_info: n_embd_k_gqa     = 2048
0.00.039.114 I print_info: n_embd_v_gqa     = 2048
0.00.039.115 I print_info: f_norm_eps       = 1.0e-05
0.00.039.115 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.115 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.116 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.116 I print_info: f_logit_scale    = 0.0e+00
0.00.039.120 I print_info: n_ff             = 8192
0.00.039.120 I print_info: n_expert         = 0
0.00.039.120 I print_info: n_expert_used    = 0
0.00.039.120 I print_info: causal attn      = 1
0.00.039.120 I print_info: pooling type     = 0
0.00.039.120 I print_info: rope type        = 2
0.00.039.120 I print_info: rope scaling     = linear
0.00.039.121 I print_info: freq_base_train  = 10000.0
0.00.039.121 I print_info: freq_scale_train = 1
0.00.039.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.123 I print_info: rope_finetuned   = unknown
0.00.039.123 I print_info: ssm_d_conv       = 0
0.00.039.123 I print_info: ssm_d_inner      = 0
0.00.039.123 I print_info: ssm_d_state      = 0
0.00.039.123 I print_info: ssm_dt_rank      = 0
0.00.039.123 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.123 I print_info: model type       = 1.4B
0.00.039.124 I print_info: model params     = 1.41 B
0.00.039.124 I print_info: general.name     = 1.4B
0.00.039.124 I print_info: vocab type       = BPE
0.00.039.125 I print_info: n_vocab          = 50304
0.00.039.125 I print_info: n_merges         = 50009
0.00.039.125 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: LF token         = 187 ''
0.00.039.126 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: max token length = 1024
0.00.039.129 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.579 I load_tensors: offloading output layer to GPU
0.00.597.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.619 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.621 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.292 I llama_init_from_model: n_seq_max     = 1
0.00.599.296 I llama_init_from_model: n_ctx         = 128
0.00.599.297 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.297 I llama_init_from_model: n_batch       = 128
0.00.599.298 I llama_init_from_model: n_ubatch      = 128
0.00.599.298 I llama_init_from_model: flash_attn    = 0
0.00.599.300 I llama_init_from_model: freq_base     = 10000.0
0.00.599.301 I llama_init_from_model: freq_scale    = 1
0.00.599.301 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.307 I ggml_metal_init: allocating
0.00.599.452 I ggml_metal_init: found device: Apple M4
0.00.599.468 I ggml_metal_init: picking default device: Apple M4
0.00.601.151 I ggml_metal_init: using embedded metal library
0.00.607.680 I ggml_metal_init: GPU name:   Apple M4
0.00.607.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.687 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.687 I ggml_metal_init: simdgroup reduction   = true
0.00.607.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.688 I ggml_metal_init: has residency sets    = true
0.00.607.688 I ggml_metal_init: has bfloat            = true
0.00.607.689 I ggml_metal_init: use bfloat            = true
0.00.607.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.242 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.650 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.628.654 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.827 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.631.829 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.631.829 I llama_init_from_model: graph nodes  = 967
0.00.631.830 I llama_init_from_model: graph splits = 2
0.00.631.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.613 I 
0.00.664.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.723 I perplexity: tokenizing the input ..
0.00.672.065 I perplexity: tokenization took 7.343 ms
0.00.672.081 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.330 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.811.745 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.811.768 I llama_perf_context_print:        load time =     655.40 ms
0.00.811.769 I llama_perf_context_print: prompt eval time =     137.37 ms /   128 tokens (    1.07 ms per token,   931.76 tokens per second)
0.00.811.769 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.770 I llama_perf_context_print:       total time =     147.16 ms /   129 tokens
0.00.812.118 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.080s
sys	0m0.143s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.293 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.303 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.303 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.304 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.305 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.305 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.307 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.308 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.978 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.591 I llama_model_loader: - type  f32:  194 tensors
0.00.023.591 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.592 I print_info: file format = GGUF V3 (latest)
0.00.023.592 I print_info: file type   = Q6_K
0.00.023.593 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.286 I load: special tokens cache size = 25
0.00.037.475 I load: token to piece cache size = 0.2984 MB
0.00.037.489 I print_info: arch             = gptneox
0.00.037.490 I print_info: vocab_only       = 0
0.00.037.490 I print_info: n_ctx_train      = 2048
0.00.037.490 I print_info: n_embd           = 2048
0.00.037.491 I print_info: n_layer          = 24
0.00.037.493 I print_info: n_head           = 16
0.00.037.494 I print_info: n_head_kv        = 16
0.00.037.494 I print_info: n_rot            = 32
0.00.037.494 I print_info: n_swa            = 0
0.00.037.494 I print_info: n_embd_head_k    = 128
0.00.037.494 I print_info: n_embd_head_v    = 128
0.00.037.495 I print_info: n_gqa            = 1
0.00.037.496 I print_info: n_embd_k_gqa     = 2048
0.00.037.497 I print_info: n_embd_v_gqa     = 2048
0.00.037.497 I print_info: f_norm_eps       = 1.0e-05
0.00.037.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.498 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.498 I print_info: f_logit_scale    = 0.0e+00
0.00.037.499 I print_info: n_ff             = 8192
0.00.037.499 I print_info: n_expert         = 0
0.00.037.499 I print_info: n_expert_used    = 0
0.00.037.499 I print_info: causal attn      = 1
0.00.037.499 I print_info: pooling type     = 0
0.00.037.499 I print_info: rope type        = 2
0.00.037.500 I print_info: rope scaling     = linear
0.00.037.500 I print_info: freq_base_train  = 10000.0
0.00.037.500 I print_info: freq_scale_train = 1
0.00.037.500 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.501 I print_info: rope_finetuned   = unknown
0.00.037.501 I print_info: ssm_d_conv       = 0
0.00.037.501 I print_info: ssm_d_inner      = 0
0.00.037.501 I print_info: ssm_d_state      = 0
0.00.037.501 I print_info: ssm_dt_rank      = 0
0.00.037.501 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.501 I print_info: model type       = 1.4B
0.00.037.502 I print_info: model params     = 1.41 B
0.00.037.502 I print_info: general.name     = 1.4B
0.00.037.502 I print_info: vocab type       = BPE
0.00.037.503 I print_info: n_vocab          = 50304
0.00.037.503 I print_info: n_merges         = 50009
0.00.037.503 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.503 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.503 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.503 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.504 I print_info: LF token         = 187 ''
0.00.037.504 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.504 I print_info: max token length = 1024
0.00.037.504 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.922 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.926 I load_tensors: offloading output layer to GPU
0.00.634.927 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.952 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.634.953 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.636.565 I llama_init_from_model: n_seq_max     = 1
0.00.636.568 I llama_init_from_model: n_ctx         = 2048
0.00.636.568 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.636.569 I llama_init_from_model: n_batch       = 2048
0.00.636.569 I llama_init_from_model: n_ubatch      = 512
0.00.636.570 I llama_init_from_model: flash_attn    = 0
0.00.636.570 I llama_init_from_model: freq_base     = 10000.0
0.00.636.571 I llama_init_from_model: freq_scale    = 1
0.00.636.572 I ggml_metal_init: allocating
0.00.636.617 I ggml_metal_init: found device: Apple M4
0.00.636.627 I ggml_metal_init: picking default device: Apple M4
0.00.637.775 I ggml_metal_init: using embedded metal library
0.00.643.752 I ggml_metal_init: GPU name:   Apple M4
0.00.643.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.758 I ggml_metal_init: simdgroup reduction   = true
0.00.643.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.758 I ggml_metal_init: has residency sets    = true
0.00.643.759 I ggml_metal_init: has bfloat            = true
0.00.643.759 I ggml_metal_init: use bfloat            = true
0.00.643.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.764 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.636 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.716 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.714.722 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.714.744 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.006 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.007 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.007 I llama_init_from_model: graph nodes  = 967
0.00.719.008 I llama_init_from_model: graph splits = 2
0.00.719.012 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.083 I main: llama threadpool init, n_threads = 4
0.00.784.132 I 
0.00.784.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.152 I 
0.00.784.324 I sampler seed: 1234
0.00.784.329 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.345 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.345 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.663.003 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.663.004 I llama_perf_context_print:        load time =     774.48 ms
0.01.663.004 I llama_perf_context_print: prompt eval time =      57.51 ms /     7 tokens (    8.22 ms per token,   121.72 tokens per second)
0.01.663.006 I llama_perf_context_print:        eval time =     818.33 ms /    63 runs   (   12.99 ms per token,    76.99 tokens per second)
0.01.663.007 I llama_perf_context_print:       total time =     879.70 ms /    70 tokens
0.01.663.266 I ggml_metal_free: deallocating

real	0m1.684s
user	0m0.107s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4867 (8acdacb3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.025 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.664 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.675 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.445 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.167 I llama_model_loader: - type  f32:  194 tensors
0.00.024.168 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.168 I print_info: file format = GGUF V3 (latest)
0.00.024.169 I print_info: file type   = Q6_K
0.00.024.170 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.469 I load: special tokens cache size = 25
0.00.038.865 I load: token to piece cache size = 0.2984 MB
0.00.038.878 I print_info: arch             = gptneox
0.00.038.879 I print_info: vocab_only       = 0
0.00.038.879 I print_info: n_ctx_train      = 2048
0.00.038.879 I print_info: n_embd           = 2048
0.00.038.880 I print_info: n_layer          = 24
0.00.038.884 I print_info: n_head           = 16
0.00.038.884 I print_info: n_head_kv        = 16
0.00.038.885 I print_info: n_rot            = 32
0.00.038.885 I print_info: n_swa            = 0
0.00.038.885 I print_info: n_embd_head_k    = 128
0.00.038.885 I print_info: n_embd_head_v    = 128
0.00.038.886 I print_info: n_gqa            = 1
0.00.038.886 I print_info: n_embd_k_gqa     = 2048
0.00.038.887 I print_info: n_embd_v_gqa     = 2048
0.00.038.888 I print_info: f_norm_eps       = 1.0e-05
0.00.038.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.888 I print_info: f_logit_scale    = 0.0e+00
0.00.038.889 I print_info: n_ff             = 8192
0.00.038.889 I print_info: n_expert         = 0
0.00.038.889 I print_info: n_expert_used    = 0
0.00.038.889 I print_info: causal attn      = 1
0.00.038.890 I print_info: pooling type     = 0
0.00.038.890 I print_info: rope type        = 2
0.00.038.890 I print_info: rope scaling     = linear
0.00.038.890 I print_info: freq_base_train  = 10000.0
0.00.038.892 I print_info: freq_scale_train = 1
0.00.038.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.892 I print_info: rope_finetuned   = unknown
0.00.038.893 I print_info: ssm_d_conv       = 0
0.00.038.893 I print_info: ssm_d_inner      = 0
0.00.038.893 I print_info: ssm_d_state      = 0
0.00.038.893 I print_info: ssm_dt_rank      = 0
0.00.038.893 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.893 I print_info: model type       = 1.4B
0.00.038.893 I print_info: model params     = 1.41 B
0.00.038.894 I print_info: general.name     = 1.4B
0.00.038.894 I print_info: vocab type       = BPE
0.00.038.894 I print_info: n_vocab          = 50304
0.00.038.895 I print_info: n_merges         = 50009
0.00.038.895 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.895 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.895 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.895 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.895 I print_info: LF token         = 187 ''
0.00.038.896 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: max token length = 1024
0.00.038.896 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.535 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.551 I load_tensors: offloading output layer to GPU
0.00.619.552 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.588 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.619.628 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.621.066 I llama_init_from_model: n_seq_max     = 1
0.00.621.068 I llama_init_from_model: n_ctx         = 128
0.00.621.069 I llama_init_from_model: n_ctx_per_seq = 128
0.00.621.069 I llama_init_from_model: n_batch       = 128
0.00.621.070 I llama_init_from_model: n_ubatch      = 128
0.00.621.070 I llama_init_from_model: flash_attn    = 0
0.00.621.072 I llama_init_from_model: freq_base     = 10000.0
0.00.621.072 I llama_init_from_model: freq_scale    = 1
0.00.621.073 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.075 I ggml_metal_init: allocating
0.00.621.127 I ggml_metal_init: found device: Apple M4
0.00.621.139 I ggml_metal_init: picking default device: Apple M4
0.00.622.488 I ggml_metal_init: using embedded metal library
0.00.628.863 I ggml_metal_init: GPU name:   Apple M4
0.00.628.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.870 I ggml_metal_init: simdgroup reduction   = true
0.00.628.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.870 I ggml_metal_init: has residency sets    = true
0.00.628.871 I ggml_metal_init: has bfloat            = true
0.00.628.871 I ggml_metal_init: use bfloat            = true
0.00.628.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.821 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.216 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.649.220 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.247 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.652.592 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.652.594 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.652.594 I llama_init_from_model: graph nodes  = 967
0.00.652.594 I llama_init_from_model: graph splits = 2
0.00.652.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.652.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.009 I 
0.00.688.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.112 I perplexity: tokenizing the input ..
0.00.695.134 I perplexity: tokenization took 7.019 ms
0.00.695.142 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.884 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.829.217 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.829.246 I llama_perf_context_print:        load time =     678.98 ms
0.00.829.247 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.73 tokens per second)
0.00.829.250 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.250 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.829.607 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.079s
sys	0m0.152s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4867 (8acdacb3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140c064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140c06b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140c06f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140c0a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140c0a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140c0ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140c0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140c0b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140c0b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140c0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140c0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140c0c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140c0d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140c0d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140c0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140c0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140c0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140c0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140c0fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140c10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140c10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140c11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140c11ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140c12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140c12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140c12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140c133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140c13a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140c13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140c14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140c14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140c14d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140c15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140c154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140c15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140c15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140c163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140c16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140c16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140c17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140c17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140c17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140c17f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140c18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140c186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140c18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140c190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140c19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140c19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140c1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140c1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140c1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140c1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140c1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140c1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140c1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140c1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140c1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140c1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140c1d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140c1d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140c1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140c1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140c1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140c1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140c1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140c1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140c1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140c1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140c1ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140c20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140c208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140c20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140c212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140c21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140c21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140c222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140c227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140c22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140c23290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140c237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140c23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140c24120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140c24610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140c24bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140c25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140c25720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140c25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140c26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140c26830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140c26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140c27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140c27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140c27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140c284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140c28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140c29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140c19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140c29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140c29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140c2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140c2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140c2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140c2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140c2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140c2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140c2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140c2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140c2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140c2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140c2d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140c2ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140c2e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140c2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140c2ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140c2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140c2f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140c2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140c30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140c30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140c30d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140c31230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140c31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140c31c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140c32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140c32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140c32b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140c33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140c33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140c33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140c33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140c34430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140c34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140c34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140c35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140c35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140c35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140c36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140c36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140c36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140c37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140c37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140c37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140c38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140c38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140c38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140c38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140c39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140c39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140c39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140c3a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140c3a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140c3ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140c3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140c3b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140c3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140c3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140c3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140c3cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140c3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140c3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140c3da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140c3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140c3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140c3e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140c3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140c3f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140c3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140c3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140c40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140c40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140c40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140c41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140c41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140c41b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140c42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140c42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140c42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140c42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140c43430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140c43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140c43e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140c44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140c44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140c44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140c45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140c45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140c45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140c46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140c46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140c46b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140c47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140c47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140c47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140c47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140c48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140c48b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140c490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140c495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140c49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140c49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140c4a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140c4ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140c4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140c4b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140c4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140c4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140c4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140c4c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140c4cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140c4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140c4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140c4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140c4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140c4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140c4ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140c4f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140c4fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140c50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140c50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140c50be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140c51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140c51740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140c51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140c522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140c52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140c52e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140c533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140c53960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140c53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140c544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140c54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140c55020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140c555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140c55b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140c56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140c566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140c56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140c57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140c577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140c57da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140c58350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140c58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140c58eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140c59460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140c59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140c59fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140c5a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140c5ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140c5b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140c5b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140c5bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140c5c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140c5c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140c5cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140c5d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140c5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140c5de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140c5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140c5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140c5ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140c5f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140c5fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140c60070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140c60620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140c60bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140c61180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140c61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140c61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140c62130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140c62630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140c62b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140c63030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140c63530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140c63a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140c63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140c64430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140c64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140c64e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140c65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140c65830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140c65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x140c66230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x140c66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x140c66c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x140c67130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x140c67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x140c67b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x140c68030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x140c68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x140c68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x140c68f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140c69430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140c69e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140c6a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140c6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140c6b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140c6b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140c6bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140c6c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140c6c730 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.684.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140e04bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140e05030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140e054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140e05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140e05d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140e061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140e06660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140e06ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140e06f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140e073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140e07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140e07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140e08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140e091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140e099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140e0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140e0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140e0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140e0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140e0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140e0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140e0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140e0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140e0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140e0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140e0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140e0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140e0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140e0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140e0fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140e0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140e10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140e10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140e10f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140e11410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140e118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140e11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140e121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140e12690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140e12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140e12fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140e13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140e13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140e13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140e14250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140e146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140e14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140e15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140e154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140e15970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140e15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140e162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140e16750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140e16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140e17090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140e17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140e177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140e17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140e17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140e18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140e18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140e18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140e190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140e19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140e199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140e19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140e1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140e1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140e1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140e1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140e1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140e1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140e1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140e1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140e1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140e1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140e1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140e1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140e1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140e1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140e1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140e1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140e1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140e1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140e1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140e1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140e1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140e1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140e20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140e208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140e20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140e21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140e21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140e21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140e21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140e22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140e227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140e22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140e230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140e23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140e23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140e23df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140e24260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140e246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140e24b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140e24fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140e25420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140e25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140e25d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140e26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140e265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140e26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140e26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140e27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140e277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140e27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140e28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140e284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140e28960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140e28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140e29240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140e296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140e29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140e29f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140e2a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140e2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140e2ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140e2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140e2b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140e2ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140e2c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140e2c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140e2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140e2d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140e2d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140e2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140e2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140e2e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140e2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140e2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140e2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140e2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140e2f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140e2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140e30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140e305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140e30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140e30e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140e312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140e31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140e31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140e32040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140e324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140e32920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140e32d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140e33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140e33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140e33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140e33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140e343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140e34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140e34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140e35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140e35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140e35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140e37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140e375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140e39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140e3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140e3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140e3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140e3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140e3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140e3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140e3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140e3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140e3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140e3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140e3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140e3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140e40130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140e406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140e40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140e412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140e41760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140e41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140e420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140e428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140e42bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140e43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140e43710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140e43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140e44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140e44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140e44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140e45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140e45930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140e45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140e46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140e46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140e46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140e475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140e47b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140e48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140e486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140e48c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140e49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140e497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140e49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140e4a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140e4a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140e4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140e4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140e4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140e4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140e4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140e4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140e4d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140e4d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140e4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140e4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140e4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140e4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140e4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140e4f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140e4fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140e503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140e50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140e50f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140e514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140e51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140e52040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140e525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140e52ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140e53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140e53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140e53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140e54810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140e54dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140e55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140e55ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140e56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140e56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140e56f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140e57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140e57930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140e57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140e58330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140e58830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140e58d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140e59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140e59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140e59c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140e5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140e5a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140e5ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140e5b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x140e5b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x140e5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x140e5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x140e5c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x140e5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x140e5ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x140e5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x140e5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x140e5dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x140e5e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140e5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140e5f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140e5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140e5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140e606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140e60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140e610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140e61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140e61a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140d05810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140d05c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140d060f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140d06560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140d069d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140d06e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140d072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140d07720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140d07b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140d08000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140d08470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140d08b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140d096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140d09e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140d0a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140d0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140d0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140d0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140d0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140d0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140d0d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140d0d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140d0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140d0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140d0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140d0f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140d0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140d0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140d0fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140d10080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140d10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140d10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140d11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140d116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140d11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140d12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140d124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140d12950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140d12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140d13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140d13730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140d13bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140d14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140d14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140d149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140d14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140d152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140d15790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140d15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140d160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140d16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140d16a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140d16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140d17350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140d177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140d17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140d18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140d183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140d186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140d18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140d18f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140d19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140d19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140d19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140d1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140d1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140d1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140d1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140d1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140d1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140d1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140d1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140d1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140d1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140d1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140d1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140d1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140d1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140d1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140d1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140d1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140d1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140d1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140d1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140d1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140d1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140d202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140d20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140d20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140d21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140d214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140d21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140d21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140d22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140d22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140d22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140d22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140d233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140d23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140d23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140d24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140d24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140d250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140d25680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140d25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140d261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140d26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140d26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140d272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140d278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140d27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140d28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140d289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140d28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140d29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140d29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140d29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140d2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140d2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140d2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140d2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140d2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140d2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140d2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140d2c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140d2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140d2d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140d2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140d2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140d2df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140d2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140d2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140d2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140d2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140d2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140d2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140d30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140d30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140d30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140d31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140d31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140d31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140d32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140d32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140d32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140d32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140d33460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140d33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140d33e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140d34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140d34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140d34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140d35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140d35760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140d35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140d36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140d36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140d36b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140d37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140d37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140d37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140d37f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140d38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140d38960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140d38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140d39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140d39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140d39d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140d3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140d3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140d3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140d3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140d3b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140d3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140d3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140d3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140d3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140d3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140d3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140d3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140d3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140d3e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140d3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140d3ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140d3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140d3f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140d3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140d40160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140d40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140d40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140d41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140d41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140d41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140d41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140d42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140d42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140d43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140d43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140d43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140d44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140d44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140d44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140d450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140d45360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140d45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140d45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140d464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140d46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140d46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140d472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140d47b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140d47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140d48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140d48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140d48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140d494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140d49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140d4a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140d4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140d4ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140d4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140d4b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140d4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140d4c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140d4c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140d4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140d4d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140d4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140d4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140d4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140d4e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140d4efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140d4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140d4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140d500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140d50660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140d50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140d511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140d51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140d51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140d522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140d52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140d52e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140d533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140d53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140d53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140d544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140d54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140d55050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140d55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140d55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140d56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140d56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140d56cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140d57270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140d57820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140d57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140d58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140d58930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140d58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140d59490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140d59a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140d59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140d5a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140d5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140d5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140d5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140d5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140d5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140d5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140d5cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140d5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140d5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140d5da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140d5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140d5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140d5e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140d5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140d5f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140d5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140d5fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140d60260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x140d60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x140d60c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x140d61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x140d61660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x140d61b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x140d62060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x140d62560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x140d62a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x140d62f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x140d63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140d63960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140d64370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140d64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140d651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140d658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140d65b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140d66320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140d667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140d66c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.775s
user	0m0.277s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4867 (8acdacb3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e70b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e70bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e70c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e70c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e70ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e70d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e70d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e70df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e70e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e70ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e70ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e70f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e70ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e7106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e710ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e711600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e712440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e712b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e713330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e713a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e714170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e714890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e715130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e715850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e715cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e716190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e716830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e716cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e717430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e717b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e717de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e718280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e718720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e718bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e719500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e7199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e719e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e71a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e71a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e71ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e71b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e71b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e71b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e71bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e71c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e71cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e71d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e71d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e71da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e71dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e71e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e71e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e71eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e71f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e71f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e71fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e71ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e720290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e720730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e720bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e721510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e7219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e721e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e7222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e722790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e722c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e7230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e723570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e723a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e723f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e7244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e724a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e7254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e7259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e725f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e726490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e7269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e726f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e727480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e7279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e727f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e728470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e7289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e728f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e729460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e7299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e729f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e72a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e72a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e72aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e72b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e72b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e71c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e72be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e72c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e72cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e72d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e72d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e72daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e72e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e72e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e72eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e72f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e72f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e72fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e730020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e730570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e730ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e730f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e731400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e7318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e731d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e7321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e732680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e732b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e732fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e733460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e733900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e733da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e734240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e7346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e734b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e735020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e7354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e735960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e735e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e7362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e736740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e737080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e737520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e7379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e737e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e738300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e7387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e738c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e7390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e739580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e739a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e739ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e73a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e73a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e73aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e73b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e73b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e73ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e73bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e73c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e73c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e73cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e73d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e73d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e73dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e73df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e73e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e73e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e73ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e73f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e73f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e73fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e73ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e740480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e740920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e740dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e741260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e741700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e741ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e742040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e7424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e742980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e7432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e743760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e743c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e7440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e744540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e7449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e744e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e745320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e7457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e745c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e746100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e7465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e746a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e746ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e747380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e747820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e747cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e748210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e748760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e749200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e7496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e749b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e749fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e74a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e74a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e74adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e74b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e74b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e74bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e74c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e74c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e74ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e74ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e74d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e74dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e74df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e74e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e74ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e74f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e74f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e74fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e750120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e7506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e750c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e751230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e7517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e751d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e752340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e7528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e752ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e753450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e753a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e753fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e754560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e754b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e7550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e755670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e755c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e7561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e756780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e756d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e7572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e757890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e757e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e7583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e7589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e758f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e759500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e759ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e75a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e75a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e75abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e75b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e75b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e75bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e75c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e75c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e75cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e75d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e75d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e75def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e75e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e75ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e75f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e75f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e75fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e760110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e7606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e760c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e761220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e7617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e761cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e7621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e7626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e762bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e7630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e7635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e763ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e763fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e7644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e7649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e764ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e7653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e7658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e765dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e7662d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e7667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e766cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e7671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e7676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e767bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e7680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e7685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e768ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e768fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e7694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e769ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e76a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e76ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e76b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e76b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e76be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e76c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e76c7d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f80f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f80fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f810570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f810a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f810eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f811350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f8117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f811c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f812130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f8125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f812a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f812f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f8133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f813850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f813cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f814190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f814630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f814ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f814f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f815410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f8158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f8161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f816690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f816b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f816fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f817470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f817730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f8179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f817e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f8182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f818740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f818bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f819020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f819490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f819900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f819d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11dc04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11dc046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11dc04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11dc04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11dc053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11dc05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11dc05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11dc06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11dc065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11dc06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11dc06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11dc07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11dc07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11dc07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11dc08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11dc084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11dc08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11dc08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11dc09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11dc09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11dc09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11dc09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11dc0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11dc0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11dc0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11dc0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11dc0b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11dc0ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11dc0be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11dc0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11dc0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11dc0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11dc0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11dc0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11dc0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11dc0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11dc0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11dc0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11dc0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11dc0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11dc0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11dc0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11dc0fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11dc10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11dc10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11dc109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11dc10e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11dc112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11dc11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11dc11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11dc12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11dc12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11dc128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11dc12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11dc131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11dc13640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11dc13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11dc13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11dc14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11dc14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11dc14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11dc150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11dc15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11dc159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11dc15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11dc162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11dc16710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11dc16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11dc16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11dc17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11dc178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11dc17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11dc181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11dc18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11dc18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11dc18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11dc19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11dc197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11dc19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11dc1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11dc1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11dc1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11dc1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11dc1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11dc1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11dc1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11dc1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11dc1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11dc1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11dc1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11dc1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11dc1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11dc1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11dc1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11dc1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11dc1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11dc1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11dc1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11dc1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11dc200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11dc20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11dc20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11dc20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11dc21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11dc21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11dc21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11dc22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11dc22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11dc22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11dc23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11dc23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11dc23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11dc23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11dc24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11dc24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11dc24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11dc25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11dc25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11dc25d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11dc26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11dc26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11dc26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11dc27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11dc27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11dc27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11dc28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11dc28590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11dc28b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11dc290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11dc296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11dc29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11dc2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11dc2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11dc2ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11dc2b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11dc2b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11dc2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11dc2bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11dc2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11dc2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11dc2cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11dc2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11dc2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11dc2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11dc2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11dc2e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11dc2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11dc2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11dc2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11dc30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11dc30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11dc30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11dc31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11dc31740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11dc31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11dc322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11dc32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11dc32e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11dc333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11dc33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11dc33f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11dc344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11dc34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11dc35020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11dc355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11dc35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11dc36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11dc366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11dc36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11dc37240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11dc377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11dc37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11dc38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11dc38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11dc38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11dc39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11dc39a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11dc39fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11dc3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11dc3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11dc3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11dc3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11dc3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11dc3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11dc3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11dc3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11dc3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11dc3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11dc3de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11dc3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11dc3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11dc3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11dc3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11dc3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11dc40070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11dc40620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11dc40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11dc41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11dc41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11dc41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11dc42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11dc42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11dc42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11dc43190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11dc43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11dc43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11dc44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11dc44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11dc44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11dc44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11dc45490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11dc45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11dc45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11dc46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11dc46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11dc46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11dc47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11dc47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11dc47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11dc48190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11dc48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11dc48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11dc49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11dc49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11dc49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11dc49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11dc4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11dc4b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11dc4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11dc4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11dc4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11dc4c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11dc4cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11dc4d290 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e74e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e7575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e756490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e753160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e75fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e75d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e75b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e759210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e7514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e74ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e753cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e754dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e75a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e756ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e75ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e751aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e752bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e759d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e75bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e754820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e755930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e75ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e757b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e758100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e752600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e753710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e7603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e75dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e74f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e758c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e74e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e7503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e760980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e755ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e769790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e75e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e754270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e756a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e75a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e752050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e75c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e750f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e75f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e75caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e7586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e7614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e74fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e760f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e74f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e75f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e7597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e75b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e75e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e75d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e755380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e7176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e76ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e76cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e76d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e76d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e76d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e76d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e76db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e76ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e76e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e76e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e76e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e76e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e76eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e76ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e76f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e76f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e76f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e76f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e76fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e76fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e770190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e770450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e770710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e7709d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e770c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e770f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e771210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e7714d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e771790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e771a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e771d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e771fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e772290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e772550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e772810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e772ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e772d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e773050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e773310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e7735d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e773890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e773b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e773e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e7740d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e774390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e774650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e774910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e774bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e774e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e775150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e775410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e7756d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e775990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e775c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e775f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e7761d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e776490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e776750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e776a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e776cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e776f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e777250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e777510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e7777d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e777a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e777d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e778010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e7782d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e778590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e778850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e778b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e778dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e779090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e779350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e779610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e7798d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e779b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e779e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e77a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e77a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e77a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e77a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e77ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e77aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e77b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e77b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e77b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e77b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e77bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e77bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e77c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e77c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e77c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e77ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e77cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e77cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e77d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e77d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e77d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e77dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e77dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e77e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e77e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e77e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e77e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e77eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e77ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e77f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e77f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e77f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e77f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e77fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e77fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e780150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e780410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e7806d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e780990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e780c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e780f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e7811d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e781490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e781750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e781a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e781cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e781f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e782250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e782510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e7827d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e782a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e782f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e7833d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e783870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e783d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e7841b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e784650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e784af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e785040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e785590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e785ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e786030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e7862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e7865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e786ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e786fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e7874b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e7879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e787ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e788480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e788980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e788e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e789580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e789840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e789d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e78a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e78aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e78b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e78b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e78bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e78c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e78c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e78cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e78d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e78d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e78de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e78e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e78e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e78efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e78f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e78fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e7900f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e7906b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e790c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e791230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e7917f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e791db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e792370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e792930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e792ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e7934b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e793a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e794030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e7945f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e794bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e795170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e795730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e795cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e7962b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e796870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e796e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e7973f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e7979b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e797f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e798530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e798af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e7990b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e799670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e799c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e79a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e79a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e79ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e79b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e79b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e79beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e79c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e79ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e79cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e79d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e79db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e79e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e79e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e79ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e79f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e79f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e79fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e7a00b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e7a05b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e7a0ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e7a0fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e7a14b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e7a19b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e7a1eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e7a23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e7a28b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e7a2db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e7a32b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e7a37b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e7a3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e7a41b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e7a46b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e7a4bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e7a50b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e7a55b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e7a5ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e7a5fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e7a64b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e7a69b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e7a73c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e7a7ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e7a8200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e7a8920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e7a8be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e7a9370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e7a9630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e7a9b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.954s
user	0m0.230s
sys	0m0.188s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.09 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.52 sec*proc (2 tests)

Total Test time (real) =   1.54 sec
        1.56 real         0.51 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.25 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.65 sec
        0.65 real         0.13 user         0.09 sys
```
