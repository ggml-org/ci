### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.52 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.89 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.89 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  190.59 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.86 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.00 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.25 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 251.53 sec*proc (29 tests)

Total Test time (real) = 251.54 sec

real	4m11.603s
user	8m28.598s
sys	0m7.206s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.05 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.22 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.75 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.50 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.39 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.33 sec*proc (29 tests)

Total Test time (real) =  54.35 sec

real	0m54.361s
user	1m16.982s
sys	0m6.018s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.115 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.203 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.050 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.059 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.060 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.061 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.061 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.062 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.063 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.063 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.064 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.066 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.069 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.070 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.070 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.071 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.071 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.072 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.075 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.468 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.471 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.471 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.471 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.472 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.473 I llama_model_loader: - type  f32:  124 tensors
0.00.024.473 I llama_model_loader: - type  f16:   73 tensors
0.00.024.474 I print_info: file format = GGUF V3 (latest)
0.00.024.475 I print_info: file type   = F16
0.00.024.476 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.028.454 I load: special tokens cache size = 5
0.00.030.634 I load: token to piece cache size = 0.2032 MB
0.00.030.638 I print_info: arch             = bert
0.00.030.638 I print_info: vocab_only       = 0
0.00.030.639 I print_info: n_ctx_train      = 512
0.00.030.639 I print_info: n_embd           = 384
0.00.030.639 I print_info: n_layer          = 12
0.00.030.642 I print_info: n_head           = 12
0.00.030.643 I print_info: n_head_kv        = 12
0.00.030.643 I print_info: n_rot            = 32
0.00.030.643 I print_info: n_swa            = 0
0.00.030.643 I print_info: n_embd_head_k    = 32
0.00.030.644 I print_info: n_embd_head_v    = 32
0.00.030.645 I print_info: n_gqa            = 1
0.00.030.646 I print_info: n_embd_k_gqa     = 384
0.00.030.646 I print_info: n_embd_v_gqa     = 384
0.00.030.647 I print_info: f_norm_eps       = 1.0e-12
0.00.030.650 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.030.650 I print_info: f_clamp_kqv      = 0.0e+00
0.00.030.650 I print_info: f_max_alibi_bias = 0.0e+00
0.00.030.651 I print_info: f_logit_scale    = 0.0e+00
0.00.030.651 I print_info: n_ff             = 1536
0.00.030.652 I print_info: n_expert         = 0
0.00.030.652 I print_info: n_expert_used    = 0
0.00.030.652 I print_info: causal attn      = 0
0.00.030.652 I print_info: pooling type     = 2
0.00.030.652 I print_info: rope type        = 2
0.00.030.653 I print_info: rope scaling     = linear
0.00.030.653 I print_info: freq_base_train  = 10000.0
0.00.030.654 I print_info: freq_scale_train = 1
0.00.030.656 I print_info: n_ctx_orig_yarn  = 512
0.00.030.656 I print_info: rope_finetuned   = unknown
0.00.030.657 I print_info: ssm_d_conv       = 0
0.00.030.657 I print_info: ssm_d_inner      = 0
0.00.030.657 I print_info: ssm_d_state      = 0
0.00.030.657 I print_info: ssm_dt_rank      = 0
0.00.030.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.030.658 I print_info: model type       = 33M
0.00.030.658 I print_info: model params     = 33.21 M
0.00.030.658 I print_info: general.name     = Bge Small
0.00.030.659 I print_info: vocab type       = WPM
0.00.030.659 I print_info: n_vocab          = 30522
0.00.030.659 I print_info: n_merges         = 0
0.00.030.660 I print_info: BOS token        = 101 '[CLS]'
0.00.030.660 I print_info: UNK token        = 100 '[UNK]'
0.00.030.665 I print_info: SEP token        = 102 '[SEP]'
0.00.030.665 I print_info: PAD token        = 0 '[PAD]'
0.00.030.666 I print_info: MASK token       = 103 '[MASK]'
0.00.030.666 I print_info: LF token         = 0 '[PAD]'
0.00.030.666 I print_info: max token length = 21
0.00.030.667 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.033.737 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.739 I load_tensors: offloading output layer to GPU
0.00.033.739 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.763 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.765 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.025 I llama_init_from_model: n_seq_max     = 1
0.00.034.026 I llama_init_from_model: n_ctx         = 512
0.00.034.027 I llama_init_from_model: n_ctx_per_seq = 512
0.00.034.027 I llama_init_from_model: n_batch       = 2048
0.00.034.027 I llama_init_from_model: n_ubatch      = 2048
0.00.034.028 I llama_init_from_model: flash_attn    = 0
0.00.034.028 I llama_init_from_model: freq_base     = 10000.0
0.00.034.028 I llama_init_from_model: freq_scale    = 1
0.00.034.029 I ggml_metal_init: allocating
0.00.034.035 I ggml_metal_init: found device: Apple M4
0.00.034.039 I ggml_metal_init: picking default device: Apple M4
0.00.034.729 I ggml_metal_init: using embedded metal library
0.00.038.592 I ggml_metal_init: GPU name:   Apple M4
0.00.038.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.596 I ggml_metal_init: simdgroup reduction   = true
0.00.038.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.596 I ggml_metal_init: has residency sets    = true
0.00.038.596 I ggml_metal_init: has bfloat            = true
0.00.038.597 I ggml_metal_init: use bfloat            = true
0.00.038.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.049.991 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.050.664 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.050.666 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.050.686 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.051.899 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.051.900 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.051.901 I llama_init_from_model: graph nodes  = 429
0.00.051.901 I llama_init_from_model: graph splits = 2
0.00.051.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.051.903 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.057.760 I 
0.00.057.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.439 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.062.294 I llama_perf_context_print:        load time =      43.54 ms
0.00.062.299 I llama_perf_context_print: prompt eval time =       3.72 ms /     9 tokens (    0.41 ms per token,  2419.35 tokens per second)
0.00.062.300 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.062.301 I llama_perf_context_print:       total time =       4.53 ms /    10 tokens
0.00.062.448 I ggml_metal_free: deallocating

real	0m0.239s
user	0m0.046s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.048 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.585 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.345 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.350 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.351 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.351 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.351 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.352 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.352 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.353 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.353 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.354 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.356 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.356 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.356 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.357 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.357 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.357 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.837 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.502 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.503 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.504 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.504 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.504 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.505 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.505 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.505 I llama_model_loader: - type  f32:  124 tensors
0.00.015.506 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.506 I print_info: file format = GGUF V3 (latest)
0.00.015.507 I print_info: file type   = Q8_0
0.00.015.507 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.925 I load: special tokens cache size = 5
0.00.019.264 I load: token to piece cache size = 0.2032 MB
0.00.019.267 I print_info: arch             = bert
0.00.019.267 I print_info: vocab_only       = 0
0.00.019.267 I print_info: n_ctx_train      = 512
0.00.019.268 I print_info: n_embd           = 384
0.00.019.268 I print_info: n_layer          = 12
0.00.019.271 I print_info: n_head           = 12
0.00.019.271 I print_info: n_head_kv        = 12
0.00.019.271 I print_info: n_rot            = 32
0.00.019.272 I print_info: n_swa            = 0
0.00.019.272 I print_info: n_embd_head_k    = 32
0.00.019.272 I print_info: n_embd_head_v    = 32
0.00.019.272 I print_info: n_gqa            = 1
0.00.019.273 I print_info: n_embd_k_gqa     = 384
0.00.019.273 I print_info: n_embd_v_gqa     = 384
0.00.019.274 I print_info: f_norm_eps       = 1.0e-12
0.00.019.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.275 I print_info: f_logit_scale    = 0.0e+00
0.00.019.275 I print_info: n_ff             = 1536
0.00.019.275 I print_info: n_expert         = 0
0.00.019.276 I print_info: n_expert_used    = 0
0.00.019.276 I print_info: causal attn      = 0
0.00.019.276 I print_info: pooling type     = 2
0.00.019.276 I print_info: rope type        = 2
0.00.019.276 I print_info: rope scaling     = linear
0.00.019.276 I print_info: freq_base_train  = 10000.0
0.00.019.277 I print_info: freq_scale_train = 1
0.00.019.277 I print_info: n_ctx_orig_yarn  = 512
0.00.019.277 I print_info: rope_finetuned   = unknown
0.00.019.277 I print_info: ssm_d_conv       = 0
0.00.019.277 I print_info: ssm_d_inner      = 0
0.00.019.277 I print_info: ssm_d_state      = 0
0.00.019.277 I print_info: ssm_dt_rank      = 0
0.00.019.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.278 I print_info: model type       = 33M
0.00.019.278 I print_info: model params     = 33.21 M
0.00.019.278 I print_info: general.name     = Bge Small
0.00.019.280 I print_info: vocab type       = WPM
0.00.019.281 I print_info: n_vocab          = 30522
0.00.019.281 I print_info: n_merges         = 0
0.00.019.281 I print_info: BOS token        = 101 '[CLS]'
0.00.019.281 I print_info: UNK token        = 100 '[UNK]'
0.00.019.281 I print_info: SEP token        = 102 '[SEP]'
0.00.019.283 I print_info: PAD token        = 0 '[PAD]'
0.00.019.283 I print_info: MASK token       = 103 '[MASK]'
0.00.019.283 I print_info: LF token         = 0 '[PAD]'
0.00.019.283 I print_info: max token length = 21
0.00.019.284 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.057 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.058 I load_tensors: offloading output layer to GPU
0.00.021.058 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.064 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.064 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.244 I llama_init_from_model: n_seq_max     = 1
0.00.021.245 I llama_init_from_model: n_ctx         = 512
0.00.021.245 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.246 I llama_init_from_model: n_batch       = 2048
0.00.021.246 I llama_init_from_model: n_ubatch      = 2048
0.00.021.246 I llama_init_from_model: flash_attn    = 0
0.00.021.246 I llama_init_from_model: freq_base     = 10000.0
0.00.021.247 I llama_init_from_model: freq_scale    = 1
0.00.021.247 I ggml_metal_init: allocating
0.00.021.254 I ggml_metal_init: found device: Apple M4
0.00.021.258 I ggml_metal_init: picking default device: Apple M4
0.00.021.778 I ggml_metal_init: using embedded metal library
0.00.024.323 I ggml_metal_init: GPU name:   Apple M4
0.00.024.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.326 I ggml_metal_init: simdgroup reduction   = true
0.00.024.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.327 I ggml_metal_init: has residency sets    = true
0.00.024.327 I ggml_metal_init: has bfloat            = true
0.00.024.327 I ggml_metal_init: use bfloat            = true
0.00.024.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.577 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.181 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.183 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.197 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.177 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.178 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.178 I llama_init_from_model: graph nodes  = 429
0.00.036.178 I llama_init_from_model: graph splits = 2
0.00.036.180 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.180 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.673 I 
0.00.039.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.197 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.456 I llama_perf_context_print:        load time =      30.08 ms
0.00.043.457 I llama_perf_context_print: prompt eval time =       3.14 ms /     9 tokens (    0.35 ms per token,  2863.51 tokens per second)
0.00.043.458 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.458 I llama_perf_context_print:       total time =       3.78 ms /    10 tokens
0.00.043.662 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.288 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.141 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.437 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.445 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.449 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.450 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.450 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.452 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.452 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.453 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.454 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.457 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.460 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.460 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.461 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.462 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.684 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.079 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.082 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.082 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.083 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.083 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.083 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.084 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.084 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.085 I llama_model_loader: - type  f32:   40 tensors
0.00.049.085 I llama_model_loader: - type  f16:   30 tensors
0.00.049.086 I print_info: file format = GGUF V3 (latest)
0.00.049.086 I print_info: file type   = F16
0.00.049.087 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.193 W load: empty token at index 5
0.00.058.082 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.492 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.526 I load: special tokens cache size = 5
0.00.317.241 I load: token to piece cache size = 1.5060 MB
0.00.317.247 I print_info: arch             = jina-bert-v2
0.00.317.247 I print_info: vocab_only       = 0
0.00.317.248 I print_info: n_ctx_train      = 8192
0.00.317.248 I print_info: n_embd           = 384
0.00.317.248 I print_info: n_layer          = 4
0.00.317.255 I print_info: n_head           = 12
0.00.317.256 I print_info: n_head_kv        = 12
0.00.317.256 I print_info: n_rot            = 32
0.00.317.256 I print_info: n_swa            = 0
0.00.317.256 I print_info: n_embd_head_k    = 32
0.00.317.256 I print_info: n_embd_head_v    = 32
0.00.317.257 I print_info: n_gqa            = 1
0.00.317.257 I print_info: n_embd_k_gqa     = 384
0.00.317.261 I print_info: n_embd_v_gqa     = 384
0.00.317.262 I print_info: f_norm_eps       = 1.0e-12
0.00.317.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.317.264 I print_info: f_clamp_kqv      = 0.0e+00
0.00.317.265 I print_info: f_max_alibi_bias = 8.0e+00
0.00.317.265 I print_info: f_logit_scale    = 0.0e+00
0.00.317.266 I print_info: n_ff             = 1536
0.00.317.266 I print_info: n_expert         = 0
0.00.317.266 I print_info: n_expert_used    = 0
0.00.317.266 I print_info: causal attn      = 0
0.00.317.266 I print_info: pooling type     = -1
0.00.317.266 I print_info: rope type        = -1
0.00.317.266 I print_info: rope scaling     = linear
0.00.317.268 I print_info: freq_base_train  = 10000.0
0.00.317.268 I print_info: freq_scale_train = 1
0.00.317.268 I print_info: n_ctx_orig_yarn  = 8192
0.00.317.268 I print_info: rope_finetuned   = unknown
0.00.317.268 I print_info: ssm_d_conv       = 0
0.00.317.269 I print_info: ssm_d_inner      = 0
0.00.317.269 I print_info: ssm_d_state      = 0
0.00.317.269 I print_info: ssm_dt_rank      = 0
0.00.317.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.317.269 I print_info: model type       = 33M
0.00.317.269 I print_info: model params     = 32.90 M
0.00.317.270 I print_info: general.name     = Jina Bert Implementation
0.00.317.271 I print_info: vocab type       = BPE
0.00.317.271 I print_info: n_vocab          = 61056
0.00.317.271 I print_info: n_merges         = 39382
0.00.317.272 I print_info: BOS token        = 0 '<s>'
0.00.317.272 I print_info: EOS token        = 2 '</s>'
0.00.317.277 I print_info: UNK token        = 3 '<unk>'
0.00.317.278 I print_info: SEP token        = 2 '</s>'
0.00.317.278 I print_info: PAD token        = 1 '<pad>'
0.00.317.278 I print_info: MASK token       = 4 '<mask>'
0.00.317.279 I print_info: EOG token        = 2 '</s>'
0.00.317.279 I print_info: max token length = 45
0.00.317.279 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.319.627 I load_tensors: offloading 4 repeating layers to GPU
0.00.319.628 I load_tensors: offloading output layer to GPU
0.00.319.629 I load_tensors: offloaded 5/5 layers to GPU
0.00.319.656 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.319.657 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.320.031 I llama_init_from_model: n_seq_max     = 1
0.00.320.032 I llama_init_from_model: n_ctx         = 8192
0.00.320.032 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.320.032 I llama_init_from_model: n_batch       = 2048
0.00.320.033 I llama_init_from_model: n_ubatch      = 2048
0.00.320.033 I llama_init_from_model: flash_attn    = 0
0.00.320.033 I llama_init_from_model: freq_base     = 10000.0
0.00.320.034 I llama_init_from_model: freq_scale    = 1
0.00.320.035 I ggml_metal_init: allocating
0.00.320.038 I ggml_metal_init: found device: Apple M4
0.00.320.042 I ggml_metal_init: picking default device: Apple M4
0.00.321.008 I ggml_metal_init: using embedded metal library
0.00.323.744 I ggml_metal_init: GPU name:   Apple M4
0.00.323.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.323.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.323.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.323.747 I ggml_metal_init: simdgroup reduction   = true
0.00.323.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.323.747 I ggml_metal_init: has residency sets    = true
0.00.323.747 I ggml_metal_init: has bfloat            = true
0.00.323.747 I ggml_metal_init: use bfloat            = true
0.00.323.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.323.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.333.393 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.336.680 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.336.682 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.336.704 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.343.408 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.343.410 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.343.410 I llama_init_from_model: graph nodes  = 154
0.00.343.410 I llama_init_from_model: graph splits = 2
0.00.343.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.343.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.992 I 
0.00.352.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.352.116 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.352.116 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.352.120 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.352.120 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.352.124 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.352.125 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.352.676 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.356.194 I llama_perf_context_print:        load time =     328.82 ms
0.00.356.195 I llama_perf_context_print: prompt eval time =       3.51 ms /    62 tokens (    0.06 ms per token, 17663.82 tokens per second)
0.00.356.196 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.356.196 I llama_perf_context_print:       total time =       4.20 ms /    63 tokens
0.00.356.433 I ggml_metal_free: deallocating

real	0m1.065s
user	0m0.324s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.156 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.374 I main: llama backend init
0.00.000.380 I main: load the model and apply lora adapter, if any
0.00.086.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.099.045 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.099.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.099.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.099.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.099.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.099.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.099.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.099.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.099.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.099.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.099.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.099.077 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.099.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.099.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.099.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.099.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.099.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.105.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.108.077 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.114.799 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.114.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.114.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.114.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.114.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.114.811 I llama_model_loader: - type  f32:  194 tensors
0.00.114.812 I llama_model_loader: - type  f16:   98 tensors
0.00.114.814 I print_info: file format = GGUF V3 (latest)
0.00.114.816 I print_info: file type   = all F32 (guessed)
0.00.114.820 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.132.891 I load: special tokens cache size = 25
0.00.142.630 I load: token to piece cache size = 0.2984 MB
0.00.142.635 I print_info: arch             = gptneox
0.00.142.635 I print_info: vocab_only       = 0
0.00.142.636 I print_info: n_ctx_train      = 2048
0.00.142.636 I print_info: n_embd           = 2048
0.00.142.638 I print_info: n_layer          = 24
0.00.142.645 I print_info: n_head           = 16
0.00.142.646 I print_info: n_head_kv        = 16
0.00.142.646 I print_info: n_rot            = 32
0.00.142.646 I print_info: n_swa            = 0
0.00.142.647 I print_info: n_embd_head_k    = 128
0.00.142.647 I print_info: n_embd_head_v    = 128
0.00.142.648 I print_info: n_gqa            = 1
0.00.142.650 I print_info: n_embd_k_gqa     = 2048
0.00.142.651 I print_info: n_embd_v_gqa     = 2048
0.00.142.652 I print_info: f_norm_eps       = 1.0e-05
0.00.142.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.142.653 I print_info: f_clamp_kqv      = 0.0e+00
0.00.142.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.142.653 I print_info: f_logit_scale    = 0.0e+00
0.00.142.654 I print_info: n_ff             = 8192
0.00.142.654 I print_info: n_expert         = 0
0.00.142.654 I print_info: n_expert_used    = 0
0.00.142.655 I print_info: causal attn      = 1
0.00.142.655 I print_info: pooling type     = 0
0.00.142.655 I print_info: rope type        = 2
0.00.142.655 I print_info: rope scaling     = linear
0.00.142.656 I print_info: freq_base_train  = 10000.0
0.00.142.656 I print_info: freq_scale_train = 1
0.00.142.656 I print_info: n_ctx_orig_yarn  = 2048
0.00.142.657 I print_info: rope_finetuned   = unknown
0.00.142.657 I print_info: ssm_d_conv       = 0
0.00.142.657 I print_info: ssm_d_inner      = 0
0.00.142.657 I print_info: ssm_d_state      = 0
0.00.142.657 I print_info: ssm_dt_rank      = 0
0.00.142.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.142.658 I print_info: model type       = 1.4B
0.00.142.658 I print_info: model params     = 1.41 B
0.00.142.658 I print_info: general.name     = 1.4B
0.00.142.659 I print_info: vocab type       = BPE
0.00.142.659 I print_info: n_vocab          = 50304
0.00.142.659 I print_info: n_merges         = 50009
0.00.142.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.142.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.142.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.142.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.142.661 I print_info: LF token         = 187 ''
0.00.142.661 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.142.662 I print_info: max token length = 1024
0.00.142.668 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.186.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.186.626 I load_tensors: offloading output layer to GPU
0.00.186.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.186.651 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.186.652 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.187.044 I llama_init_from_model: n_seq_max     = 1
0.00.187.045 I llama_init_from_model: n_ctx         = 2048
0.00.187.045 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.187.045 I llama_init_from_model: n_batch       = 2048
0.00.187.045 I llama_init_from_model: n_ubatch      = 512
0.00.187.046 I llama_init_from_model: flash_attn    = 0
0.00.187.047 I llama_init_from_model: freq_base     = 10000.0
0.00.187.047 I llama_init_from_model: freq_scale    = 1
0.00.187.048 I ggml_metal_init: allocating
0.00.187.068 I ggml_metal_init: found device: Apple M4
0.00.187.074 I ggml_metal_init: picking default device: Apple M4
0.00.187.721 I ggml_metal_init: using embedded metal library
0.00.247.046 I ggml_metal_init: GPU name:   Apple M4
0.00.247.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.247.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.247.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.247.054 I ggml_metal_init: simdgroup reduction   = true
0.00.247.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.247.054 I ggml_metal_init: has residency sets    = true
0.00.247.054 I ggml_metal_init: has bfloat            = true
0.00.247.054 I ggml_metal_init: use bfloat            = true
0.00.247.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.247.058 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.724 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.193 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.434.199 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.434.245 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.508 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.438.510 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.438.511 I llama_init_from_model: graph nodes  = 967
0.00.438.511 I llama_init_from_model: graph splits = 2
0.00.438.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.438.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.438.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.344 I main: llama threadpool init, n_threads = 4
0.00.510.382 I 
0.00.510.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.399 I 
0.00.510.452 I sampler seed: 1234
0.00.510.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.487 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.489 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.489 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.349.025 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.02.349.026 I llama_perf_context_print:        load time =     422.46 ms
0.02.349.026 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.97 tokens per second)
0.02.349.027 I llama_perf_context_print:        eval time =    1791.51 ms /    63 runs   (   28.44 ms per token,    35.17 tokens per second)
0.02.349.027 I llama_perf_context_print:       total time =    1839.73 ms /    70 tokens
0.02.349.288 I ggml_metal_free: deallocating

real	0m2.666s
user	0m0.143s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.852 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.724 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.616 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.621 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.256 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.420 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.422 I llama_model_loader: - type  f32:  194 tensors
0.00.051.422 I llama_model_loader: - type  f16:   98 tensors
0.00.051.423 I print_info: file format = GGUF V3 (latest)
0.00.051.423 I print_info: file type   = all F32 (guessed)
0.00.051.429 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.230 I load: special tokens cache size = 25
0.00.070.939 I load: token to piece cache size = 0.2984 MB
0.00.070.943 I print_info: arch             = gptneox
0.00.070.943 I print_info: vocab_only       = 0
0.00.070.944 I print_info: n_ctx_train      = 2048
0.00.070.944 I print_info: n_embd           = 2048
0.00.070.944 I print_info: n_layer          = 24
0.00.070.947 I print_info: n_head           = 16
0.00.070.948 I print_info: n_head_kv        = 16
0.00.070.949 I print_info: n_rot            = 32
0.00.070.949 I print_info: n_swa            = 0
0.00.070.949 I print_info: n_embd_head_k    = 128
0.00.070.949 I print_info: n_embd_head_v    = 128
0.00.070.950 I print_info: n_gqa            = 1
0.00.070.950 I print_info: n_embd_k_gqa     = 2048
0.00.070.951 I print_info: n_embd_v_gqa     = 2048
0.00.070.952 I print_info: f_norm_eps       = 1.0e-05
0.00.070.952 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.954 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.955 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.955 I print_info: f_logit_scale    = 0.0e+00
0.00.070.955 I print_info: n_ff             = 8192
0.00.070.956 I print_info: n_expert         = 0
0.00.070.957 I print_info: n_expert_used    = 0
0.00.070.957 I print_info: causal attn      = 1
0.00.070.957 I print_info: pooling type     = 0
0.00.070.957 I print_info: rope type        = 2
0.00.070.958 I print_info: rope scaling     = linear
0.00.070.958 I print_info: freq_base_train  = 10000.0
0.00.070.958 I print_info: freq_scale_train = 1
0.00.070.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.959 I print_info: rope_finetuned   = unknown
0.00.070.959 I print_info: ssm_d_conv       = 0
0.00.070.959 I print_info: ssm_d_inner      = 0
0.00.070.959 I print_info: ssm_d_state      = 0
0.00.070.959 I print_info: ssm_dt_rank      = 0
0.00.070.959 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.963 I print_info: model type       = 1.4B
0.00.070.964 I print_info: model params     = 1.41 B
0.00.070.964 I print_info: general.name     = 1.4B
0.00.070.965 I print_info: vocab type       = BPE
0.00.070.965 I print_info: n_vocab          = 50304
0.00.070.965 I print_info: n_merges         = 50009
0.00.070.965 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.966 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.967 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.967 I print_info: LF token         = 187 ''
0.00.070.968 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.968 I print_info: max token length = 1024
0.00.070.968 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.039.104 I load_tensors: offloading 24 repeating layers to GPU
0.01.039.108 I load_tensors: offloading output layer to GPU
0.01.039.108 I load_tensors: offloaded 25/25 layers to GPU
0.01.039.134 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.039.136 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.040.059 I llama_init_from_model: n_seq_max     = 1
0.01.040.060 I llama_init_from_model: n_ctx         = 128
0.01.040.060 I llama_init_from_model: n_ctx_per_seq = 128
0.01.040.060 I llama_init_from_model: n_batch       = 128
0.01.040.061 I llama_init_from_model: n_ubatch      = 128
0.01.040.061 I llama_init_from_model: flash_attn    = 0
0.01.040.062 I llama_init_from_model: freq_base     = 10000.0
0.01.040.062 I llama_init_from_model: freq_scale    = 1
0.01.040.062 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.040.066 I ggml_metal_init: allocating
0.01.040.114 I ggml_metal_init: found device: Apple M4
0.01.040.121 I ggml_metal_init: picking default device: Apple M4
0.01.041.169 I ggml_metal_init: using embedded metal library
0.01.045.192 I ggml_metal_init: GPU name:   Apple M4
0.01.045.194 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.045.194 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.045.195 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.045.195 I ggml_metal_init: simdgroup reduction   = true
0.01.045.195 I ggml_metal_init: simdgroup matrix mul. = true
0.01.045.196 I ggml_metal_init: has residency sets    = true
0.01.045.196 I ggml_metal_init: has bfloat            = true
0.01.045.196 I ggml_metal_init: use bfloat            = true
0.01.045.196 I ggml_metal_init: hasUnifiedMemory      = true
0.01.045.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.056.454 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.058.341 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.058.343 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.058.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.060.047 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.060.048 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.060.048 I llama_init_from_model: graph nodes  = 967
0.01.060.049 I llama_init_from_model: graph splits = 2
0.01.060.050 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.060.051 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.096.791 I 
0.01.096.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.096.828 I perplexity: tokenizing the input ..
0.01.102.263 I perplexity: tokenization took 5.433 ms
0.01.102.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.221.341 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.222.685 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.222.720 I llama_perf_context_print:        load time =    1077.05 ms
0.01.222.722 I llama_perf_context_print: prompt eval time =     118.72 ms /   128 tokens (    0.93 ms per token,  1078.21 tokens per second)
0.01.222.722 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.222.723 I llama_perf_context_print:       total time =     125.93 ms /   129 tokens
0.01.223.120 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.098s
sys	0m0.225s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.350 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.596 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.606 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.734 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.734 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.735 I llama_model_loader: - type  f32:  194 tensors
0.00.035.735 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.736 I print_info: file format = GGUF V3 (latest)
0.00.035.737 I print_info: file type   = Q8_0
0.00.035.738 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.161 I load: special tokens cache size = 25
0.00.050.074 I load: token to piece cache size = 0.2984 MB
0.00.050.079 I print_info: arch             = gptneox
0.00.050.079 I print_info: vocab_only       = 0
0.00.050.080 I print_info: n_ctx_train      = 2048
0.00.050.082 I print_info: n_embd           = 2048
0.00.050.082 I print_info: n_layer          = 24
0.00.050.089 I print_info: n_head           = 16
0.00.050.090 I print_info: n_head_kv        = 16
0.00.050.090 I print_info: n_rot            = 32
0.00.050.090 I print_info: n_swa            = 0
0.00.050.090 I print_info: n_embd_head_k    = 128
0.00.050.091 I print_info: n_embd_head_v    = 128
0.00.050.093 I print_info: n_gqa            = 1
0.00.050.094 I print_info: n_embd_k_gqa     = 2048
0.00.050.094 I print_info: n_embd_v_gqa     = 2048
0.00.050.095 I print_info: f_norm_eps       = 1.0e-05
0.00.050.095 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.095 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.095 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.096 I print_info: f_logit_scale    = 0.0e+00
0.00.050.097 I print_info: n_ff             = 8192
0.00.050.097 I print_info: n_expert         = 0
0.00.050.097 I print_info: n_expert_used    = 0
0.00.050.098 I print_info: causal attn      = 1
0.00.050.099 I print_info: pooling type     = 0
0.00.050.099 I print_info: rope type        = 2
0.00.050.099 I print_info: rope scaling     = linear
0.00.050.100 I print_info: freq_base_train  = 10000.0
0.00.050.100 I print_info: freq_scale_train = 1
0.00.050.100 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.100 I print_info: rope_finetuned   = unknown
0.00.050.101 I print_info: ssm_d_conv       = 0
0.00.050.101 I print_info: ssm_d_inner      = 0
0.00.050.101 I print_info: ssm_d_state      = 0
0.00.050.101 I print_info: ssm_dt_rank      = 0
0.00.050.101 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.101 I print_info: model type       = 1.4B
0.00.050.101 I print_info: model params     = 1.41 B
0.00.050.102 I print_info: general.name     = 1.4B
0.00.050.102 I print_info: vocab type       = BPE
0.00.050.103 I print_info: n_vocab          = 50304
0.00.050.103 I print_info: n_merges         = 50009
0.00.050.103 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.103 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.103 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.103 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.104 I print_info: LF token         = 187 ''
0.00.050.104 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.104 I print_info: max token length = 1024
0.00.050.104 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.02.574.079 I load_tensors: offloading 24 repeating layers to GPU
0.02.574.085 I load_tensors: offloading output layer to GPU
0.02.574.086 I load_tensors: offloaded 25/25 layers to GPU
0.02.574.110 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.02.574.112 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.02.574.873 I llama_init_from_model: n_seq_max     = 1
0.02.574.875 I llama_init_from_model: n_ctx         = 2048
0.02.574.875 I llama_init_from_model: n_ctx_per_seq = 2048
0.02.574.876 I llama_init_from_model: n_batch       = 2048
0.02.574.876 I llama_init_from_model: n_ubatch      = 512
0.02.574.876 I llama_init_from_model: flash_attn    = 0
0.02.574.877 I llama_init_from_model: freq_base     = 10000.0
0.02.574.877 I llama_init_from_model: freq_scale    = 1
0.02.574.879 I ggml_metal_init: allocating
0.02.574.891 I ggml_metal_init: found device: Apple M4
0.02.574.898 I ggml_metal_init: picking default device: Apple M4
0.02.576.194 I ggml_metal_init: using embedded metal library
0.02.581.656 I ggml_metal_init: GPU name:   Apple M4
0.02.581.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.02.581.660 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.02.581.660 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.02.581.661 I ggml_metal_init: simdgroup reduction   = true
0.02.581.661 I ggml_metal_init: simdgroup matrix mul. = true
0.02.581.661 I ggml_metal_init: has residency sets    = true
0.02.581.662 I ggml_metal_init: has bfloat            = true
0.02.581.662 I ggml_metal_init: use bfloat            = true
0.02.581.663 I ggml_metal_init: hasUnifiedMemory      = true
0.02.581.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.02.597.393 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.02.648.247 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.02.648.254 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.02.648.302 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.02.652.908 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.02.652.911 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.02.652.911 I llama_init_from_model: graph nodes  = 967
0.02.652.911 I llama_init_from_model: graph splits = 2
0.02.652.916 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.02.653.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.02.653.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.02.707.710 I main: llama threadpool init, n_threads = 4
0.02.707.753 I 
0.02.707.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.02.707.769 I 
0.02.707.937 I sampler seed: 1234
0.02.707.942 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.02.707.953 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.02.707.953 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.02.707.953 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.03.812.793 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.03.812.794 I llama_perf_context_print:        load time =    2696.65 ms
0.03.812.795 I llama_perf_context_print: prompt eval time =      48.93 ms /     7 tokens (    6.99 ms per token,   143.07 tokens per second)
0.03.812.795 I llama_perf_context_print:        eval time =    1053.13 ms /    63 runs   (   16.72 ms per token,    59.82 tokens per second)
0.03.812.796 I llama_perf_context_print:       total time =    1105.78 ms /    70 tokens
0.03.813.101 I ggml_metal_free: deallocating

real	0m3.833s
user	0m0.108s
sys	0m0.261s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.777 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.481 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.405 I llama_model_loader: - type  f32:  194 tensors
0.00.026.405 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.406 I print_info: file format = GGUF V3 (latest)
0.00.026.407 I print_info: file type   = Q8_0
0.00.026.408 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.749 I load: special tokens cache size = 25
0.00.040.843 I load: token to piece cache size = 0.2984 MB
0.00.040.848 I print_info: arch             = gptneox
0.00.040.848 I print_info: vocab_only       = 0
0.00.040.848 I print_info: n_ctx_train      = 2048
0.00.040.849 I print_info: n_embd           = 2048
0.00.040.849 I print_info: n_layer          = 24
0.00.040.854 I print_info: n_head           = 16
0.00.040.855 I print_info: n_head_kv        = 16
0.00.040.855 I print_info: n_rot            = 32
0.00.040.855 I print_info: n_swa            = 0
0.00.040.855 I print_info: n_embd_head_k    = 128
0.00.040.855 I print_info: n_embd_head_v    = 128
0.00.040.859 I print_info: n_gqa            = 1
0.00.040.860 I print_info: n_embd_k_gqa     = 2048
0.00.040.861 I print_info: n_embd_v_gqa     = 2048
0.00.040.861 I print_info: f_norm_eps       = 1.0e-05
0.00.040.861 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.862 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.862 I print_info: f_logit_scale    = 0.0e+00
0.00.040.862 I print_info: n_ff             = 8192
0.00.040.863 I print_info: n_expert         = 0
0.00.040.863 I print_info: n_expert_used    = 0
0.00.040.863 I print_info: causal attn      = 1
0.00.040.866 I print_info: pooling type     = 0
0.00.040.867 I print_info: rope type        = 2
0.00.040.867 I print_info: rope scaling     = linear
0.00.040.867 I print_info: freq_base_train  = 10000.0
0.00.040.867 I print_info: freq_scale_train = 1
0.00.040.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.868 I print_info: rope_finetuned   = unknown
0.00.040.868 I print_info: ssm_d_conv       = 0
0.00.040.868 I print_info: ssm_d_inner      = 0
0.00.040.868 I print_info: ssm_d_state      = 0
0.00.040.868 I print_info: ssm_dt_rank      = 0
0.00.040.868 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.870 I print_info: model type       = 1.4B
0.00.040.870 I print_info: model params     = 1.41 B
0.00.040.870 I print_info: general.name     = 1.4B
0.00.040.871 I print_info: vocab type       = BPE
0.00.040.871 I print_info: n_vocab          = 50304
0.00.040.871 I print_info: n_merges         = 50009
0.00.040.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.872 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.872 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.872 I print_info: LF token         = 187 ''
0.00.040.872 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: max token length = 1024
0.00.040.873 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.929.206 I load_tensors: offloading 24 repeating layers to GPU
0.00.929.214 I load_tensors: offloading output layer to GPU
0.00.929.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.929.246 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.929.249 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.930.436 I llama_init_from_model: n_seq_max     = 1
0.00.930.438 I llama_init_from_model: n_ctx         = 128
0.00.930.438 I llama_init_from_model: n_ctx_per_seq = 128
0.00.930.439 I llama_init_from_model: n_batch       = 128
0.00.930.439 I llama_init_from_model: n_ubatch      = 128
0.00.930.439 I llama_init_from_model: flash_attn    = 0
0.00.930.440 I llama_init_from_model: freq_base     = 10000.0
0.00.930.441 I llama_init_from_model: freq_scale    = 1
0.00.930.441 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.930.443 I ggml_metal_init: allocating
0.00.930.532 I ggml_metal_init: found device: Apple M4
0.00.930.544 I ggml_metal_init: picking default device: Apple M4
0.00.931.929 I ggml_metal_init: using embedded metal library
0.00.937.496 I ggml_metal_init: GPU name:   Apple M4
0.00.937.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.937.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.937.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.937.501 I ggml_metal_init: simdgroup reduction   = true
0.00.937.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.937.502 I ggml_metal_init: has residency sets    = true
0.00.937.502 I ggml_metal_init: has bfloat            = true
0.00.937.502 I ggml_metal_init: use bfloat            = true
0.00.937.503 I ggml_metal_init: hasUnifiedMemory      = true
0.00.937.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.953.443 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.956.685 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.956.689 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.956.724 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.959.740 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.959.741 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.959.742 I llama_init_from_model: graph nodes  = 967
0.00.959.742 I llama_init_from_model: graph splits = 2
0.00.959.745 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.959.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.985.887 I 
0.00.985.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.985.962 I perplexity: tokenizing the input ..
0.00.992.865 I perplexity: tokenization took 6.898 ms
0.00.992.872 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.130.507 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.131.858 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.131.890 I llama_perf_context_print:        load time =     976.10 ms
0.01.131.893 I llama_perf_context_print: prompt eval time =     136.68 ms /   128 tokens (    1.07 ms per token,   936.50 tokens per second)
0.01.131.894 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.131.894 I llama_perf_context_print:       total time =     146.01 ms /   129 tokens
0.01.132.257 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.078s
sys	0m0.180s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.016.122 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.173 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.187 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.188 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.188 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.189 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.127 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.030 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.030 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.031 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.031 I llama_model_loader: - type  f32:  194 tensors
0.00.043.032 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.032 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.033 I print_info: file format = GGUF V3 (latest)
0.00.043.033 I print_info: file type   = Q4_0
0.00.043.035 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.798 I load: special tokens cache size = 25
0.00.057.081 I load: token to piece cache size = 0.2984 MB
0.00.057.085 I print_info: arch             = gptneox
0.00.057.085 I print_info: vocab_only       = 0
0.00.057.085 I print_info: n_ctx_train      = 2048
0.00.057.086 I print_info: n_embd           = 2048
0.00.057.086 I print_info: n_layer          = 24
0.00.057.089 I print_info: n_head           = 16
0.00.057.089 I print_info: n_head_kv        = 16
0.00.057.090 I print_info: n_rot            = 32
0.00.057.090 I print_info: n_swa            = 0
0.00.057.090 I print_info: n_embd_head_k    = 128
0.00.057.090 I print_info: n_embd_head_v    = 128
0.00.057.091 I print_info: n_gqa            = 1
0.00.057.092 I print_info: n_embd_k_gqa     = 2048
0.00.057.093 I print_info: n_embd_v_gqa     = 2048
0.00.057.093 I print_info: f_norm_eps       = 1.0e-05
0.00.057.093 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.094 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.094 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.094 I print_info: f_logit_scale    = 0.0e+00
0.00.057.095 I print_info: n_ff             = 8192
0.00.057.095 I print_info: n_expert         = 0
0.00.057.095 I print_info: n_expert_used    = 0
0.00.057.095 I print_info: causal attn      = 1
0.00.057.095 I print_info: pooling type     = 0
0.00.057.095 I print_info: rope type        = 2
0.00.057.095 I print_info: rope scaling     = linear
0.00.057.099 I print_info: freq_base_train  = 10000.0
0.00.057.099 I print_info: freq_scale_train = 1
0.00.057.099 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.099 I print_info: rope_finetuned   = unknown
0.00.057.099 I print_info: ssm_d_conv       = 0
0.00.057.100 I print_info: ssm_d_inner      = 0
0.00.057.100 I print_info: ssm_d_state      = 0
0.00.057.100 I print_info: ssm_dt_rank      = 0
0.00.057.100 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.100 I print_info: model type       = 1.4B
0.00.057.100 I print_info: model params     = 1.41 B
0.00.057.101 I print_info: general.name     = 1.4B
0.00.057.101 I print_info: vocab type       = BPE
0.00.057.101 I print_info: n_vocab          = 50304
0.00.057.101 I print_info: n_merges         = 50009
0.00.057.102 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.102 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.102 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.102 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.102 I print_info: LF token         = 187 ''
0.00.057.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.123 I print_info: max token length = 1024
0.00.057.124 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.673.767 I load_tensors: offloading 24 repeating layers to GPU
0.01.673.772 I load_tensors: offloading output layer to GPU
0.01.673.773 I load_tensors: offloaded 25/25 layers to GPU
0.01.673.795 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.01.673.796 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.01.674.505 I llama_init_from_model: n_seq_max     = 1
0.01.674.507 I llama_init_from_model: n_ctx         = 2048
0.01.674.508 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.674.508 I llama_init_from_model: n_batch       = 2048
0.01.674.508 I llama_init_from_model: n_ubatch      = 512
0.01.674.509 I llama_init_from_model: flash_attn    = 0
0.01.674.510 I llama_init_from_model: freq_base     = 10000.0
0.01.674.510 I llama_init_from_model: freq_scale    = 1
0.01.674.511 I ggml_metal_init: allocating
0.01.674.546 I ggml_metal_init: found device: Apple M4
0.01.674.557 I ggml_metal_init: picking default device: Apple M4
0.01.675.582 I ggml_metal_init: using embedded metal library
0.01.688.223 I ggml_metal_init: GPU name:   Apple M4
0.01.688.231 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.688.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.688.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.688.233 I ggml_metal_init: simdgroup reduction   = true
0.01.688.233 I ggml_metal_init: simdgroup matrix mul. = true
0.01.688.234 I ggml_metal_init: has residency sets    = true
0.01.688.234 I ggml_metal_init: has bfloat            = true
0.01.688.234 I ggml_metal_init: use bfloat            = true
0.01.688.236 I ggml_metal_init: hasUnifiedMemory      = true
0.01.688.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.706.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.738.855 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.738.865 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.738.906 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.743.753 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.743.756 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.743.756 I llama_init_from_model: graph nodes  = 967
0.01.743.756 I llama_init_from_model: graph splits = 2
0.01.743.760 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.743.873 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.743.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.799.824 I main: llama threadpool init, n_threads = 4
0.01.799.881 I 
0.01.799.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.799.899 I 
0.01.800.075 I sampler seed: 1234
0.01.800.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.800.100 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.800.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.800.100 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.02.489.440 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.02.489.440 I llama_perf_context_print:        load time =    1783.01 ms
0.02.489.441 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   142.99 tokens per second)
0.02.489.442 I llama_perf_context_print:        eval time =     637.53 ms /    63 runs   (   10.12 ms per token,    98.82 tokens per second)
0.02.489.442 I llama_perf_context_print:       total time =     690.30 ms /    70 tokens
0.02.489.667 I ggml_metal_free: deallocating

real	0m2.529s
user	0m0.108s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.392 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.874 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.711 I llama_model_loader: - type  f32:  194 tensors
0.00.026.711 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.712 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.713 I print_info: file format = GGUF V3 (latest)
0.00.026.713 I print_info: file type   = Q4_0
0.00.026.714 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.094 I load: special tokens cache size = 25
0.00.041.016 I load: token to piece cache size = 0.2984 MB
0.00.041.021 I print_info: arch             = gptneox
0.00.041.021 I print_info: vocab_only       = 0
0.00.041.021 I print_info: n_ctx_train      = 2048
0.00.041.021 I print_info: n_embd           = 2048
0.00.041.022 I print_info: n_layer          = 24
0.00.041.026 I print_info: n_head           = 16
0.00.041.027 I print_info: n_head_kv        = 16
0.00.041.027 I print_info: n_rot            = 32
0.00.041.027 I print_info: n_swa            = 0
0.00.041.027 I print_info: n_embd_head_k    = 128
0.00.041.027 I print_info: n_embd_head_v    = 128
0.00.041.028 I print_info: n_gqa            = 1
0.00.041.030 I print_info: n_embd_k_gqa     = 2048
0.00.041.031 I print_info: n_embd_v_gqa     = 2048
0.00.041.032 I print_info: f_norm_eps       = 1.0e-05
0.00.041.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.032 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.032 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.032 I print_info: f_logit_scale    = 0.0e+00
0.00.041.033 I print_info: n_ff             = 8192
0.00.041.033 I print_info: n_expert         = 0
0.00.041.033 I print_info: n_expert_used    = 0
0.00.041.033 I print_info: causal attn      = 1
0.00.041.034 I print_info: pooling type     = 0
0.00.041.034 I print_info: rope type        = 2
0.00.041.036 I print_info: rope scaling     = linear
0.00.041.036 I print_info: freq_base_train  = 10000.0
0.00.041.037 I print_info: freq_scale_train = 1
0.00.041.037 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.037 I print_info: rope_finetuned   = unknown
0.00.041.038 I print_info: ssm_d_conv       = 0
0.00.041.038 I print_info: ssm_d_inner      = 0
0.00.041.040 I print_info: ssm_d_state      = 0
0.00.041.040 I print_info: ssm_dt_rank      = 0
0.00.041.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.040 I print_info: model type       = 1.4B
0.00.041.040 I print_info: model params     = 1.41 B
0.00.041.040 I print_info: general.name     = 1.4B
0.00.041.041 I print_info: vocab type       = BPE
0.00.041.041 I print_info: n_vocab          = 50304
0.00.041.041 I print_info: n_merges         = 50009
0.00.041.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.042 I print_info: LF token         = 187 ''
0.00.041.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.042 I print_info: max token length = 1024
0.00.041.043 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.332 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.351 I load_tensors: offloading output layer to GPU
0.00.617.352 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.396 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.617.398 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.618.875 I llama_init_from_model: n_seq_max     = 1
0.00.618.879 I llama_init_from_model: n_ctx         = 128
0.00.618.880 I llama_init_from_model: n_ctx_per_seq = 128
0.00.618.880 I llama_init_from_model: n_batch       = 128
0.00.618.880 I llama_init_from_model: n_ubatch      = 128
0.00.618.881 I llama_init_from_model: flash_attn    = 0
0.00.618.883 I llama_init_from_model: freq_base     = 10000.0
0.00.618.883 I llama_init_from_model: freq_scale    = 1
0.00.618.884 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.618.886 I ggml_metal_init: allocating
0.00.618.992 I ggml_metal_init: found device: Apple M4
0.00.619.007 I ggml_metal_init: picking default device: Apple M4
0.00.621.071 I ggml_metal_init: using embedded metal library
0.00.626.908 I ggml_metal_init: GPU name:   Apple M4
0.00.626.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.927 I ggml_metal_init: simdgroup reduction   = true
0.00.626.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.928 I ggml_metal_init: has residency sets    = true
0.00.626.928 I ggml_metal_init: has bfloat            = true
0.00.626.928 I ggml_metal_init: use bfloat            = true
0.00.626.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.299 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.650.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.650.968 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.012 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.654.309 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.654.311 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.654.311 I llama_init_from_model: graph nodes  = 967
0.00.654.312 I llama_init_from_model: graph splits = 2
0.00.654.316 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.741 I 
0.00.681.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.805 I perplexity: tokenizing the input ..
0.00.688.155 I perplexity: tokenization took 6.348 ms
0.00.688.160 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.485 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.821.824 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.821.848 I llama_perf_context_print:        load time =     671.34 ms
0.00.821.849 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.51 tokens per second)
0.00.821.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.851 I llama_perf_context_print:       total time =     140.11 ms /   129 tokens
0.00.822.259 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.080s
sys	0m0.129s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.101 I main: llama backend init
0.00.000.103 I main: load the model and apply lora adapter, if any
0.00.015.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.039.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.413 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.057.415 I llama_model_loader: - type  f32:  194 tensors
0.00.057.419 I llama_model_loader: - type q4_1:   97 tensors
0.00.057.420 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.422 I print_info: file format = GGUF V3 (latest)
0.00.057.423 I print_info: file type   = Q4_1
0.00.057.427 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.077.244 I load: special tokens cache size = 25
0.00.088.621 I load: token to piece cache size = 0.2984 MB
0.00.088.626 I print_info: arch             = gptneox
0.00.088.626 I print_info: vocab_only       = 0
0.00.088.627 I print_info: n_ctx_train      = 2048
0.00.088.627 I print_info: n_embd           = 2048
0.00.088.627 I print_info: n_layer          = 24
0.00.088.631 I print_info: n_head           = 16
0.00.088.632 I print_info: n_head_kv        = 16
0.00.088.632 I print_info: n_rot            = 32
0.00.088.633 I print_info: n_swa            = 0
0.00.088.633 I print_info: n_embd_head_k    = 128
0.00.088.633 I print_info: n_embd_head_v    = 128
0.00.088.634 I print_info: n_gqa            = 1
0.00.088.635 I print_info: n_embd_k_gqa     = 2048
0.00.088.636 I print_info: n_embd_v_gqa     = 2048
0.00.088.637 I print_info: f_norm_eps       = 1.0e-05
0.00.088.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.640 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.642 I print_info: f_logit_scale    = 0.0e+00
0.00.088.643 I print_info: n_ff             = 8192
0.00.088.643 I print_info: n_expert         = 0
0.00.088.644 I print_info: n_expert_used    = 0
0.00.088.644 I print_info: causal attn      = 1
0.00.088.644 I print_info: pooling type     = 0
0.00.088.644 I print_info: rope type        = 2
0.00.088.645 I print_info: rope scaling     = linear
0.00.088.645 I print_info: freq_base_train  = 10000.0
0.00.088.647 I print_info: freq_scale_train = 1
0.00.088.648 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.648 I print_info: rope_finetuned   = unknown
0.00.088.648 I print_info: ssm_d_conv       = 0
0.00.088.648 I print_info: ssm_d_inner      = 0
0.00.088.648 I print_info: ssm_d_state      = 0
0.00.088.648 I print_info: ssm_dt_rank      = 0
0.00.088.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.649 I print_info: model type       = 1.4B
0.00.088.650 I print_info: model params     = 1.41 B
0.00.088.650 I print_info: general.name     = 1.4B
0.00.088.651 I print_info: vocab type       = BPE
0.00.088.651 I print_info: n_vocab          = 50304
0.00.088.651 I print_info: n_merges         = 50009
0.00.088.652 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.652 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.652 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.653 I print_info: LF token         = 187 ''
0.00.088.653 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.653 I print_info: max token length = 1024
0.00.088.654 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.700.015 I load_tensors: offloading 24 repeating layers to GPU
0.01.700.033 I load_tensors: offloading output layer to GPU
0.01.700.034 I load_tensors: offloaded 25/25 layers to GPU
0.01.700.068 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.01.700.069 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.01.701.627 I llama_init_from_model: n_seq_max     = 1
0.01.701.631 I llama_init_from_model: n_ctx         = 2048
0.01.701.631 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.701.632 I llama_init_from_model: n_batch       = 2048
0.01.701.632 I llama_init_from_model: n_ubatch      = 512
0.01.701.633 I llama_init_from_model: flash_attn    = 0
0.01.701.635 I llama_init_from_model: freq_base     = 10000.0
0.01.701.636 I llama_init_from_model: freq_scale    = 1
0.01.701.638 I ggml_metal_init: allocating
0.01.701.705 I ggml_metal_init: found device: Apple M4
0.01.701.718 I ggml_metal_init: picking default device: Apple M4
0.01.703.574 I ggml_metal_init: using embedded metal library
0.01.708.944 I ggml_metal_init: GPU name:   Apple M4
0.01.708.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.708.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.708.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.708.964 I ggml_metal_init: simdgroup reduction   = true
0.01.708.964 I ggml_metal_init: simdgroup matrix mul. = true
0.01.708.964 I ggml_metal_init: has residency sets    = true
0.01.708.965 I ggml_metal_init: has bfloat            = true
0.01.708.965 I ggml_metal_init: use bfloat            = true
0.01.708.967 I ggml_metal_init: hasUnifiedMemory      = true
0.01.708.971 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.729.723 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.784.204 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.784.208 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.784.245 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.788.444 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.788.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.788.445 I llama_init_from_model: graph nodes  = 967
0.01.788.446 I llama_init_from_model: graph splits = 2
0.01.788.450 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.788.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.788.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.843.537 I main: llama threadpool init, n_threads = 4
0.01.843.583 I 
0.01.843.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.843.597 I 
0.01.843.706 I sampler seed: 1234
0.01.843.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.843.722 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.843.722 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.843.724 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.02.579.791 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.02.579.792 I llama_perf_context_print:        load time =    1826.93 ms
0.02.579.792 I llama_perf_context_print: prompt eval time =      48.97 ms /     7 tokens (    7.00 ms per token,   142.93 tokens per second)
0.02.579.794 I llama_perf_context_print:        eval time =     684.47 ms /    63 runs   (   10.86 ms per token,    92.04 tokens per second)
0.02.579.795 I llama_perf_context_print:       total time =     736.93 ms /    70 tokens
0.02.580.039 I ggml_metal_free: deallocating

real	0m2.629s
user	0m0.140s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.506 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.201 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.019 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.895 I llama_model_loader: - type  f32:  194 tensors
0.00.025.895 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.895 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.896 I print_info: file format = GGUF V3 (latest)
0.00.025.901 I print_info: file type   = Q4_1
0.00.025.903 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.875 I load: special tokens cache size = 25
0.00.039.992 I load: token to piece cache size = 0.2984 MB
0.00.039.997 I print_info: arch             = gptneox
0.00.039.997 I print_info: vocab_only       = 0
0.00.039.997 I print_info: n_ctx_train      = 2048
0.00.039.997 I print_info: n_embd           = 2048
0.00.039.998 I print_info: n_layer          = 24
0.00.040.002 I print_info: n_head           = 16
0.00.040.003 I print_info: n_head_kv        = 16
0.00.040.003 I print_info: n_rot            = 32
0.00.040.003 I print_info: n_swa            = 0
0.00.040.003 I print_info: n_embd_head_k    = 128
0.00.040.003 I print_info: n_embd_head_v    = 128
0.00.040.007 I print_info: n_gqa            = 1
0.00.040.008 I print_info: n_embd_k_gqa     = 2048
0.00.040.008 I print_info: n_embd_v_gqa     = 2048
0.00.040.009 I print_info: f_norm_eps       = 1.0e-05
0.00.040.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.011 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.011 I print_info: f_logit_scale    = 0.0e+00
0.00.040.011 I print_info: n_ff             = 8192
0.00.040.011 I print_info: n_expert         = 0
0.00.040.012 I print_info: n_expert_used    = 0
0.00.040.012 I print_info: causal attn      = 1
0.00.040.015 I print_info: pooling type     = 0
0.00.040.015 I print_info: rope type        = 2
0.00.040.016 I print_info: rope scaling     = linear
0.00.040.016 I print_info: freq_base_train  = 10000.0
0.00.040.016 I print_info: freq_scale_train = 1
0.00.040.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.017 I print_info: rope_finetuned   = unknown
0.00.040.018 I print_info: ssm_d_conv       = 0
0.00.040.018 I print_info: ssm_d_inner      = 0
0.00.040.018 I print_info: ssm_d_state      = 0
0.00.040.018 I print_info: ssm_dt_rank      = 0
0.00.040.018 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.019 I print_info: model type       = 1.4B
0.00.040.019 I print_info: model params     = 1.41 B
0.00.040.019 I print_info: general.name     = 1.4B
0.00.040.020 I print_info: vocab type       = BPE
0.00.040.020 I print_info: n_vocab          = 50304
0.00.040.020 I print_info: n_merges         = 50009
0.00.040.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.022 I print_info: LF token         = 187 ''
0.00.040.022 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.024 I print_info: max token length = 1024
0.00.040.025 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.530 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.557 I load_tensors: offloading output layer to GPU
0.00.711.557 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.596 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.711.598 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.713.042 I llama_init_from_model: n_seq_max     = 1
0.00.713.047 I llama_init_from_model: n_ctx         = 128
0.00.713.048 I llama_init_from_model: n_ctx_per_seq = 128
0.00.713.049 I llama_init_from_model: n_batch       = 128
0.00.713.049 I llama_init_from_model: n_ubatch      = 128
0.00.713.049 I llama_init_from_model: flash_attn    = 0
0.00.713.052 I llama_init_from_model: freq_base     = 10000.0
0.00.713.052 I llama_init_from_model: freq_scale    = 1
0.00.713.053 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.713.056 I ggml_metal_init: allocating
0.00.713.146 I ggml_metal_init: found device: Apple M4
0.00.713.162 I ggml_metal_init: picking default device: Apple M4
0.00.715.094 I ggml_metal_init: using embedded metal library
0.00.721.859 I ggml_metal_init: GPU name:   Apple M4
0.00.721.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.871 I ggml_metal_init: simdgroup reduction   = true
0.00.721.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.872 I ggml_metal_init: has residency sets    = true
0.00.721.872 I ggml_metal_init: has bfloat            = true
0.00.721.873 I ggml_metal_init: use bfloat            = true
0.00.721.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.666 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.744.430 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.744.480 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.679 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.747.681 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.747.682 I llama_init_from_model: graph nodes  = 967
0.00.747.682 I llama_init_from_model: graph splits = 2
0.00.747.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.747.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.510 I 
0.00.770.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.558 I perplexity: tokenizing the input ..
0.00.777.356 I perplexity: tokenization took 6.793 ms
0.00.777.365 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.792 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.902.124 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.902.146 I llama_perf_context_print:        load time =     760.99 ms
0.00.902.147 I llama_perf_context_print: prompt eval time =     122.47 ms /   128 tokens (    0.96 ms per token,  1045.17 tokens per second)
0.00.902.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.902.148 I llama_perf_context_print:       total time =     131.64 ms /   129 tokens
0.00.902.586 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.080s
sys	0m0.144s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.778 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.797 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.800 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.751 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.038.054 I llama_model_loader: - type  f32:  194 tensors
0.00.038.054 I llama_model_loader: - type q5_0:   97 tensors
0.00.038.054 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.055 I print_info: file format = GGUF V3 (latest)
0.00.038.055 I print_info: file type   = Q5_0
0.00.038.056 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.846 I load: special tokens cache size = 25
0.00.054.203 I load: token to piece cache size = 0.2984 MB
0.00.054.206 I print_info: arch             = gptneox
0.00.054.206 I print_info: vocab_only       = 0
0.00.054.206 I print_info: n_ctx_train      = 2048
0.00.054.206 I print_info: n_embd           = 2048
0.00.054.207 I print_info: n_layer          = 24
0.00.054.210 I print_info: n_head           = 16
0.00.054.210 I print_info: n_head_kv        = 16
0.00.054.211 I print_info: n_rot            = 32
0.00.054.211 I print_info: n_swa            = 0
0.00.054.211 I print_info: n_embd_head_k    = 128
0.00.054.211 I print_info: n_embd_head_v    = 128
0.00.054.212 I print_info: n_gqa            = 1
0.00.054.213 I print_info: n_embd_k_gqa     = 2048
0.00.054.213 I print_info: n_embd_v_gqa     = 2048
0.00.054.214 I print_info: f_norm_eps       = 1.0e-05
0.00.054.214 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.214 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.215 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.215 I print_info: f_logit_scale    = 0.0e+00
0.00.054.215 I print_info: n_ff             = 8192
0.00.054.216 I print_info: n_expert         = 0
0.00.054.216 I print_info: n_expert_used    = 0
0.00.054.216 I print_info: causal attn      = 1
0.00.054.216 I print_info: pooling type     = 0
0.00.054.216 I print_info: rope type        = 2
0.00.054.216 I print_info: rope scaling     = linear
0.00.054.217 I print_info: freq_base_train  = 10000.0
0.00.054.219 I print_info: freq_scale_train = 1
0.00.054.219 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.219 I print_info: rope_finetuned   = unknown
0.00.054.220 I print_info: ssm_d_conv       = 0
0.00.054.221 I print_info: ssm_d_inner      = 0
0.00.054.221 I print_info: ssm_d_state      = 0
0.00.054.221 I print_info: ssm_dt_rank      = 0
0.00.054.221 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.222 I print_info: model type       = 1.4B
0.00.054.222 I print_info: model params     = 1.41 B
0.00.054.222 I print_info: general.name     = 1.4B
0.00.054.223 I print_info: vocab type       = BPE
0.00.054.223 I print_info: n_vocab          = 50304
0.00.054.223 I print_info: n_merges         = 50009
0.00.054.223 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.223 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.223 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.224 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.224 I print_info: LF token         = 187 ''
0.00.054.227 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.228 I print_info: max token length = 1024
0.00.054.228 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.538.894 I load_tensors: offloading 24 repeating layers to GPU
0.01.538.908 I load_tensors: offloading output layer to GPU
0.01.538.909 I load_tensors: offloaded 25/25 layers to GPU
0.01.538.947 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.01.538.948 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.01.540.240 I llama_init_from_model: n_seq_max     = 1
0.01.540.246 I llama_init_from_model: n_ctx         = 2048
0.01.540.247 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.540.248 I llama_init_from_model: n_batch       = 2048
0.01.540.248 I llama_init_from_model: n_ubatch      = 512
0.01.540.249 I llama_init_from_model: flash_attn    = 0
0.01.540.250 I llama_init_from_model: freq_base     = 10000.0
0.01.540.251 I llama_init_from_model: freq_scale    = 1
0.01.540.257 I ggml_metal_init: allocating
0.01.540.399 I ggml_metal_init: found device: Apple M4
0.01.540.414 I ggml_metal_init: picking default device: Apple M4
0.01.542.183 I ggml_metal_init: using embedded metal library
0.01.546.766 I ggml_metal_init: GPU name:   Apple M4
0.01.546.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.546.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.546.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.546.778 I ggml_metal_init: simdgroup reduction   = true
0.01.546.778 I ggml_metal_init: simdgroup matrix mul. = true
0.01.546.778 I ggml_metal_init: has residency sets    = true
0.01.546.779 I ggml_metal_init: has bfloat            = true
0.01.546.779 I ggml_metal_init: use bfloat            = true
0.01.546.780 I ggml_metal_init: hasUnifiedMemory      = true
0.01.546.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.558.783 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.590.344 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.590.350 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.590.384 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.594.300 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.594.303 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.594.303 I llama_init_from_model: graph nodes  = 967
0.01.594.303 I llama_init_from_model: graph splits = 2
0.01.594.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.594.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.594.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.650.433 I main: llama threadpool init, n_threads = 4
0.01.650.482 I 
0.01.650.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.650.498 I 
0.01.650.655 I sampler seed: 1234
0.01.650.660 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.650.671 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.650.671 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.650.671 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.445.292 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.02.445.293 I llama_perf_context_print:        load time =    1639.96 ms
0.02.445.294 I llama_perf_context_print: prompt eval time =      42.90 ms /     7 tokens (    6.13 ms per token,   163.17 tokens per second)
0.02.445.295 I llama_perf_context_print:        eval time =     748.80 ms /    63 runs   (   11.89 ms per token,    84.13 tokens per second)
0.02.445.295 I llama_perf_context_print:       total time =     795.55 ms /    70 tokens
0.02.445.534 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.105s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.625 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.279 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.291 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.293 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.294 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.091 I llama_model_loader: - type  f32:  194 tensors
0.00.027.092 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.092 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.093 I print_info: file format = GGUF V3 (latest)
0.00.027.093 I print_info: file type   = Q5_0
0.00.027.098 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.501 I load: special tokens cache size = 25
0.00.041.603 I load: token to piece cache size = 0.2984 MB
0.00.041.607 I print_info: arch             = gptneox
0.00.041.607 I print_info: vocab_only       = 0
0.00.041.608 I print_info: n_ctx_train      = 2048
0.00.041.608 I print_info: n_embd           = 2048
0.00.041.608 I print_info: n_layer          = 24
0.00.041.612 I print_info: n_head           = 16
0.00.041.613 I print_info: n_head_kv        = 16
0.00.041.613 I print_info: n_rot            = 32
0.00.041.613 I print_info: n_swa            = 0
0.00.041.613 I print_info: n_embd_head_k    = 128
0.00.041.614 I print_info: n_embd_head_v    = 128
0.00.041.614 I print_info: n_gqa            = 1
0.00.041.615 I print_info: n_embd_k_gqa     = 2048
0.00.041.619 I print_info: n_embd_v_gqa     = 2048
0.00.041.619 I print_info: f_norm_eps       = 1.0e-05
0.00.041.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.620 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.620 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.620 I print_info: f_logit_scale    = 0.0e+00
0.00.041.621 I print_info: n_ff             = 8192
0.00.041.621 I print_info: n_expert         = 0
0.00.041.621 I print_info: n_expert_used    = 0
0.00.041.621 I print_info: causal attn      = 1
0.00.041.621 I print_info: pooling type     = 0
0.00.041.621 I print_info: rope type        = 2
0.00.041.622 I print_info: rope scaling     = linear
0.00.041.622 I print_info: freq_base_train  = 10000.0
0.00.041.622 I print_info: freq_scale_train = 1
0.00.041.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.623 I print_info: rope_finetuned   = unknown
0.00.041.623 I print_info: ssm_d_conv       = 0
0.00.041.623 I print_info: ssm_d_inner      = 0
0.00.041.623 I print_info: ssm_d_state      = 0
0.00.041.623 I print_info: ssm_dt_rank      = 0
0.00.041.623 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.624 I print_info: model type       = 1.4B
0.00.041.625 I print_info: model params     = 1.41 B
0.00.041.625 I print_info: general.name     = 1.4B
0.00.041.625 I print_info: vocab type       = BPE
0.00.041.626 I print_info: n_vocab          = 50304
0.00.041.626 I print_info: n_merges         = 50009
0.00.041.626 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.626 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.626 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.626 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.627 I print_info: LF token         = 187 ''
0.00.041.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.627 I print_info: max token length = 1024
0.00.041.627 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.753.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.753.341 I load_tensors: offloading output layer to GPU
0.00.753.342 I load_tensors: offloaded 25/25 layers to GPU
0.00.753.375 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.753.381 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.755.144 I llama_init_from_model: n_seq_max     = 1
0.00.755.150 I llama_init_from_model: n_ctx         = 128
0.00.755.151 I llama_init_from_model: n_ctx_per_seq = 128
0.00.755.152 I llama_init_from_model: n_batch       = 128
0.00.755.152 I llama_init_from_model: n_ubatch      = 128
0.00.755.153 I llama_init_from_model: flash_attn    = 0
0.00.755.167 I llama_init_from_model: freq_base     = 10000.0
0.00.755.168 I llama_init_from_model: freq_scale    = 1
0.00.755.169 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.755.172 I ggml_metal_init: allocating
0.00.755.240 I ggml_metal_init: found device: Apple M4
0.00.755.264 I ggml_metal_init: picking default device: Apple M4
0.00.757.067 I ggml_metal_init: using embedded metal library
0.00.764.151 I ggml_metal_init: GPU name:   Apple M4
0.00.764.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.764.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.764.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.764.164 I ggml_metal_init: simdgroup reduction   = true
0.00.764.164 I ggml_metal_init: simdgroup matrix mul. = true
0.00.764.164 I ggml_metal_init: has residency sets    = true
0.00.764.165 I ggml_metal_init: has bfloat            = true
0.00.764.165 I ggml_metal_init: use bfloat            = true
0.00.764.166 I ggml_metal_init: hasUnifiedMemory      = true
0.00.764.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.782.267 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.786.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.786.062 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.786.114 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.789.223 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.789.224 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.789.225 I llama_init_from_model: graph nodes  = 967
0.00.789.225 I llama_init_from_model: graph splits = 2
0.00.789.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.789.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.275 I 
0.00.817.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.325 I perplexity: tokenizing the input ..
0.00.824.182 I perplexity: tokenization took 6.853 ms
0.00.824.192 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.971.557 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.972.910 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.972.931 I llama_perf_context_print:        load time =     806.64 ms
0.00.972.932 I llama_perf_context_print: prompt eval time =     146.41 ms /   128 tokens (    1.14 ms per token,   874.25 tokens per second)
0.00.972.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.972.934 I llama_perf_context_print:       total time =     155.66 ms /   129 tokens
0.00.973.300 I ggml_metal_free: deallocating

real	0m0.989s
user	0m0.081s
sys	0m0.162s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.020.495 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.040.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.719 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.729 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.730 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.054.858 I llama_model_loader: - type  f32:  194 tensors
0.00.054.858 I llama_model_loader: - type q5_1:   97 tensors
0.00.054.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.860 I print_info: file format = GGUF V3 (latest)
0.00.054.866 I print_info: file type   = Q5_1
0.00.054.867 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.073.905 I load: special tokens cache size = 25
0.00.086.770 I load: token to piece cache size = 0.2984 MB
0.00.086.774 I print_info: arch             = gptneox
0.00.086.775 I print_info: vocab_only       = 0
0.00.086.775 I print_info: n_ctx_train      = 2048
0.00.086.775 I print_info: n_embd           = 2048
0.00.086.776 I print_info: n_layer          = 24
0.00.086.779 I print_info: n_head           = 16
0.00.086.781 I print_info: n_head_kv        = 16
0.00.086.781 I print_info: n_rot            = 32
0.00.086.781 I print_info: n_swa            = 0
0.00.086.782 I print_info: n_embd_head_k    = 128
0.00.086.782 I print_info: n_embd_head_v    = 128
0.00.086.783 I print_info: n_gqa            = 1
0.00.086.784 I print_info: n_embd_k_gqa     = 2048
0.00.086.785 I print_info: n_embd_v_gqa     = 2048
0.00.086.786 I print_info: f_norm_eps       = 1.0e-05
0.00.086.786 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.789 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.791 I print_info: f_logit_scale    = 0.0e+00
0.00.086.794 I print_info: n_ff             = 8192
0.00.086.795 I print_info: n_expert         = 0
0.00.086.795 I print_info: n_expert_used    = 0
0.00.086.795 I print_info: causal attn      = 1
0.00.086.795 I print_info: pooling type     = 0
0.00.086.796 I print_info: rope type        = 2
0.00.086.796 I print_info: rope scaling     = linear
0.00.086.796 I print_info: freq_base_train  = 10000.0
0.00.086.797 I print_info: freq_scale_train = 1
0.00.086.797 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.798 I print_info: rope_finetuned   = unknown
0.00.086.798 I print_info: ssm_d_conv       = 0
0.00.086.798 I print_info: ssm_d_inner      = 0
0.00.086.798 I print_info: ssm_d_state      = 0
0.00.086.798 I print_info: ssm_dt_rank      = 0
0.00.086.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.799 I print_info: model type       = 1.4B
0.00.086.799 I print_info: model params     = 1.41 B
0.00.086.800 I print_info: general.name     = 1.4B
0.00.086.805 I print_info: vocab type       = BPE
0.00.086.806 I print_info: n_vocab          = 50304
0.00.086.806 I print_info: n_merges         = 50009
0.00.086.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.808 I print_info: LF token         = 187 ''
0.00.086.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.809 I print_info: max token length = 1024
0.00.086.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.048.906 I load_tensors: offloading 24 repeating layers to GPU
0.01.048.910 I load_tensors: offloading output layer to GPU
0.01.048.912 I load_tensors: offloaded 25/25 layers to GPU
0.01.048.935 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.01.048.938 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.01.050.318 I llama_init_from_model: n_seq_max     = 1
0.01.050.320 I llama_init_from_model: n_ctx         = 2048
0.01.050.321 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.050.322 I llama_init_from_model: n_batch       = 2048
0.01.050.322 I llama_init_from_model: n_ubatch      = 512
0.01.050.323 I llama_init_from_model: flash_attn    = 0
0.01.050.323 I llama_init_from_model: freq_base     = 10000.0
0.01.050.324 I llama_init_from_model: freq_scale    = 1
0.01.050.325 I ggml_metal_init: allocating
0.01.050.338 I ggml_metal_init: found device: Apple M4
0.01.050.347 I ggml_metal_init: picking default device: Apple M4
0.01.051.688 I ggml_metal_init: using embedded metal library
0.01.057.660 I ggml_metal_init: GPU name:   Apple M4
0.01.057.664 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.057.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.057.666 I ggml_metal_init: simdgroup reduction   = true
0.01.057.666 I ggml_metal_init: simdgroup matrix mul. = true
0.01.057.666 I ggml_metal_init: has residency sets    = true
0.01.057.666 I ggml_metal_init: has bfloat            = true
0.01.057.667 I ggml_metal_init: use bfloat            = true
0.01.057.667 I ggml_metal_init: hasUnifiedMemory      = true
0.01.057.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.074.594 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.128.957 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.128.961 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.128.994 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.133.633 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.133.635 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.133.635 I llama_init_from_model: graph nodes  = 967
0.01.133.635 I llama_init_from_model: graph splits = 2
0.01.133.642 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.133.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.133.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.191.194 I main: llama threadpool init, n_threads = 4
0.01.191.240 I 
0.01.191.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.191.255 I 
0.01.191.433 I sampler seed: 1234
0.01.191.438 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.191.461 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.191.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.191.462 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.043.576 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.02.043.576 I llama_perf_context_print:        load time =    1170.00 ms
0.02.043.578 I llama_perf_context_print: prompt eval time =      53.40 ms /     7 tokens (    7.63 ms per token,   131.10 tokens per second)
0.02.043.578 I llama_perf_context_print:        eval time =     795.79 ms /    63 runs   (   12.63 ms per token,    79.17 tokens per second)
0.02.043.579 I llama_perf_context_print:       total time =     853.07 ms /    70 tokens
0.02.043.835 I ggml_metal_free: deallocating

real	0m2.089s
user	0m0.134s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.411 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.595 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.405 I llama_model_loader: - type  f32:  194 tensors
0.00.025.405 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.405 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.406 I print_info: file format = GGUF V3 (latest)
0.00.025.411 I print_info: file type   = Q5_1
0.00.025.413 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.696 I load: special tokens cache size = 25
0.00.039.870 I load: token to piece cache size = 0.2984 MB
0.00.039.875 I print_info: arch             = gptneox
0.00.039.875 I print_info: vocab_only       = 0
0.00.039.875 I print_info: n_ctx_train      = 2048
0.00.039.875 I print_info: n_embd           = 2048
0.00.039.876 I print_info: n_layer          = 24
0.00.039.880 I print_info: n_head           = 16
0.00.039.881 I print_info: n_head_kv        = 16
0.00.039.881 I print_info: n_rot            = 32
0.00.039.881 I print_info: n_swa            = 0
0.00.039.882 I print_info: n_embd_head_k    = 128
0.00.039.882 I print_info: n_embd_head_v    = 128
0.00.039.883 I print_info: n_gqa            = 1
0.00.039.883 I print_info: n_embd_k_gqa     = 2048
0.00.039.884 I print_info: n_embd_v_gqa     = 2048
0.00.039.884 I print_info: f_norm_eps       = 1.0e-05
0.00.039.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.885 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.885 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.885 I print_info: f_logit_scale    = 0.0e+00
0.00.039.886 I print_info: n_ff             = 8192
0.00.039.886 I print_info: n_expert         = 0
0.00.039.886 I print_info: n_expert_used    = 0
0.00.039.886 I print_info: causal attn      = 1
0.00.039.886 I print_info: pooling type     = 0
0.00.039.886 I print_info: rope type        = 2
0.00.039.887 I print_info: rope scaling     = linear
0.00.039.887 I print_info: freq_base_train  = 10000.0
0.00.039.887 I print_info: freq_scale_train = 1
0.00.039.887 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.888 I print_info: rope_finetuned   = unknown
0.00.039.888 I print_info: ssm_d_conv       = 0
0.00.039.888 I print_info: ssm_d_inner      = 0
0.00.039.888 I print_info: ssm_d_state      = 0
0.00.039.888 I print_info: ssm_dt_rank      = 0
0.00.039.888 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.889 I print_info: model type       = 1.4B
0.00.039.889 I print_info: model params     = 1.41 B
0.00.039.889 I print_info: general.name     = 1.4B
0.00.039.890 I print_info: vocab type       = BPE
0.00.039.890 I print_info: n_vocab          = 50304
0.00.039.890 I print_info: n_merges         = 50009
0.00.039.891 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: LF token         = 187 ''
0.00.039.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: max token length = 1024
0.00.039.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.918 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.937 I load_tensors: offloading output layer to GPU
0.00.621.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.973 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.974 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.623.179 I llama_init_from_model: n_seq_max     = 1
0.00.623.187 I llama_init_from_model: n_ctx         = 128
0.00.623.188 I llama_init_from_model: n_ctx_per_seq = 128
0.00.623.188 I llama_init_from_model: n_batch       = 128
0.00.623.189 I llama_init_from_model: n_ubatch      = 128
0.00.623.189 I llama_init_from_model: flash_attn    = 0
0.00.623.192 I llama_init_from_model: freq_base     = 10000.0
0.00.623.192 I llama_init_from_model: freq_scale    = 1
0.00.623.193 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.623.197 I ggml_metal_init: allocating
0.00.623.276 I ggml_metal_init: found device: Apple M4
0.00.623.291 I ggml_metal_init: picking default device: Apple M4
0.00.625.324 I ggml_metal_init: using embedded metal library
0.00.631.939 I ggml_metal_init: GPU name:   Apple M4
0.00.631.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.945 I ggml_metal_init: simdgroup reduction   = true
0.00.631.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.946 I ggml_metal_init: has residency sets    = true
0.00.631.946 I ggml_metal_init: has bfloat            = true
0.00.631.946 I ggml_metal_init: use bfloat            = true
0.00.631.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.952 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.138 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.472 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.652.476 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.511 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.538 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.655.540 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.655.540 I llama_init_from_model: graph nodes  = 967
0.00.655.541 I llama_init_from_model: graph splits = 2
0.00.655.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.348 I 
0.00.681.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.400 I perplexity: tokenizing the input ..
0.00.686.926 I perplexity: tokenization took 5.527 ms
0.00.686.931 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.315 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.822.652 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.822.675 I llama_perf_context_print:        load time =     671.92 ms
0.00.822.676 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.34 tokens per second)
0.00.822.676 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.677 I llama_perf_context_print:       total time =     141.33 ms /   129 tokens
0.00.823.092 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.078s
sys	0m0.144s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.730 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.393 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.396 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.396 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.397 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.398 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.398 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.090 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.821 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.821 I llama_model_loader: - type  f32:  194 tensors
0.00.025.821 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.821 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.822 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.822 I print_info: file format = GGUF V3 (latest)
0.00.025.822 I print_info: file type   = Q2_K - Medium
0.00.025.823 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.992 I load: special tokens cache size = 25
0.00.039.971 I load: token to piece cache size = 0.2984 MB
0.00.039.973 I print_info: arch             = gptneox
0.00.039.973 I print_info: vocab_only       = 0
0.00.039.974 I print_info: n_ctx_train      = 2048
0.00.039.974 I print_info: n_embd           = 2048
0.00.039.974 I print_info: n_layer          = 24
0.00.039.977 I print_info: n_head           = 16
0.00.039.978 I print_info: n_head_kv        = 16
0.00.039.978 I print_info: n_rot            = 32
0.00.039.978 I print_info: n_swa            = 0
0.00.039.978 I print_info: n_embd_head_k    = 128
0.00.039.979 I print_info: n_embd_head_v    = 128
0.00.039.979 I print_info: n_gqa            = 1
0.00.039.980 I print_info: n_embd_k_gqa     = 2048
0.00.039.981 I print_info: n_embd_v_gqa     = 2048
0.00.039.982 I print_info: f_norm_eps       = 1.0e-05
0.00.039.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.982 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.982 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.982 I print_info: f_logit_scale    = 0.0e+00
0.00.039.983 I print_info: n_ff             = 8192
0.00.039.983 I print_info: n_expert         = 0
0.00.039.983 I print_info: n_expert_used    = 0
0.00.039.983 I print_info: causal attn      = 1
0.00.039.984 I print_info: pooling type     = 0
0.00.039.984 I print_info: rope type        = 2
0.00.039.984 I print_info: rope scaling     = linear
0.00.039.984 I print_info: freq_base_train  = 10000.0
0.00.039.985 I print_info: freq_scale_train = 1
0.00.039.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.985 I print_info: rope_finetuned   = unknown
0.00.039.985 I print_info: ssm_d_conv       = 0
0.00.039.985 I print_info: ssm_d_inner      = 0
0.00.039.986 I print_info: ssm_d_state      = 0
0.00.039.988 I print_info: ssm_dt_rank      = 0
0.00.039.988 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.988 I print_info: model type       = 1.4B
0.00.039.989 I print_info: model params     = 1.41 B
0.00.039.989 I print_info: general.name     = 1.4B
0.00.039.989 I print_info: vocab type       = BPE
0.00.039.990 I print_info: n_vocab          = 50304
0.00.039.990 I print_info: n_merges         = 50009
0.00.039.990 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.991 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.991 I print_info: LF token         = 187 ''
0.00.039.991 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.991 I print_info: max token length = 1024
0.00.039.992 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.346.783 I load_tensors: offloading 24 repeating layers to GPU
0.00.346.798 I load_tensors: offloading output layer to GPU
0.00.346.799 I load_tensors: offloaded 25/25 layers to GPU
0.00.346.833 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.346.834 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.348.341 I llama_init_from_model: n_seq_max     = 1
0.00.348.346 I llama_init_from_model: n_ctx         = 2048
0.00.348.347 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.348.347 I llama_init_from_model: n_batch       = 2048
0.00.348.347 I llama_init_from_model: n_ubatch      = 512
0.00.348.348 I llama_init_from_model: flash_attn    = 0
0.00.348.350 I llama_init_from_model: freq_base     = 10000.0
0.00.348.350 I llama_init_from_model: freq_scale    = 1
0.00.348.353 I ggml_metal_init: allocating
0.00.348.448 I ggml_metal_init: found device: Apple M4
0.00.348.461 I ggml_metal_init: picking default device: Apple M4
0.00.350.322 I ggml_metal_init: using embedded metal library
0.00.355.863 I ggml_metal_init: GPU name:   Apple M4
0.00.355.882 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.883 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.884 I ggml_metal_init: simdgroup reduction   = true
0.00.355.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.885 I ggml_metal_init: has residency sets    = true
0.00.355.885 I ggml_metal_init: has bfloat            = true
0.00.355.885 I ggml_metal_init: use bfloat            = true
0.00.355.887 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.771 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.262 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.438.270 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.438.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.442.286 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.442.289 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.442.289 I llama_init_from_model: graph nodes  = 967
0.00.442.289 I llama_init_from_model: graph splits = 2
0.00.442.295 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.442.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.442.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.053 I main: llama threadpool init, n_threads = 4
0.00.504.095 I 
0.00.504.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.504.111 I 
0.00.504.289 I sampler seed: 1234
0.00.504.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.305 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.182.304 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.182.305 I llama_perf_context_print:        load time =     492.62 ms
0.01.182.305 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.98 tokens per second)
0.01.182.307 I llama_perf_context_print:        eval time =     634.77 ms /    63 runs   (   10.08 ms per token,    99.25 tokens per second)
0.01.182.308 I llama_perf_context_print:       total time =     678.95 ms /    70 tokens
0.01.182.565 I ggml_metal_free: deallocating

real	0m1.202s
user	0m0.112s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.996 I llama_model_loader: - type  f32:  194 tensors
0.00.027.998 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.998 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.998 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.999 I print_info: file format = GGUF V3 (latest)
0.00.027.999 I print_info: file type   = Q2_K - Medium
0.00.028.000 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.036.401 I load: special tokens cache size = 25
0.00.042.514 I load: token to piece cache size = 0.2984 MB
0.00.042.518 I print_info: arch             = gptneox
0.00.042.519 I print_info: vocab_only       = 0
0.00.042.519 I print_info: n_ctx_train      = 2048
0.00.042.519 I print_info: n_embd           = 2048
0.00.042.519 I print_info: n_layer          = 24
0.00.042.524 I print_info: n_head           = 16
0.00.042.525 I print_info: n_head_kv        = 16
0.00.042.525 I print_info: n_rot            = 32
0.00.042.525 I print_info: n_swa            = 0
0.00.042.525 I print_info: n_embd_head_k    = 128
0.00.042.526 I print_info: n_embd_head_v    = 128
0.00.042.526 I print_info: n_gqa            = 1
0.00.042.527 I print_info: n_embd_k_gqa     = 2048
0.00.042.528 I print_info: n_embd_v_gqa     = 2048
0.00.042.528 I print_info: f_norm_eps       = 1.0e-05
0.00.042.529 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.529 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.529 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.529 I print_info: f_logit_scale    = 0.0e+00
0.00.042.530 I print_info: n_ff             = 8192
0.00.042.533 I print_info: n_expert         = 0
0.00.042.533 I print_info: n_expert_used    = 0
0.00.042.533 I print_info: causal attn      = 1
0.00.042.533 I print_info: pooling type     = 0
0.00.042.533 I print_info: rope type        = 2
0.00.042.533 I print_info: rope scaling     = linear
0.00.042.534 I print_info: freq_base_train  = 10000.0
0.00.042.534 I print_info: freq_scale_train = 1
0.00.042.534 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.534 I print_info: rope_finetuned   = unknown
0.00.042.535 I print_info: ssm_d_conv       = 0
0.00.042.535 I print_info: ssm_d_inner      = 0
0.00.042.535 I print_info: ssm_d_state      = 0
0.00.042.535 I print_info: ssm_dt_rank      = 0
0.00.042.535 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.535 I print_info: model type       = 1.4B
0.00.042.536 I print_info: model params     = 1.41 B
0.00.042.536 I print_info: general.name     = 1.4B
0.00.042.536 I print_info: vocab type       = BPE
0.00.042.538 I print_info: n_vocab          = 50304
0.00.042.538 I print_info: n_merges         = 50009
0.00.042.538 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.538 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.538 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.538 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.538 I print_info: LF token         = 187 ''
0.00.042.539 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.539 I print_info: max token length = 1024
0.00.042.539 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.357.714 I load_tensors: offloading 24 repeating layers to GPU
0.00.357.729 I load_tensors: offloading output layer to GPU
0.00.357.729 I load_tensors: offloaded 25/25 layers to GPU
0.00.357.762 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.357.763 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.359.198 I llama_init_from_model: n_seq_max     = 1
0.00.359.202 I llama_init_from_model: n_ctx         = 128
0.00.359.203 I llama_init_from_model: n_ctx_per_seq = 128
0.00.359.203 I llama_init_from_model: n_batch       = 128
0.00.359.204 I llama_init_from_model: n_ubatch      = 128
0.00.359.204 I llama_init_from_model: flash_attn    = 0
0.00.359.206 I llama_init_from_model: freq_base     = 10000.0
0.00.359.207 I llama_init_from_model: freq_scale    = 1
0.00.359.207 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.359.210 I ggml_metal_init: allocating
0.00.359.287 I ggml_metal_init: found device: Apple M4
0.00.359.300 I ggml_metal_init: picking default device: Apple M4
0.00.361.194 I ggml_metal_init: using embedded metal library
0.00.367.004 I ggml_metal_init: GPU name:   Apple M4
0.00.367.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.367.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.367.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.367.026 I ggml_metal_init: simdgroup reduction   = true
0.00.367.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.367.027 I ggml_metal_init: has residency sets    = true
0.00.367.027 I ggml_metal_init: has bfloat            = true
0.00.367.028 I ggml_metal_init: use bfloat            = true
0.00.367.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.367.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.389.336 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.393.240 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.393.247 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.393.300 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.396.834 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.396.836 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.396.836 I llama_init_from_model: graph nodes  = 967
0.00.396.837 I llama_init_from_model: graph splits = 2
0.00.396.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.396.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.423.770 I 
0.00.423.810 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.423.815 I perplexity: tokenizing the input ..
0.00.428.892 I perplexity: tokenization took 5.075 ms
0.00.428.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.895 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.383 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.404 I llama_perf_context_print:        load time =     411.90 ms
0.00.562.405 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.52 tokens per second)
0.00.562.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.406 I llama_perf_context_print:       total time =     138.64 ms /   129 tokens
0.00.562.821 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.099s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.437 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.165 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.166 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.168 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.988 I llama_model_loader: - type  f32:  194 tensors
0.00.025.988 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.988 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.988 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.989 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.989 I print_info: file format = GGUF V3 (latest)
0.00.025.990 I print_info: file type   = Q3_K - Medium
0.00.025.991 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.195 I load: special tokens cache size = 25
0.00.040.230 I load: token to piece cache size = 0.2984 MB
0.00.040.233 I print_info: arch             = gptneox
0.00.040.233 I print_info: vocab_only       = 0
0.00.040.234 I print_info: n_ctx_train      = 2048
0.00.040.234 I print_info: n_embd           = 2048
0.00.040.234 I print_info: n_layer          = 24
0.00.040.237 I print_info: n_head           = 16
0.00.040.238 I print_info: n_head_kv        = 16
0.00.040.238 I print_info: n_rot            = 32
0.00.040.238 I print_info: n_swa            = 0
0.00.040.239 I print_info: n_embd_head_k    = 128
0.00.040.239 I print_info: n_embd_head_v    = 128
0.00.040.240 I print_info: n_gqa            = 1
0.00.040.240 I print_info: n_embd_k_gqa     = 2048
0.00.040.241 I print_info: n_embd_v_gqa     = 2048
0.00.040.241 I print_info: f_norm_eps       = 1.0e-05
0.00.040.244 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.245 I print_info: f_logit_scale    = 0.0e+00
0.00.040.245 I print_info: n_ff             = 8192
0.00.040.249 I print_info: n_expert         = 0
0.00.040.249 I print_info: n_expert_used    = 0
0.00.040.249 I print_info: causal attn      = 1
0.00.040.251 I print_info: pooling type     = 0
0.00.040.251 I print_info: rope type        = 2
0.00.040.251 I print_info: rope scaling     = linear
0.00.040.251 I print_info: freq_base_train  = 10000.0
0.00.040.252 I print_info: freq_scale_train = 1
0.00.040.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.252 I print_info: rope_finetuned   = unknown
0.00.040.252 I print_info: ssm_d_conv       = 0
0.00.040.253 I print_info: ssm_d_inner      = 0
0.00.040.253 I print_info: ssm_d_state      = 0
0.00.040.253 I print_info: ssm_dt_rank      = 0
0.00.040.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.253 I print_info: model type       = 1.4B
0.00.040.254 I print_info: model params     = 1.41 B
0.00.040.254 I print_info: general.name     = 1.4B
0.00.040.254 I print_info: vocab type       = BPE
0.00.040.254 I print_info: n_vocab          = 50304
0.00.040.255 I print_info: n_merges         = 50009
0.00.040.255 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.255 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.256 I print_info: LF token         = 187 ''
0.00.040.257 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: max token length = 1024
0.00.040.257 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.439.362 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.379 I load_tensors: offloading output layer to GPU
0.00.439.380 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.416 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.417 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.440.883 I llama_init_from_model: n_seq_max     = 1
0.00.440.886 I llama_init_from_model: n_ctx         = 2048
0.00.440.886 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.440.887 I llama_init_from_model: n_batch       = 2048
0.00.440.887 I llama_init_from_model: n_ubatch      = 512
0.00.440.888 I llama_init_from_model: flash_attn    = 0
0.00.440.890 I llama_init_from_model: freq_base     = 10000.0
0.00.440.890 I llama_init_from_model: freq_scale    = 1
0.00.440.892 I ggml_metal_init: allocating
0.00.440.972 I ggml_metal_init: found device: Apple M4
0.00.440.986 I ggml_metal_init: picking default device: Apple M4
0.00.442.837 I ggml_metal_init: using embedded metal library
0.00.449.300 I ggml_metal_init: GPU name:   Apple M4
0.00.449.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.308 I ggml_metal_init: simdgroup reduction   = true
0.00.449.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.308 I ggml_metal_init: has residency sets    = true
0.00.449.308 I ggml_metal_init: has bfloat            = true
0.00.449.309 I ggml_metal_init: use bfloat            = true
0.00.449.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.166 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.523.804 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.523.811 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.523.846 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.970 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.528.972 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.528.973 I llama_init_from_model: graph nodes  = 967
0.00.528.973 I llama_init_from_model: graph splits = 2
0.00.528.979 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.529.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.529.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.510 I main: llama threadpool init, n_threads = 4
0.00.587.555 I 
0.00.587.572 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.572 I 
0.00.587.725 I sampler seed: 1234
0.00.587.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.587.750 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.587.751 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.587.751 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.330.555 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.330.556 I llama_perf_context_print:        load time =     577.37 ms
0.01.330.558 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.27 tokens per second)
0.01.330.558 I llama_perf_context_print:        eval time =     690.70 ms /    63 runs   (   10.96 ms per token,    91.21 tokens per second)
0.01.330.559 I llama_perf_context_print:       total time =     743.74 ms /    70 tokens
0.01.330.810 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.222 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.460 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.461 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.463 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.466 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.323 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.162 I llama_model_loader: - type  f32:  194 tensors
0.00.025.162 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.162 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.163 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.163 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.164 I print_info: file format = GGUF V3 (latest)
0.00.025.164 I print_info: file type   = Q3_K - Medium
0.00.025.166 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.632 I load: special tokens cache size = 25
0.00.039.552 I load: token to piece cache size = 0.2984 MB
0.00.039.557 I print_info: arch             = gptneox
0.00.039.557 I print_info: vocab_only       = 0
0.00.039.558 I print_info: n_ctx_train      = 2048
0.00.039.558 I print_info: n_embd           = 2048
0.00.039.558 I print_info: n_layer          = 24
0.00.039.563 I print_info: n_head           = 16
0.00.039.566 I print_info: n_head_kv        = 16
0.00.039.566 I print_info: n_rot            = 32
0.00.039.567 I print_info: n_swa            = 0
0.00.039.567 I print_info: n_embd_head_k    = 128
0.00.039.567 I print_info: n_embd_head_v    = 128
0.00.039.568 I print_info: n_gqa            = 1
0.00.039.568 I print_info: n_embd_k_gqa     = 2048
0.00.039.569 I print_info: n_embd_v_gqa     = 2048
0.00.039.570 I print_info: f_norm_eps       = 1.0e-05
0.00.039.570 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.574 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.574 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.574 I print_info: f_logit_scale    = 0.0e+00
0.00.039.575 I print_info: n_ff             = 8192
0.00.039.575 I print_info: n_expert         = 0
0.00.039.575 I print_info: n_expert_used    = 0
0.00.039.575 I print_info: causal attn      = 1
0.00.039.576 I print_info: pooling type     = 0
0.00.039.577 I print_info: rope type        = 2
0.00.039.577 I print_info: rope scaling     = linear
0.00.039.577 I print_info: freq_base_train  = 10000.0
0.00.039.578 I print_info: freq_scale_train = 1
0.00.039.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.578 I print_info: rope_finetuned   = unknown
0.00.039.578 I print_info: ssm_d_conv       = 0
0.00.039.578 I print_info: ssm_d_inner      = 0
0.00.039.578 I print_info: ssm_d_state      = 0
0.00.039.579 I print_info: ssm_dt_rank      = 0
0.00.039.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.579 I print_info: model type       = 1.4B
0.00.039.579 I print_info: model params     = 1.41 B
0.00.039.580 I print_info: general.name     = 1.4B
0.00.039.580 I print_info: vocab type       = BPE
0.00.039.580 I print_info: n_vocab          = 50304
0.00.039.580 I print_info: n_merges         = 50009
0.00.039.581 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: LF token         = 187 ''
0.00.039.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.588 I print_info: max token length = 1024
0.00.039.588 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.507 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.520 I load_tensors: offloading output layer to GPU
0.00.445.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.555 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.557 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.447.056 I llama_init_from_model: n_seq_max     = 1
0.00.447.061 I llama_init_from_model: n_ctx         = 128
0.00.447.061 I llama_init_from_model: n_ctx_per_seq = 128
0.00.447.062 I llama_init_from_model: n_batch       = 128
0.00.447.063 I llama_init_from_model: n_ubatch      = 128
0.00.447.063 I llama_init_from_model: flash_attn    = 0
0.00.447.065 I llama_init_from_model: freq_base     = 10000.0
0.00.447.066 I llama_init_from_model: freq_scale    = 1
0.00.447.067 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.447.069 I ggml_metal_init: allocating
0.00.447.153 I ggml_metal_init: found device: Apple M4
0.00.447.170 I ggml_metal_init: picking default device: Apple M4
0.00.449.207 I ggml_metal_init: using embedded metal library
0.00.455.116 I ggml_metal_init: GPU name:   Apple M4
0.00.455.125 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.126 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.128 I ggml_metal_init: simdgroup reduction   = true
0.00.455.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.129 I ggml_metal_init: has residency sets    = true
0.00.455.129 I ggml_metal_init: has bfloat            = true
0.00.455.129 I ggml_metal_init: use bfloat            = true
0.00.455.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.397 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.477.871 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.477.877 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.477.938 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.481.133 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.481.135 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.481.136 I llama_init_from_model: graph nodes  = 967
0.00.481.136 I llama_init_from_model: graph splits = 2
0.00.481.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.481.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.509 I 
0.00.508.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.556 I perplexity: tokenizing the input ..
0.00.513.971 I perplexity: tokenization took 5.413 ms
0.00.513.978 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.367 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.707 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.727 I llama_perf_context_print:        load time =     499.28 ms
0.00.649.728 I llama_perf_context_print: prompt eval time =     133.43 ms /   128 tokens (    1.04 ms per token,   959.29 tokens per second)
0.00.649.729 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.729 I llama_perf_context_print:       total time =     141.22 ms /   129 tokens
0.00.650.131 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.079s
sys	0m0.110s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.287 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.208 I llama_model_loader: - type  f32:  194 tensors
0.00.025.208 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.208 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.209 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.209 I print_info: file format = GGUF V3 (latest)
0.00.025.210 I print_info: file type   = Q4_K - Medium
0.00.025.214 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.921 I load: special tokens cache size = 25
0.00.038.737 I load: token to piece cache size = 0.2984 MB
0.00.038.740 I print_info: arch             = gptneox
0.00.038.740 I print_info: vocab_only       = 0
0.00.038.740 I print_info: n_ctx_train      = 2048
0.00.038.740 I print_info: n_embd           = 2048
0.00.038.740 I print_info: n_layer          = 24
0.00.038.743 I print_info: n_head           = 16
0.00.038.744 I print_info: n_head_kv        = 16
0.00.038.744 I print_info: n_rot            = 32
0.00.038.744 I print_info: n_swa            = 0
0.00.038.744 I print_info: n_embd_head_k    = 128
0.00.038.744 I print_info: n_embd_head_v    = 128
0.00.038.745 I print_info: n_gqa            = 1
0.00.038.746 I print_info: n_embd_k_gqa     = 2048
0.00.038.746 I print_info: n_embd_v_gqa     = 2048
0.00.038.747 I print_info: f_norm_eps       = 1.0e-05
0.00.038.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.748 I print_info: f_logit_scale    = 0.0e+00
0.00.038.748 I print_info: n_ff             = 8192
0.00.038.749 I print_info: n_expert         = 0
0.00.038.751 I print_info: n_expert_used    = 0
0.00.038.751 I print_info: causal attn      = 1
0.00.038.752 I print_info: pooling type     = 0
0.00.038.752 I print_info: rope type        = 2
0.00.038.752 I print_info: rope scaling     = linear
0.00.038.753 I print_info: freq_base_train  = 10000.0
0.00.038.753 I print_info: freq_scale_train = 1
0.00.038.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.753 I print_info: rope_finetuned   = unknown
0.00.038.755 I print_info: ssm_d_conv       = 0
0.00.038.755 I print_info: ssm_d_inner      = 0
0.00.038.755 I print_info: ssm_d_state      = 0
0.00.038.755 I print_info: ssm_dt_rank      = 0
0.00.038.755 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.756 I print_info: model type       = 1.4B
0.00.038.756 I print_info: model params     = 1.41 B
0.00.038.756 I print_info: general.name     = 1.4B
0.00.038.757 I print_info: vocab type       = BPE
0.00.038.757 I print_info: n_vocab          = 50304
0.00.038.757 I print_info: n_merges         = 50009
0.00.038.757 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.757 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.758 I print_info: LF token         = 187 ''
0.00.038.762 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.762 I print_info: max token length = 1024
0.00.038.762 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.526 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.535 I load_tensors: offloading output layer to GPU
0.00.538.536 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.562 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.563 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.073 I llama_init_from_model: n_seq_max     = 1
0.00.540.075 I llama_init_from_model: n_ctx         = 2048
0.00.540.076 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.540.077 I llama_init_from_model: n_batch       = 2048
0.00.540.077 I llama_init_from_model: n_ubatch      = 512
0.00.540.077 I llama_init_from_model: flash_attn    = 0
0.00.540.080 I llama_init_from_model: freq_base     = 10000.0
0.00.540.080 I llama_init_from_model: freq_scale    = 1
0.00.540.082 I ggml_metal_init: allocating
0.00.540.135 I ggml_metal_init: found device: Apple M4
0.00.540.149 I ggml_metal_init: picking default device: Apple M4
0.00.541.820 I ggml_metal_init: using embedded metal library
0.00.547.322 I ggml_metal_init: GPU name:   Apple M4
0.00.547.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.547.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.547.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.547.335 I ggml_metal_init: simdgroup reduction   = true
0.00.547.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.547.335 I ggml_metal_init: has residency sets    = true
0.00.547.336 I ggml_metal_init: has bfloat            = true
0.00.547.336 I ggml_metal_init: use bfloat            = true
0.00.547.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.547.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.567.476 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.867 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.627.876 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.627.910 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.270 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.632.272 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.632.272 I llama_init_from_model: graph nodes  = 967
0.00.632.272 I llama_init_from_model: graph splits = 2
0.00.632.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.632.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.632.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.507 I main: llama threadpool init, n_threads = 4
0.00.690.555 I 
0.00.690.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.570 I 
0.00.690.740 I sampler seed: 1234
0.00.690.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.791 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.791 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.453.422 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.453.422 I llama_perf_context_print:        load time =     680.52 ms
0.01.453.423 I llama_perf_context_print: prompt eval time =      57.88 ms /     7 tokens (    8.27 ms per token,   120.94 tokens per second)
0.01.453.424 I llama_perf_context_print:        eval time =     701.84 ms /    63 runs   (   11.14 ms per token,    89.76 tokens per second)
0.01.453.424 I llama_perf_context_print:       total time =     763.61 ms /    70 tokens
0.01.453.625 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.306 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.434 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.286 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.196 I llama_model_loader: - type  f32:  194 tensors
0.00.026.196 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.196 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.197 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.197 I print_info: file format = GGUF V3 (latest)
0.00.026.198 I print_info: file type   = Q4_K - Medium
0.00.026.199 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.235 I load: special tokens cache size = 25
0.00.040.386 I load: token to piece cache size = 0.2984 MB
0.00.040.391 I print_info: arch             = gptneox
0.00.040.391 I print_info: vocab_only       = 0
0.00.040.391 I print_info: n_ctx_train      = 2048
0.00.040.391 I print_info: n_embd           = 2048
0.00.040.392 I print_info: n_layer          = 24
0.00.040.396 I print_info: n_head           = 16
0.00.040.397 I print_info: n_head_kv        = 16
0.00.040.397 I print_info: n_rot            = 32
0.00.040.397 I print_info: n_swa            = 0
0.00.040.398 I print_info: n_embd_head_k    = 128
0.00.040.398 I print_info: n_embd_head_v    = 128
0.00.040.399 I print_info: n_gqa            = 1
0.00.040.399 I print_info: n_embd_k_gqa     = 2048
0.00.040.400 I print_info: n_embd_v_gqa     = 2048
0.00.040.403 I print_info: f_norm_eps       = 1.0e-05
0.00.040.403 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.403 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.404 I print_info: f_logit_scale    = 0.0e+00
0.00.040.404 I print_info: n_ff             = 8192
0.00.040.404 I print_info: n_expert         = 0
0.00.040.405 I print_info: n_expert_used    = 0
0.00.040.405 I print_info: causal attn      = 1
0.00.040.405 I print_info: pooling type     = 0
0.00.040.405 I print_info: rope type        = 2
0.00.040.406 I print_info: rope scaling     = linear
0.00.040.406 I print_info: freq_base_train  = 10000.0
0.00.040.407 I print_info: freq_scale_train = 1
0.00.040.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.431 I print_info: rope_finetuned   = unknown
0.00.040.433 I print_info: ssm_d_conv       = 0
0.00.040.433 I print_info: ssm_d_inner      = 0
0.00.040.434 I print_info: ssm_d_state      = 0
0.00.040.434 I print_info: ssm_dt_rank      = 0
0.00.040.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.434 I print_info: model type       = 1.4B
0.00.040.435 I print_info: model params     = 1.41 B
0.00.040.435 I print_info: general.name     = 1.4B
0.00.040.435 I print_info: vocab type       = BPE
0.00.040.436 I print_info: n_vocab          = 50304
0.00.040.436 I print_info: n_merges         = 50009
0.00.040.436 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.436 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.436 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.439 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.439 I print_info: LF token         = 187 ''
0.00.040.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.439 I print_info: max token length = 1024
0.00.040.440 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.736 I load_tensors: offloading output layer to GPU
0.00.547.736 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.770 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.771 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.549.450 I llama_init_from_model: n_seq_max     = 1
0.00.549.454 I llama_init_from_model: n_ctx         = 128
0.00.549.454 I llama_init_from_model: n_ctx_per_seq = 128
0.00.549.455 I llama_init_from_model: n_batch       = 128
0.00.549.455 I llama_init_from_model: n_ubatch      = 128
0.00.549.455 I llama_init_from_model: flash_attn    = 0
0.00.549.458 I llama_init_from_model: freq_base     = 10000.0
0.00.549.459 I llama_init_from_model: freq_scale    = 1
0.00.549.459 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.549.463 I ggml_metal_init: allocating
0.00.549.523 I ggml_metal_init: found device: Apple M4
0.00.549.537 I ggml_metal_init: picking default device: Apple M4
0.00.551.254 I ggml_metal_init: using embedded metal library
0.00.558.061 I ggml_metal_init: GPU name:   Apple M4
0.00.558.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.070 I ggml_metal_init: simdgroup reduction   = true
0.00.558.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.071 I ggml_metal_init: has residency sets    = true
0.00.558.071 I ggml_metal_init: has bfloat            = true
0.00.558.071 I ggml_metal_init: use bfloat            = true
0.00.558.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.722 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.579.134 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.579.138 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.579.175 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.582.076 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.582.078 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.582.078 I llama_init_from_model: graph nodes  = 967
0.00.582.079 I llama_init_from_model: graph splits = 2
0.00.582.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.582.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.562 I 
0.00.610.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.631 I perplexity: tokenizing the input ..
0.00.617.722 I perplexity: tokenization took 7.088 ms
0.00.617.730 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.966 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.766.378 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.766.396 I llama_perf_context_print:        load time =     600.24 ms
0.00.766.396 I llama_perf_context_print: prompt eval time =     146.26 ms /   128 tokens (    1.14 ms per token,   875.15 tokens per second)
0.00.766.397 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.766.397 I llama_perf_context_print:       total time =     155.84 ms /   129 tokens
0.00.766.790 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.080s
sys	0m0.138s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.850 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.855 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.861 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.867 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.429 I llama_model_loader: - type  f32:  194 tensors
0.00.025.429 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.429 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.430 I print_info: file format = GGUF V3 (latest)
0.00.025.431 I print_info: file type   = Q5_K - Medium
0.00.025.431 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.565 I load: special tokens cache size = 25
0.00.039.568 I load: token to piece cache size = 0.2984 MB
0.00.039.571 I print_info: arch             = gptneox
0.00.039.572 I print_info: vocab_only       = 0
0.00.039.572 I print_info: n_ctx_train      = 2048
0.00.039.572 I print_info: n_embd           = 2048
0.00.039.572 I print_info: n_layer          = 24
0.00.039.575 I print_info: n_head           = 16
0.00.039.576 I print_info: n_head_kv        = 16
0.00.039.576 I print_info: n_rot            = 32
0.00.039.577 I print_info: n_swa            = 0
0.00.039.577 I print_info: n_embd_head_k    = 128
0.00.039.578 I print_info: n_embd_head_v    = 128
0.00.039.578 I print_info: n_gqa            = 1
0.00.039.579 I print_info: n_embd_k_gqa     = 2048
0.00.039.580 I print_info: n_embd_v_gqa     = 2048
0.00.039.580 I print_info: f_norm_eps       = 1.0e-05
0.00.039.581 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.581 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.581 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.581 I print_info: f_logit_scale    = 0.0e+00
0.00.039.582 I print_info: n_ff             = 8192
0.00.039.582 I print_info: n_expert         = 0
0.00.039.582 I print_info: n_expert_used    = 0
0.00.039.582 I print_info: causal attn      = 1
0.00.039.582 I print_info: pooling type     = 0
0.00.039.582 I print_info: rope type        = 2
0.00.039.584 I print_info: rope scaling     = linear
0.00.039.585 I print_info: freq_base_train  = 10000.0
0.00.039.585 I print_info: freq_scale_train = 1
0.00.039.586 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.587 I print_info: rope_finetuned   = unknown
0.00.039.587 I print_info: ssm_d_conv       = 0
0.00.039.587 I print_info: ssm_d_inner      = 0
0.00.039.587 I print_info: ssm_d_state      = 0
0.00.039.587 I print_info: ssm_dt_rank      = 0
0.00.039.587 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.588 I print_info: model type       = 1.4B
0.00.039.588 I print_info: model params     = 1.41 B
0.00.039.588 I print_info: general.name     = 1.4B
0.00.039.589 I print_info: vocab type       = BPE
0.00.039.589 I print_info: n_vocab          = 50304
0.00.039.589 I print_info: n_merges         = 50009
0.00.039.589 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: LF token         = 187 ''
0.00.039.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: max token length = 1024
0.00.039.599 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.379 I load_tensors: offloading output layer to GPU
0.00.604.380 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.412 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.604.414 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.605.905 I llama_init_from_model: n_seq_max     = 1
0.00.605.910 I llama_init_from_model: n_ctx         = 2048
0.00.605.910 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.911 I llama_init_from_model: n_batch       = 2048
0.00.605.911 I llama_init_from_model: n_ubatch      = 512
0.00.605.912 I llama_init_from_model: flash_attn    = 0
0.00.605.913 I llama_init_from_model: freq_base     = 10000.0
0.00.605.914 I llama_init_from_model: freq_scale    = 1
0.00.605.917 I ggml_metal_init: allocating
0.00.605.973 I ggml_metal_init: found device: Apple M4
0.00.605.986 I ggml_metal_init: picking default device: Apple M4
0.00.608.027 I ggml_metal_init: using embedded metal library
0.00.614.549 I ggml_metal_init: GPU name:   Apple M4
0.00.614.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.554 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.555 I ggml_metal_init: simdgroup reduction   = true
0.00.614.555 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.555 I ggml_metal_init: has residency sets    = true
0.00.614.556 I ggml_metal_init: has bfloat            = true
0.00.614.556 I ggml_metal_init: use bfloat            = true
0.00.614.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.856 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.866 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.903 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.534 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.697.536 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.697.536 I llama_init_from_model: graph nodes  = 967
0.00.697.537 I llama_init_from_model: graph splits = 2
0.00.697.543 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.677 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.915 I main: llama threadpool init, n_threads = 4
0.00.761.954 I 
0.00.761.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.967 I 
0.00.762.119 I sampler seed: 1234
0.00.762.123 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.166 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.169 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.170 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.603.819 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49511.85 tokens per second)
0.01.603.820 I llama_perf_context_print:        load time =     752.35 ms
0.01.603.820 I llama_perf_context_print: prompt eval time =      51.68 ms /     7 tokens (    7.38 ms per token,   135.45 tokens per second)
0.01.603.821 I llama_perf_context_print:        eval time =     787.33 ms /    63 runs   (   12.50 ms per token,    80.02 tokens per second)
0.01.603.821 I llama_perf_context_print:       total time =     842.62 ms /    70 tokens
0.01.604.056 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.111s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.104 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.105 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.105 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.106 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.562 I llama_model_loader: - type  f32:  194 tensors
0.00.024.563 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.563 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.564 I print_info: file format = GGUF V3 (latest)
0.00.024.564 I print_info: file type   = Q5_K - Medium
0.00.024.565 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.601 I load: special tokens cache size = 25
0.00.038.531 I load: token to piece cache size = 0.2984 MB
0.00.038.535 I print_info: arch             = gptneox
0.00.038.535 I print_info: vocab_only       = 0
0.00.038.535 I print_info: n_ctx_train      = 2048
0.00.038.536 I print_info: n_embd           = 2048
0.00.038.536 I print_info: n_layer          = 24
0.00.038.539 I print_info: n_head           = 16
0.00.038.540 I print_info: n_head_kv        = 16
0.00.038.540 I print_info: n_rot            = 32
0.00.038.540 I print_info: n_swa            = 0
0.00.038.541 I print_info: n_embd_head_k    = 128
0.00.038.541 I print_info: n_embd_head_v    = 128
0.00.038.542 I print_info: n_gqa            = 1
0.00.038.542 I print_info: n_embd_k_gqa     = 2048
0.00.038.543 I print_info: n_embd_v_gqa     = 2048
0.00.038.544 I print_info: f_norm_eps       = 1.0e-05
0.00.038.544 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.544 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.544 I print_info: f_logit_scale    = 0.0e+00
0.00.038.547 I print_info: n_ff             = 8192
0.00.038.548 I print_info: n_expert         = 0
0.00.038.549 I print_info: n_expert_used    = 0
0.00.038.549 I print_info: causal attn      = 1
0.00.038.549 I print_info: pooling type     = 0
0.00.038.549 I print_info: rope type        = 2
0.00.038.549 I print_info: rope scaling     = linear
0.00.038.550 I print_info: freq_base_train  = 10000.0
0.00.038.550 I print_info: freq_scale_train = 1
0.00.038.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.552 I print_info: rope_finetuned   = unknown
0.00.038.552 I print_info: ssm_d_conv       = 0
0.00.038.552 I print_info: ssm_d_inner      = 0
0.00.038.552 I print_info: ssm_d_state      = 0
0.00.038.552 I print_info: ssm_dt_rank      = 0
0.00.038.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.552 I print_info: model type       = 1.4B
0.00.038.553 I print_info: model params     = 1.41 B
0.00.038.553 I print_info: general.name     = 1.4B
0.00.038.553 I print_info: vocab type       = BPE
0.00.038.554 I print_info: n_vocab          = 50304
0.00.038.554 I print_info: n_merges         = 50009
0.00.038.554 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.555 I print_info: LF token         = 187 ''
0.00.038.556 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.556 I print_info: max token length = 1024
0.00.038.556 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.173 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.178 I load_tensors: offloading output layer to GPU
0.00.598.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.201 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.204 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.643 I llama_init_from_model: n_seq_max     = 1
0.00.599.645 I llama_init_from_model: n_ctx         = 128
0.00.599.645 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.646 I llama_init_from_model: n_batch       = 128
0.00.599.646 I llama_init_from_model: n_ubatch      = 128
0.00.599.646 I llama_init_from_model: flash_attn    = 0
0.00.599.648 I llama_init_from_model: freq_base     = 10000.0
0.00.599.648 I llama_init_from_model: freq_scale    = 1
0.00.599.649 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.650 I ggml_metal_init: allocating
0.00.599.694 I ggml_metal_init: found device: Apple M4
0.00.599.705 I ggml_metal_init: picking default device: Apple M4
0.00.601.160 I ggml_metal_init: using embedded metal library
0.00.607.214 I ggml_metal_init: GPU name:   Apple M4
0.00.607.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.220 I ggml_metal_init: simdgroup reduction   = true
0.00.607.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.220 I ggml_metal_init: has residency sets    = true
0.00.607.220 I ggml_metal_init: has bfloat            = true
0.00.607.221 I ggml_metal_init: use bfloat            = true
0.00.607.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.286 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.771 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.778 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.824 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.948 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.630.950 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.630.950 I llama_init_from_model: graph nodes  = 967
0.00.630.950 I llama_init_from_model: graph splits = 2
0.00.630.953 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.752 I 
0.00.665.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.832 I perplexity: tokenizing the input ..
0.00.671.227 I perplexity: tokenization took 5.393 ms
0.00.671.233 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.652 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.985 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.010 I llama_perf_context_print:        load time =     656.68 ms
0.00.813.011 I llama_perf_context_print: prompt eval time =     139.87 ms /   128 tokens (    1.09 ms per token,   915.16 tokens per second)
0.00.813.012 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.012 I llama_perf_context_print:       total time =     147.26 ms /   129 tokens
0.00.813.403 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.076s
sys	0m0.141s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.270 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.273 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.273 I llama_model_loader: - type  f32:  194 tensors
0.00.026.273 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.274 I print_info: file format = GGUF V3 (latest)
0.00.026.274 I print_info: file type   = Q6_K
0.00.026.275 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.063 I load: special tokens cache size = 25
0.00.040.017 I load: token to piece cache size = 0.2984 MB
0.00.040.020 I print_info: arch             = gptneox
0.00.040.020 I print_info: vocab_only       = 0
0.00.040.020 I print_info: n_ctx_train      = 2048
0.00.040.020 I print_info: n_embd           = 2048
0.00.040.021 I print_info: n_layer          = 24
0.00.040.023 I print_info: n_head           = 16
0.00.040.024 I print_info: n_head_kv        = 16
0.00.040.024 I print_info: n_rot            = 32
0.00.040.024 I print_info: n_swa            = 0
0.00.040.025 I print_info: n_embd_head_k    = 128
0.00.040.025 I print_info: n_embd_head_v    = 128
0.00.040.027 I print_info: n_gqa            = 1
0.00.040.028 I print_info: n_embd_k_gqa     = 2048
0.00.040.029 I print_info: n_embd_v_gqa     = 2048
0.00.040.034 I print_info: f_norm_eps       = 1.0e-05
0.00.040.034 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.034 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.035 I print_info: f_logit_scale    = 0.0e+00
0.00.040.035 I print_info: n_ff             = 8192
0.00.040.036 I print_info: n_expert         = 0
0.00.040.036 I print_info: n_expert_used    = 0
0.00.040.037 I print_info: causal attn      = 1
0.00.040.037 I print_info: pooling type     = 0
0.00.040.037 I print_info: rope type        = 2
0.00.040.038 I print_info: rope scaling     = linear
0.00.040.038 I print_info: freq_base_train  = 10000.0
0.00.040.038 I print_info: freq_scale_train = 1
0.00.040.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.039 I print_info: rope_finetuned   = unknown
0.00.040.039 I print_info: ssm_d_conv       = 0
0.00.040.039 I print_info: ssm_d_inner      = 0
0.00.040.039 I print_info: ssm_d_state      = 0
0.00.040.041 I print_info: ssm_dt_rank      = 0
0.00.040.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.041 I print_info: model type       = 1.4B
0.00.040.041 I print_info: model params     = 1.41 B
0.00.040.041 I print_info: general.name     = 1.4B
0.00.040.042 I print_info: vocab type       = BPE
0.00.040.042 I print_info: n_vocab          = 50304
0.00.040.042 I print_info: n_merges         = 50009
0.00.040.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.044 I print_info: LF token         = 187 ''
0.00.040.044 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.045 I print_info: max token length = 1024
0.00.040.045 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.651 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.674 I load_tensors: offloading output layer to GPU
0.00.643.675 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.713 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.643.715 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.644.954 I llama_init_from_model: n_seq_max     = 1
0.00.644.957 I llama_init_from_model: n_ctx         = 2048
0.00.644.958 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.644.958 I llama_init_from_model: n_batch       = 2048
0.00.644.959 I llama_init_from_model: n_ubatch      = 512
0.00.644.959 I llama_init_from_model: flash_attn    = 0
0.00.644.962 I llama_init_from_model: freq_base     = 10000.0
0.00.644.963 I llama_init_from_model: freq_scale    = 1
0.00.644.967 I ggml_metal_init: allocating
0.00.645.038 I ggml_metal_init: found device: Apple M4
0.00.645.053 I ggml_metal_init: picking default device: Apple M4
0.00.646.803 I ggml_metal_init: using embedded metal library
0.00.653.208 I ggml_metal_init: GPU name:   Apple M4
0.00.653.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.214 I ggml_metal_init: simdgroup reduction   = true
0.00.653.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.215 I ggml_metal_init: has residency sets    = true
0.00.653.215 I ggml_metal_init: has bfloat            = true
0.00.653.215 I ggml_metal_init: use bfloat            = true
0.00.653.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.206 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.246 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.926 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.927 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.928 I llama_init_from_model: graph nodes  = 967
0.00.734.928 I llama_init_from_model: graph splits = 2
0.00.734.933 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.722 I main: llama threadpool init, n_threads = 4
0.00.795.775 I 
0.00.795.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.793 I 
0.00.795.928 I sampler seed: 1234
0.00.795.933 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.944 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.945 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.945 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.716.344 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.716.345 I llama_perf_context_print:        load time =     785.22 ms
0.01.716.346 I llama_perf_context_print: prompt eval time =      54.20 ms /     7 tokens (    7.74 ms per token,   129.16 tokens per second)
0.01.716.347 I llama_perf_context_print:        eval time =     863.28 ms /    63 runs   (   13.70 ms per token,    72.98 tokens per second)
0.01.716.348 I llama_perf_context_print:       total time =     921.31 ms /    70 tokens
0.01.716.607 I ggml_metal_free: deallocating

real	0m1.735s
user	0m0.111s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4710 (8a8c4ceb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.567 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.554 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.565 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.571 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.361 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.365 I llama_model_loader: - type  f32:  194 tensors
0.00.027.365 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.366 I print_info: file format = GGUF V3 (latest)
0.00.027.367 I print_info: file type   = Q6_K
0.00.027.368 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.747 I load: special tokens cache size = 25
0.00.041.832 I load: token to piece cache size = 0.2984 MB
0.00.041.837 I print_info: arch             = gptneox
0.00.041.837 I print_info: vocab_only       = 0
0.00.041.838 I print_info: n_ctx_train      = 2048
0.00.041.838 I print_info: n_embd           = 2048
0.00.041.838 I print_info: n_layer          = 24
0.00.041.843 I print_info: n_head           = 16
0.00.041.844 I print_info: n_head_kv        = 16
0.00.041.844 I print_info: n_rot            = 32
0.00.041.844 I print_info: n_swa            = 0
0.00.041.844 I print_info: n_embd_head_k    = 128
0.00.041.844 I print_info: n_embd_head_v    = 128
0.00.041.845 I print_info: n_gqa            = 1
0.00.041.846 I print_info: n_embd_k_gqa     = 2048
0.00.041.846 I print_info: n_embd_v_gqa     = 2048
0.00.041.847 I print_info: f_norm_eps       = 1.0e-05
0.00.041.848 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.848 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.848 I print_info: f_logit_scale    = 0.0e+00
0.00.041.848 I print_info: n_ff             = 8192
0.00.041.849 I print_info: n_expert         = 0
0.00.041.849 I print_info: n_expert_used    = 0
0.00.041.852 I print_info: causal attn      = 1
0.00.041.852 I print_info: pooling type     = 0
0.00.041.852 I print_info: rope type        = 2
0.00.041.852 I print_info: rope scaling     = linear
0.00.041.853 I print_info: freq_base_train  = 10000.0
0.00.041.853 I print_info: freq_scale_train = 1
0.00.041.853 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.854 I print_info: rope_finetuned   = unknown
0.00.041.854 I print_info: ssm_d_conv       = 0
0.00.041.854 I print_info: ssm_d_inner      = 0
0.00.041.855 I print_info: ssm_d_state      = 0
0.00.041.855 I print_info: ssm_dt_rank      = 0
0.00.041.855 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.855 I print_info: model type       = 1.4B
0.00.041.855 I print_info: model params     = 1.41 B
0.00.041.856 I print_info: general.name     = 1.4B
0.00.041.856 I print_info: vocab type       = BPE
0.00.041.856 I print_info: n_vocab          = 50304
0.00.041.856 I print_info: n_merges         = 50009
0.00.041.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.858 I print_info: LF token         = 187 ''
0.00.041.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.858 I print_info: max token length = 1024
0.00.041.858 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.292.283 I load_tensors: offloading 24 repeating layers to GPU
0.00.292.288 I load_tensors: offloading output layer to GPU
0.00.292.289 I load_tensors: offloaded 25/25 layers to GPU
0.00.292.314 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.292.318 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.293.691 I llama_init_from_model: n_seq_max     = 1
0.00.293.694 I llama_init_from_model: n_ctx         = 128
0.00.293.694 I llama_init_from_model: n_ctx_per_seq = 128
0.00.293.695 I llama_init_from_model: n_batch       = 128
0.00.293.695 I llama_init_from_model: n_ubatch      = 128
0.00.293.695 I llama_init_from_model: flash_attn    = 0
0.00.293.696 I llama_init_from_model: freq_base     = 10000.0
0.00.293.697 I llama_init_from_model: freq_scale    = 1
0.00.293.697 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.293.699 I ggml_metal_init: allocating
0.00.293.769 I ggml_metal_init: found device: Apple M4
0.00.293.779 I ggml_metal_init: picking default device: Apple M4
0.00.295.163 I ggml_metal_init: using embedded metal library
0.00.301.319 I ggml_metal_init: GPU name:   Apple M4
0.00.301.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.301.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.301.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.301.325 I ggml_metal_init: simdgroup reduction   = true
0.00.301.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.301.326 I ggml_metal_init: has residency sets    = true
0.00.301.326 I ggml_metal_init: has bfloat            = true
0.00.301.326 I ggml_metal_init: use bfloat            = true
0.00.301.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.301.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.318.187 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.321.597 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.321.601 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.321.646 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.324.819 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.324.820 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.324.821 I llama_init_from_model: graph nodes  = 967
0.00.324.821 I llama_init_from_model: graph splits = 2
0.00.324.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.324.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.307 I 
0.00.356.371 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.377 I perplexity: tokenizing the input ..
0.00.361.581 I perplexity: tokenization took 5.202 ms
0.00.361.584 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.500.669 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.502.020 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.502.041 I llama_perf_context_print:        load time =     344.72 ms
0.00.502.042 I llama_perf_context_print: prompt eval time =     138.85 ms /   128 tokens (    1.08 ms per token,   921.83 tokens per second)
0.00.502.043 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.502.043 I llama_perf_context_print:       total time =     145.74 ms /   129 tokens
0.00.502.427 I ggml_metal_free: deallocating

real	0m0.518s
user	0m0.076s
sys	0m0.098s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4710 (8a8c4ceb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143a053a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143a05a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143a05e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143a062f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143a06760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143a06bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143a071a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143a07750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143a07d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143a08200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143a08700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143a08c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143a09720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143a09ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143a0a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143a0ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143a0b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143a0bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143a0c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143a0cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143a0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143a0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143a0e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143a0e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143a0f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143a0f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143a0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143a10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143a10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143a10d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143a11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143a114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143a11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143a122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143a12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143a12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143a12ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143a13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143a13800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143a13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143a14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143a145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143a14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143a14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143a151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143a157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143a15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143a16720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143a16d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143a17340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143a17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143a17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143a18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143a18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143a19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143a19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143a19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143a19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143a1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143a1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143a1b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143a1b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143a1b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143a1be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143a1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143a1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143a1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143a1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143a1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143a1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143a1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143a1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143a1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143a1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143a1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143a1fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143a20240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143a20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143a20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143a21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143a21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143a21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143a22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143a22770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143a22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143a23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143a23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143a23cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143a24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143a24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143a24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143a251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143a25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143a25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143a261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143a26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143a16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143a26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143a27350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143a278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143a27df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143a28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143a28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143a28de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143a29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143a29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143a29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143a2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143a2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143a2adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143a2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143a2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143a2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143a2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143a2c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143a2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143a2cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143a2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143a2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143a2dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143a2e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143a2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143a2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143a2efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143a2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143a2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143a2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143a30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143a30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143a31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143a314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143a31980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143a31e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143a322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143a32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143a32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143a330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143a33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143a339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143a33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143a34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143a347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143a34c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143a35100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143a355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143a35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143a35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143a36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143a36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143a36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143a37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143a37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143a37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143a37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143a383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143a38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143a38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143a391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143a39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143a39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143a39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143a3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143a3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143a3ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143a3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143a3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143a3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143a3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143a3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143a3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143a3cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143a3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143a3d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143a3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143a3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143a3e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143a3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143a3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143a3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143a3fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143a400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143a40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143a40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143a40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143a41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143a417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143a41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143a42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143a425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143a42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143a42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143a43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143a43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143a43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143a44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143a44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143a44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143a45490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143a45c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143a46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143a463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143a469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143a47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143a477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143a47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143a48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143a485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143a48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143a492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143a49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143a49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143a4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143a4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143a4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143a4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143a4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143a4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143a4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143a4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143a4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143a4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143a4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143a4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143a4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143a4e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143a4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143a4f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143a4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143a4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143a50260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143a507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143a50d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143a51250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143a517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143a51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143a52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143a52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143a52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143a53230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143a53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143a53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143a54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143a54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143a54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143a55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143a55760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143a55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143a56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143a56750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143a56ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143a571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143a57740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143a57c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143a581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143a58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143a58c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143a591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143a59720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143a59c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143a5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143a5a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143a5ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143a5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143a5b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143a5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143a5c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143a5c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143a5c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143a5ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143a5d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143a5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143a5dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143a5e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143a5e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143a5e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143a5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143a5f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143a5f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143a5fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143a601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143a608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143a60ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143a61710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143a61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143a620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143a628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143a62ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143a631b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.737.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11b404bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11b405040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11b4054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11b405920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11b405d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11b406200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11b406670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11b406ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11b406f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11b4073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11b407830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11b407f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11b408a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11b4091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11b409a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11b40a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11b40a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11b40af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11b40b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11b40bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11b40c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11b40cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11b40d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11b40da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11b40e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11b40e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11b40e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11b40eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11b40efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11b40f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11b40f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11b40fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11b410230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11b4104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11b410960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11b410dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11b411240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11b4116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11b411b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11b411f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11b412400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11b412870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11b412ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11b413150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11b4135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11b413a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11b413ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11b414310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11b414780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11b414bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11b415060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11b4154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11b415940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11b415db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11b416220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11b416690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11b416c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11b417100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11b417570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11b4179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11b417e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11b4182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11b418730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11b418ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11b419010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11b419480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11b4198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11b419d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11b41a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11b41a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11b41aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11b41af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11b41b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11b41b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11b41bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11b41c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11b41c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11b41c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11b41ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11b41d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11b41d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11b41db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11b41dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11b41e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11b41e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11b41ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11b41f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11b41f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11b41fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11b41ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11b420370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11b4207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11b420c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11b4210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11b421530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11b4219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11b421e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11b422280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11b4226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11b422b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11b422fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11b423440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11b4238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11b423d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11b424190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11b424600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11b424a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11b424ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11b425350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11b4257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11b425c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11b4260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11b426510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11b426980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11b426df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11b427260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11b4276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11b427b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11b427fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11b428420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11b428890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11b428d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11b429170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11b4295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11b429a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11b429ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11b42a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11b42a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11b42ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11b42b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11b42b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11b42b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11b42bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11b42c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11b42c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11b42cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11b42cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11b42d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11b42d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11b42dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11b42e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11b42e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11b42ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11b42eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11b42f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11b42f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11b42fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11b430060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11b4304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11b430940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11b430db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11b431220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11b431690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11b431b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11b431f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11b4323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11b432850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11b432cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11b433130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11b4335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11b433a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11b433e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11b4342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11b434760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11b434bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11b435040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11b435c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11b435f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11b4361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11b436660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11b436ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11b436f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11b4373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11b437820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11b437c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11b438100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11b438570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11b4389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11b438e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11b4392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11b439730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11b439ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11b43a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11b43a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11b43a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11b43ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11b43b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11b43b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11b43bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11b43bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11b43c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11b43c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11b43cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11b43d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11b43d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11b43d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11b43de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11b43e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11b43e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11b43eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11b43eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11b43f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11b43f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11b43fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11b440340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11b4407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11b440c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11b441090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11b4415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11b441ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11b442630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11b4428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11b442eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11b443470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11b443a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11b443ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11b4445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11b444b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11b445130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11b4456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11b445cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11b446270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11b446830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11b446df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11b4473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11b447970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11b447f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11b4484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11b448ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11b449070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11b449630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11b449bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11b44a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11b44a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11b44ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11b44b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11b44b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11b44be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11b44c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11b44c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11b44cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11b44d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11b44db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11b44e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11b44e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11b44ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11b44f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11b44f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11b44fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11b450370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11b450930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11b450ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11b4514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11b451a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11b452030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11b4525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11b452bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11b453170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11b453730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11b453cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11b4542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11b454870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11b454e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11b4553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11b4559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11b455f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11b456530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11b456af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11b456ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11b4574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b4579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11b457ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11b4583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11b4588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11b458df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11b4592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b4597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11b459cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11b45a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11b45a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11b45abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11b45b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11b45b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11b45c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11b45c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11b45ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11b45d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11b45d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11b45e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11b45e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11b45e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11b3044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11b304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11b304dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11b305230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11b3056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11b305b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11b305f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11b3063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11b306860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11b306cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11b307140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11b307860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11b308380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11b308b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11b309340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11b309a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11b30a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11b30a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11b30afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11b30b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11b30be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11b30c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11b30cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11b30d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11b30da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11b30dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11b30e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11b30e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11b30e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11b30ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11b30f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11b30f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11b30fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11b30fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11b3102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11b310710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11b310b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11b310ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11b311460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11b3118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11b311d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11b3121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11b312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11b312a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11b312f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11b313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11b3137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11b313c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11b3140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11b314530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11b3149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11b314e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11b315280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11b3156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11b315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11b315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11b316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11b316a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11b316eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11b317320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11b317790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11b317c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11b318070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11b3184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11b318950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11b318dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11b319230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11b3196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11b319b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11b319f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11b31a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11b31a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11b31acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11b31b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11b31b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11b31ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11b31be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11b31c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11b31c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11b31cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11b31d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11b31d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11b31d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11b31dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11b31e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11b31e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11b31eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11b31ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11b31f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11b31f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11b31fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11b320120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11b320590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11b320a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11b320e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11b3212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11b321750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11b321bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11b322030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11b3224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11b322910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11b322d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11b3231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11b323a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11b323d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11b3241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11b324620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11b324a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11b324f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11b325370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11b3257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11b325c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11b3260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11b326530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11b3269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11b326e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11b327280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11b3276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11b327b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11b327fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11b328440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11b3288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11b328d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11b329190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11b329600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11b329a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11b329ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11b32a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11b32a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11b32ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11b32b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11b32b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11b32b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11b32bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11b32c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11b32c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11b32cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11b32cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11b32d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11b32d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11b32dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11b32e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11b32e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11b32ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11b32eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11b32f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11b32f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11b32fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11b330080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11b3304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11b330960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11b330dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11b331240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11b3316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11b331b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11b331f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11b332400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11b332870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11b332ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11b333150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11b3335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11b333a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11b333ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11b334310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11b334780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11b334bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11b335060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11b3354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11b335940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11b335db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11b336220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11b336690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11b336b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11b336f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11b3373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11b337850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11b337cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11b338130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11b3385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11b338a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11b338e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11b3392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11b339760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11b339bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11b33a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11b33a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11b33a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11b33ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11b33b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11b33b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11b33bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11b33bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11b33c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11b33c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11b33cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11b33d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11b33d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11b33d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11b33de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11b33e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11b33e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11b33ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11b33f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11b33f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11b33f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11b33fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11b3401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11b340650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11b340ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11b340f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11b341ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11b341d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11b342030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11b3424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11b342910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11b342d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11b3431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11b343660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11b343ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11b343f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11b3443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11b344820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11b344c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11b345100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11b345570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11b3459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11b345e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11b3462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11b346730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11b346ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11b347010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11b347480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11b3478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11b347d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11b3481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11b348640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11b348ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11b348f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11b349390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11b349800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11b349c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11b34a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11b34a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11b34a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11b34ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11b34b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11b34b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11b34bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11b34bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11b34c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11b34c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11b34cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11b34d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11b34d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11b34da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11b34df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11b34e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11b34e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11b34ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11b34f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11b34f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11b34f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11b34fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11b350280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11b3506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11b350b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11b350fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11b351440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11b3518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11b351d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b352190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11b352600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11b352a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11b352ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11b353350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11b3537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b353c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11b3540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11b354510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11b354980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11b354df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11b355260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11b3556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11b356140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11b356860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11b356f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11b3576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11b357960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11b357dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11b3583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11b3589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.797s
user	0m0.280s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4710 (8a8c4ceb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15060b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15060bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15060c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15060c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15060cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15060d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15060d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15060dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15060e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15060e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15060ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15060f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15060fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150610510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150610d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150611b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150612280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1506129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150613170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150613fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1506146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150614f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150615690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150615f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1506173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150617b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1506183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150618900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150618bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150619500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1506199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15061a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15061a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15061ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15061b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15061b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15061b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15061be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15061c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15061cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15061d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15061d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15061df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15061e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15061ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15061f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15061f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15061fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1506202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1506205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1506213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150621b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150622450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1506228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150622d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150623230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1506236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150624010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1506244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150624950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150624df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150626330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150626880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150627320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150627dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150628310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150628860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150629850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15062a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15062a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15062ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15062b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15062b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15062bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15062c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15062c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15062cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15061ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15062d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15062d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15062dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15062e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15062e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15062eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15062f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15062f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15062fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150631400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150631ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150632340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1506327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150632c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150633120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1506335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150633f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1506343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150634840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150634ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150635620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150635f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1506368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150636d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1506371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150637fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150638900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150638da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150639240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1506396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150639b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15063a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15063a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15063a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15063ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15063b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15063b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15063bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15063c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15063c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15063c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15063ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15063d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15063d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15063dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15063e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15063e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15063ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15063eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15063f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15063f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15063fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150640140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1506405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1506413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x150641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1506421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150642640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150642ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150643420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1506438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150643d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150644200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1506446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150644b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150644fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150645480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150645920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150645dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150646260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150646ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150647040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1506474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150647980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150647e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1506482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150648760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150648c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1506490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1506495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150649b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15064a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15064a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15064a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15064aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15064b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15064bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15064c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15064c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15064ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15064d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15064d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15064de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15064e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15064e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15064ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15064f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15064f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15064fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1506503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150650900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150650e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1506513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1506518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150651e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1506528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150652e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150653380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1506538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150653e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1506548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150654e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150655360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1506558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150655e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150656350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1506568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150656df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150657340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150657890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150657de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150658330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150658880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150658dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150659320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150659dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15065a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15065a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15065adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15065b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15065b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15065bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15065c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15065c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15065cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15065d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15065d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15065dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15065e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15065e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15065ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15065f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15065f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15065fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1506602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150660800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150660d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1506612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1506617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150661d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1506621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150662680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150662b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150662fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150663460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150663900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150663da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150664240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1506646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150664b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150665020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1506654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150665960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150665e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1506662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1506667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150666f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150667630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150668470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150668730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150668f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1506691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1506697f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.361 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15070adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15070b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15070b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15070bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15070bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15070c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15070c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15070ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15070d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15070d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15070dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15070e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15070ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15070f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15070fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150710350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150710a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150711190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1507118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150712080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1507127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150712ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1507135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150713d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150714420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1507146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1507149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1507156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150715bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150716100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150716570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150716830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150716ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150717110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150717670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150717b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150718070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150718570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150718a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150718f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150719470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15071a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15071a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15071abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15071b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15071b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15071b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15071bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15071c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15071c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15071cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15071d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15071d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15071da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15071e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15071e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15071eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15071f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15071f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15071fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15071ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1507203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150720860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150720d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1507211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150721640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150721ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150721f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150722970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150722ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150723410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150723960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150723eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150724400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150724950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150724ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1507253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150725940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150725e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1507263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150726e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1507273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x150727920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150727e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1507283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x150728910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150728e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1507293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150729900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150729e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15072a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15072a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15072ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15072b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15072b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15072be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15072c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15072c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15072ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15072d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15072d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15072de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15072e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15072e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15072ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15072f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15072f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15072fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1507301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150730680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150730b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150731460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150731900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150731da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150732240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1507326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150732b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150733020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1507334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150733960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150733e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1507342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150734740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150734be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150735080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150735520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1507359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150735e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1507367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150736c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1507370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150737580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150737a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150737ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150738360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150738800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150738ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150739140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1507395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150739a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150739f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15073a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15073a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15073ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15073b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15073b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15073bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15073bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15073c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15073c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15073cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15073d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15073d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15073db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15073dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15073e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15073e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15073edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15073f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15073f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15073fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150740040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1507404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150740980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150740e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1507412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150741760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150741c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1507420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1507429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150742e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150743320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1507437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150743c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150744100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1507445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150744a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150744ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150745380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150745820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150745cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150746160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150746600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150746aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150746ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150747540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150747a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150747fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1507482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1507488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150748ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1507494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150749cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15074a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15074a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15074aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15074b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15074b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15074bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15074c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15074c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15074cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15074d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15074d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15074ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15074e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15074e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15074eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15074f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15074f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15074fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1507502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150750830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150750d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1507512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150751820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150751d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1507522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150752810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150752d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1507532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150753800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150753d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1507542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1507547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150754d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150755290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1507557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150755d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150756280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1507567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150756d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150757270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1507577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150757d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150758260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1507587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150758d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150759250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1507597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150759cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15075a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15075a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15075ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15075b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15075b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15075bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15075c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15075c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15075ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15075d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15075d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15075dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15075e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15075e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15075eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15075f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15075f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15075fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150760080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150760520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1507609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150760e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150761300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1507617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150761c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1507620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150762580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150762a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150762ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150763360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150763800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150763ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1507641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150764910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150765030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150765750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150766130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150766920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150766be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1507671f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147c046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147c04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147c04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147c05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147c058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147c05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147c06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147c065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147c06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147c06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147c07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147c07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147c08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147c08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147c09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147c09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147c0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147c0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147c0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147c0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147c0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147c0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147c0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147c0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147c0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147c0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147c0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147c0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147c0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147c0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147c0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147c0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147c0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147c10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147c104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147c10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147c10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147c111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147c11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147c11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147c11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147c123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147c12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147c12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147c13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147c13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147c139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147c13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147c142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147c14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147c14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147c15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147c15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147c158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147c15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147c16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147c16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147c170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147c17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147c17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147c17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147c18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147c186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147c18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147c18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147c19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147c198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147c19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147c1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147c1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147c1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147c1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147c1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147c1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147c1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147c1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147c1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147c1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147c1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147c1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147c1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147c1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147c1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147c1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147c1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147c1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147c1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147c1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147c1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147c1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147c20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147c20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147c20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147c21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147c214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147c21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147c22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147c226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147c22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147c22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147c233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147c23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147c23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147c243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147c24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147c24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147c25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147c25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147c259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147c25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147c262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147c26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147c26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147c27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147c27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147c278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147c27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147c281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147c28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147c28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147c28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147c29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147c29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147c29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147c2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147c2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147c2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147c2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147c2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147c2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147c2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147c2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147c2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147c2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147c2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147c2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147c2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147c2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147c2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147c2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147c2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147c2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147c2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147c2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147c2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147c2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147c30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147c306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147c30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147c30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147c31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147c318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147c31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147c32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147c32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147c32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147c32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147c33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147c337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147c33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147c340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147c34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147c34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147c34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147c35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147c356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147c35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147c35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147c36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147c36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147c36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147c37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147c375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147c37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147c37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147c38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147c387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147c38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147c39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147c394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147c39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147c39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147c3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147c3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147c3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147c3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147c3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147c3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147c3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147c3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147c3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147c3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147c3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147c3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147c3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147c3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147c3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147c3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147c3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147c3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147c3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147c3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147c3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147c3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147c403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147c40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147c40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147c41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147c41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147c41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147c42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147c426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147c42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147c42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147c433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147c43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147c43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147c44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147c445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147c44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147c44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147c45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147c45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147c45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147c46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147c464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147c46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147c46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147c47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147c47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147c47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147c47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147c483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147c48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147c48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147c49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147c49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147c49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147c49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147c4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147c4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147c4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147c4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147c4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147c4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147c4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147c4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147c4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147c4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147c4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147c4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147c4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147c4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147c4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147c4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147c4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147c4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147c4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147c4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147c4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147c50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147c50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147c508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147c50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147c511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147c51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147c51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147c51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147c52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147c52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147c52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147c530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147c53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147c539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147c53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147c542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147c54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147c54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147c54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147c55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147c558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147c56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147c56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147c57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147c578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147c57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147c57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147c585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147c58be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.960s
user	0m0.231s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.80 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.24 sec*proc (2 tests)

Total Test time (real) =   2.26 sec
        2.28 real         0.52 user         0.29 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.57 real         0.13 user         0.08 sys
```
