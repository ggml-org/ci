Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.623s
user	0m0.925s
sys	0m1.295s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Built target llava
[ 27%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-barrier
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-fns
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-embedding
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-infill
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-cli
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Generating index.html.gz.hpp
[ 81%] Built target llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-passkey
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-quantize
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Built target llama-save-load-state
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-llava-clip-quantize-cli
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-vdot
[ 98%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.180s
user	0m6.678s
sys	0m10.099s

main: quantize time =  2345.37 ms
main:    total time =  2345.37 ms

main: quantize time =  2195.29 ms
main:    total time =  2195.29 ms

main: quantize time =  2638.26 ms
main:    total time =  2638.26 ms

main: quantize time =  2064.39 ms
main:    total time =  2064.39 ms

main: quantize time =  1390.39 ms
main:    total time =  1390.39 ms

main: quantize time =  6333.47 ms
main:    total time =  6333.47 ms

main: quantize time =  6219.88 ms
main:    total time =  6219.88 ms

main: quantize time =  7626.72 ms
main:    total time =  7626.72 ms

main: quantize time =  6137.83 ms
main:    total time =  6137.83 ms

main: quantize time =  4576.86 ms
main:    total time =  4576.86 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.228 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.384 I main: llama backend init
0.00.000.389 I main: load the model and apply lora adapter, if any
0.00.042.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.055.342 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.055.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.055.366 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.055.366 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.055.367 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.055.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.055.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.055.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.055.372 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.055.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.055.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.055.373 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.055.374 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.055.375 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.055.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.055.386 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.055.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.065.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.687 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.073.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.693 I llama_model_loader: - type  f32:  194 tensors
0.00.073.693 I llama_model_loader: - type  f16:   98 tensors
0.00.073.695 I print_info: file format = GGUF V3 (latest)
0.00.073.696 I print_info: file type   = all F32 (guessed)
0.00.073.697 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.087.850 I load: special tokens cache size = 25
0.00.096.907 I load: token to piece cache size = 0.2984 MB
0.00.096.912 I print_info: arch             = gptneox
0.00.096.913 I print_info: vocab_only       = 0
0.00.096.913 I print_info: n_ctx_train      = 2048
0.00.096.913 I print_info: n_embd           = 2048
0.00.096.913 I print_info: n_layer          = 24
0.00.096.919 I print_info: n_head           = 16
0.00.096.920 I print_info: n_head_kv        = 16
0.00.096.920 I print_info: n_rot            = 32
0.00.096.920 I print_info: n_swa            = 0
0.00.096.921 I print_info: n_embd_head_k    = 128
0.00.096.921 I print_info: n_embd_head_v    = 128
0.00.096.922 I print_info: n_gqa            = 1
0.00.096.923 I print_info: n_embd_k_gqa     = 2048
0.00.096.924 I print_info: n_embd_v_gqa     = 2048
0.00.096.925 I print_info: f_norm_eps       = 1.0e-05
0.00.096.926 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.926 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.926 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.926 I print_info: f_logit_scale    = 0.0e+00
0.00.096.931 I print_info: n_ff             = 8192
0.00.096.931 I print_info: n_expert         = 0
0.00.096.931 I print_info: n_expert_used    = 0
0.00.096.931 I print_info: causal attn      = 1
0.00.096.932 I print_info: pooling type     = 0
0.00.096.932 I print_info: rope type        = 2
0.00.096.932 I print_info: rope scaling     = linear
0.00.096.932 I print_info: freq_base_train  = 10000.0
0.00.096.933 I print_info: freq_scale_train = 1
0.00.096.933 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.933 I print_info: rope_finetuned   = unknown
0.00.096.933 I print_info: ssm_d_conv       = 0
0.00.096.936 I print_info: ssm_d_inner      = 0
0.00.096.936 I print_info: ssm_d_state      = 0
0.00.096.937 I print_info: ssm_dt_rank      = 0
0.00.096.937 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.937 I print_info: model type       = 1.4B
0.00.096.937 I print_info: model params     = 1.41 B
0.00.096.938 I print_info: general.name     = 1.4B
0.00.096.939 I print_info: vocab type       = BPE
0.00.096.939 I print_info: n_vocab          = 50304
0.00.096.939 I print_info: n_merges         = 50009
0.00.096.939 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.940 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.940 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.940 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.940 I print_info: LF token         = 187 'Ċ'
0.00.096.941 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.941 I print_info: max token length = 1024
0.00.096.942 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.139.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.139.525 I load_tensors: offloading output layer to GPU
0.00.139.526 I load_tensors: offloaded 25/25 layers to GPU
0.00.139.549 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.139.551 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.139.909 I llama_init_from_model: n_seq_max     = 1
0.00.139.910 I llama_init_from_model: n_ctx         = 2048
0.00.139.910 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.139.911 I llama_init_from_model: n_batch       = 2048
0.00.139.911 I llama_init_from_model: n_ubatch      = 512
0.00.139.911 I llama_init_from_model: flash_attn    = 0
0.00.139.912 I llama_init_from_model: freq_base     = 10000.0
0.00.139.912 I llama_init_from_model: freq_scale    = 1
0.00.139.912 I ggml_metal_init: allocating
0.00.139.931 I ggml_metal_init: found device: Apple M4
0.00.139.936 I ggml_metal_init: picking default device: Apple M4
0.00.140.522 I ggml_metal_init: using embedded metal library
0.00.143.034 I ggml_metal_init: GPU name:   Apple M4
0.00.143.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.143.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.143.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.143.038 I ggml_metal_init: simdgroup reduction   = true
0.00.143.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.143.038 I ggml_metal_init: has residency sets    = true
0.00.143.038 I ggml_metal_init: has bfloat            = true
0.00.143.038 I ggml_metal_init: use bfloat            = true
0.00.143.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.143.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.157.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.186.074 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.186.087 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.186.123 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.190.344 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.190.347 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.190.348 I llama_init_from_model: graph nodes  = 967
0.00.190.348 I llama_init_from_model: graph splits = 2
0.00.190.351 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.190.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.190.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.257.574 I main: llama threadpool init, n_threads = 4
0.00.257.608 I 
0.00.257.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.257.635 I 
0.00.257.674 I sampler seed: 1234
0.00.257.678 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.257.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.257.704 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.257.704 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.101.080 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.02.101.080 I llama_perf_context_print:        load time =     213.75 ms
0.02.101.081 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.91 tokens per second)
0.02.101.082 I llama_perf_context_print:        eval time =    1796.86 ms /    63 runs   (   28.52 ms per token,    35.06 tokens per second)
0.02.101.082 I llama_perf_context_print:       total time =    1844.34 ms /    70 tokens
0.02.101.384 I ggml_metal_free: deallocating

real	0m2.403s
user	0m0.130s
sys	0m0.135s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.063 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.103 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.009.567 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.917 I llama_model_loader: - type  f32:  194 tensors
0.00.027.917 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.918 I print_info: file format = GGUF V3 (latest)
0.00.027.920 I print_info: file type   = Q8_0
0.00.027.921 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.920 I load: special tokens cache size = 25
0.00.042.005 I load: token to piece cache size = 0.2984 MB
0.00.042.008 I print_info: arch             = gptneox
0.00.042.008 I print_info: vocab_only       = 0
0.00.042.008 I print_info: n_ctx_train      = 2048
0.00.042.008 I print_info: n_embd           = 2048
0.00.042.009 I print_info: n_layer          = 24
0.00.042.013 I print_info: n_head           = 16
0.00.042.016 I print_info: n_head_kv        = 16
0.00.042.016 I print_info: n_rot            = 32
0.00.042.016 I print_info: n_swa            = 0
0.00.042.016 I print_info: n_embd_head_k    = 128
0.00.042.016 I print_info: n_embd_head_v    = 128
0.00.042.017 I print_info: n_gqa            = 1
0.00.042.018 I print_info: n_embd_k_gqa     = 2048
0.00.042.018 I print_info: n_embd_v_gqa     = 2048
0.00.042.019 I print_info: f_norm_eps       = 1.0e-05
0.00.042.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.020 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.020 I print_info: f_logit_scale    = 0.0e+00
0.00.042.021 I print_info: n_ff             = 8192
0.00.042.021 I print_info: n_expert         = 0
0.00.042.021 I print_info: n_expert_used    = 0
0.00.042.022 I print_info: causal attn      = 1
0.00.042.022 I print_info: pooling type     = 0
0.00.042.022 I print_info: rope type        = 2
0.00.042.022 I print_info: rope scaling     = linear
0.00.042.022 I print_info: freq_base_train  = 10000.0
0.00.042.023 I print_info: freq_scale_train = 1
0.00.042.023 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.023 I print_info: rope_finetuned   = unknown
0.00.042.024 I print_info: ssm_d_conv       = 0
0.00.042.024 I print_info: ssm_d_inner      = 0
0.00.042.024 I print_info: ssm_d_state      = 0
0.00.042.024 I print_info: ssm_dt_rank      = 0
0.00.042.024 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.024 I print_info: model type       = 1.4B
0.00.042.025 I print_info: model params     = 1.41 B
0.00.042.025 I print_info: general.name     = 1.4B
0.00.042.025 I print_info: vocab type       = BPE
0.00.042.026 I print_info: n_vocab          = 50304
0.00.042.026 I print_info: n_merges         = 50009
0.00.042.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.026 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.026 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.027 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.027 I print_info: LF token         = 187 'Ċ'
0.00.042.027 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.028 I print_info: max token length = 1024
0.00.042.028 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.101.346 I load_tensors: offloading 24 repeating layers to GPU
0.01.101.352 I load_tensors: offloading output layer to GPU
0.01.101.353 I load_tensors: offloaded 25/25 layers to GPU
0.01.101.377 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.101.379 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.102.522 I llama_init_from_model: n_seq_max     = 1
0.01.102.524 I llama_init_from_model: n_ctx         = 2048
0.01.102.524 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.102.525 I llama_init_from_model: n_batch       = 2048
0.01.102.525 I llama_init_from_model: n_ubatch      = 512
0.01.102.526 I llama_init_from_model: flash_attn    = 0
0.01.102.527 I llama_init_from_model: freq_base     = 10000.0
0.01.102.527 I llama_init_from_model: freq_scale    = 1
0.01.102.528 I ggml_metal_init: allocating
0.01.102.545 I ggml_metal_init: found device: Apple M4
0.01.102.553 I ggml_metal_init: picking default device: Apple M4
0.01.103.892 I ggml_metal_init: using embedded metal library
0.01.109.399 I ggml_metal_init: GPU name:   Apple M4
0.01.109.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.109.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.109.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.109.404 I ggml_metal_init: simdgroup reduction   = true
0.01.109.405 I ggml_metal_init: simdgroup matrix mul. = true
0.01.109.405 I ggml_metal_init: has residency sets    = true
0.01.109.405 I ggml_metal_init: has bfloat            = true
0.01.109.405 I ggml_metal_init: use bfloat            = true
0.01.109.406 I ggml_metal_init: hasUnifiedMemory      = true
0.01.109.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.126.175 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.179.251 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.179.258 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.179.280 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.183.568 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.183.570 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.183.571 I llama_init_from_model: graph nodes  = 967
0.01.183.571 I llama_init_from_model: graph splits = 2
0.01.183.577 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.183.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.183.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.240.440 I main: llama threadpool init, n_threads = 4
0.01.240.483 I 
0.01.240.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.240.508 I 
0.01.240.661 I sampler seed: 1234
0.01.240.665 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.240.686 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.240.686 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.240.686 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.336.623 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.02.336.624 I llama_perf_context_print:        load time =    1230.13 ms
0.02.336.625 I llama_perf_context_print: prompt eval time =      48.90 ms /     7 tokens (    6.99 ms per token,   143.15 tokens per second)
0.02.336.625 I llama_perf_context_print:        eval time =    1044.15 ms /    63 runs   (   16.57 ms per token,    60.34 tokens per second)
0.02.336.628 I llama_perf_context_print:       total time =    1096.92 ms /    70 tokens
0.02.336.915 I ggml_metal_free: deallocating

real	0m2.355s
user	0m0.108s
sys	0m0.264s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.015.093 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.344 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.604 I llama_model_loader: - type  f32:  194 tensors
0.00.040.605 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.605 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.606 I print_info: file format = GGUF V3 (latest)
0.00.040.607 I print_info: file type   = Q4_0
0.00.040.608 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.677 I load: special tokens cache size = 25
0.00.058.698 I load: token to piece cache size = 0.2984 MB
0.00.058.701 I print_info: arch             = gptneox
0.00.058.702 I print_info: vocab_only       = 0
0.00.058.702 I print_info: n_ctx_train      = 2048
0.00.058.702 I print_info: n_embd           = 2048
0.00.058.702 I print_info: n_layer          = 24
0.00.058.707 I print_info: n_head           = 16
0.00.058.708 I print_info: n_head_kv        = 16
0.00.058.708 I print_info: n_rot            = 32
0.00.058.709 I print_info: n_swa            = 0
0.00.058.709 I print_info: n_embd_head_k    = 128
0.00.058.709 I print_info: n_embd_head_v    = 128
0.00.058.710 I print_info: n_gqa            = 1
0.00.058.712 I print_info: n_embd_k_gqa     = 2048
0.00.058.713 I print_info: n_embd_v_gqa     = 2048
0.00.058.714 I print_info: f_norm_eps       = 1.0e-05
0.00.058.714 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.715 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.715 I print_info: f_logit_scale    = 0.0e+00
0.00.058.716 I print_info: n_ff             = 8192
0.00.058.716 I print_info: n_expert         = 0
0.00.058.716 I print_info: n_expert_used    = 0
0.00.058.716 I print_info: causal attn      = 1
0.00.058.717 I print_info: pooling type     = 0
0.00.058.717 I print_info: rope type        = 2
0.00.058.717 I print_info: rope scaling     = linear
0.00.058.718 I print_info: freq_base_train  = 10000.0
0.00.058.718 I print_info: freq_scale_train = 1
0.00.058.718 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.719 I print_info: rope_finetuned   = unknown
0.00.058.719 I print_info: ssm_d_conv       = 0
0.00.058.719 I print_info: ssm_d_inner      = 0
0.00.058.719 I print_info: ssm_d_state      = 0
0.00.058.719 I print_info: ssm_dt_rank      = 0
0.00.058.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.720 I print_info: model type       = 1.4B
0.00.058.720 I print_info: model params     = 1.41 B
0.00.058.720 I print_info: general.name     = 1.4B
0.00.058.721 I print_info: vocab type       = BPE
0.00.058.722 I print_info: n_vocab          = 50304
0.00.058.722 I print_info: n_merges         = 50009
0.00.058.722 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.722 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.723 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.723 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.726 I print_info: LF token         = 187 'Ċ'
0.00.058.726 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.726 I print_info: max token length = 1024
0.00.058.727 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.694 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.710 I load_tensors: offloading output layer to GPU
0.00.648.711 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.744 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.648.745 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.650.266 I llama_init_from_model: n_seq_max     = 1
0.00.650.269 I llama_init_from_model: n_ctx         = 2048
0.00.650.269 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.270 I llama_init_from_model: n_batch       = 2048
0.00.650.270 I llama_init_from_model: n_ubatch      = 512
0.00.650.270 I llama_init_from_model: flash_attn    = 0
0.00.650.273 I llama_init_from_model: freq_base     = 10000.0
0.00.650.274 I llama_init_from_model: freq_scale    = 1
0.00.650.276 I ggml_metal_init: allocating
0.00.650.355 I ggml_metal_init: found device: Apple M4
0.00.650.367 I ggml_metal_init: picking default device: Apple M4
0.00.652.247 I ggml_metal_init: using embedded metal library
0.00.658.334 I ggml_metal_init: GPU name:   Apple M4
0.00.658.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.342 I ggml_metal_init: simdgroup reduction   = true
0.00.658.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.342 I ggml_metal_init: has residency sets    = true
0.00.658.343 I ggml_metal_init: has bfloat            = true
0.00.658.343 I ggml_metal_init: use bfloat            = true
0.00.658.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.771 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.586 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.593 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.196 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.199 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.199 I llama_init_from_model: graph nodes  = 967
0.00.735.199 I llama_init_from_model: graph splits = 2
0.00.735.204 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.230 I main: llama threadpool init, n_threads = 4
0.00.792.275 I 
0.00.792.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.300 I 
0.00.792.479 I sampler seed: 1234
0.00.792.483 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.494 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.494 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.494 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.478.024 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.478.025 I llama_perf_context_print:        load time =     776.40 ms
0.01.478.026 I llama_perf_context_print: prompt eval time =      49.36 ms /     7 tokens (    7.05 ms per token,   141.80 tokens per second)
0.01.478.026 I llama_perf_context_print:        eval time =     633.28 ms /    63 runs   (   10.05 ms per token,    99.48 tokens per second)
0.01.478.027 I llama_perf_context_print:       total time =     686.53 ms /    70 tokens
0.01.478.255 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.116s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.556 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.538 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.542 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.550 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.556 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.864 I llama_model_loader: - type  f32:  194 tensors
0.00.038.865 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.865 I print_info: file format = GGUF V3 (latest)
0.00.038.866 I print_info: file type   = Q4_1
0.00.038.867 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.974 I load: special tokens cache size = 25
0.00.055.128 I load: token to piece cache size = 0.2984 MB
0.00.055.131 I print_info: arch             = gptneox
0.00.055.132 I print_info: vocab_only       = 0
0.00.055.132 I print_info: n_ctx_train      = 2048
0.00.055.132 I print_info: n_embd           = 2048
0.00.055.132 I print_info: n_layer          = 24
0.00.055.135 I print_info: n_head           = 16
0.00.055.136 I print_info: n_head_kv        = 16
0.00.055.136 I print_info: n_rot            = 32
0.00.055.136 I print_info: n_swa            = 0
0.00.055.136 I print_info: n_embd_head_k    = 128
0.00.055.139 I print_info: n_embd_head_v    = 128
0.00.055.139 I print_info: n_gqa            = 1
0.00.055.140 I print_info: n_embd_k_gqa     = 2048
0.00.055.141 I print_info: n_embd_v_gqa     = 2048
0.00.055.141 I print_info: f_norm_eps       = 1.0e-05
0.00.055.146 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.147 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.147 I print_info: f_logit_scale    = 0.0e+00
0.00.055.149 I print_info: n_ff             = 8192
0.00.055.149 I print_info: n_expert         = 0
0.00.055.149 I print_info: n_expert_used    = 0
0.00.055.149 I print_info: causal attn      = 1
0.00.055.150 I print_info: pooling type     = 0
0.00.055.150 I print_info: rope type        = 2
0.00.055.151 I print_info: rope scaling     = linear
0.00.055.152 I print_info: freq_base_train  = 10000.0
0.00.055.152 I print_info: freq_scale_train = 1
0.00.055.152 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.152 I print_info: rope_finetuned   = unknown
0.00.055.152 I print_info: ssm_d_conv       = 0
0.00.055.152 I print_info: ssm_d_inner      = 0
0.00.055.153 I print_info: ssm_d_state      = 0
0.00.055.153 I print_info: ssm_dt_rank      = 0
0.00.055.153 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.153 I print_info: model type       = 1.4B
0.00.055.154 I print_info: model params     = 1.41 B
0.00.055.154 I print_info: general.name     = 1.4B
0.00.055.154 I print_info: vocab type       = BPE
0.00.055.154 I print_info: n_vocab          = 50304
0.00.055.154 I print_info: n_merges         = 50009
0.00.055.155 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.156 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.156 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.157 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.157 I print_info: LF token         = 187 'Ċ'
0.00.055.157 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.157 I print_info: max token length = 1024
0.00.055.158 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.726.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.726.329 I load_tensors: offloading output layer to GPU
0.00.726.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.726.364 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.726.365 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.727.648 I llama_init_from_model: n_seq_max     = 1
0.00.727.650 I llama_init_from_model: n_ctx         = 2048
0.00.727.651 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.727.651 I llama_init_from_model: n_batch       = 2048
0.00.727.652 I llama_init_from_model: n_ubatch      = 512
0.00.727.652 I llama_init_from_model: flash_attn    = 0
0.00.727.654 I llama_init_from_model: freq_base     = 10000.0
0.00.727.655 I llama_init_from_model: freq_scale    = 1
0.00.727.657 I ggml_metal_init: allocating
0.00.727.734 I ggml_metal_init: found device: Apple M4
0.00.727.748 I ggml_metal_init: picking default device: Apple M4
0.00.729.596 I ggml_metal_init: using embedded metal library
0.00.736.326 I ggml_metal_init: GPU name:   Apple M4
0.00.736.330 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.332 I ggml_metal_init: simdgroup reduction   = true
0.00.736.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.333 I ggml_metal_init: has residency sets    = true
0.00.736.333 I ggml_metal_init: has bfloat            = true
0.00.736.334 I ggml_metal_init: use bfloat            = true
0.00.736.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.754.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.806.786 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.806.792 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.806.817 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.811.943 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.811.946 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.811.946 I llama_init_from_model: graph nodes  = 967
0.00.811.946 I llama_init_from_model: graph splits = 2
0.00.811.952 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.812.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.812.068 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.866.986 I main: llama threadpool init, n_threads = 4
0.00.867.032 I 
0.00.867.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.867.057 I 
0.00.867.207 I sampler seed: 1234
0.00.867.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.867.255 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.867.256 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.867.256 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.595.354 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.595.355 I llama_perf_context_print:        load time =     854.73 ms
0.01.595.356 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.01.595.356 I llama_perf_context_print:        eval time =     676.21 ms /    63 runs   (   10.73 ms per token,    93.17 tokens per second)
0.01.595.358 I llama_perf_context_print:       total time =     729.07 ms /    70 tokens
0.01.595.604 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.112s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.752 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.032 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.033 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.033 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.033 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.034 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.710 I llama_model_loader: - type  f32:  194 tensors
0.00.025.710 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.710 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.711 I print_info: file format = GGUF V3 (latest)
0.00.025.711 I print_info: file type   = Q5_0
0.00.025.712 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.849 I load: special tokens cache size = 25
0.00.039.668 I load: token to piece cache size = 0.2984 MB
0.00.039.671 I print_info: arch             = gptneox
0.00.039.671 I print_info: vocab_only       = 0
0.00.039.671 I print_info: n_ctx_train      = 2048
0.00.039.671 I print_info: n_embd           = 2048
0.00.039.672 I print_info: n_layer          = 24
0.00.039.675 I print_info: n_head           = 16
0.00.039.675 I print_info: n_head_kv        = 16
0.00.039.675 I print_info: n_rot            = 32
0.00.039.675 I print_info: n_swa            = 0
0.00.039.676 I print_info: n_embd_head_k    = 128
0.00.039.676 I print_info: n_embd_head_v    = 128
0.00.039.677 I print_info: n_gqa            = 1
0.00.039.677 I print_info: n_embd_k_gqa     = 2048
0.00.039.678 I print_info: n_embd_v_gqa     = 2048
0.00.039.679 I print_info: f_norm_eps       = 1.0e-05
0.00.039.679 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.679 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.679 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.680 I print_info: f_logit_scale    = 0.0e+00
0.00.039.680 I print_info: n_ff             = 8192
0.00.039.680 I print_info: n_expert         = 0
0.00.039.681 I print_info: n_expert_used    = 0
0.00.039.681 I print_info: causal attn      = 1
0.00.039.681 I print_info: pooling type     = 0
0.00.039.682 I print_info: rope type        = 2
0.00.039.691 I print_info: rope scaling     = linear
0.00.039.693 I print_info: freq_base_train  = 10000.0
0.00.039.695 I print_info: freq_scale_train = 1
0.00.039.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.695 I print_info: rope_finetuned   = unknown
0.00.039.695 I print_info: ssm_d_conv       = 0
0.00.039.696 I print_info: ssm_d_inner      = 0
0.00.039.696 I print_info: ssm_d_state      = 0
0.00.039.697 I print_info: ssm_dt_rank      = 0
0.00.039.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.697 I print_info: model type       = 1.4B
0.00.039.697 I print_info: model params     = 1.41 B
0.00.039.698 I print_info: general.name     = 1.4B
0.00.039.698 I print_info: vocab type       = BPE
0.00.039.698 I print_info: n_vocab          = 50304
0.00.039.698 I print_info: n_merges         = 50009
0.00.039.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: LF token         = 187 'Ċ'
0.00.039.699 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: max token length = 1024
0.00.039.700 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.721.231 I load_tensors: offloading 24 repeating layers to GPU
0.00.721.245 I load_tensors: offloading output layer to GPU
0.00.721.246 I load_tensors: offloaded 25/25 layers to GPU
0.00.721.281 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.721.283 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.722.983 I llama_init_from_model: n_seq_max     = 1
0.00.722.986 I llama_init_from_model: n_ctx         = 2048
0.00.722.987 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.722.987 I llama_init_from_model: n_batch       = 2048
0.00.722.988 I llama_init_from_model: n_ubatch      = 512
0.00.722.988 I llama_init_from_model: flash_attn    = 0
0.00.722.990 I llama_init_from_model: freq_base     = 10000.0
0.00.722.990 I llama_init_from_model: freq_scale    = 1
0.00.722.992 I ggml_metal_init: allocating
0.00.723.007 I ggml_metal_init: found device: Apple M4
0.00.723.017 I ggml_metal_init: picking default device: Apple M4
0.00.724.502 I ggml_metal_init: using embedded metal library
0.00.730.746 I ggml_metal_init: GPU name:   Apple M4
0.00.730.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.730.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.730.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.730.752 I ggml_metal_init: simdgroup reduction   = true
0.00.730.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.730.753 I ggml_metal_init: has residency sets    = true
0.00.730.753 I ggml_metal_init: has bfloat            = true
0.00.730.753 I ggml_metal_init: use bfloat            = true
0.00.730.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.730.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.457 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.800.722 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.800.729 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.800.757 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.805.383 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.805.386 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.805.386 I llama_init_from_model: graph nodes  = 967
0.00.805.386 I llama_init_from_model: graph splits = 2
0.00.805.393 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.805.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.805.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.551 I main: llama threadpool init, n_threads = 4
0.00.860.596 I 
0.00.860.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.619 I 
0.00.860.873 I sampler seed: 1234
0.00.860.882 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.860.897 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.860.899 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.860.899 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.640.278 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.640.279 I llama_perf_context_print:        load time =     851.07 ms
0.01.640.280 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.47 tokens per second)
0.01.640.280 I llama_perf_context_print:        eval time =     733.47 ms /    63 runs   (   11.64 ms per token,    85.89 tokens per second)
0.01.640.281 I llama_perf_context_print:       total time =     780.45 ms /    70 tokens
0.01.640.563 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.559 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.568 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.305 I llama_model_loader: - type  f32:  194 tensors
0.00.026.305 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.305 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.306 I print_info: file format = GGUF V3 (latest)
0.00.026.306 I print_info: file type   = Q5_1
0.00.026.310 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.613 I load: special tokens cache size = 25
0.00.040.675 I load: token to piece cache size = 0.2984 MB
0.00.040.678 I print_info: arch             = gptneox
0.00.040.678 I print_info: vocab_only       = 0
0.00.040.678 I print_info: n_ctx_train      = 2048
0.00.040.678 I print_info: n_embd           = 2048
0.00.040.679 I print_info: n_layer          = 24
0.00.040.682 I print_info: n_head           = 16
0.00.040.682 I print_info: n_head_kv        = 16
0.00.040.683 I print_info: n_rot            = 32
0.00.040.685 I print_info: n_swa            = 0
0.00.040.685 I print_info: n_embd_head_k    = 128
0.00.040.685 I print_info: n_embd_head_v    = 128
0.00.040.686 I print_info: n_gqa            = 1
0.00.040.687 I print_info: n_embd_k_gqa     = 2048
0.00.040.687 I print_info: n_embd_v_gqa     = 2048
0.00.040.688 I print_info: f_norm_eps       = 1.0e-05
0.00.040.688 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.688 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.689 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.689 I print_info: f_logit_scale    = 0.0e+00
0.00.040.689 I print_info: n_ff             = 8192
0.00.040.690 I print_info: n_expert         = 0
0.00.040.690 I print_info: n_expert_used    = 0
0.00.040.690 I print_info: causal attn      = 1
0.00.040.690 I print_info: pooling type     = 0
0.00.040.690 I print_info: rope type        = 2
0.00.040.691 I print_info: rope scaling     = linear
0.00.040.691 I print_info: freq_base_train  = 10000.0
0.00.040.693 I print_info: freq_scale_train = 1
0.00.040.693 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.693 I print_info: rope_finetuned   = unknown
0.00.040.693 I print_info: ssm_d_conv       = 0
0.00.040.693 I print_info: ssm_d_inner      = 0
0.00.040.693 I print_info: ssm_d_state      = 0
0.00.040.694 I print_info: ssm_dt_rank      = 0
0.00.040.694 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.694 I print_info: model type       = 1.4B
0.00.040.694 I print_info: model params     = 1.41 B
0.00.040.694 I print_info: general.name     = 1.4B
0.00.040.695 I print_info: vocab type       = BPE
0.00.040.695 I print_info: n_vocab          = 50304
0.00.040.695 I print_info: n_merges         = 50009
0.00.040.700 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.700 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.700 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.701 I print_info: LF token         = 187 'Ċ'
0.00.040.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.701 I print_info: max token length = 1024
0.00.040.701 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.592 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.606 I load_tensors: offloading output layer to GPU
0.00.603.607 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.642 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.603.643 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.605.156 I llama_init_from_model: n_seq_max     = 1
0.00.605.162 I llama_init_from_model: n_ctx         = 2048
0.00.605.163 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.605.163 I llama_init_from_model: n_batch       = 2048
0.00.605.164 I llama_init_from_model: n_ubatch      = 512
0.00.605.164 I llama_init_from_model: flash_attn    = 0
0.00.605.165 I llama_init_from_model: freq_base     = 10000.0
0.00.605.166 I llama_init_from_model: freq_scale    = 1
0.00.605.169 I ggml_metal_init: allocating
0.00.605.217 I ggml_metal_init: found device: Apple M4
0.00.605.231 I ggml_metal_init: picking default device: Apple M4
0.00.606.997 I ggml_metal_init: using embedded metal library
0.00.613.571 I ggml_metal_init: GPU name:   Apple M4
0.00.613.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.579 I ggml_metal_init: simdgroup reduction   = true
0.00.613.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.580 I ggml_metal_init: has residency sets    = true
0.00.613.580 I ggml_metal_init: has bfloat            = true
0.00.613.580 I ggml_metal_init: use bfloat            = true
0.00.613.581 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.583 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.209 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.879 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.684.886 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.684.911 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.784 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.689.786 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.689.787 I llama_init_from_model: graph nodes  = 967
0.00.689.787 I llama_init_from_model: graph splits = 2
0.00.689.793 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.689.926 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.689.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.577 I main: llama threadpool init, n_threads = 4
0.00.748.619 I 
0.00.748.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.641 I 
0.00.748.800 I sampler seed: 1234
0.00.748.804 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.815 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.815 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.815 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.592.624 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.592.625 I llama_perf_context_print:        load time =     738.01 ms
0.01.592.626 I llama_perf_context_print: prompt eval time =      52.26 ms /     7 tokens (    7.47 ms per token,   133.94 tokens per second)
0.01.592.627 I llama_perf_context_print:        eval time =     788.57 ms /    63 runs   (   12.52 ms per token,    79.89 tokens per second)
0.01.592.627 I llama_perf_context_print:       total time =     844.75 ms /    70 tokens
0.01.592.887 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.110s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.754 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.509 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.511 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.512 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.512 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.516 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.370 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.190 I llama_model_loader: - type  f32:  194 tensors
0.00.025.190 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.191 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.191 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.192 I print_info: file format = GGUF V3 (latest)
0.00.025.192 I print_info: file type   = Q2_K - Medium
0.00.025.193 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.083 I load: special tokens cache size = 25
0.00.039.251 I load: token to piece cache size = 0.2984 MB
0.00.039.254 I print_info: arch             = gptneox
0.00.039.254 I print_info: vocab_only       = 0
0.00.039.254 I print_info: n_ctx_train      = 2048
0.00.039.254 I print_info: n_embd           = 2048
0.00.039.254 I print_info: n_layer          = 24
0.00.039.257 I print_info: n_head           = 16
0.00.039.258 I print_info: n_head_kv        = 16
0.00.039.258 I print_info: n_rot            = 32
0.00.039.259 I print_info: n_swa            = 0
0.00.039.259 I print_info: n_embd_head_k    = 128
0.00.039.259 I print_info: n_embd_head_v    = 128
0.00.039.260 I print_info: n_gqa            = 1
0.00.039.261 I print_info: n_embd_k_gqa     = 2048
0.00.039.262 I print_info: n_embd_v_gqa     = 2048
0.00.039.263 I print_info: f_norm_eps       = 1.0e-05
0.00.039.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.264 I print_info: f_logit_scale    = 0.0e+00
0.00.039.264 I print_info: n_ff             = 8192
0.00.039.264 I print_info: n_expert         = 0
0.00.039.265 I print_info: n_expert_used    = 0
0.00.039.265 I print_info: causal attn      = 1
0.00.039.265 I print_info: pooling type     = 0
0.00.039.267 I print_info: rope type        = 2
0.00.039.267 I print_info: rope scaling     = linear
0.00.039.267 I print_info: freq_base_train  = 10000.0
0.00.039.268 I print_info: freq_scale_train = 1
0.00.039.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.268 I print_info: rope_finetuned   = unknown
0.00.039.268 I print_info: ssm_d_conv       = 0
0.00.039.269 I print_info: ssm_d_inner      = 0
0.00.039.269 I print_info: ssm_d_state      = 0
0.00.039.269 I print_info: ssm_dt_rank      = 0
0.00.039.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.269 I print_info: model type       = 1.4B
0.00.039.271 I print_info: model params     = 1.41 B
0.00.039.271 I print_info: general.name     = 1.4B
0.00.039.271 I print_info: vocab type       = BPE
0.00.039.271 I print_info: n_vocab          = 50304
0.00.039.272 I print_info: n_merges         = 50009
0.00.039.272 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.273 I print_info: LF token         = 187 'Ċ'
0.00.039.273 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.273 I print_info: max token length = 1024
0.00.039.277 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.350.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.350.813 I load_tensors: offloading output layer to GPU
0.00.350.814 I load_tensors: offloaded 25/25 layers to GPU
0.00.350.847 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.350.848 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.554 I llama_init_from_model: n_seq_max     = 1
0.00.352.560 I llama_init_from_model: n_ctx         = 2048
0.00.352.561 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.352.561 I llama_init_from_model: n_batch       = 2048
0.00.352.561 I llama_init_from_model: n_ubatch      = 512
0.00.352.562 I llama_init_from_model: flash_attn    = 0
0.00.352.564 I llama_init_from_model: freq_base     = 10000.0
0.00.352.564 I llama_init_from_model: freq_scale    = 1
0.00.352.566 I ggml_metal_init: allocating
0.00.352.673 I ggml_metal_init: found device: Apple M4
0.00.352.687 I ggml_metal_init: picking default device: Apple M4
0.00.354.583 I ggml_metal_init: using embedded metal library
0.00.360.110 I ggml_metal_init: GPU name:   Apple M4
0.00.360.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.360.129 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.360.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.360.131 I ggml_metal_init: simdgroup reduction   = true
0.00.360.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.360.131 I ggml_metal_init: has residency sets    = true
0.00.360.132 I ggml_metal_init: has bfloat            = true
0.00.360.132 I ggml_metal_init: use bfloat            = true
0.00.360.136 I ggml_metal_init: hasUnifiedMemory      = true
0.00.360.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.660 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.435.534 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.435.544 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.435.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.782 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.439.784 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.439.784 I llama_init_from_model: graph nodes  = 967
0.00.439.784 I llama_init_from_model: graph splits = 2
0.00.439.791 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.920 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.915 I main: llama threadpool init, n_threads = 4
0.00.495.953 I 
0.00.495.978 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.978 I 
0.00.496.108 I sampler seed: 1234
0.00.496.113 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.160 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.162 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.176.121 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.176.121 I llama_perf_context_print:        load time =     485.45 ms
0.01.176.124 I llama_perf_context_print: prompt eval time =      40.69 ms /     7 tokens (    5.81 ms per token,   172.05 tokens per second)
0.01.176.125 I llama_perf_context_print:        eval time =     636.40 ms /    63 runs   (   10.10 ms per token,    98.99 tokens per second)
0.01.176.127 I llama_perf_context_print:       total time =     680.91 ms /    70 tokens
0.01.176.361 I ggml_metal_free: deallocating

real	0m1.194s
user	0m0.111s
sys	0m0.164s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.748 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.759 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.760 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.760 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.764 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.764 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.764 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.576 I llama_model_loader: - type  f32:  194 tensors
0.00.025.577 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.577 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.577 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.578 I print_info: file format = GGUF V3 (latest)
0.00.025.578 I print_info: file type   = Q3_K - Medium
0.00.025.579 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.820 I load: special tokens cache size = 25
0.00.039.956 I load: token to piece cache size = 0.2984 MB
0.00.039.959 I print_info: arch             = gptneox
0.00.039.959 I print_info: vocab_only       = 0
0.00.039.959 I print_info: n_ctx_train      = 2048
0.00.039.960 I print_info: n_embd           = 2048
0.00.039.960 I print_info: n_layer          = 24
0.00.039.962 I print_info: n_head           = 16
0.00.039.963 I print_info: n_head_kv        = 16
0.00.039.963 I print_info: n_rot            = 32
0.00.039.963 I print_info: n_swa            = 0
0.00.039.964 I print_info: n_embd_head_k    = 128
0.00.039.964 I print_info: n_embd_head_v    = 128
0.00.039.964 I print_info: n_gqa            = 1
0.00.039.965 I print_info: n_embd_k_gqa     = 2048
0.00.039.966 I print_info: n_embd_v_gqa     = 2048
0.00.039.966 I print_info: f_norm_eps       = 1.0e-05
0.00.039.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.967 I print_info: f_logit_scale    = 0.0e+00
0.00.039.968 I print_info: n_ff             = 8192
0.00.039.973 I print_info: n_expert         = 0
0.00.039.973 I print_info: n_expert_used    = 0
0.00.039.975 I print_info: causal attn      = 1
0.00.039.976 I print_info: pooling type     = 0
0.00.039.976 I print_info: rope type        = 2
0.00.039.976 I print_info: rope scaling     = linear
0.00.039.977 I print_info: freq_base_train  = 10000.0
0.00.039.977 I print_info: freq_scale_train = 1
0.00.039.977 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.979 I print_info: rope_finetuned   = unknown
0.00.039.979 I print_info: ssm_d_conv       = 0
0.00.039.979 I print_info: ssm_d_inner      = 0
0.00.039.979 I print_info: ssm_d_state      = 0
0.00.039.979 I print_info: ssm_dt_rank      = 0
0.00.039.979 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.980 I print_info: model type       = 1.4B
0.00.039.980 I print_info: model params     = 1.41 B
0.00.039.980 I print_info: general.name     = 1.4B
0.00.039.981 I print_info: vocab type       = BPE
0.00.039.981 I print_info: n_vocab          = 50304
0.00.039.981 I print_info: n_merges         = 50009
0.00.039.981 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: LF token         = 187 'Ċ'
0.00.039.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: max token length = 1024
0.00.039.983 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.647 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.657 I load_tensors: offloading output layer to GPU
0.00.440.659 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.688 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.689 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.166 I llama_init_from_model: n_seq_max     = 1
0.00.442.168 I llama_init_from_model: n_ctx         = 2048
0.00.442.169 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.442.170 I llama_init_from_model: n_batch       = 2048
0.00.442.170 I llama_init_from_model: n_ubatch      = 512
0.00.442.171 I llama_init_from_model: flash_attn    = 0
0.00.442.172 I llama_init_from_model: freq_base     = 10000.0
0.00.442.173 I llama_init_from_model: freq_scale    = 1
0.00.442.176 I ggml_metal_init: allocating
0.00.442.215 I ggml_metal_init: found device: Apple M4
0.00.442.228 I ggml_metal_init: picking default device: Apple M4
0.00.443.953 I ggml_metal_init: using embedded metal library
0.00.449.965 I ggml_metal_init: GPU name:   Apple M4
0.00.449.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.970 I ggml_metal_init: simdgroup reduction   = true
0.00.449.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.970 I ggml_metal_init: has residency sets    = true
0.00.449.970 I ggml_metal_init: has bfloat            = true
0.00.449.971 I ggml_metal_init: use bfloat            = true
0.00.449.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.757 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.503.728 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.503.737 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.503.759 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.509.335 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.509.337 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.509.338 I llama_init_from_model: graph nodes  = 967
0.00.509.338 I llama_init_from_model: graph splits = 2
0.00.509.345 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.509.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.509.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.549 I main: llama threadpool init, n_threads = 4
0.00.573.591 I 
0.00.573.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.573.613 I 
0.00.573.761 I sampler seed: 1234
0.00.573.766 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.573.776 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.573.777 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.573.777 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.326.427 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.01.326.428 I llama_perf_context_print:        load time =     563.89 ms
0.01.326.429 I llama_perf_context_print: prompt eval time =      46.36 ms /     7 tokens (    6.62 ms per token,   151.00 tokens per second)
0.01.326.429 I llama_perf_context_print:        eval time =     703.29 ms /    63 runs   (   11.16 ms per token,    89.58 tokens per second)
0.01.326.430 I llama_perf_context_print:       total time =     753.61 ms /    70 tokens
0.01.326.686 I ggml_metal_free: deallocating

real	0m1.343s
user	0m0.108s
sys	0m0.167s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.693 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.373 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.373 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.373 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.268 I llama_model_loader: - type  f32:  194 tensors
0.00.026.268 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.269 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.269 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.269 I print_info: file format = GGUF V3 (latest)
0.00.026.270 I print_info: file type   = Q4_K - Medium
0.00.026.271 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.230 I load: special tokens cache size = 25
0.00.040.284 I load: token to piece cache size = 0.2984 MB
0.00.040.286 I print_info: arch             = gptneox
0.00.040.287 I print_info: vocab_only       = 0
0.00.040.287 I print_info: n_ctx_train      = 2048
0.00.040.287 I print_info: n_embd           = 2048
0.00.040.287 I print_info: n_layer          = 24
0.00.040.290 I print_info: n_head           = 16
0.00.040.291 I print_info: n_head_kv        = 16
0.00.040.291 I print_info: n_rot            = 32
0.00.040.291 I print_info: n_swa            = 0
0.00.040.291 I print_info: n_embd_head_k    = 128
0.00.040.291 I print_info: n_embd_head_v    = 128
0.00.040.294 I print_info: n_gqa            = 1
0.00.040.295 I print_info: n_embd_k_gqa     = 2048
0.00.040.295 I print_info: n_embd_v_gqa     = 2048
0.00.040.296 I print_info: f_norm_eps       = 1.0e-05
0.00.040.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.297 I print_info: f_logit_scale    = 0.0e+00
0.00.040.297 I print_info: n_ff             = 8192
0.00.040.298 I print_info: n_expert         = 0
0.00.040.298 I print_info: n_expert_used    = 0
0.00.040.298 I print_info: causal attn      = 1
0.00.040.298 I print_info: pooling type     = 0
0.00.040.298 I print_info: rope type        = 2
0.00.040.299 I print_info: rope scaling     = linear
0.00.040.299 I print_info: freq_base_train  = 10000.0
0.00.040.300 I print_info: freq_scale_train = 1
0.00.040.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.300 I print_info: rope_finetuned   = unknown
0.00.040.300 I print_info: ssm_d_conv       = 0
0.00.040.300 I print_info: ssm_d_inner      = 0
0.00.040.301 I print_info: ssm_d_state      = 0
0.00.040.301 I print_info: ssm_dt_rank      = 0
0.00.040.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.301 I print_info: model type       = 1.4B
0.00.040.302 I print_info: model params     = 1.41 B
0.00.040.302 I print_info: general.name     = 1.4B
0.00.040.302 I print_info: vocab type       = BPE
0.00.040.303 I print_info: n_vocab          = 50304
0.00.040.303 I print_info: n_merges         = 50009
0.00.040.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.305 I print_info: LF token         = 187 'Ċ'
0.00.040.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.306 I print_info: max token length = 1024
0.00.040.306 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.132 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.148 I load_tensors: offloading output layer to GPU
0.00.533.149 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.185 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.186 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.534.762 I llama_init_from_model: n_seq_max     = 1
0.00.534.765 I llama_init_from_model: n_ctx         = 2048
0.00.534.766 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.534.766 I llama_init_from_model: n_batch       = 2048
0.00.534.767 I llama_init_from_model: n_ubatch      = 512
0.00.534.767 I llama_init_from_model: flash_attn    = 0
0.00.534.769 I llama_init_from_model: freq_base     = 10000.0
0.00.534.770 I llama_init_from_model: freq_scale    = 1
0.00.534.772 I ggml_metal_init: allocating
0.00.534.848 I ggml_metal_init: found device: Apple M4
0.00.534.861 I ggml_metal_init: picking default device: Apple M4
0.00.536.814 I ggml_metal_init: using embedded metal library
0.00.543.527 I ggml_metal_init: GPU name:   Apple M4
0.00.543.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.533 I ggml_metal_init: simdgroup reduction   = true
0.00.543.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.533 I ggml_metal_init: has residency sets    = true
0.00.543.534 I ggml_metal_init: has bfloat            = true
0.00.543.534 I ggml_metal_init: use bfloat            = true
0.00.543.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.560.439 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.784 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.607.789 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.607.813 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.143 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.612.145 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.612.145 I llama_init_from_model: graph nodes  = 967
0.00.612.145 I llama_init_from_model: graph splits = 2
0.00.612.151 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.612.277 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.612.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.178 I main: llama threadpool init, n_threads = 4
0.00.667.219 I 
0.00.667.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.240 I 
0.00.667.392 I sampler seed: 1234
0.00.667.396 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.417 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.417 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.417 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.428.398 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.428.399 I llama_perf_context_print:        load time =     656.75 ms
0.01.428.400 I llama_perf_context_print: prompt eval time =      46.72 ms /     7 tokens (    6.67 ms per token,   149.84 tokens per second)
0.01.428.401 I llama_perf_context_print:        eval time =     711.38 ms /    63 runs   (   11.29 ms per token,    88.56 tokens per second)
0.01.428.401 I llama_perf_context_print:       total time =     761.95 ms /    70 tokens
0.01.428.661 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.109s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.214 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.214 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.388 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.305 I llama_model_loader: - type  f32:  194 tensors
0.00.028.305 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.305 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.306 I print_info: file format = GGUF V3 (latest)
0.00.028.307 I print_info: file type   = Q5_K - Medium
0.00.028.308 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.731 I load: special tokens cache size = 25
0.00.042.964 I load: token to piece cache size = 0.2984 MB
0.00.042.968 I print_info: arch             = gptneox
0.00.042.968 I print_info: vocab_only       = 0
0.00.042.968 I print_info: n_ctx_train      = 2048
0.00.042.968 I print_info: n_embd           = 2048
0.00.042.968 I print_info: n_layer          = 24
0.00.042.973 I print_info: n_head           = 16
0.00.042.974 I print_info: n_head_kv        = 16
0.00.042.974 I print_info: n_rot            = 32
0.00.042.974 I print_info: n_swa            = 0
0.00.042.974 I print_info: n_embd_head_k    = 128
0.00.042.974 I print_info: n_embd_head_v    = 128
0.00.042.975 I print_info: n_gqa            = 1
0.00.042.976 I print_info: n_embd_k_gqa     = 2048
0.00.042.977 I print_info: n_embd_v_gqa     = 2048
0.00.042.977 I print_info: f_norm_eps       = 1.0e-05
0.00.042.978 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.978 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.978 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.978 I print_info: f_logit_scale    = 0.0e+00
0.00.042.979 I print_info: n_ff             = 8192
0.00.042.979 I print_info: n_expert         = 0
0.00.042.979 I print_info: n_expert_used    = 0
0.00.042.979 I print_info: causal attn      = 1
0.00.042.979 I print_info: pooling type     = 0
0.00.042.984 I print_info: rope type        = 2
0.00.042.985 I print_info: rope scaling     = linear
0.00.042.985 I print_info: freq_base_train  = 10000.0
0.00.042.985 I print_info: freq_scale_train = 1
0.00.042.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.986 I print_info: rope_finetuned   = unknown
0.00.042.986 I print_info: ssm_d_conv       = 0
0.00.042.986 I print_info: ssm_d_inner      = 0
0.00.042.986 I print_info: ssm_d_state      = 0
0.00.042.986 I print_info: ssm_dt_rank      = 0
0.00.042.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.986 I print_info: model type       = 1.4B
0.00.042.987 I print_info: model params     = 1.41 B
0.00.042.987 I print_info: general.name     = 1.4B
0.00.042.987 I print_info: vocab type       = BPE
0.00.042.987 I print_info: n_vocab          = 50304
0.00.042.988 I print_info: n_merges         = 50009
0.00.042.988 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.992 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.992 I print_info: LF token         = 187 'Ċ'
0.00.042.993 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.993 I print_info: max token length = 1024
0.00.042.994 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.277 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.289 I load_tensors: offloading output layer to GPU
0.00.660.290 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.323 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.660.324 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.661.600 I llama_init_from_model: n_seq_max     = 1
0.00.661.604 I llama_init_from_model: n_ctx         = 2048
0.00.661.605 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.661.605 I llama_init_from_model: n_batch       = 2048
0.00.661.606 I llama_init_from_model: n_ubatch      = 512
0.00.661.606 I llama_init_from_model: flash_attn    = 0
0.00.661.608 I llama_init_from_model: freq_base     = 10000.0
0.00.661.608 I llama_init_from_model: freq_scale    = 1
0.00.661.611 I ggml_metal_init: allocating
0.00.661.674 I ggml_metal_init: found device: Apple M4
0.00.661.687 I ggml_metal_init: picking default device: Apple M4
0.00.663.312 I ggml_metal_init: using embedded metal library
0.00.667.805 I ggml_metal_init: GPU name:   Apple M4
0.00.667.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.811 I ggml_metal_init: simdgroup reduction   = true
0.00.667.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.811 I ggml_metal_init: has residency sets    = true
0.00.667.811 I ggml_metal_init: has bfloat            = true
0.00.667.811 I ggml_metal_init: use bfloat            = true
0.00.667.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.394 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.635 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.713.643 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.711 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.780 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.781 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.781 I llama_init_from_model: graph nodes  = 967
0.00.717.782 I llama_init_from_model: graph splits = 2
0.00.717.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.842 I main: llama threadpool init, n_threads = 4
0.00.784.885 I 
0.00.784.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.907 I 
0.00.785.073 I sampler seed: 1234
0.00.785.077 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.088 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.089 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.632.751 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.632.751 I llama_perf_context_print:        load time =     774.10 ms
0.01.632.752 I llama_perf_context_print: prompt eval time =      51.34 ms /     7 tokens (    7.33 ms per token,   136.35 tokens per second)
0.01.632.753 I llama_perf_context_print:        eval time =     793.90 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.632.753 I llama_perf_context_print:       total time =     848.62 ms /    70 tokens
0.01.633.023 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.104s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.100 I main: load the model and apply lora adapter, if any
0.00.012.423 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.020.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.567 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.029.579 I llama_model_loader: - type  f32:  194 tensors
0.00.029.579 I llama_model_loader: - type q6_K:   98 tensors
0.00.029.580 I print_info: file format = GGUF V3 (latest)
0.00.029.580 I print_info: file type   = Q6_K
0.00.029.582 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.038.013 I load: special tokens cache size = 25
0.00.044.279 I load: token to piece cache size = 0.2984 MB
0.00.044.286 I print_info: arch             = gptneox
0.00.044.286 I print_info: vocab_only       = 0
0.00.044.288 I print_info: n_ctx_train      = 2048
0.00.044.288 I print_info: n_embd           = 2048
0.00.044.288 I print_info: n_layer          = 24
0.00.044.293 I print_info: n_head           = 16
0.00.044.294 I print_info: n_head_kv        = 16
0.00.044.294 I print_info: n_rot            = 32
0.00.044.294 I print_info: n_swa            = 0
0.00.044.295 I print_info: n_embd_head_k    = 128
0.00.044.295 I print_info: n_embd_head_v    = 128
0.00.044.295 I print_info: n_gqa            = 1
0.00.044.296 I print_info: n_embd_k_gqa     = 2048
0.00.044.297 I print_info: n_embd_v_gqa     = 2048
0.00.044.297 I print_info: f_norm_eps       = 1.0e-05
0.00.044.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.298 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.299 I print_info: f_logit_scale    = 0.0e+00
0.00.044.301 I print_info: n_ff             = 8192
0.00.044.301 I print_info: n_expert         = 0
0.00.044.301 I print_info: n_expert_used    = 0
0.00.044.302 I print_info: causal attn      = 1
0.00.044.302 I print_info: pooling type     = 0
0.00.044.302 I print_info: rope type        = 2
0.00.044.302 I print_info: rope scaling     = linear
0.00.044.302 I print_info: freq_base_train  = 10000.0
0.00.044.303 I print_info: freq_scale_train = 1
0.00.044.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.303 I print_info: rope_finetuned   = unknown
0.00.044.303 I print_info: ssm_d_conv       = 0
0.00.044.303 I print_info: ssm_d_inner      = 0
0.00.044.303 I print_info: ssm_d_state      = 0
0.00.044.303 I print_info: ssm_dt_rank      = 0
0.00.044.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.304 I print_info: model type       = 1.4B
0.00.044.305 I print_info: model params     = 1.41 B
0.00.044.305 I print_info: general.name     = 1.4B
0.00.044.305 I print_info: vocab type       = BPE
0.00.044.305 I print_info: n_vocab          = 50304
0.00.044.306 I print_info: n_merges         = 50009
0.00.044.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.306 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.306 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.306 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.307 I print_info: LF token         = 187 'Ċ'
0.00.044.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.308 I print_info: max token length = 1024
0.00.044.309 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.431 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.444 I load_tensors: offloading output layer to GPU
0.00.663.445 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.485 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.663.486 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.664.533 I llama_init_from_model: n_seq_max     = 1
0.00.664.536 I llama_init_from_model: n_ctx         = 2048
0.00.664.537 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.664.537 I llama_init_from_model: n_batch       = 2048
0.00.664.537 I llama_init_from_model: n_ubatch      = 512
0.00.664.538 I llama_init_from_model: flash_attn    = 0
0.00.664.540 I llama_init_from_model: freq_base     = 10000.0
0.00.664.540 I llama_init_from_model: freq_scale    = 1
0.00.664.542 I ggml_metal_init: allocating
0.00.664.588 I ggml_metal_init: found device: Apple M4
0.00.664.600 I ggml_metal_init: picking default device: Apple M4
0.00.666.276 I ggml_metal_init: using embedded metal library
0.00.672.643 I ggml_metal_init: GPU name:   Apple M4
0.00.672.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.649 I ggml_metal_init: simdgroup reduction   = true
0.00.672.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.649 I ggml_metal_init: has residency sets    = true
0.00.672.650 I ggml_metal_init: has bfloat            = true
0.00.672.650 I ggml_metal_init: use bfloat            = true
0.00.672.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.695 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.026 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.042 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.745.066 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.054 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.056 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.057 I llama_init_from_model: graph nodes  = 967
0.00.750.057 I llama_init_from_model: graph splits = 2
0.00.750.062 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.495 I main: llama threadpool init, n_threads = 4
0.00.818.539 I 
0.00.818.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.562 I 
0.00.818.739 I sampler seed: 1234
0.00.818.743 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.755 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.755 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.702.165 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.702.165 I llama_perf_context_print:        load time =     805.33 ms
0.01.702.166 I llama_perf_context_print: prompt eval time =      54.10 ms /     7 tokens (    7.73 ms per token,   129.40 tokens per second)
0.01.702.168 I llama_perf_context_print:        eval time =     826.36 ms /    63 runs   (   13.12 ms per token,    76.24 tokens per second)
0.01.702.168 I llama_perf_context_print:       total time =     884.41 ms /    70 tokens
0.01.702.439 I ggml_metal_free: deallocating

real	0m1.721s
user	0m0.111s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.220 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.446 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.750 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.766 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.729 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.762 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.645 I llama_model_loader: - type  f32:  194 tensors
0.00.035.645 I llama_model_loader: - type  f16:   98 tensors
0.00.035.646 I print_info: file format = GGUF V3 (latest)
0.00.035.647 I print_info: file type   = all F32 (guessed)
0.00.035.648 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.044.145 I load: special tokens cache size = 25
0.00.050.427 I load: token to piece cache size = 0.2984 MB
0.00.050.431 I print_info: arch             = gptneox
0.00.050.432 I print_info: vocab_only       = 0
0.00.050.432 I print_info: n_ctx_train      = 2048
0.00.050.432 I print_info: n_embd           = 2048
0.00.050.432 I print_info: n_layer          = 24
0.00.050.436 I print_info: n_head           = 16
0.00.050.437 I print_info: n_head_kv        = 16
0.00.050.437 I print_info: n_rot            = 32
0.00.050.438 I print_info: n_swa            = 0
0.00.050.438 I print_info: n_embd_head_k    = 128
0.00.050.438 I print_info: n_embd_head_v    = 128
0.00.050.439 I print_info: n_gqa            = 1
0.00.050.440 I print_info: n_embd_k_gqa     = 2048
0.00.050.440 I print_info: n_embd_v_gqa     = 2048
0.00.050.441 I print_info: f_norm_eps       = 1.0e-05
0.00.050.441 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.442 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.444 I print_info: f_logit_scale    = 0.0e+00
0.00.050.445 I print_info: n_ff             = 8192
0.00.050.445 I print_info: n_expert         = 0
0.00.050.445 I print_info: n_expert_used    = 0
0.00.050.445 I print_info: causal attn      = 1
0.00.050.445 I print_info: pooling type     = 0
0.00.050.446 I print_info: rope type        = 2
0.00.050.446 I print_info: rope scaling     = linear
0.00.050.446 I print_info: freq_base_train  = 10000.0
0.00.050.446 I print_info: freq_scale_train = 1
0.00.050.447 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.448 I print_info: rope_finetuned   = unknown
0.00.050.448 I print_info: ssm_d_conv       = 0
0.00.050.448 I print_info: ssm_d_inner      = 0
0.00.050.448 I print_info: ssm_d_state      = 0
0.00.050.448 I print_info: ssm_dt_rank      = 0
0.00.050.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.449 I print_info: model type       = 1.4B
0.00.050.449 I print_info: model params     = 1.41 B
0.00.050.449 I print_info: general.name     = 1.4B
0.00.050.449 I print_info: vocab type       = BPE
0.00.050.451 I print_info: n_vocab          = 50304
0.00.050.451 I print_info: n_merges         = 50009
0.00.050.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.452 I print_info: LF token         = 187 'Ċ'
0.00.050.452 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.452 I print_info: max token length = 1024
0.00.050.453 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.938.489 I load_tensors: offloading 24 repeating layers to GPU
0.00.938.495 I load_tensors: offloading output layer to GPU
0.00.938.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.938.523 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.938.524 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.939.201 I llama_init_from_model: n_seq_max     = 1
0.00.939.202 I llama_init_from_model: n_ctx         = 128
0.00.939.203 I llama_init_from_model: n_ctx_per_seq = 128
0.00.939.203 I llama_init_from_model: n_batch       = 128
0.00.939.203 I llama_init_from_model: n_ubatch      = 128
0.00.939.203 I llama_init_from_model: flash_attn    = 0
0.00.939.205 I llama_init_from_model: freq_base     = 10000.0
0.00.939.205 I llama_init_from_model: freq_scale    = 1
0.00.939.205 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.939.206 I ggml_metal_init: allocating
0.00.939.292 I ggml_metal_init: found device: Apple M4
0.00.939.298 I ggml_metal_init: picking default device: Apple M4
0.00.940.491 I ggml_metal_init: using embedded metal library
0.00.944.333 I ggml_metal_init: GPU name:   Apple M4
0.00.944.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.944.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.944.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.944.338 I ggml_metal_init: simdgroup reduction   = true
0.00.944.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.944.338 I ggml_metal_init: has residency sets    = true
0.00.944.338 I ggml_metal_init: has bfloat            = true
0.00.944.338 I ggml_metal_init: use bfloat            = true
0.00.944.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.944.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.955.921 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.957.669 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.957.675 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.957.688 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.959.341 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.959.342 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.959.343 I llama_init_from_model: graph nodes  = 967
0.00.959.343 I llama_init_from_model: graph splits = 2
0.00.959.344 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.959.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.993.567 I 
0.00.993.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.993.635 I perplexity: tokenizing the input ..
0.00.998.912 I perplexity: tokenization took 5.275 ms
0.00.998.934 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.117.405 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.118.749 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.118.762 I llama_perf_context_print:        load time =     976.11 ms
0.01.118.763 I llama_perf_context_print: prompt eval time =     118.16 ms /   128 tokens (    0.92 ms per token,  1083.24 tokens per second)
0.01.118.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.118.764 I llama_perf_context_print:       total time =     125.20 ms /   129 tokens
0.01.119.151 I ggml_metal_free: deallocating

real	0m1.293s
user	0m0.076s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.616 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.835 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.842 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.842 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.842 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.454 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.454 I llama_model_loader: - type  f32:  194 tensors
0.00.025.455 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.455 I print_info: file format = GGUF V3 (latest)
0.00.025.456 I print_info: file type   = Q8_0
0.00.025.457 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.637 I load: special tokens cache size = 25
0.00.040.004 I load: token to piece cache size = 0.2984 MB
0.00.040.009 I print_info: arch             = gptneox
0.00.040.009 I print_info: vocab_only       = 0
0.00.040.009 I print_info: n_ctx_train      = 2048
0.00.040.010 I print_info: n_embd           = 2048
0.00.040.010 I print_info: n_layer          = 24
0.00.040.014 I print_info: n_head           = 16
0.00.040.015 I print_info: n_head_kv        = 16
0.00.040.015 I print_info: n_rot            = 32
0.00.040.015 I print_info: n_swa            = 0
0.00.040.015 I print_info: n_embd_head_k    = 128
0.00.040.016 I print_info: n_embd_head_v    = 128
0.00.040.016 I print_info: n_gqa            = 1
0.00.040.017 I print_info: n_embd_k_gqa     = 2048
0.00.040.018 I print_info: n_embd_v_gqa     = 2048
0.00.040.018 I print_info: f_norm_eps       = 1.0e-05
0.00.040.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.019 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.019 I print_info: f_logit_scale    = 0.0e+00
0.00.040.020 I print_info: n_ff             = 8192
0.00.040.020 I print_info: n_expert         = 0
0.00.040.020 I print_info: n_expert_used    = 0
0.00.040.020 I print_info: causal attn      = 1
0.00.040.020 I print_info: pooling type     = 0
0.00.040.020 I print_info: rope type        = 2
0.00.040.021 I print_info: rope scaling     = linear
0.00.040.021 I print_info: freq_base_train  = 10000.0
0.00.040.021 I print_info: freq_scale_train = 1
0.00.040.021 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.022 I print_info: rope_finetuned   = unknown
0.00.040.022 I print_info: ssm_d_conv       = 0
0.00.040.022 I print_info: ssm_d_inner      = 0
0.00.040.022 I print_info: ssm_d_state      = 0
0.00.040.022 I print_info: ssm_dt_rank      = 0
0.00.040.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.025 I print_info: model type       = 1.4B
0.00.040.025 I print_info: model params     = 1.41 B
0.00.040.026 I print_info: general.name     = 1.4B
0.00.040.046 I print_info: vocab type       = BPE
0.00.040.048 I print_info: n_vocab          = 50304
0.00.040.048 I print_info: n_merges         = 50009
0.00.040.048 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.051 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.052 I print_info: LF token         = 187 'Ċ'
0.00.040.052 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.052 I print_info: max token length = 1024
0.00.040.053 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.916.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.916.415 I load_tensors: offloading output layer to GPU
0.00.916.416 I load_tensors: offloaded 25/25 layers to GPU
0.00.916.445 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.916.447 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.917.737 I llama_init_from_model: n_seq_max     = 1
0.00.917.739 I llama_init_from_model: n_ctx         = 128
0.00.917.739 I llama_init_from_model: n_ctx_per_seq = 128
0.00.917.740 I llama_init_from_model: n_batch       = 128
0.00.917.740 I llama_init_from_model: n_ubatch      = 128
0.00.917.740 I llama_init_from_model: flash_attn    = 0
0.00.917.742 I llama_init_from_model: freq_base     = 10000.0
0.00.917.742 I llama_init_from_model: freq_scale    = 1
0.00.917.742 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.917.744 I ggml_metal_init: allocating
0.00.917.826 I ggml_metal_init: found device: Apple M4
0.00.917.836 I ggml_metal_init: picking default device: Apple M4
0.00.919.198 I ggml_metal_init: using embedded metal library
0.00.924.962 I ggml_metal_init: GPU name:   Apple M4
0.00.924.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.924.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.924.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.924.967 I ggml_metal_init: simdgroup reduction   = true
0.00.924.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.924.968 I ggml_metal_init: has residency sets    = true
0.00.924.968 I ggml_metal_init: has bfloat            = true
0.00.924.968 I ggml_metal_init: use bfloat            = true
0.00.924.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.924.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.940.781 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.944.040 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.944.043 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.944.065 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.947.065 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.947.066 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.947.067 I llama_init_from_model: graph nodes  = 967
0.00.947.067 I llama_init_from_model: graph splits = 2
0.00.947.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.947.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.971.761 I 
0.00.971.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.971.831 I perplexity: tokenizing the input ..
0.00.978.803 I perplexity: tokenization took 6.969 ms
0.00.978.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.104.533 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.105.805 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.105.816 I llama_perf_context_print:        load time =     962.14 ms
0.01.105.817 I llama_perf_context_print: prompt eval time =     124.69 ms /   128 tokens (    0.97 ms per token,  1026.53 tokens per second)
0.01.105.817 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.105.818 I llama_perf_context_print:       total time =     134.06 ms /   129 tokens
0.01.106.158 I ggml_metal_free: deallocating

real	0m1.120s
user	0m0.079s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.183 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.910 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.924 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.925 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.926 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.926 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.927 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.927 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.929 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.706 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.706 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.707 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.707 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.707 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.708 I llama_model_loader: - type  f32:  194 tensors
0.00.028.708 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.708 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.709 I print_info: file format = GGUF V3 (latest)
0.00.028.710 I print_info: file type   = Q4_0
0.00.028.711 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.037.179 I load: special tokens cache size = 25
0.00.043.442 I load: token to piece cache size = 0.2984 MB
0.00.043.446 I print_info: arch             = gptneox
0.00.043.446 I print_info: vocab_only       = 0
0.00.043.446 I print_info: n_ctx_train      = 2048
0.00.043.447 I print_info: n_embd           = 2048
0.00.043.447 I print_info: n_layer          = 24
0.00.043.451 I print_info: n_head           = 16
0.00.043.452 I print_info: n_head_kv        = 16
0.00.043.452 I print_info: n_rot            = 32
0.00.043.453 I print_info: n_swa            = 0
0.00.043.453 I print_info: n_embd_head_k    = 128
0.00.043.453 I print_info: n_embd_head_v    = 128
0.00.043.454 I print_info: n_gqa            = 1
0.00.043.455 I print_info: n_embd_k_gqa     = 2048
0.00.043.455 I print_info: n_embd_v_gqa     = 2048
0.00.043.456 I print_info: f_norm_eps       = 1.0e-05
0.00.043.456 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.456 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.457 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.457 I print_info: f_logit_scale    = 0.0e+00
0.00.043.457 I print_info: n_ff             = 8192
0.00.043.458 I print_info: n_expert         = 0
0.00.043.458 I print_info: n_expert_used    = 0
0.00.043.458 I print_info: causal attn      = 1
0.00.043.458 I print_info: pooling type     = 0
0.00.043.458 I print_info: rope type        = 2
0.00.043.458 I print_info: rope scaling     = linear
0.00.043.459 I print_info: freq_base_train  = 10000.0
0.00.043.459 I print_info: freq_scale_train = 1
0.00.043.459 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.463 I print_info: rope_finetuned   = unknown
0.00.043.463 I print_info: ssm_d_conv       = 0
0.00.043.463 I print_info: ssm_d_inner      = 0
0.00.043.463 I print_info: ssm_d_state      = 0
0.00.043.463 I print_info: ssm_dt_rank      = 0
0.00.043.463 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.463 I print_info: model type       = 1.4B
0.00.043.464 I print_info: model params     = 1.41 B
0.00.043.464 I print_info: general.name     = 1.4B
0.00.043.464 I print_info: vocab type       = BPE
0.00.043.465 I print_info: n_vocab          = 50304
0.00.043.465 I print_info: n_merges         = 50009
0.00.043.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.467 I print_info: LF token         = 187 'Ċ'
0.00.043.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.467 I print_info: max token length = 1024
0.00.043.468 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.629.441 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.466 I load_tensors: offloading output layer to GPU
0.00.629.467 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.508 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.629.510 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.630.797 I llama_init_from_model: n_seq_max     = 1
0.00.630.799 I llama_init_from_model: n_ctx         = 128
0.00.630.800 I llama_init_from_model: n_ctx_per_seq = 128
0.00.630.801 I llama_init_from_model: n_batch       = 128
0.00.630.801 I llama_init_from_model: n_ubatch      = 128
0.00.630.801 I llama_init_from_model: flash_attn    = 0
0.00.630.804 I llama_init_from_model: freq_base     = 10000.0
0.00.630.804 I llama_init_from_model: freq_scale    = 1
0.00.630.805 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.807 I ggml_metal_init: allocating
0.00.630.921 I ggml_metal_init: found device: Apple M4
0.00.630.938 I ggml_metal_init: picking default device: Apple M4
0.00.632.840 I ggml_metal_init: using embedded metal library
0.00.638.654 I ggml_metal_init: GPU name:   Apple M4
0.00.638.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.667 I ggml_metal_init: simdgroup reduction   = true
0.00.638.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.668 I ggml_metal_init: has residency sets    = true
0.00.638.668 I ggml_metal_init: has bfloat            = true
0.00.638.669 I ggml_metal_init: use bfloat            = true
0.00.638.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.758 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.474 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.661.479 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.518 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.762 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.664.764 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.664.764 I llama_init_from_model: graph nodes  = 967
0.00.664.765 I llama_init_from_model: graph splits = 2
0.00.664.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.071 I 
0.00.692.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.143 I perplexity: tokenizing the input ..
0.00.698.706 I perplexity: tokenization took 6.559 ms
0.00.698.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.839 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.836.175 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.836.187 I llama_perf_context_print:        load time =     680.88 ms
0.00.836.188 I llama_perf_context_print: prompt eval time =     135.16 ms /   128 tokens (    1.06 ms per token,   947.03 tokens per second)
0.00.836.189 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.189 I llama_perf_context_print:       total time =     144.12 ms /   129 tokens
0.00.836.609 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.081s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.913 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.919 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.932 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.932 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.563 I llama_model_loader: - type  f32:  194 tensors
0.00.025.563 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.564 I print_info: file format = GGUF V3 (latest)
0.00.025.565 I print_info: file type   = Q4_1
0.00.025.566 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.073 I load: special tokens cache size = 25
0.00.040.184 I load: token to piece cache size = 0.2984 MB
0.00.040.188 I print_info: arch             = gptneox
0.00.040.189 I print_info: vocab_only       = 0
0.00.040.189 I print_info: n_ctx_train      = 2048
0.00.040.189 I print_info: n_embd           = 2048
0.00.040.189 I print_info: n_layer          = 24
0.00.040.194 I print_info: n_head           = 16
0.00.040.195 I print_info: n_head_kv        = 16
0.00.040.195 I print_info: n_rot            = 32
0.00.040.196 I print_info: n_swa            = 0
0.00.040.196 I print_info: n_embd_head_k    = 128
0.00.040.197 I print_info: n_embd_head_v    = 128
0.00.040.197 I print_info: n_gqa            = 1
0.00.040.198 I print_info: n_embd_k_gqa     = 2048
0.00.040.200 I print_info: n_embd_v_gqa     = 2048
0.00.040.201 I print_info: f_norm_eps       = 1.0e-05
0.00.040.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.201 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.202 I print_info: f_logit_scale    = 0.0e+00
0.00.040.202 I print_info: n_ff             = 8192
0.00.040.202 I print_info: n_expert         = 0
0.00.040.202 I print_info: n_expert_used    = 0
0.00.040.202 I print_info: causal attn      = 1
0.00.040.203 I print_info: pooling type     = 0
0.00.040.203 I print_info: rope type        = 2
0.00.040.203 I print_info: rope scaling     = linear
0.00.040.203 I print_info: freq_base_train  = 10000.0
0.00.040.204 I print_info: freq_scale_train = 1
0.00.040.204 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.205 I print_info: rope_finetuned   = unknown
0.00.040.205 I print_info: ssm_d_conv       = 0
0.00.040.205 I print_info: ssm_d_inner      = 0
0.00.040.205 I print_info: ssm_d_state      = 0
0.00.040.205 I print_info: ssm_dt_rank      = 0
0.00.040.205 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.206 I print_info: model type       = 1.4B
0.00.040.206 I print_info: model params     = 1.41 B
0.00.040.206 I print_info: general.name     = 1.4B
0.00.040.207 I print_info: vocab type       = BPE
0.00.040.207 I print_info: n_vocab          = 50304
0.00.040.208 I print_info: n_merges         = 50009
0.00.040.208 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.208 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: LF token         = 187 'Ċ'
0.00.040.209 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: max token length = 1024
0.00.040.210 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.699.244 I load_tensors: offloading 24 repeating layers to GPU
0.00.699.263 I load_tensors: offloading output layer to GPU
0.00.699.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.699.298 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.699.300 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.700.665 I llama_init_from_model: n_seq_max     = 1
0.00.700.673 I llama_init_from_model: n_ctx         = 128
0.00.700.673 I llama_init_from_model: n_ctx_per_seq = 128
0.00.700.674 I llama_init_from_model: n_batch       = 128
0.00.700.674 I llama_init_from_model: n_ubatch      = 128
0.00.700.674 I llama_init_from_model: flash_attn    = 0
0.00.700.678 I llama_init_from_model: freq_base     = 10000.0
0.00.700.678 I llama_init_from_model: freq_scale    = 1
0.00.700.679 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.700.681 I ggml_metal_init: allocating
0.00.700.761 I ggml_metal_init: found device: Apple M4
0.00.700.777 I ggml_metal_init: picking default device: Apple M4
0.00.702.694 I ggml_metal_init: using embedded metal library
0.00.708.404 I ggml_metal_init: GPU name:   Apple M4
0.00.708.412 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.708.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.708.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.708.415 I ggml_metal_init: simdgroup reduction   = true
0.00.708.415 I ggml_metal_init: simdgroup matrix mul. = true
0.00.708.416 I ggml_metal_init: has residency sets    = true
0.00.708.416 I ggml_metal_init: has bfloat            = true
0.00.708.416 I ggml_metal_init: use bfloat            = true
0.00.708.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.708.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.963 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.506 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.731.514 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.731.542 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.837 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.734.839 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.734.840 I llama_init_from_model: graph nodes  = 967
0.00.734.840 I llama_init_from_model: graph splits = 2
0.00.734.843 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.734.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.448 I 
0.00.759.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.519 I perplexity: tokenizing the input ..
0.00.765.971 I perplexity: tokenization took 6.45 ms
0.00.765.993 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.902.921 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.904.253 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.904.267 I llama_perf_context_print:        load time =     750.08 ms
0.00.904.268 I llama_perf_context_print: prompt eval time =     136.30 ms /   128 tokens (    1.06 ms per token,   939.13 tokens per second)
0.00.904.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.904.269 I llama_perf_context_print:       total time =     144.82 ms /   129 tokens
0.00.904.643 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.081s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.327 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.849 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.855 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.864 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.865 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.866 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.868 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.868 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.868 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.699 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.700 I llama_model_loader: - type  f32:  194 tensors
0.00.026.700 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.701 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.702 I print_info: file format = GGUF V3 (latest)
0.00.026.702 I print_info: file type   = Q5_0
0.00.026.703 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.899 I load: special tokens cache size = 25
0.00.040.962 I load: token to piece cache size = 0.2984 MB
0.00.040.967 I print_info: arch             = gptneox
0.00.040.967 I print_info: vocab_only       = 0
0.00.040.968 I print_info: n_ctx_train      = 2048
0.00.040.968 I print_info: n_embd           = 2048
0.00.040.968 I print_info: n_layer          = 24
0.00.040.973 I print_info: n_head           = 16
0.00.040.976 I print_info: n_head_kv        = 16
0.00.040.977 I print_info: n_rot            = 32
0.00.040.977 I print_info: n_swa            = 0
0.00.040.977 I print_info: n_embd_head_k    = 128
0.00.040.977 I print_info: n_embd_head_v    = 128
0.00.040.978 I print_info: n_gqa            = 1
0.00.040.978 I print_info: n_embd_k_gqa     = 2048
0.00.040.979 I print_info: n_embd_v_gqa     = 2048
0.00.040.979 I print_info: f_norm_eps       = 1.0e-05
0.00.040.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.980 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.980 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.980 I print_info: f_logit_scale    = 0.0e+00
0.00.040.981 I print_info: n_ff             = 8192
0.00.040.981 I print_info: n_expert         = 0
0.00.040.982 I print_info: n_expert_used    = 0
0.00.040.983 I print_info: causal attn      = 1
0.00.040.983 I print_info: pooling type     = 0
0.00.040.983 I print_info: rope type        = 2
0.00.040.983 I print_info: rope scaling     = linear
0.00.040.983 I print_info: freq_base_train  = 10000.0
0.00.040.984 I print_info: freq_scale_train = 1
0.00.040.984 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.984 I print_info: rope_finetuned   = unknown
0.00.040.984 I print_info: ssm_d_conv       = 0
0.00.040.985 I print_info: ssm_d_inner      = 0
0.00.040.985 I print_info: ssm_d_state      = 0
0.00.040.985 I print_info: ssm_dt_rank      = 0
0.00.040.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.985 I print_info: model type       = 1.4B
0.00.040.985 I print_info: model params     = 1.41 B
0.00.040.986 I print_info: general.name     = 1.4B
0.00.040.986 I print_info: vocab type       = BPE
0.00.040.986 I print_info: n_vocab          = 50304
0.00.040.986 I print_info: n_merges         = 50009
0.00.040.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.988 I print_info: LF token         = 187 'Ċ'
0.00.040.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.988 I print_info: max token length = 1024
0.00.040.989 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.713.488 I load_tensors: offloading 24 repeating layers to GPU
0.00.713.511 I load_tensors: offloading output layer to GPU
0.00.713.511 I load_tensors: offloaded 25/25 layers to GPU
0.00.713.552 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.713.560 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.714.998 I llama_init_from_model: n_seq_max     = 1
0.00.715.007 I llama_init_from_model: n_ctx         = 128
0.00.715.007 I llama_init_from_model: n_ctx_per_seq = 128
0.00.715.008 I llama_init_from_model: n_batch       = 128
0.00.715.008 I llama_init_from_model: n_ubatch      = 128
0.00.715.009 I llama_init_from_model: flash_attn    = 0
0.00.715.012 I llama_init_from_model: freq_base     = 10000.0
0.00.715.012 I llama_init_from_model: freq_scale    = 1
0.00.715.013 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.715.018 I ggml_metal_init: allocating
0.00.715.121 I ggml_metal_init: found device: Apple M4
0.00.715.138 I ggml_metal_init: picking default device: Apple M4
0.00.717.089 I ggml_metal_init: using embedded metal library
0.00.723.279 I ggml_metal_init: GPU name:   Apple M4
0.00.723.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.723.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.723.288 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.723.288 I ggml_metal_init: simdgroup reduction   = true
0.00.723.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.723.289 I ggml_metal_init: has residency sets    = true
0.00.723.289 I ggml_metal_init: has bfloat            = true
0.00.723.290 I ggml_metal_init: use bfloat            = true
0.00.723.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.723.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.741.957 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.435 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.745.439 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.745.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.741 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.748.743 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.748.744 I llama_init_from_model: graph nodes  = 967
0.00.748.744 I llama_init_from_model: graph splits = 2
0.00.748.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.748.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.824 I 
0.00.775.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.899 I perplexity: tokenizing the input ..
0.00.781.994 I perplexity: tokenization took 6.092 ms
0.00.782.011 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.916.897 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.918.238 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.918.251 I llama_perf_context_print:        load time =     765.49 ms
0.00.918.253 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.14 tokens per second)
0.00.918.254 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.918.254 I llama_perf_context_print:       total time =     142.43 ms /   129 tokens
0.00.918.692 I ggml_metal_free: deallocating

real	0m0.934s
user	0m0.079s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.004 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.013 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.968 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.929 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.931 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.931 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.932 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.932 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.933 I llama_model_loader: - type  f32:  194 tensors
0.00.025.933 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.933 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.934 I print_info: file format = GGUF V3 (latest)
0.00.025.935 I print_info: file type   = Q5_1
0.00.025.936 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.360 I load: special tokens cache size = 25
0.00.040.483 I load: token to piece cache size = 0.2984 MB
0.00.040.488 I print_info: arch             = gptneox
0.00.040.488 I print_info: vocab_only       = 0
0.00.040.488 I print_info: n_ctx_train      = 2048
0.00.040.488 I print_info: n_embd           = 2048
0.00.040.489 I print_info: n_layer          = 24
0.00.040.493 I print_info: n_head           = 16
0.00.040.494 I print_info: n_head_kv        = 16
0.00.040.494 I print_info: n_rot            = 32
0.00.040.494 I print_info: n_swa            = 0
0.00.040.494 I print_info: n_embd_head_k    = 128
0.00.040.494 I print_info: n_embd_head_v    = 128
0.00.040.499 I print_info: n_gqa            = 1
0.00.040.499 I print_info: n_embd_k_gqa     = 2048
0.00.040.500 I print_info: n_embd_v_gqa     = 2048
0.00.040.501 I print_info: f_norm_eps       = 1.0e-05
0.00.040.501 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.501 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.501 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.501 I print_info: f_logit_scale    = 0.0e+00
0.00.040.502 I print_info: n_ff             = 8192
0.00.040.502 I print_info: n_expert         = 0
0.00.040.502 I print_info: n_expert_used    = 0
0.00.040.502 I print_info: causal attn      = 1
0.00.040.503 I print_info: pooling type     = 0
0.00.040.504 I print_info: rope type        = 2
0.00.040.504 I print_info: rope scaling     = linear
0.00.040.505 I print_info: freq_base_train  = 10000.0
0.00.040.511 I print_info: freq_scale_train = 1
0.00.040.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.512 I print_info: rope_finetuned   = unknown
0.00.040.512 I print_info: ssm_d_conv       = 0
0.00.040.512 I print_info: ssm_d_inner      = 0
0.00.040.512 I print_info: ssm_d_state      = 0
0.00.040.512 I print_info: ssm_dt_rank      = 0
0.00.040.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.513 I print_info: model type       = 1.4B
0.00.040.513 I print_info: model params     = 1.41 B
0.00.040.513 I print_info: general.name     = 1.4B
0.00.040.514 I print_info: vocab type       = BPE
0.00.040.514 I print_info: n_vocab          = 50304
0.00.040.514 I print_info: n_merges         = 50009
0.00.040.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.515 I print_info: LF token         = 187 'Ċ'
0.00.040.516 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.516 I print_info: max token length = 1024
0.00.040.516 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.922 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.941 I load_tensors: offloading output layer to GPU
0.00.615.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.975 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.615.976 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.617.362 I llama_init_from_model: n_seq_max     = 1
0.00.617.367 I llama_init_from_model: n_ctx         = 128
0.00.617.367 I llama_init_from_model: n_ctx_per_seq = 128
0.00.617.368 I llama_init_from_model: n_batch       = 128
0.00.617.368 I llama_init_from_model: n_ubatch      = 128
0.00.617.369 I llama_init_from_model: flash_attn    = 0
0.00.617.371 I llama_init_from_model: freq_base     = 10000.0
0.00.617.371 I llama_init_from_model: freq_scale    = 1
0.00.617.372 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.374 I ggml_metal_init: allocating
0.00.617.461 I ggml_metal_init: found device: Apple M4
0.00.617.477 I ggml_metal_init: picking default device: Apple M4
0.00.619.441 I ggml_metal_init: using embedded metal library
0.00.626.103 I ggml_metal_init: GPU name:   Apple M4
0.00.626.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.112 I ggml_metal_init: simdgroup reduction   = true
0.00.626.112 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.112 I ggml_metal_init: has residency sets    = true
0.00.626.113 I ggml_metal_init: has bfloat            = true
0.00.626.113 I ggml_metal_init: use bfloat            = true
0.00.626.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.589 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.882 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.646.885 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.907 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.176 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.178 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.178 I llama_init_from_model: graph nodes  = 967
0.00.650.178 I llama_init_from_model: graph splits = 2
0.00.650.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.821 I 
0.00.675.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.897 I perplexity: tokenizing the input ..
0.00.681.954 I perplexity: tokenization took 6.055 ms
0.00.681.966 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.363 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.817.706 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.817.719 I llama_perf_context_print:        load time =     665.99 ms
0.00.817.720 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.23 tokens per second)
0.00.817.720 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.721 I llama_perf_context_print:       total time =     141.90 ms /   129 tokens
0.00.818.102 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.079s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.266 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.217 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.218 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.218 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.219 I llama_model_loader: - type  f32:  194 tensors
0.00.026.219 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.220 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.220 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.221 I print_info: file format = GGUF V3 (latest)
0.00.026.221 I print_info: file type   = Q2_K - Medium
0.00.026.222 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.690 I load: special tokens cache size = 25
0.00.040.735 I load: token to piece cache size = 0.2984 MB
0.00.040.739 I print_info: arch             = gptneox
0.00.040.740 I print_info: vocab_only       = 0
0.00.040.740 I print_info: n_ctx_train      = 2048
0.00.040.740 I print_info: n_embd           = 2048
0.00.040.740 I print_info: n_layer          = 24
0.00.040.745 I print_info: n_head           = 16
0.00.040.745 I print_info: n_head_kv        = 16
0.00.040.746 I print_info: n_rot            = 32
0.00.040.747 I print_info: n_swa            = 0
0.00.040.747 I print_info: n_embd_head_k    = 128
0.00.040.750 I print_info: n_embd_head_v    = 128
0.00.040.750 I print_info: n_gqa            = 1
0.00.040.751 I print_info: n_embd_k_gqa     = 2048
0.00.040.751 I print_info: n_embd_v_gqa     = 2048
0.00.040.752 I print_info: f_norm_eps       = 1.0e-05
0.00.040.752 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.752 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.754 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.754 I print_info: f_logit_scale    = 0.0e+00
0.00.040.754 I print_info: n_ff             = 8192
0.00.040.755 I print_info: n_expert         = 0
0.00.040.755 I print_info: n_expert_used    = 0
0.00.040.756 I print_info: causal attn      = 1
0.00.040.757 I print_info: pooling type     = 0
0.00.040.757 I print_info: rope type        = 2
0.00.040.757 I print_info: rope scaling     = linear
0.00.040.758 I print_info: freq_base_train  = 10000.0
0.00.040.758 I print_info: freq_scale_train = 1
0.00.040.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.758 I print_info: rope_finetuned   = unknown
0.00.040.758 I print_info: ssm_d_conv       = 0
0.00.040.758 I print_info: ssm_d_inner      = 0
0.00.040.758 I print_info: ssm_d_state      = 0
0.00.040.759 I print_info: ssm_dt_rank      = 0
0.00.040.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.760 I print_info: model type       = 1.4B
0.00.040.760 I print_info: model params     = 1.41 B
0.00.040.760 I print_info: general.name     = 1.4B
0.00.040.761 I print_info: vocab type       = BPE
0.00.040.761 I print_info: n_vocab          = 50304
0.00.040.761 I print_info: n_merges         = 50009
0.00.040.761 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.761 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.762 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.762 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.762 I print_info: LF token         = 187 'Ċ'
0.00.040.762 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.762 I print_info: max token length = 1024
0.00.040.763 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.360.769 I load_tensors: offloading 24 repeating layers to GPU
0.00.360.784 I load_tensors: offloading output layer to GPU
0.00.360.785 I load_tensors: offloaded 25/25 layers to GPU
0.00.360.817 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.360.818 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.362.181 I llama_init_from_model: n_seq_max     = 1
0.00.362.189 I llama_init_from_model: n_ctx         = 128
0.00.362.189 I llama_init_from_model: n_ctx_per_seq = 128
0.00.362.190 I llama_init_from_model: n_batch       = 128
0.00.362.190 I llama_init_from_model: n_ubatch      = 128
0.00.362.190 I llama_init_from_model: flash_attn    = 0
0.00.362.193 I llama_init_from_model: freq_base     = 10000.0
0.00.362.194 I llama_init_from_model: freq_scale    = 1
0.00.362.194 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.362.197 I ggml_metal_init: allocating
0.00.362.274 I ggml_metal_init: found device: Apple M4
0.00.362.288 I ggml_metal_init: picking default device: Apple M4
0.00.364.199 I ggml_metal_init: using embedded metal library
0.00.370.020 I ggml_metal_init: GPU name:   Apple M4
0.00.370.038 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.370.039 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.370.039 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.370.041 I ggml_metal_init: simdgroup reduction   = true
0.00.370.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.370.041 I ggml_metal_init: has residency sets    = true
0.00.370.042 I ggml_metal_init: has bfloat            = true
0.00.370.042 I ggml_metal_init: use bfloat            = true
0.00.370.045 I ggml_metal_init: hasUnifiedMemory      = true
0.00.370.050 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.392.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.396.836 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.396.842 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.396.875 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.400.381 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.400.383 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.400.383 I llama_init_from_model: graph nodes  = 967
0.00.400.383 I llama_init_from_model: graph splits = 2
0.00.400.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.400.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.960 I 
0.00.433.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.057 I perplexity: tokenizing the input ..
0.00.439.396 I perplexity: tokenization took 6.334 ms
0.00.439.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.578.186 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.528 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.541 I llama_perf_context_print:        load time =     422.69 ms
0.00.579.541 I llama_perf_context_print: prompt eval time =     138.14 ms /   128 tokens (    1.08 ms per token,   926.60 tokens per second)
0.00.579.542 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.542 I llama_perf_context_print:       total time =     146.58 ms /   129 tokens
0.00.579.901 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.083s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.131 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.647 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.663 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.663 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.664 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.611 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.612 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.612 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.614 I llama_model_loader: - type  f32:  194 tensors
0.00.025.615 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.615 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.615 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.615 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.616 I print_info: file format = GGUF V3 (latest)
0.00.025.620 I print_info: file type   = Q3_K - Medium
0.00.025.622 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.734 I load: special tokens cache size = 25
0.00.039.682 I load: token to piece cache size = 0.2984 MB
0.00.039.685 I print_info: arch             = gptneox
0.00.039.686 I print_info: vocab_only       = 0
0.00.039.686 I print_info: n_ctx_train      = 2048
0.00.039.686 I print_info: n_embd           = 2048
0.00.039.686 I print_info: n_layer          = 24
0.00.039.690 I print_info: n_head           = 16
0.00.039.691 I print_info: n_head_kv        = 16
0.00.039.691 I print_info: n_rot            = 32
0.00.039.692 I print_info: n_swa            = 0
0.00.039.692 I print_info: n_embd_head_k    = 128
0.00.039.692 I print_info: n_embd_head_v    = 128
0.00.039.693 I print_info: n_gqa            = 1
0.00.039.693 I print_info: n_embd_k_gqa     = 2048
0.00.039.694 I print_info: n_embd_v_gqa     = 2048
0.00.039.695 I print_info: f_norm_eps       = 1.0e-05
0.00.039.696 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.697 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.697 I print_info: f_logit_scale    = 0.0e+00
0.00.039.697 I print_info: n_ff             = 8192
0.00.039.698 I print_info: n_expert         = 0
0.00.039.698 I print_info: n_expert_used    = 0
0.00.039.698 I print_info: causal attn      = 1
0.00.039.698 I print_info: pooling type     = 0
0.00.039.698 I print_info: rope type        = 2
0.00.039.699 I print_info: rope scaling     = linear
0.00.039.699 I print_info: freq_base_train  = 10000.0
0.00.039.699 I print_info: freq_scale_train = 1
0.00.039.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.700 I print_info: rope_finetuned   = unknown
0.00.039.700 I print_info: ssm_d_conv       = 0
0.00.039.700 I print_info: ssm_d_inner      = 0
0.00.039.700 I print_info: ssm_d_state      = 0
0.00.039.700 I print_info: ssm_dt_rank      = 0
0.00.039.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.701 I print_info: model type       = 1.4B
0.00.039.701 I print_info: model params     = 1.41 B
0.00.039.701 I print_info: general.name     = 1.4B
0.00.039.702 I print_info: vocab type       = BPE
0.00.039.702 I print_info: n_vocab          = 50304
0.00.039.702 I print_info: n_merges         = 50009
0.00.039.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: LF token         = 187 'Ċ'
0.00.039.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.704 I print_info: max token length = 1024
0.00.039.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.316 I load_tensors: offloading 24 repeating layers to GPU
0.00.455.339 I load_tensors: offloading output layer to GPU
0.00.455.340 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.375 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.376 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.456.741 I llama_init_from_model: n_seq_max     = 1
0.00.456.745 I llama_init_from_model: n_ctx         = 128
0.00.456.745 I llama_init_from_model: n_ctx_per_seq = 128
0.00.456.746 I llama_init_from_model: n_batch       = 128
0.00.456.746 I llama_init_from_model: n_ubatch      = 128
0.00.456.746 I llama_init_from_model: flash_attn    = 0
0.00.456.749 I llama_init_from_model: freq_base     = 10000.0
0.00.456.750 I llama_init_from_model: freq_scale    = 1
0.00.456.750 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.456.753 I ggml_metal_init: allocating
0.00.456.831 I ggml_metal_init: found device: Apple M4
0.00.456.847 I ggml_metal_init: picking default device: Apple M4
0.00.458.753 I ggml_metal_init: using embedded metal library
0.00.464.573 I ggml_metal_init: GPU name:   Apple M4
0.00.464.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.593 I ggml_metal_init: simdgroup reduction   = true
0.00.464.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.594 I ggml_metal_init: has residency sets    = true
0.00.464.594 I ggml_metal_init: has bfloat            = true
0.00.464.594 I ggml_metal_init: use bfloat            = true
0.00.464.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.977 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.739 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.489.747 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.489.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.493.009 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.493.010 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.493.011 I llama_init_from_model: graph nodes  = 967
0.00.493.011 I llama_init_from_model: graph splits = 2
0.00.493.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.493.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.010 I 
0.00.522.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.087 I perplexity: tokenizing the input ..
0.00.528.854 I perplexity: tokenization took 6.762 ms
0.00.528.876 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.668.785 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.670.136 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.670.156 I llama_perf_context_print:        load time =     512.87 ms
0.00.670.157 I llama_perf_context_print: prompt eval time =     138.88 ms /   128 tokens (    1.08 ms per token,   921.69 tokens per second)
0.00.670.158 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.158 I llama_perf_context_print:       total time =     148.15 ms /   129 tokens
0.00.670.618 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.083s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.389 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.392 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.184 I llama_model_loader: - type  f32:  194 tensors
0.00.026.185 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.185 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.185 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.186 I print_info: file format = GGUF V3 (latest)
0.00.026.187 I print_info: file type   = Q4_K - Medium
0.00.026.188 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.669 I load: special tokens cache size = 25
0.00.040.691 I load: token to piece cache size = 0.2984 MB
0.00.040.695 I print_info: arch             = gptneox
0.00.040.695 I print_info: vocab_only       = 0
0.00.040.695 I print_info: n_ctx_train      = 2048
0.00.040.695 I print_info: n_embd           = 2048
0.00.040.696 I print_info: n_layer          = 24
0.00.040.700 I print_info: n_head           = 16
0.00.040.701 I print_info: n_head_kv        = 16
0.00.040.701 I print_info: n_rot            = 32
0.00.040.701 I print_info: n_swa            = 0
0.00.040.702 I print_info: n_embd_head_k    = 128
0.00.040.702 I print_info: n_embd_head_v    = 128
0.00.040.702 I print_info: n_gqa            = 1
0.00.040.703 I print_info: n_embd_k_gqa     = 2048
0.00.040.704 I print_info: n_embd_v_gqa     = 2048
0.00.040.704 I print_info: f_norm_eps       = 1.0e-05
0.00.040.705 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.705 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.705 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.706 I print_info: f_logit_scale    = 0.0e+00
0.00.040.706 I print_info: n_ff             = 8192
0.00.040.707 I print_info: n_expert         = 0
0.00.040.707 I print_info: n_expert_used    = 0
0.00.040.707 I print_info: causal attn      = 1
0.00.040.707 I print_info: pooling type     = 0
0.00.040.707 I print_info: rope type        = 2
0.00.040.707 I print_info: rope scaling     = linear
0.00.040.711 I print_info: freq_base_train  = 10000.0
0.00.040.711 I print_info: freq_scale_train = 1
0.00.040.711 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.712 I print_info: rope_finetuned   = unknown
0.00.040.712 I print_info: ssm_d_conv       = 0
0.00.040.712 I print_info: ssm_d_inner      = 0
0.00.040.712 I print_info: ssm_d_state      = 0
0.00.040.712 I print_info: ssm_dt_rank      = 0
0.00.040.712 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.712 I print_info: model type       = 1.4B
0.00.040.713 I print_info: model params     = 1.41 B
0.00.040.713 I print_info: general.name     = 1.4B
0.00.040.713 I print_info: vocab type       = BPE
0.00.040.713 I print_info: n_vocab          = 50304
0.00.040.714 I print_info: n_merges         = 50009
0.00.040.716 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: LF token         = 187 'Ċ'
0.00.040.716 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.717 I print_info: max token length = 1024
0.00.040.718 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.546.270 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.291 I load_tensors: offloading output layer to GPU
0.00.546.292 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.324 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.546.325 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.547.682 I llama_init_from_model: n_seq_max     = 1
0.00.547.688 I llama_init_from_model: n_ctx         = 128
0.00.547.688 I llama_init_from_model: n_ctx_per_seq = 128
0.00.547.689 I llama_init_from_model: n_batch       = 128
0.00.547.689 I llama_init_from_model: n_ubatch      = 128
0.00.547.690 I llama_init_from_model: flash_attn    = 0
0.00.547.693 I llama_init_from_model: freq_base     = 10000.0
0.00.547.693 I llama_init_from_model: freq_scale    = 1
0.00.547.694 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.547.697 I ggml_metal_init: allocating
0.00.547.785 I ggml_metal_init: found device: Apple M4
0.00.547.801 I ggml_metal_init: picking default device: Apple M4
0.00.549.692 I ggml_metal_init: using embedded metal library
0.00.556.255 I ggml_metal_init: GPU name:   Apple M4
0.00.556.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.556.262 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.556.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.556.263 I ggml_metal_init: simdgroup reduction   = true
0.00.556.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.556.264 I ggml_metal_init: has residency sets    = true
0.00.556.264 I ggml_metal_init: has bfloat            = true
0.00.556.264 I ggml_metal_init: use bfloat            = true
0.00.556.265 I ggml_metal_init: hasUnifiedMemory      = true
0.00.556.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.453 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.578.878 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.578.881 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.578.903 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.581.931 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.581.933 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.581.933 I llama_init_from_model: graph nodes  = 967
0.00.581.934 I llama_init_from_model: graph splits = 2
0.00.581.938 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.581.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.028 I 
0.00.608.091 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.107 I perplexity: tokenizing the input ..
0.00.615.293 I perplexity: tokenization took 7.182 ms
0.00.615.320 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.124 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.752.446 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.752.460 I llama_perf_context_print:        load time =     598.20 ms
0.00.752.462 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.80 tokens per second)
0.00.752.463 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.463 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.752.865 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.082s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.274 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.643 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.645 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.645 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.645 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.647 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.647 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.648 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.648 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.341 I llama_model_loader: - type  f32:  194 tensors
0.00.026.341 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.342 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.342 I print_info: file format = GGUF V3 (latest)
0.00.026.343 I print_info: file type   = Q5_K - Medium
0.00.026.344 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.793 I load: special tokens cache size = 25
0.00.040.931 I load: token to piece cache size = 0.2984 MB
0.00.040.935 I print_info: arch             = gptneox
0.00.040.935 I print_info: vocab_only       = 0
0.00.040.935 I print_info: n_ctx_train      = 2048
0.00.040.935 I print_info: n_embd           = 2048
0.00.040.935 I print_info: n_layer          = 24
0.00.040.939 I print_info: n_head           = 16
0.00.040.940 I print_info: n_head_kv        = 16
0.00.040.940 I print_info: n_rot            = 32
0.00.040.940 I print_info: n_swa            = 0
0.00.040.941 I print_info: n_embd_head_k    = 128
0.00.040.941 I print_info: n_embd_head_v    = 128
0.00.040.942 I print_info: n_gqa            = 1
0.00.040.942 I print_info: n_embd_k_gqa     = 2048
0.00.040.943 I print_info: n_embd_v_gqa     = 2048
0.00.040.944 I print_info: f_norm_eps       = 1.0e-05
0.00.040.944 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.944 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.944 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.945 I print_info: f_logit_scale    = 0.0e+00
0.00.040.945 I print_info: n_ff             = 8192
0.00.040.945 I print_info: n_expert         = 0
0.00.040.946 I print_info: n_expert_used    = 0
0.00.040.946 I print_info: causal attn      = 1
0.00.040.946 I print_info: pooling type     = 0
0.00.040.946 I print_info: rope type        = 2
0.00.040.946 I print_info: rope scaling     = linear
0.00.040.948 I print_info: freq_base_train  = 10000.0
0.00.040.948 I print_info: freq_scale_train = 1
0.00.040.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.949 I print_info: rope_finetuned   = unknown
0.00.040.949 I print_info: ssm_d_conv       = 0
0.00.040.949 I print_info: ssm_d_inner      = 0
0.00.040.949 I print_info: ssm_d_state      = 0
0.00.040.949 I print_info: ssm_dt_rank      = 0
0.00.040.952 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.952 I print_info: model type       = 1.4B
0.00.040.952 I print_info: model params     = 1.41 B
0.00.040.952 I print_info: general.name     = 1.4B
0.00.040.953 I print_info: vocab type       = BPE
0.00.040.953 I print_info: n_vocab          = 50304
0.00.040.953 I print_info: n_merges         = 50009
0.00.040.953 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.954 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.954 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.954 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.954 I print_info: LF token         = 187 'Ċ'
0.00.040.955 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.955 I print_info: max token length = 1024
0.00.040.955 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.850 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.872 I load_tensors: offloading output layer to GPU
0.00.621.873 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.912 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.621.913 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.623.287 I llama_init_from_model: n_seq_max     = 1
0.00.623.291 I llama_init_from_model: n_ctx         = 128
0.00.623.291 I llama_init_from_model: n_ctx_per_seq = 128
0.00.623.292 I llama_init_from_model: n_batch       = 128
0.00.623.292 I llama_init_from_model: n_ubatch      = 128
0.00.623.292 I llama_init_from_model: flash_attn    = 0
0.00.623.295 I llama_init_from_model: freq_base     = 10000.0
0.00.623.295 I llama_init_from_model: freq_scale    = 1
0.00.623.296 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.623.298 I ggml_metal_init: allocating
0.00.623.375 I ggml_metal_init: found device: Apple M4
0.00.623.389 I ggml_metal_init: picking default device: Apple M4
0.00.625.270 I ggml_metal_init: using embedded metal library
0.00.631.906 I ggml_metal_init: GPU name:   Apple M4
0.00.631.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.914 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.915 I ggml_metal_init: simdgroup reduction   = true
0.00.631.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.915 I ggml_metal_init: has residency sets    = true
0.00.631.916 I ggml_metal_init: has bfloat            = true
0.00.631.916 I ggml_metal_init: use bfloat            = true
0.00.631.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.713 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.195 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.221 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.265 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.657.267 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.657.268 I llama_init_from_model: graph nodes  = 967
0.00.657.268 I llama_init_from_model: graph splits = 2
0.00.657.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.657.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.808 I 
0.00.689.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.895 I perplexity: tokenizing the input ..
0.00.696.377 I perplexity: tokenization took 6.479 ms
0.00.696.395 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.574 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.838.926 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.838.942 I llama_perf_context_print:        load time =     679.53 ms
0.00.838.942 I llama_perf_context_print: prompt eval time =     140.73 ms /   128 tokens (    1.10 ms per token,   909.56 tokens per second)
0.00.838.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.943 I llama_perf_context_print:       total time =     149.13 ms /   129 tokens
0.00.839.439 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.081s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.858 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.858 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.860 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.860 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.860 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.860 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.861 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.862 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.687 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.688 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.690 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.690 I llama_model_loader: - type  f32:  194 tensors
0.00.025.691 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.692 I print_info: file format = GGUF V3 (latest)
0.00.025.692 I print_info: file type   = Q6_K
0.00.025.693 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.799 I load: special tokens cache size = 25
0.00.039.833 I load: token to piece cache size = 0.2984 MB
0.00.039.836 I print_info: arch             = gptneox
0.00.039.836 I print_info: vocab_only       = 0
0.00.039.837 I print_info: n_ctx_train      = 2048
0.00.039.837 I print_info: n_embd           = 2048
0.00.039.837 I print_info: n_layer          = 24
0.00.039.841 I print_info: n_head           = 16
0.00.039.842 I print_info: n_head_kv        = 16
0.00.039.843 I print_info: n_rot            = 32
0.00.039.843 I print_info: n_swa            = 0
0.00.039.843 I print_info: n_embd_head_k    = 128
0.00.039.846 I print_info: n_embd_head_v    = 128
0.00.039.846 I print_info: n_gqa            = 1
0.00.039.847 I print_info: n_embd_k_gqa     = 2048
0.00.039.849 I print_info: n_embd_v_gqa     = 2048
0.00.039.850 I print_info: f_norm_eps       = 1.0e-05
0.00.039.850 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.850 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.851 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.851 I print_info: f_logit_scale    = 0.0e+00
0.00.039.851 I print_info: n_ff             = 8192
0.00.039.852 I print_info: n_expert         = 0
0.00.039.852 I print_info: n_expert_used    = 0
0.00.039.852 I print_info: causal attn      = 1
0.00.039.852 I print_info: pooling type     = 0
0.00.039.852 I print_info: rope type        = 2
0.00.039.852 I print_info: rope scaling     = linear
0.00.039.854 I print_info: freq_base_train  = 10000.0
0.00.039.855 I print_info: freq_scale_train = 1
0.00.039.855 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.855 I print_info: rope_finetuned   = unknown
0.00.039.855 I print_info: ssm_d_conv       = 0
0.00.039.855 I print_info: ssm_d_inner      = 0
0.00.039.855 I print_info: ssm_d_state      = 0
0.00.039.857 I print_info: ssm_dt_rank      = 0
0.00.039.857 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.857 I print_info: model type       = 1.4B
0.00.039.858 I print_info: model params     = 1.41 B
0.00.039.858 I print_info: general.name     = 1.4B
0.00.039.858 I print_info: vocab type       = BPE
0.00.039.858 I print_info: n_vocab          = 50304
0.00.039.858 I print_info: n_merges         = 50009
0.00.039.859 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.859 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.859 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.859 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.859 I print_info: LF token         = 187 'Ċ'
0.00.039.860 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: max token length = 1024
0.00.039.860 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.200.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.200.957 I load_tensors: offloading output layer to GPU
0.00.200.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.200.989 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.200.996 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.201.982 I llama_init_from_model: n_seq_max     = 1
0.00.201.984 I llama_init_from_model: n_ctx         = 128
0.00.201.984 I llama_init_from_model: n_ctx_per_seq = 128
0.00.201.984 I llama_init_from_model: n_batch       = 128
0.00.201.985 I llama_init_from_model: n_ubatch      = 128
0.00.201.985 I llama_init_from_model: flash_attn    = 0
0.00.201.986 I llama_init_from_model: freq_base     = 10000.0
0.00.201.987 I llama_init_from_model: freq_scale    = 1
0.00.201.988 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.201.989 I ggml_metal_init: allocating
0.00.202.023 I ggml_metal_init: found device: Apple M4
0.00.202.034 I ggml_metal_init: picking default device: Apple M4
0.00.203.135 I ggml_metal_init: using embedded metal library
0.00.208.605 I ggml_metal_init: GPU name:   Apple M4
0.00.208.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.208.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.208.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.208.613 I ggml_metal_init: simdgroup reduction   = true
0.00.208.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.208.613 I ggml_metal_init: has residency sets    = true
0.00.208.614 I ggml_metal_init: has bfloat            = true
0.00.208.614 I ggml_metal_init: use bfloat            = true
0.00.208.615 I ggml_metal_init: hasUnifiedMemory      = true
0.00.208.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.224.179 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.227.535 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.227.541 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.227.568 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.230.268 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.230.269 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.230.270 I llama_init_from_model: graph nodes  = 967
0.00.230.270 I llama_init_from_model: graph splits = 2
0.00.230.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.230.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.266.247 I 
0.00.266.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.266.327 I perplexity: tokenizing the input ..
0.00.271.618 I perplexity: tokenization took 5.289 ms
0.00.271.636 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.411.543 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.412.885 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.412.899 I llama_perf_context_print:        load time =     256.40 ms
0.00.412.900 I llama_perf_context_print: prompt eval time =     139.25 ms /   128 tokens (    1.09 ms per token,   919.20 tokens per second)
0.00.412.901 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.412.901 I llama_perf_context_print:       total time =     146.65 ms /   129 tokens
0.00.413.271 I ggml_metal_free: deallocating

real	0m0.429s
user	0m0.075s
sys	0m0.080s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.361 I build: 4692 (c3d6af7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.664 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.384 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.388 I llama_model_loader: - type  f32:  194 tensors
0.00.055.388 I llama_model_loader: - type  f16:   98 tensors
0.00.055.389 I print_info: file format = GGUF V3 (latest)
0.00.055.390 I print_info: file type   = all F32 (guessed)
0.00.055.391 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.115 I load: special tokens cache size = 25
0.00.073.241 I load: token to piece cache size = 0.2984 MB
0.00.073.245 I print_info: arch             = gptneox
0.00.073.245 I print_info: vocab_only       = 0
0.00.073.245 I print_info: n_ctx_train      = 2048
0.00.073.245 I print_info: n_embd           = 2048
0.00.073.246 I print_info: n_layer          = 24
0.00.073.249 I print_info: n_head           = 16
0.00.073.250 I print_info: n_head_kv        = 16
0.00.073.251 I print_info: n_rot            = 32
0.00.073.251 I print_info: n_swa            = 0
0.00.073.253 I print_info: n_embd_head_k    = 128
0.00.073.253 I print_info: n_embd_head_v    = 128
0.00.073.254 I print_info: n_gqa            = 1
0.00.073.255 I print_info: n_embd_k_gqa     = 2048
0.00.073.255 I print_info: n_embd_v_gqa     = 2048
0.00.073.256 I print_info: f_norm_eps       = 1.0e-05
0.00.073.256 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.256 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.257 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.257 I print_info: f_logit_scale    = 0.0e+00
0.00.073.258 I print_info: n_ff             = 8192
0.00.073.258 I print_info: n_expert         = 0
0.00.073.258 I print_info: n_expert_used    = 0
0.00.073.259 I print_info: causal attn      = 1
0.00.073.259 I print_info: pooling type     = 0
0.00.073.264 I print_info: rope type        = 2
0.00.073.266 I print_info: rope scaling     = linear
0.00.073.266 I print_info: freq_base_train  = 10000.0
0.00.073.267 I print_info: freq_scale_train = 1
0.00.073.267 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.267 I print_info: rope_finetuned   = unknown
0.00.073.267 I print_info: ssm_d_conv       = 0
0.00.073.269 I print_info: ssm_d_inner      = 0
0.00.073.269 I print_info: ssm_d_state      = 0
0.00.073.269 I print_info: ssm_dt_rank      = 0
0.00.073.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.269 I print_info: model type       = 1.4B
0.00.073.270 I print_info: model params     = 1.41 B
0.00.073.270 I print_info: general.name     = 1.4B
0.00.073.271 I print_info: vocab type       = BPE
0.00.073.272 I print_info: n_vocab          = 50304
0.00.073.272 I print_info: n_merges         = 50009
0.00.073.272 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.272 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.272 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.273 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.273 I print_info: LF token         = 187 'Ċ'
0.00.073.274 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.275 I print_info: max token length = 1024
0.00.073.275 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.367.011 I load_tensors: offloading 24 repeating layers to GPU
0.01.367.014 I load_tensors: offloading output layer to GPU
0.01.367.015 I load_tensors: offloaded 25/25 layers to GPU
0.01.367.039 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.367.041 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.368.195 I llama_init_from_model: n_seq_max     = 1
0.01.368.197 I llama_init_from_model: n_ctx         = 128
0.01.368.197 I llama_init_from_model: n_ctx_per_seq = 128
0.01.368.197 I llama_init_from_model: n_batch       = 128
0.01.368.197 I llama_init_from_model: n_ubatch      = 128
0.01.368.198 I llama_init_from_model: flash_attn    = 0
0.01.368.198 I llama_init_from_model: freq_base     = 10000.0
0.01.368.199 I llama_init_from_model: freq_scale    = 1
0.01.368.199 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.368.200 I ggml_metal_init: allocating
0.01.368.257 I ggml_metal_init: found device: Apple M4
0.01.368.266 I ggml_metal_init: picking default device: Apple M4
0.01.369.337 I ggml_metal_init: using embedded metal library
0.01.373.287 I ggml_metal_init: GPU name:   Apple M4
0.01.373.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.373.290 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.373.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.373.291 I ggml_metal_init: simdgroup reduction   = true
0.01.373.291 I ggml_metal_init: simdgroup matrix mul. = true
0.01.373.291 I ggml_metal_init: has residency sets    = true
0.01.373.291 I ggml_metal_init: has bfloat            = true
0.01.373.291 I ggml_metal_init: use bfloat            = true
0.01.373.292 I ggml_metal_init: hasUnifiedMemory      = true
0.01.373.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.384.566 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.386.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.386.350 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.386.366 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.388.081 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.388.082 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.388.083 I llama_init_from_model: graph nodes  = 967
0.01.388.083 I llama_init_from_model: graph splits = 2
0.01.388.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.388.085 I 
0.01.388.120 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.388.122 I compute_imatrix: tokenizing the input ..
0.01.392.293 I compute_imatrix: tokenization took 4.17 ms
0.01.392.295 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.662.274 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.664.851 I llama_perf_context_print:        load time =    1640.18 ms
0.01.664.852 I llama_perf_context_print: prompt eval time =     268.18 ms /   128 tokens (    2.10 ms per token,   477.29 tokens per second)
0.01.664.853 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.664.853 I llama_perf_context_print:       total time =    1642.75 ms /   129 tokens
0.01.665.373 I ggml_metal_free: deallocating

real	0m1.853s
user	0m0.128s
sys	0m0.271s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4692 (c3d6af7c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138604ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138608560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1386089d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138608e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1386092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138609720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138609b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13860a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13860a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13860a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13860ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13860b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13860bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13860c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13860ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13860d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13860dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13860e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13860eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13860f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13860fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138610160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138611b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138612950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138612dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1386148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1386155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138616340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1386167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138617090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138617de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1386189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1386192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13861a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13861a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13861abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13861ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13861b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13861b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13861bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13861c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13861c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13861ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13861cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13861d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13861d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13861de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13861e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13861e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13861ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13861f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13861f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13861fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138620240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1386207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138620da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138621350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138621eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138623570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138623b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1386240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138624680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138624c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1386251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138625790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1386262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1386268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138626e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138627400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1386279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1386180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1386286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138629550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13862a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13862a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13862ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13862b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13862b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13862bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13862c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13862c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13862ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13862d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13862d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13862de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13862e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13862e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13862ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13862f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13862f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13862fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138630190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138630690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138632e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138633390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138633890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138633d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138634290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138634790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138635690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138637490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138638390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138638890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138639c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13863a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13863a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13863ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13863b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13863b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13863ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13863bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13863c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13863c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13863ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13863d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13863d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13863dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13863e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13863e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13863ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13863f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13863f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13863fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138640090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138640590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138640a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138640f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138643790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138643c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138644190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138644690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138645590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138645a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138646990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138646f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1386474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138647aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138648660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138648c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13864a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13864a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13864adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13864b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13864ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13864bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13864c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13864cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13864d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13864d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13864db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13864e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13864e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13864eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13864f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13864f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13864fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138650090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1386505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138651080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1386515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138652070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1386525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138652b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138653060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1386535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138653b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138654050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1386545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138654af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138655040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138655590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138655ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138656030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138656580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138656ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138657020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138657570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138657ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138658010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138658560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138658ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138659000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138659550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138659aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138659ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13865a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13865aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13865afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13865b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13865ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13865bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13865c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13865ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13865cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13865d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13865da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13865dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13865e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13865ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13865efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13865f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13865f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13865fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1386602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138660770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1386610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1386619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1386627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1386635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138663fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1386646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138664de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138665500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138665c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138665ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1386666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138666fa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.751.786 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11de04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11de04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11de05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11de05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11de05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11de06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11de065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11de06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11de06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11de07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11de07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11de07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11de08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11de09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11de09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11de0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11de0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11de0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11de0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11de0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11de0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11de0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11de0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11de0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11de0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11de0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11de0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11de0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11de0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11de0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11de0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11de0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11de10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11de10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11de108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11de10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11de11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11de11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11de11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11de11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11de12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11de127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11de12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11de130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11de13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11de13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11de13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11de14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11de146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11de14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11de14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11de15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11de15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11de15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11de16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11de165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11de16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11de17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11de174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11de17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11de17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11de18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11de18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11de18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11de18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11de193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11de19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11de19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11de1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11de1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11de1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11de1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11de1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11de1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11de1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11de1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11de1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11de1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11de1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11de1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11de1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11de1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11de1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11de1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11de1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11de1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11de1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11de1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11de1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11de1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11de202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11de20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11de20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11de21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11de21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11de218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11de21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11de221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11de22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11de22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11de22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11de23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11de23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11de23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11de240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11de24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11de249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11de24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11de252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11de25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11de25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11de25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11de26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11de268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11de26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11de271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11de27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11de27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11de27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11de28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11de287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11de28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11de290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11de29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11de299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11de29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11de2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11de2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11de2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11de2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11de2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11de2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11de2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11de2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11de2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11de2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11de2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11de2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11de2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11de2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11de2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11de2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11de2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11de2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11de2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11de2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11de2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11de2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11de30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11de30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11de30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11de31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11de315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11de31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11de31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11de32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11de327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11de32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11de33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11de334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11de33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11de33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11de34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11de346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11de34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11de34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11de35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11de35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11de36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11de365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11de36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11de36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11de37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11de37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11de37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11de38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11de384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11de38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11de38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11de39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11de39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11de39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11de39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11de3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11de3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11de3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11de3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11de3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11de3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11de3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11de3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11de3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11de3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11de3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11de3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11de3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11de3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11de3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11de3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11de3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11de3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11de3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11de3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11de3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11de40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11de40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11de40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11de40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11de41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11de41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11de42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11de42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11de42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11de433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11de43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11de43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11de44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11de44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11de45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11de45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11de45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11de461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11de46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11de46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11de47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11de478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11de47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11de48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11de48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11de48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11de49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11de49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11de4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11de4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11de4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11de4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11de4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11de4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11de4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11de4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11de4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11de4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11de4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11de4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11de4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11de4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11de4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11de4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11de4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11de502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11de50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11de50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11de51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11de519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11de51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11de52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11de52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11de530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11de53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11de53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11de54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11de547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11de54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11de55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11de55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11de55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11de56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11de56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11de56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11de57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11de57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11de57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11de58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11de58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11de58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11de59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11de59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11de59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11de5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11de5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11de5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11de5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11de5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11de5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11de5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11de5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11de5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11de5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11de5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11de5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11de5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f1044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f104950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f104dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f105230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f1056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f105b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f105f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f1063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f106860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f106cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f107140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f107860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f108380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f108b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f109340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f109a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f10a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f10a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f10afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f10b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f10be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f10c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f10cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f10d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f10da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f10dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f10e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f10e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f10e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f10ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f10f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f10f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f10fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f10fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f1102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f110710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f110b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f110ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f111460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f1118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f111d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f1121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f112620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f112a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f112f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f113370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f1137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f113c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f1140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f114530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f1149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f114e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f115280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f1156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f115b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f115fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f116540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f116a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f116eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f117320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f117790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f117c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f118070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f1184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f118950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f118dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f119230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f1196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f119b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f119f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f11a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f11a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f11acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f11b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f11b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f11ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f11be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f11c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f11c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f11cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f11d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f11d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f11d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f11dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f11e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f11e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f11eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f11ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f11f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f11f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f11fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f120120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f120590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f120a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f120e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f1212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f121750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f121bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f122030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f1224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f122910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f122d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f1231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f123a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f123d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f1241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f124620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f124a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f124f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f125370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f1257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f125c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f1260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f126530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f1269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f126e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f127280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f1276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f127b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f127fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f128440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f1288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f128d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f129190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f129600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f129a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f129ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f12a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f12a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f12ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f12b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f12b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f12b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f12bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f12c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f12c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f12cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f12cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f12d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f12d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f12dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f12e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f12e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f12ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f12eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f12f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f12f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f12fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f130080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f1304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f130960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f130dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f131240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f1316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f131b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f131f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f132400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f132870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f132ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f133150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f1335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f133a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f133ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f134310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f134780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f134bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f135060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f1354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f135940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f135db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f136220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f136690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f136b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f136f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f1373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f137850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f137cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f138130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f1385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f138a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f138e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f1392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f139760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f139bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f13a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f13a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f13a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f13ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f13b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f13b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f13bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f13bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f13c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f13c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f13cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f13d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f13d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f13d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f13de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f13e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f13e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f13ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f13f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f13f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f13f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f13fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f1401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f140650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f140ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f140f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f141ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f141d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f142030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f1424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f142910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f142d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f1431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f143660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f143ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f143f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f1443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f144820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f144c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f145100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f145570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f1459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f145e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f1462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f146730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f146ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f147010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f147480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f1478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f147d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f1481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f148640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f148ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f148f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f149390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f149800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f149c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f14a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f14a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f14a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f14ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f14b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f14b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f14bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f14bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f14c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f14c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f14cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f14d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f14d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f14da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f14df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f14e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f14e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f14ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f14f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f14f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f14f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f14fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f150280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f1506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f150b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f150fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f151440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f1518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f151d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f152190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f152600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f152a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f152ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f153350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f1537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f153c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f1540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f154510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f154980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f154df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f155260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f1556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f156140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f156860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f156f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f1576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f157960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f157dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f1583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f1589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.796s
user	0m0.281s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4692 (c3d6af7c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15170d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15170dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15170e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15170e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15170ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15170f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15170f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15170ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151710500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151710a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151710f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151711400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1517126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151713600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151714b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151715330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151715a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151716170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151717130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151717850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151717b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151718120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151718d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1517192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151719590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151719a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151719cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15171a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15171aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15171ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15171b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15171b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15171bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15171c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15171c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15171c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15171cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15171d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15171d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15171d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15171dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15171e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15171ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15171f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15171fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151720150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151720760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151720d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151721380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1517224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151722770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151723570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151723830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151724170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151724610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151724ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1517253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151725890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1517261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151726670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151726b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151726fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151727500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151727a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151727fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1517284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151728a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151728f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1517294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151729a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151729f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15172a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15172aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15172af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15172b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15172ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15172bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15172c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15172ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15172cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15172d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15172d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15172df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15172e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15172e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15172ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15171ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15172f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15172fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1517300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1517305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151730b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151731090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1517315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151731b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151732080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1517325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151732b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151733070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1517335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151734060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151734500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1517349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1517352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151735780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151735c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1517360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151736560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151736a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151736ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151737340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1517377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151737c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151738120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1517385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151738a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151738f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1517393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151739ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15173a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15173a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15173aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15173af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15173b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15173b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15173bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15173c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15173c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15173cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15173cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15173d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15173d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15173dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15173e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15173e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15173eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15173f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15173f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15173f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15173fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1517402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151740740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151740be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151741080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151741520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1517419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151741e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151742300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1517427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151742c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1517430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151743580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151743a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151743ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151744360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151744800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151744ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151745140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1517455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151745f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1517463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151746860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151746d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1517471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151747640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151747ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151747f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151748420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1517488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151749200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1517496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151749b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151749fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15174a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15174a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15174adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15174b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15174b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15174bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15174c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15174c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15174ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15174d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15174d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15174dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15174e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15174e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15174ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15174f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15174f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15174fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151750490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151750930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151750dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151751580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151751ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151752020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151752570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151752ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151753010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151753560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151753ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151754000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151754550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151754aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151754ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151755540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151755a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151755fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151756530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151756a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151756fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151757520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151757a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151757fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151758510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151758a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151758fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151759500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151759a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151759fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15175a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15175aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15175af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15175b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15175ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15175bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15175c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15175ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15175cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15175d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15175da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15175df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15175e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15175ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15175ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15175f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15175f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15175ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151760490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1517609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151760f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151761480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1517619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151761f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151762470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1517629c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151762f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151763460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1517639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151763f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1517643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151764840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151764ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151765180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151765620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151765ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151765f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151766400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1517668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151766d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1517671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151767680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151767b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151767fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151768460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1517689b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1517690d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1517697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151769f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15176a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15176a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15176b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15176b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15176b9b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152005fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152006430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152009090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152009350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152009610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152009eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15200a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15200a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15200adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15200b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15200b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15200b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15200c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15200cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15200d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15200dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15200e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15200e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15200f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15200f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1520100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152010810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152010f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152011d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152012030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152012640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152012c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152013260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152013a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152013ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1520141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152014a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152014f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152015240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1520156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152015b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1520164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152016960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152016e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1520172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152017740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152017be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152017ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1520184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152018ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1520190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1520196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152019cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15201a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15201a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15201af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15201b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15201bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15201c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15201c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15201c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15201cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15201d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15201dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15201e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15201e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15201e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15201ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15201f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15201f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15201fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1520200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152020560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152020a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152020ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152021340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152021890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152021de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152022330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152022880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152023320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152023870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152023dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152024310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152024860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152024db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152025300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152025850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152025da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1520262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152026840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1520272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152027830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152027d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1520282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152028820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1520292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152029810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152029d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15202a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15202a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15202ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15202b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15202b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15202bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15202c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15202c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15202cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15202d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15202d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15202dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15202e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15202e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15202ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15202f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15202f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15202fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15202fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152030380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152030820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152030cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152031160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152031600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152031f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1520323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152032880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152032d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1520331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152033660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152033b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152033fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152034440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1520348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152034d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152035220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1520356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152035b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152036000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1520364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152036940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152036de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152037280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152037720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152037bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152038060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152038500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1520389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152038e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1520392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152039780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15203a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15203a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15203aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15203aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15203b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15203b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15203bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15203c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15203c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15203ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15203cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15203d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15203d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15203dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15203e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15203e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15203eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15203ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15203f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15203f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15203fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1520401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152040680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152040b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152040fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152041460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152041900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152041da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152042240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1520426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152042b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152043020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1520434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152043960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152043e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1520442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152044740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152044be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152045080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152045520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1520459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152045f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152046460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1520469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152046f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1520471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1520477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152047de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1520483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152048be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152049080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152049340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152049950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152049f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15204a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15204abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15204b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15204b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15204bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15204c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15204c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15204ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15204d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15204d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15204dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15204e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15204e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15204ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15204f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15204f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15204fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1520501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152050c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1520511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152051730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152051c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1520521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152052720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152052c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1520531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152053710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152053c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1520541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152054700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152054c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1520551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1520556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152055c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1520566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152056c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152057180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1520576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152057c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152058170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1520586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152058c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152059160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1520596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152059c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15205a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15205a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15205abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15205b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15205b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15205bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15205c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15205c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15205cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15205d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15205d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15205dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15205e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15205e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15205eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15205efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15205f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15205f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15205fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152060220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1520606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152060b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152061000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1520614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152061940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152061de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152062280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152062720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152062bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152063110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152063830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152063f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152064670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152064d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152065050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152065840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152065b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152066110 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152047a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152012900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152065dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152047480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1520480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15201abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15201a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152049c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1520122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152018d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152019390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1520199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152012f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15201b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152019fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152005b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15201b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1520098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152009b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152065310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152014470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152014730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152013520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15204a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1520486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1520066f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1520069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152066570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152066830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152066af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152066db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152067070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152067330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1520675f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1520678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152067b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152067e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1520680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1520683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152068670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152068930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152068bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152068eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152069170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152069430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1520696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1520699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152069c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152069f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15206a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15206a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15206a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15206aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15206acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15206afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15206b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15206b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15206b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15206bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15206bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15206c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15206c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15206c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15206c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15206cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15206cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15206d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15206d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15206d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15206d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15206dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15206de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15206e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15206e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15206e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15206e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15206ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15206eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15206f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15206f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15206f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15206f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15206fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15206ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152070230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1520704f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1520707b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152070a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152070d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152070ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1520712b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152071570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152071830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152071af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152071db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152072070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152072330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1520725f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1520728b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152072b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152072e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1520730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1520733b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152073670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152073930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152073bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152073eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152074170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152074430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1520746f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1520749b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152074c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152074f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1520751f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1520754b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152075770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152075a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152075cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152075fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152076270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152076530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1520767f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152076ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152076d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152077030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1520772f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1520775b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152077870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152077b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152077df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1520780b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152078370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152078630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1520788f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152078bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152078e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152079130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1520793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1520796b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152079970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152079c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152079ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15207a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15207a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15207a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15207a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15207acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15207af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15207b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15207b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15207b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15207ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15207bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15207bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15207c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15207c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15207c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15207caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15207cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15207d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15207d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15207d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15207d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15207db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15207de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15207e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15207e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15207e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15207e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15207ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15207eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15207f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15207f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15207f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15207f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15207fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15207ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1520801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1520804b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152080770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152080a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152080cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152080fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152081270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152081530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1520817f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152081ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152081d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152082030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1520822f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1520825b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152082870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152082b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152082df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1520830b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152083370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152083630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1520838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152083bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152083e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152084130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1520843f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1520846b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152084970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152084c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152084ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1520851b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152085470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152085730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1520859f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152085fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152086280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152086540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152086800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152086ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152086d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152087040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152087300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1520875c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152087880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152087b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152087e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1520880c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152088380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152088640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152088900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152088bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152088e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152089140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152089400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1520896c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152089980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152089c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152089f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15208a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15208a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15208a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15208aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15208af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15208b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15208b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15208bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15208c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15208c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15208cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15208d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15208d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15208df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15208e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15208e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15208ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15208f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15208f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15208ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152090450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1520909a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152090ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152091440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152091990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152091ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152092430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152092980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152092ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152093420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152093970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152093ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152094410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1520946d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152094990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152094e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152095390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152095890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152095d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152096290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152096790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152096c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152097190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152097690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152097b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152098090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152098590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152098a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152098f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1520999a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15209a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15209a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15209af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15209b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15209b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15209bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15209c280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.231s
sys	0m0.189s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
