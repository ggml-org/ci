### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.45 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.79 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.16 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.24 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.23 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.81 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.89 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   26.07 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.28 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 223.50 sec*proc (27 tests)

Total Test time (real) = 223.51 sec

real	3m43.545s
user	7m43.397s
sys	0m6.235s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.08 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.22 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.40 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.37 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.04 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.12 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.12 sec*proc (27 tests)

Total Test time (real) =  51.14 sec

real	0m51.146s
user	1m11.856s
sys	0m5.548s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.107 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.295 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.254 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.266 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.267 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.268 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.269 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.271 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.276 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.276 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.277 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.278 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.282 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.285 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.286 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.287 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.287 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.288 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.289 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.917 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.919 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.920 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.920 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.921 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.031.921 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.922 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.031.922 I llama_model_loader: - type  f32:  124 tensors
0.00.031.923 I llama_model_loader: - type  f16:   73 tensors
0.00.036.360 I llm_load_vocab: special tokens cache size = 5
0.00.038.651 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.655 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.656 I llm_load_print_meta: arch             = bert
0.00.038.656 I llm_load_print_meta: vocab type       = WPM
0.00.038.657 I llm_load_print_meta: n_vocab          = 30522
0.00.038.657 I llm_load_print_meta: n_merges         = 0
0.00.038.657 I llm_load_print_meta: vocab_only       = 0
0.00.038.657 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.658 I llm_load_print_meta: n_embd           = 384
0.00.038.658 I llm_load_print_meta: n_layer          = 12
0.00.038.687 I llm_load_print_meta: n_head           = 12
0.00.038.688 I llm_load_print_meta: n_head_kv        = 12
0.00.038.689 I llm_load_print_meta: n_rot            = 32
0.00.038.689 I llm_load_print_meta: n_swa            = 0
0.00.038.689 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.689 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.690 I llm_load_print_meta: n_gqa            = 1
0.00.038.691 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.692 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.693 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.694 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.695 I llm_load_print_meta: n_ff             = 1536
0.00.038.696 I llm_load_print_meta: n_expert         = 0
0.00.038.696 I llm_load_print_meta: n_expert_used    = 0
0.00.038.699 I llm_load_print_meta: causal attn      = 0
0.00.038.699 I llm_load_print_meta: pooling type     = 2
0.00.038.699 I llm_load_print_meta: rope type        = 2
0.00.038.700 I llm_load_print_meta: rope scaling     = linear
0.00.038.700 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.701 I llm_load_print_meta: freq_scale_train = 1
0.00.038.701 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.701 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.701 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.701 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.714 I llm_load_print_meta: model type       = 33M
0.00.038.715 I llm_load_print_meta: model ftype      = F16
0.00.038.715 I llm_load_print_meta: model params     = 33.21 M
0.00.038.716 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.717 I llm_load_print_meta: general.name     = Bge Small
0.00.038.717 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.718 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.718 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.718 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.719 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.720 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.720 I llm_load_print_meta: max token length = 21
0.00.040.965 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.040.967 I llm_load_tensors: offloading output layer to GPU
0.00.040.967 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.040.995 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.996 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.580 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.581 I llama_new_context_with_model: n_ctx         = 512
0.00.041.582 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.582 I llama_new_context_with_model: n_batch       = 2048
0.00.041.582 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.582 I llama_new_context_with_model: flash_attn    = 0
0.00.041.583 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.583 I llama_new_context_with_model: freq_scale    = 1
0.00.041.584 I ggml_metal_init: allocating
0.00.041.591 I ggml_metal_init: found device: Apple M4
0.00.041.593 I ggml_metal_init: picking default device: Apple M4
0.00.042.457 I ggml_metal_init: using embedded metal library
0.00.046.727 I ggml_metal_init: GPU name:   Apple M4
0.00.046.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.731 I ggml_metal_init: simdgroup reduction   = true
0.00.046.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.731 I ggml_metal_init: has bfloat            = true
0.00.046.731 I ggml_metal_init: use bfloat            = true
0.00.046.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.179 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.181 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.183 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.060.940 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.060.942 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.060.942 I llama_new_context_with_model: graph nodes  = 429
0.00.060.942 I llama_new_context_with_model: graph splits = 2
0.00.060.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.067.704 I 
0.00.067.732 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.396 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.489 I llama_perf_context_print:        load time =      47.40 ms
0.00.073.490 I llama_perf_context_print: prompt eval time =       4.92 ms /     9 tokens (    0.55 ms per token,  1828.15 tokens per second)
0.00.073.491 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.491 I llama_perf_context_print:       total time =       5.79 ms /    10 tokens
0.00.073.626 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.051s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.470 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.476 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.478 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.478 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.478 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.479 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.480 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.480 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.480 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.481 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.483 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.483 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.483 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.484 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.484 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.484 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.484 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.603 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.604 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.604 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.605 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.605 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.605 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.605 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.606 I llama_model_loader: - type  f32:  124 tensors
0.00.014.606 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.981 I llm_load_vocab: special tokens cache size = 5
0.00.018.263 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.265 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.266 I llm_load_print_meta: arch             = bert
0.00.018.266 I llm_load_print_meta: vocab type       = WPM
0.00.018.266 I llm_load_print_meta: n_vocab          = 30522
0.00.018.266 I llm_load_print_meta: n_merges         = 0
0.00.018.266 I llm_load_print_meta: vocab_only       = 0
0.00.018.266 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.267 I llm_load_print_meta: n_embd           = 384
0.00.018.267 I llm_load_print_meta: n_layer          = 12
0.00.018.277 I llm_load_print_meta: n_head           = 12
0.00.018.277 I llm_load_print_meta: n_head_kv        = 12
0.00.018.277 I llm_load_print_meta: n_rot            = 32
0.00.018.277 I llm_load_print_meta: n_swa            = 0
0.00.018.278 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.278 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.278 I llm_load_print_meta: n_gqa            = 1
0.00.018.279 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.279 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.282 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.282 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.282 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.283 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.283 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.284 I llm_load_print_meta: n_ff             = 1536
0.00.018.284 I llm_load_print_meta: n_expert         = 0
0.00.018.284 I llm_load_print_meta: n_expert_used    = 0
0.00.018.284 I llm_load_print_meta: causal attn      = 0
0.00.018.284 I llm_load_print_meta: pooling type     = 2
0.00.018.284 I llm_load_print_meta: rope type        = 2
0.00.018.285 I llm_load_print_meta: rope scaling     = linear
0.00.018.285 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.285 I llm_load_print_meta: freq_scale_train = 1
0.00.018.285 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.285 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.286 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.286 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.286 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.286 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.286 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.291 I llm_load_print_meta: model type       = 33M
0.00.018.291 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.291 I llm_load_print_meta: model params     = 33.21 M
0.00.018.292 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.292 I llm_load_print_meta: general.name     = Bge Small
0.00.018.292 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.293 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.293 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.293 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.293 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.293 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.293 I llm_load_print_meta: max token length = 21
0.00.019.636 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.637 I llm_load_tensors: offloading output layer to GPU
0.00.019.637 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.645 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.646 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.018 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.018 I llama_new_context_with_model: n_ctx         = 512
0.00.020.018 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.019 I llama_new_context_with_model: n_batch       = 2048
0.00.020.019 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.019 I llama_new_context_with_model: flash_attn    = 0
0.00.020.019 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.020 I llama_new_context_with_model: freq_scale    = 1
0.00.020.020 I ggml_metal_init: allocating
0.00.020.025 I ggml_metal_init: found device: Apple M4
0.00.020.027 I ggml_metal_init: picking default device: Apple M4
0.00.020.651 I ggml_metal_init: using embedded metal library
0.00.023.152 I ggml_metal_init: GPU name:   Apple M4
0.00.023.154 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.155 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.155 I ggml_metal_init: simdgroup reduction   = true
0.00.023.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.155 I ggml_metal_init: has bfloat            = true
0.00.023.155 I ggml_metal_init: use bfloat            = true
0.00.023.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.701 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.703 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.704 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.265 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.266 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.267 I llama_new_context_with_model: graph nodes  = 429
0.00.034.267 I llama_new_context_with_model: graph splits = 2
0.00.034.280 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.361 I 
0.00.039.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.922 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.407 I llama_perf_context_print:        load time =      30.00 ms
0.00.044.407 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2069.92 tokens per second)
0.00.044.408 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.409 I llama_perf_context_print:       total time =       5.05 ms /    10 tokens
0.00.044.587 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.127 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.636 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.212 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.217 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.219 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.224 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.224 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.225 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.226 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.227 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.227 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.228 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.228 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.232 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.233 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.233 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.341 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.341 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.341 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.342 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.342 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.342 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.343 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.343 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.344 I llama_model_loader: - type  f32:   41 tensors
0.00.049.349 I llama_model_loader: - type  f16:   29 tensors
0.00.067.398 W llm_load_vocab: empty token at index 5
0.00.072.093 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.458 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.485 I llm_load_vocab: special tokens cache size = 5
0.00.336.076 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.336.081 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.336.082 I llm_load_print_meta: arch             = jina-bert-v2
0.00.336.083 I llm_load_print_meta: vocab type       = BPE
0.00.336.084 I llm_load_print_meta: n_vocab          = 61056
0.00.336.084 I llm_load_print_meta: n_merges         = 39382
0.00.336.085 I llm_load_print_meta: vocab_only       = 0
0.00.336.085 I llm_load_print_meta: n_ctx_train      = 8192
0.00.336.086 I llm_load_print_meta: n_embd           = 384
0.00.336.086 I llm_load_print_meta: n_layer          = 4
0.00.336.119 I llm_load_print_meta: n_head           = 12
0.00.336.121 I llm_load_print_meta: n_head_kv        = 12
0.00.336.121 I llm_load_print_meta: n_rot            = 32
0.00.336.121 I llm_load_print_meta: n_swa            = 0
0.00.336.121 I llm_load_print_meta: n_embd_head_k    = 32
0.00.336.121 I llm_load_print_meta: n_embd_head_v    = 32
0.00.336.122 I llm_load_print_meta: n_gqa            = 1
0.00.336.122 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.336.123 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.336.124 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.336.125 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.336.126 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.336.127 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.336.127 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.336.128 I llm_load_print_meta: n_ff             = 1536
0.00.336.128 I llm_load_print_meta: n_expert         = 0
0.00.336.128 I llm_load_print_meta: n_expert_used    = 0
0.00.336.128 I llm_load_print_meta: causal attn      = 0
0.00.336.130 I llm_load_print_meta: pooling type     = -1
0.00.336.131 I llm_load_print_meta: rope type        = -1
0.00.336.131 I llm_load_print_meta: rope scaling     = linear
0.00.336.131 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.336.132 I llm_load_print_meta: freq_scale_train = 1
0.00.336.132 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.336.132 I llm_load_print_meta: rope_finetuned   = unknown
0.00.336.132 I llm_load_print_meta: ssm_d_conv       = 0
0.00.336.133 I llm_load_print_meta: ssm_d_inner      = 0
0.00.336.133 I llm_load_print_meta: ssm_d_state      = 0
0.00.336.133 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.336.133 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.336.156 I llm_load_print_meta: model type       = 33M
0.00.336.157 I llm_load_print_meta: model ftype      = F16
0.00.336.157 I llm_load_print_meta: model params     = 32.90 M
0.00.336.158 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.336.158 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.336.158 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.336.158 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.336.159 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.336.159 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.336.164 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.336.164 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.336.164 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.336.165 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.336.165 I llm_load_print_meta: max token length = 45
0.00.337.361 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.337.361 I llm_load_tensors: offloading output layer to GPU
0.00.337.362 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.337.387 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.337.388 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.338.228 I llama_new_context_with_model: n_seq_max     = 1
0.00.338.228 I llama_new_context_with_model: n_ctx         = 8192
0.00.338.228 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.338.229 I llama_new_context_with_model: n_batch       = 2048
0.00.338.229 I llama_new_context_with_model: n_ubatch      = 2048
0.00.338.229 I llama_new_context_with_model: flash_attn    = 0
0.00.338.229 I llama_new_context_with_model: freq_base     = 10000.0
0.00.338.230 I llama_new_context_with_model: freq_scale    = 1
0.00.338.230 I ggml_metal_init: allocating
0.00.338.233 I ggml_metal_init: found device: Apple M4
0.00.338.235 I ggml_metal_init: picking default device: Apple M4
0.00.339.241 I ggml_metal_init: using embedded metal library
0.00.342.229 I ggml_metal_init: GPU name:   Apple M4
0.00.342.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.232 I ggml_metal_init: simdgroup reduction   = true
0.00.342.232 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.232 I ggml_metal_init: has bfloat            = true
0.00.342.232 I ggml_metal_init: use bfloat            = true
0.00.342.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.354.242 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.354.246 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.354.251 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.354.838 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.354.839 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.354.839 I llama_new_context_with_model: graph nodes  = 154
0.00.354.839 I llama_new_context_with_model: graph splits = 2
0.00.354.857 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.664 I 
0.00.367.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.919 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.919 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.929 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.929 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.938 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.938 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.440 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.216 I llama_perf_context_print:        load time =     344.02 ms
0.00.372.217 I llama_perf_context_print: prompt eval time =       3.77 ms /    62 tokens (    0.06 ms per token, 16454.35 tokens per second)
0.00.372.218 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.221 I llama_perf_context_print:       total time =       4.55 ms /    63 tokens
0.00.372.354 I ggml_metal_free: deallocating

real	0m1.065s
user	0m0.343s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.105 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.237 I main: llama backend init
0.00.000.244 I main: load the model and apply lora adapter, if any
0.00.048.928 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.060.252 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.280 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.281 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.282 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.282 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.698 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.078.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.587 I llama_model_loader: - type  f32:  194 tensors
0.00.078.587 I llama_model_loader: - type  f16:   98 tensors
0.00.113.772 I llm_load_vocab: special tokens cache size = 25
0.00.121.104 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.121.108 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.121.108 I llm_load_print_meta: arch             = gptneox
0.00.121.108 I llm_load_print_meta: vocab type       = BPE
0.00.121.109 I llm_load_print_meta: n_vocab          = 50304
0.00.121.109 I llm_load_print_meta: n_merges         = 50009
0.00.121.109 I llm_load_print_meta: vocab_only       = 0
0.00.121.109 I llm_load_print_meta: n_ctx_train      = 2048
0.00.121.109 I llm_load_print_meta: n_embd           = 2048
0.00.121.109 I llm_load_print_meta: n_layer          = 24
0.00.121.132 I llm_load_print_meta: n_head           = 16
0.00.121.133 I llm_load_print_meta: n_head_kv        = 16
0.00.121.134 I llm_load_print_meta: n_rot            = 32
0.00.121.134 I llm_load_print_meta: n_swa            = 0
0.00.121.134 I llm_load_print_meta: n_embd_head_k    = 128
0.00.121.134 I llm_load_print_meta: n_embd_head_v    = 128
0.00.121.135 I llm_load_print_meta: n_gqa            = 1
0.00.121.135 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.121.136 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.121.137 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.121.137 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.121.139 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.121.139 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.121.139 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.121.140 I llm_load_print_meta: n_ff             = 8192
0.00.121.140 I llm_load_print_meta: n_expert         = 0
0.00.121.140 I llm_load_print_meta: n_expert_used    = 0
0.00.121.140 I llm_load_print_meta: causal attn      = 1
0.00.121.140 I llm_load_print_meta: pooling type     = 0
0.00.121.140 I llm_load_print_meta: rope type        = 2
0.00.121.141 I llm_load_print_meta: rope scaling     = linear
0.00.121.141 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.121.141 I llm_load_print_meta: freq_scale_train = 1
0.00.121.141 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.121.142 I llm_load_print_meta: rope_finetuned   = unknown
0.00.121.142 I llm_load_print_meta: ssm_d_conv       = 0
0.00.121.142 I llm_load_print_meta: ssm_d_inner      = 0
0.00.121.143 I llm_load_print_meta: ssm_d_state      = 0
0.00.121.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.121.143 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.121.155 I llm_load_print_meta: model type       = 1.4B
0.00.121.155 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.121.156 I llm_load_print_meta: model params     = 1.41 B
0.00.121.157 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.121.157 I llm_load_print_meta: general.name     = 1.4B
0.00.121.157 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.121.157 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.121.157 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.121.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.121.159 I llm_load_print_meta: LF token         = 128 ''
0.00.121.159 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.121.160 I llm_load_print_meta: max token length = 1024
0.00.123.752 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.123.752 I llm_load_tensors: offloading output layer to GPU
0.00.123.753 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.123.771 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.123.772 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.124.790 I llama_new_context_with_model: n_seq_max     = 1
0.00.124.792 I llama_new_context_with_model: n_ctx         = 2048
0.00.124.792 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.124.792 I llama_new_context_with_model: n_batch       = 2048
0.00.124.792 I llama_new_context_with_model: n_ubatch      = 512
0.00.124.792 I llama_new_context_with_model: flash_attn    = 0
0.00.124.793 I llama_new_context_with_model: freq_base     = 10000.0
0.00.124.793 I llama_new_context_with_model: freq_scale    = 1
0.00.124.794 I ggml_metal_init: allocating
0.00.124.803 I ggml_metal_init: found device: Apple M4
0.00.124.805 I ggml_metal_init: picking default device: Apple M4
0.00.125.525 I ggml_metal_init: using embedded metal library
0.00.135.253 I ggml_metal_init: GPU name:   Apple M4
0.00.135.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.135.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.135.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.135.256 I ggml_metal_init: simdgroup reduction   = true
0.00.135.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.135.256 I ggml_metal_init: has bfloat            = true
0.00.135.257 I ggml_metal_init: use bfloat            = true
0.00.135.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.135.257 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.180.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.180.638 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.180.659 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.181.622 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.181.624 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.181.624 I llama_new_context_with_model: graph nodes  = 967
0.00.181.624 I llama_new_context_with_model: graph splits = 2
0.00.181.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.257.929 I main: llama threadpool init, n_threads = 4
0.00.257.959 I 
0.00.257.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.257.996 I 
0.00.258.080 I sampler seed: 1234
0.00.258.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.258.108 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.258.110 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.258.110 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.109.537 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.02.109.538 I llama_perf_context_print:        load time =     208.99 ms
0.02.109.539 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.68 tokens per second)
0.02.109.540 I llama_perf_context_print:        eval time =    1804.75 ms /    63 runs   (   28.65 ms per token,    34.91 tokens per second)
0.02.109.540 I llama_perf_context_print:       total time =    1851.61 ms /    70 tokens
0.02.109.750 I ggml_metal_free: deallocating

real	0m2.448s
user	0m0.149s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.838 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.247 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.096 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.115 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.117 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.117 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.118 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.121 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.286 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.500 I llama_model_loader: - type  f32:  194 tensors
0.00.058.500 I llama_model_loader: - type  f16:   98 tensors
0.00.089.125 I llm_load_vocab: special tokens cache size = 25
0.00.095.968 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.971 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.971 I llm_load_print_meta: arch             = gptneox
0.00.095.972 I llm_load_print_meta: vocab type       = BPE
0.00.095.972 I llm_load_print_meta: n_vocab          = 50304
0.00.095.972 I llm_load_print_meta: n_merges         = 50009
0.00.095.972 I llm_load_print_meta: vocab_only       = 0
0.00.095.972 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.973 I llm_load_print_meta: n_embd           = 2048
0.00.095.973 I llm_load_print_meta: n_layer          = 24
0.00.095.987 I llm_load_print_meta: n_head           = 16
0.00.095.988 I llm_load_print_meta: n_head_kv        = 16
0.00.095.988 I llm_load_print_meta: n_rot            = 32
0.00.095.989 I llm_load_print_meta: n_swa            = 0
0.00.095.989 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.989 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.992 I llm_load_print_meta: n_gqa            = 1
0.00.095.993 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.993 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.994 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.995 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.995 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.996 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.996 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.996 I llm_load_print_meta: n_ff             = 8192
0.00.095.997 I llm_load_print_meta: n_expert         = 0
0.00.095.997 I llm_load_print_meta: n_expert_used    = 0
0.00.095.997 I llm_load_print_meta: causal attn      = 1
0.00.095.997 I llm_load_print_meta: pooling type     = 0
0.00.095.997 I llm_load_print_meta: rope type        = 2
0.00.095.997 I llm_load_print_meta: rope scaling     = linear
0.00.096.003 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.004 I llm_load_print_meta: freq_scale_train = 1
0.00.096.005 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.005 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.005 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.005 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.006 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.006 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.006 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.016 I llm_load_print_meta: model type       = 1.4B
0.00.096.017 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.096.017 I llm_load_print_meta: model params     = 1.41 B
0.00.096.018 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.096.018 I llm_load_print_meta: general.name     = 1.4B
0.00.096.018 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.018 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.019 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.019 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.019 I llm_load_print_meta: LF token         = 128 ''
0.00.096.019 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.020 I llm_load_print_meta: max token length = 1024
0.00.097.859 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.859 I llm_load_tensors: offloading output layer to GPU
0.00.097.859 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.869 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.871 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.790 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.791 I llama_new_context_with_model: n_ctx         = 128
0.00.098.791 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.792 I llama_new_context_with_model: n_batch       = 128
0.00.098.792 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.792 I llama_new_context_with_model: flash_attn    = 0
0.00.098.793 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.793 I llama_new_context_with_model: freq_scale    = 1
0.00.098.793 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.794 I ggml_metal_init: allocating
0.00.098.803 I ggml_metal_init: found device: Apple M4
0.00.098.805 I ggml_metal_init: picking default device: Apple M4
0.00.099.460 I ggml_metal_init: using embedded metal library
0.00.102.129 I ggml_metal_init: GPU name:   Apple M4
0.00.102.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.132 I ggml_metal_init: simdgroup reduction   = true
0.00.102.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.132 I ggml_metal_init: has bfloat            = true
0.00.102.132 I ggml_metal_init: use bfloat            = true
0.00.102.133 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.133 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.904 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.112.906 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.112.921 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.815 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.113.816 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.113.817 I llama_new_context_with_model: graph nodes  = 967
0.00.113.817 I llama_new_context_with_model: graph splits = 2
0.00.113.829 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.191.388 I 
0.01.191.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.191.491 I perplexity: tokenizing the input ..
0.01.204.727 I perplexity: tokenization took 13.233 ms
0.01.204.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.326.981 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.328.671 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.328.693 I llama_perf_context_print:        load time =    1165.12 ms
0.01.328.696 I llama_perf_context_print: prompt eval time =     121.23 ms /   128 tokens (    0.95 ms per token,  1055.80 tokens per second)
0.01.328.698 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.328.698 I llama_perf_context_print:       total time =     137.31 ms /   129 tokens
0.01.329.377 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.129s
sys	0m0.229s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.335 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.336 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.336 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.606 I llama_model_loader: - type  f32:  194 tensors
0.00.036.607 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.130 I llm_load_vocab: special tokens cache size = 25
0.00.066.421 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.426 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.426 I llm_load_print_meta: arch             = gptneox
0.00.066.427 I llm_load_print_meta: vocab type       = BPE
0.00.066.429 I llm_load_print_meta: n_vocab          = 50304
0.00.066.429 I llm_load_print_meta: n_merges         = 50009
0.00.066.430 I llm_load_print_meta: vocab_only       = 0
0.00.066.436 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.437 I llm_load_print_meta: n_embd           = 2048
0.00.066.437 I llm_load_print_meta: n_layer          = 24
0.00.066.453 I llm_load_print_meta: n_head           = 16
0.00.066.455 I llm_load_print_meta: n_head_kv        = 16
0.00.066.455 I llm_load_print_meta: n_rot            = 32
0.00.066.455 I llm_load_print_meta: n_swa            = 0
0.00.066.455 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.455 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.456 I llm_load_print_meta: n_gqa            = 1
0.00.066.456 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.457 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.458 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.458 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.458 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.458 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.458 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.459 I llm_load_print_meta: n_ff             = 8192
0.00.066.459 I llm_load_print_meta: n_expert         = 0
0.00.066.460 I llm_load_print_meta: n_expert_used    = 0
0.00.066.460 I llm_load_print_meta: causal attn      = 1
0.00.066.460 I llm_load_print_meta: pooling type     = 0
0.00.066.460 I llm_load_print_meta: rope type        = 2
0.00.066.460 I llm_load_print_meta: rope scaling     = linear
0.00.066.461 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.461 I llm_load_print_meta: freq_scale_train = 1
0.00.066.461 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.461 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.461 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.461 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.462 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.462 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.462 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.467 I llm_load_print_meta: model type       = 1.4B
0.00.066.467 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.468 I llm_load_print_meta: model params     = 1.41 B
0.00.066.468 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.468 I llm_load_print_meta: general.name     = 1.4B
0.00.066.469 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.469 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.469 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.469 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.469 I llm_load_print_meta: LF token         = 128 ''
0.00.066.470 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.470 I llm_load_print_meta: max token length = 1024
0.00.068.636 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.636 I llm_load_tensors: offloading output layer to GPU
0.00.068.636 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.642 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.644 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.621 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.622 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.622 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.623 I llama_new_context_with_model: n_batch       = 2048
0.00.069.623 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.623 I llama_new_context_with_model: flash_attn    = 0
0.00.069.623 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.624 I llama_new_context_with_model: freq_scale    = 1
0.00.069.624 I ggml_metal_init: allocating
0.00.069.631 I ggml_metal_init: found device: Apple M4
0.00.069.633 I ggml_metal_init: picking default device: Apple M4
0.00.070.370 I ggml_metal_init: using embedded metal library
0.00.073.097 I ggml_metal_init: GPU name:   Apple M4
0.00.073.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.099 I ggml_metal_init: simdgroup reduction   = true
0.00.073.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.100 I ggml_metal_init: has bfloat            = true
0.00.073.100 I ggml_metal_init: use bfloat            = true
0.00.073.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.638 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.648 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.673 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.802 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.803 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.804 I llama_new_context_with_model: graph nodes  = 967
0.00.109.804 I llama_new_context_with_model: graph splits = 2
0.00.109.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.323.097 I main: llama threadpool init, n_threads = 4
0.01.323.137 I 
0.01.323.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.323.165 I 
0.01.323.399 I sampler seed: 1234
0.01.323.406 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.323.445 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.323.447 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.323.447 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.410.763 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.02.410.763 I llama_perf_context_print:        load time =    1313.31 ms
0.02.410.765 I llama_perf_context_print: prompt eval time =      39.67 ms /     7 tokens (    5.67 ms per token,   176.44 tokens per second)
0.02.410.766 I llama_perf_context_print:        eval time =    1044.73 ms /    63 runs   (   16.58 ms per token,    60.30 tokens per second)
0.02.410.766 I llama_perf_context_print:       total time =    1087.67 ms /    70 tokens
0.02.410.983 I ggml_metal_free: deallocating

real	0m2.431s
user	0m0.116s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.137 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.836 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.836 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.839 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.839 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.840 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.405 I llama_model_loader: - type  f32:  194 tensors
0.00.033.406 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.298 I llm_load_vocab: special tokens cache size = 25
0.00.065.905 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.908 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.908 I llm_load_print_meta: arch             = gptneox
0.00.065.909 I llm_load_print_meta: vocab type       = BPE
0.00.065.909 I llm_load_print_meta: n_vocab          = 50304
0.00.065.909 I llm_load_print_meta: n_merges         = 50009
0.00.065.910 I llm_load_print_meta: vocab_only       = 0
0.00.065.910 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.910 I llm_load_print_meta: n_embd           = 2048
0.00.065.912 I llm_load_print_meta: n_layer          = 24
0.00.065.929 I llm_load_print_meta: n_head           = 16
0.00.065.930 I llm_load_print_meta: n_head_kv        = 16
0.00.065.930 I llm_load_print_meta: n_rot            = 32
0.00.065.930 I llm_load_print_meta: n_swa            = 0
0.00.065.930 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.931 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.932 I llm_load_print_meta: n_gqa            = 1
0.00.065.932 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.933 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.934 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.934 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.934 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.934 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.934 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.935 I llm_load_print_meta: n_ff             = 8192
0.00.065.935 I llm_load_print_meta: n_expert         = 0
0.00.065.935 I llm_load_print_meta: n_expert_used    = 0
0.00.065.935 I llm_load_print_meta: causal attn      = 1
0.00.065.936 I llm_load_print_meta: pooling type     = 0
0.00.065.936 I llm_load_print_meta: rope type        = 2
0.00.065.936 I llm_load_print_meta: rope scaling     = linear
0.00.065.936 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.938 I llm_load_print_meta: freq_scale_train = 1
0.00.065.938 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.938 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.938 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.939 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.939 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.939 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.949 I llm_load_print_meta: model type       = 1.4B
0.00.065.949 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.950 I llm_load_print_meta: model params     = 1.41 B
0.00.065.950 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.950 I llm_load_print_meta: general.name     = 1.4B
0.00.065.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.951 I llm_load_print_meta: LF token         = 128 ''
0.00.065.952 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.952 I llm_load_print_meta: max token length = 1024
0.00.068.255 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.256 I llm_load_tensors: offloading output layer to GPU
0.00.068.256 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.267 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.268 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.214 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.215 I llama_new_context_with_model: n_ctx         = 128
0.00.069.215 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.215 I llama_new_context_with_model: n_batch       = 128
0.00.069.215 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.215 I llama_new_context_with_model: flash_attn    = 0
0.00.069.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.216 I llama_new_context_with_model: freq_scale    = 1
0.00.069.216 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.217 I ggml_metal_init: allocating
0.00.069.220 I ggml_metal_init: found device: Apple M4
0.00.069.223 I ggml_metal_init: picking default device: Apple M4
0.00.069.882 I ggml_metal_init: using embedded metal library
0.00.072.435 I ggml_metal_init: GPU name:   Apple M4
0.00.072.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.437 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.437 I ggml_metal_init: simdgroup reduction   = true
0.00.072.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.438 I ggml_metal_init: has bfloat            = true
0.00.072.438 I ggml_metal_init: use bfloat            = true
0.00.072.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.389 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.404 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.368 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.369 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.369 I llama_new_context_with_model: graph nodes  = 967
0.00.084.370 I llama_new_context_with_model: graph splits = 2
0.00.084.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.931.057 I 
0.00.931.086 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.931.108 I perplexity: tokenizing the input ..
0.00.939.204 I perplexity: tokenization took 8.095 ms
0.00.939.214 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.063.719 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.064.885 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.064.903 I llama_perf_context_print:        load time =     918.74 ms
0.01.064.904 I llama_perf_context_print: prompt eval time =     124.24 ms /   128 tokens (    0.97 ms per token,  1030.24 tokens per second)
0.01.064.905 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.064.906 I llama_perf_context_print:       total time =     133.84 ms /   129 tokens
0.01.065.239 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.093s
sys	0m0.166s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.014.501 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.886 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.886 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.887 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.887 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.772 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.774 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.775 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.775 I llama_model_loader: - type  f32:  194 tensors
0.00.030.775 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.776 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.506 I llm_load_vocab: special tokens cache size = 25
0.00.058.661 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.664 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.665 I llm_load_print_meta: arch             = gptneox
0.00.058.665 I llm_load_print_meta: vocab type       = BPE
0.00.058.665 I llm_load_print_meta: n_vocab          = 50304
0.00.058.665 I llm_load_print_meta: n_merges         = 50009
0.00.058.666 I llm_load_print_meta: vocab_only       = 0
0.00.058.666 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.666 I llm_load_print_meta: n_embd           = 2048
0.00.058.666 I llm_load_print_meta: n_layer          = 24
0.00.058.681 I llm_load_print_meta: n_head           = 16
0.00.058.682 I llm_load_print_meta: n_head_kv        = 16
0.00.058.682 I llm_load_print_meta: n_rot            = 32
0.00.058.682 I llm_load_print_meta: n_swa            = 0
0.00.058.682 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.683 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.683 I llm_load_print_meta: n_gqa            = 1
0.00.058.684 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.685 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.686 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.686 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.687 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.687 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.688 I llm_load_print_meta: n_ff             = 8192
0.00.058.690 I llm_load_print_meta: n_expert         = 0
0.00.058.690 I llm_load_print_meta: n_expert_used    = 0
0.00.058.690 I llm_load_print_meta: causal attn      = 1
0.00.058.690 I llm_load_print_meta: pooling type     = 0
0.00.058.691 I llm_load_print_meta: rope type        = 2
0.00.058.691 I llm_load_print_meta: rope scaling     = linear
0.00.058.692 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.692 I llm_load_print_meta: freq_scale_train = 1
0.00.058.692 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.692 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.692 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.693 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.703 I llm_load_print_meta: model type       = 1.4B
0.00.058.703 I llm_load_print_meta: model ftype      = Q4_0
0.00.058.703 I llm_load_print_meta: model params     = 1.41 B
0.00.058.704 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.058.704 I llm_load_print_meta: general.name     = 1.4B
0.00.058.704 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.704 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.705 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.705 I llm_load_print_meta: LF token         = 128 ''
0.00.058.705 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.705 I llm_load_print_meta: max token length = 1024
0.00.060.386 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.386 I llm_load_tensors: offloading output layer to GPU
0.00.060.386 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.397 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.060.398 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.061.299 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.300 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.301 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.301 I llama_new_context_with_model: n_batch       = 2048
0.00.061.301 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.301 I llama_new_context_with_model: flash_attn    = 0
0.00.061.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.302 I llama_new_context_with_model: freq_scale    = 1
0.00.061.302 I ggml_metal_init: allocating
0.00.061.305 I ggml_metal_init: found device: Apple M4
0.00.061.307 I ggml_metal_init: picking default device: Apple M4
0.00.062.073 I ggml_metal_init: using embedded metal library
0.00.064.627 I ggml_metal_init: GPU name:   Apple M4
0.00.064.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.629 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.630 I ggml_metal_init: simdgroup reduction   = true
0.00.064.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.630 I ggml_metal_init: has bfloat            = true
0.00.064.630 I ggml_metal_init: use bfloat            = true
0.00.064.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.631 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.330 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.345 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.368 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.706 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.708 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.708 I llama_new_context_with_model: graph nodes  = 967
0.00.107.709 I llama_new_context_with_model: graph splits = 2
0.00.107.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.346 I main: llama threadpool init, n_threads = 4
0.00.717.386 I 
0.00.717.417 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.418 I 
0.00.717.649 I sampler seed: 1234
0.00.717.654 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.687 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.689 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.689 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.401.674 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.401.675 I llama_perf_context_print:        load time =     702.84 ms
0.01.401.675 I llama_perf_context_print: prompt eval time =      40.37 ms /     7 tokens (    5.77 ms per token,   173.40 tokens per second)
0.01.401.679 I llama_perf_context_print:        eval time =     640.72 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.401.681 I llama_perf_context_print:       total time =     684.33 ms /    70 tokens
0.01.401.876 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.113s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.343 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.149 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.155 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.155 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.156 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.157 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.175 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.088 I llama_model_loader: - type  f32:  194 tensors
0.00.024.089 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.089 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.035 I llm_load_vocab: special tokens cache size = 25
0.00.051.065 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.068 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.068 I llm_load_print_meta: arch             = gptneox
0.00.051.068 I llm_load_print_meta: vocab type       = BPE
0.00.051.068 I llm_load_print_meta: n_vocab          = 50304
0.00.051.069 I llm_load_print_meta: n_merges         = 50009
0.00.051.069 I llm_load_print_meta: vocab_only       = 0
0.00.051.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.069 I llm_load_print_meta: n_embd           = 2048
0.00.051.069 I llm_load_print_meta: n_layer          = 24
0.00.051.085 I llm_load_print_meta: n_head           = 16
0.00.051.085 I llm_load_print_meta: n_head_kv        = 16
0.00.051.086 I llm_load_print_meta: n_rot            = 32
0.00.051.086 I llm_load_print_meta: n_swa            = 0
0.00.051.086 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.086 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.087 I llm_load_print_meta: n_gqa            = 1
0.00.051.088 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.088 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.089 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.089 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.089 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.090 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.090 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.090 I llm_load_print_meta: n_ff             = 8192
0.00.051.090 I llm_load_print_meta: n_expert         = 0
0.00.051.091 I llm_load_print_meta: n_expert_used    = 0
0.00.051.091 I llm_load_print_meta: causal attn      = 1
0.00.051.091 I llm_load_print_meta: pooling type     = 0
0.00.051.091 I llm_load_print_meta: rope type        = 2
0.00.051.091 I llm_load_print_meta: rope scaling     = linear
0.00.051.091 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.092 I llm_load_print_meta: freq_scale_train = 1
0.00.051.092 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.092 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.092 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.092 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.092 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.093 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.103 I llm_load_print_meta: model type       = 1.4B
0.00.051.103 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.104 I llm_load_print_meta: model params     = 1.41 B
0.00.051.104 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.104 I llm_load_print_meta: general.name     = 1.4B
0.00.051.105 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: LF token         = 128 ''
0.00.051.106 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.106 I llm_load_print_meta: max token length = 1024
0.00.053.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.016 I llm_load_tensors: offloading output layer to GPU
0.00.053.017 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.027 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.029 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.935 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.936 I llama_new_context_with_model: n_ctx         = 128
0.00.053.936 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.936 I llama_new_context_with_model: n_batch       = 128
0.00.053.937 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.937 I llama_new_context_with_model: flash_attn    = 0
0.00.053.937 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.938 I llama_new_context_with_model: freq_scale    = 1
0.00.053.938 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.938 I ggml_metal_init: allocating
0.00.053.942 I ggml_metal_init: found device: Apple M4
0.00.053.944 I ggml_metal_init: picking default device: Apple M4
0.00.054.515 I ggml_metal_init: using embedded metal library
0.00.056.871 I ggml_metal_init: GPU name:   Apple M4
0.00.056.873 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.873 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.874 I ggml_metal_init: simdgroup reduction   = true
0.00.056.874 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.874 I ggml_metal_init: has bfloat            = true
0.00.056.874 I ggml_metal_init: use bfloat            = true
0.00.056.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.279 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.281 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.295 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.228 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.229 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.229 I llama_new_context_with_model: graph nodes  = 967
0.00.069.230 I llama_new_context_with_model: graph splits = 2
0.00.069.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.052 I 
0.00.621.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.096 I perplexity: tokenizing the input ..
0.00.629.327 I perplexity: tokenization took 8.229 ms
0.00.629.338 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.237 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.753.571 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.753.585 I llama_perf_context_print:        load time =     611.70 ms
0.00.753.586 I llama_perf_context_print: prompt eval time =     122.66 ms /   128 tokens (    0.96 ms per token,  1043.52 tokens per second)
0.00.753.587 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.587 I llama_perf_context_print:       total time =     132.53 ms /   129 tokens
0.00.753.954 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.080s
sys	0m0.106s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.734 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.249 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.250 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.250 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.250 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.251 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.251 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.252 I llama_model_loader: - type  f32:  194 tensors
0.00.025.252 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.252 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.446 I llm_load_vocab: special tokens cache size = 25
0.00.052.459 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.462 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.462 I llm_load_print_meta: arch             = gptneox
0.00.052.462 I llm_load_print_meta: vocab type       = BPE
0.00.052.463 I llm_load_print_meta: n_vocab          = 50304
0.00.052.463 I llm_load_print_meta: n_merges         = 50009
0.00.052.463 I llm_load_print_meta: vocab_only       = 0
0.00.052.463 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.463 I llm_load_print_meta: n_embd           = 2048
0.00.052.463 I llm_load_print_meta: n_layer          = 24
0.00.052.478 I llm_load_print_meta: n_head           = 16
0.00.052.479 I llm_load_print_meta: n_head_kv        = 16
0.00.052.479 I llm_load_print_meta: n_rot            = 32
0.00.052.480 I llm_load_print_meta: n_swa            = 0
0.00.052.480 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.480 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.481 I llm_load_print_meta: n_gqa            = 1
0.00.052.482 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.482 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.483 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.483 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.483 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.484 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.484 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.485 I llm_load_print_meta: n_ff             = 8192
0.00.052.485 I llm_load_print_meta: n_expert         = 0
0.00.052.485 I llm_load_print_meta: n_expert_used    = 0
0.00.052.485 I llm_load_print_meta: causal attn      = 1
0.00.052.485 I llm_load_print_meta: pooling type     = 0
0.00.052.485 I llm_load_print_meta: rope type        = 2
0.00.052.487 I llm_load_print_meta: rope scaling     = linear
0.00.052.488 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.488 I llm_load_print_meta: freq_scale_train = 1
0.00.052.488 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.498 I llm_load_print_meta: model type       = 1.4B
0.00.052.499 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.499 I llm_load_print_meta: model params     = 1.41 B
0.00.052.500 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.500 I llm_load_print_meta: general.name     = 1.4B
0.00.052.500 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.500 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.500 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: LF token         = 128 ''
0.00.052.501 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: max token length = 1024
0.00.054.473 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.474 I llm_load_tensors: offloading output layer to GPU
0.00.054.474 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.484 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.485 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.387 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.388 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.388 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.388 I llama_new_context_with_model: n_batch       = 2048
0.00.055.388 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.388 I llama_new_context_with_model: flash_attn    = 0
0.00.055.389 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.389 I llama_new_context_with_model: freq_scale    = 1
0.00.055.390 I ggml_metal_init: allocating
0.00.055.396 I ggml_metal_init: found device: Apple M4
0.00.055.399 I ggml_metal_init: picking default device: Apple M4
0.00.056.002 I ggml_metal_init: using embedded metal library
0.00.058.361 I ggml_metal_init: GPU name:   Apple M4
0.00.058.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.364 I ggml_metal_init: simdgroup reduction   = true
0.00.058.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.364 I ggml_metal_init: has bfloat            = true
0.00.058.364 I ggml_metal_init: use bfloat            = true
0.00.058.365 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.378 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.387 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.406 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.409 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.410 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.411 I llama_new_context_with_model: graph nodes  = 967
0.00.088.411 I llama_new_context_with_model: graph splits = 2
0.00.088.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.906.087 I main: llama threadpool init, n_threads = 4
0.00.906.140 I 
0.00.906.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.906.180 I 
0.00.906.416 I sampler seed: 1234
0.00.906.422 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.906.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.906.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.906.455 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.638.553 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.638.555 I llama_perf_context_print:        load time =     897.34 ms
0.01.638.556 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.58 tokens per second)
0.01.638.556 I llama_perf_context_print:        eval time =     685.57 ms /    63 runs   (   10.88 ms per token,    91.89 tokens per second)
0.01.638.558 I llama_perf_context_print:       total time =     732.47 ms /    70 tokens
0.01.638.752 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.764 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.950 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.955 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.854 I llama_model_loader: - type  f32:  194 tensors
0.00.024.854 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.855 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.426 I llm_load_vocab: special tokens cache size = 25
0.00.051.452 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.456 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.456 I llm_load_print_meta: arch             = gptneox
0.00.051.456 I llm_load_print_meta: vocab type       = BPE
0.00.051.456 I llm_load_print_meta: n_vocab          = 50304
0.00.051.457 I llm_load_print_meta: n_merges         = 50009
0.00.051.457 I llm_load_print_meta: vocab_only       = 0
0.00.051.457 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.457 I llm_load_print_meta: n_embd           = 2048
0.00.051.457 I llm_load_print_meta: n_layer          = 24
0.00.051.472 I llm_load_print_meta: n_head           = 16
0.00.051.474 I llm_load_print_meta: n_head_kv        = 16
0.00.051.474 I llm_load_print_meta: n_rot            = 32
0.00.051.475 I llm_load_print_meta: n_swa            = 0
0.00.051.475 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.475 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.475 I llm_load_print_meta: n_gqa            = 1
0.00.051.476 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.477 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.477 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.478 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.478 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.478 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.478 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.478 I llm_load_print_meta: n_ff             = 8192
0.00.051.479 I llm_load_print_meta: n_expert         = 0
0.00.051.479 I llm_load_print_meta: n_expert_used    = 0
0.00.051.479 I llm_load_print_meta: causal attn      = 1
0.00.051.479 I llm_load_print_meta: pooling type     = 0
0.00.051.479 I llm_load_print_meta: rope type        = 2
0.00.051.479 I llm_load_print_meta: rope scaling     = linear
0.00.051.480 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.480 I llm_load_print_meta: freq_scale_train = 1
0.00.051.480 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.480 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.480 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.481 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.481 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.481 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.481 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.490 I llm_load_print_meta: model type       = 1.4B
0.00.051.491 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.491 I llm_load_print_meta: model params     = 1.41 B
0.00.051.492 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.492 I llm_load_print_meta: general.name     = 1.4B
0.00.051.492 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.492 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.493 I llm_load_print_meta: LF token         = 128 ''
0.00.051.493 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.493 I llm_load_print_meta: max token length = 1024
0.00.053.396 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.397 I llm_load_tensors: offloading output layer to GPU
0.00.053.397 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.408 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.409 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.280 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.281 I llama_new_context_with_model: n_ctx         = 128
0.00.054.281 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.281 I llama_new_context_with_model: n_batch       = 128
0.00.054.282 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.282 I llama_new_context_with_model: flash_attn    = 0
0.00.054.282 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.282 I llama_new_context_with_model: freq_scale    = 1
0.00.054.283 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.283 I ggml_metal_init: allocating
0.00.054.286 I ggml_metal_init: found device: Apple M4
0.00.054.288 I ggml_metal_init: picking default device: Apple M4
0.00.054.880 I ggml_metal_init: using embedded metal library
0.00.057.185 I ggml_metal_init: GPU name:   Apple M4
0.00.057.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.187 I ggml_metal_init: simdgroup reduction   = true
0.00.057.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.188 I ggml_metal_init: has bfloat            = true
0.00.057.188 I ggml_metal_init: use bfloat            = true
0.00.057.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.467 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.470 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.484 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.420 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.421 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.422 I llama_new_context_with_model: graph nodes  = 967
0.00.069.422 I llama_new_context_with_model: graph splits = 2
0.00.069.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.416 I 
0.00.652.472 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.483 I perplexity: tokenizing the input ..
0.00.659.912 I perplexity: tokenization took 7.427 ms
0.00.659.925 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.065 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.783.313 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.783.332 I llama_perf_context_print:        load time =     642.64 ms
0.00.783.334 I llama_perf_context_print: prompt eval time =     121.89 ms /   128 tokens (    0.95 ms per token,  1050.08 tokens per second)
0.00.783.334 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.335 I llama_perf_context_print:       total time =     130.92 ms /   129 tokens
0.00.783.753 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.079s
sys	0m0.086s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.503 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.029.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.387 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.387 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.388 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.389 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.389 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.390 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.392 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.040.338 I llama_model_loader: - type  f32:  194 tensors
0.00.040.338 I llama_model_loader: - type q5_0:   97 tensors
0.00.040.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.072.417 I llm_load_vocab: special tokens cache size = 25
0.00.083.655 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.659 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.659 I llm_load_print_meta: arch             = gptneox
0.00.083.659 I llm_load_print_meta: vocab type       = BPE
0.00.083.660 I llm_load_print_meta: n_vocab          = 50304
0.00.083.660 I llm_load_print_meta: n_merges         = 50009
0.00.083.660 I llm_load_print_meta: vocab_only       = 0
0.00.083.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.660 I llm_load_print_meta: n_embd           = 2048
0.00.083.661 I llm_load_print_meta: n_layer          = 24
0.00.083.676 I llm_load_print_meta: n_head           = 16
0.00.083.677 I llm_load_print_meta: n_head_kv        = 16
0.00.083.677 I llm_load_print_meta: n_rot            = 32
0.00.083.678 I llm_load_print_meta: n_swa            = 0
0.00.083.678 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.679 I llm_load_print_meta: n_gqa            = 1
0.00.083.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.682 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.684 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.684 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.685 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.685 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.686 I llm_load_print_meta: n_ff             = 8192
0.00.083.686 I llm_load_print_meta: n_expert         = 0
0.00.083.686 I llm_load_print_meta: n_expert_used    = 0
0.00.083.686 I llm_load_print_meta: causal attn      = 1
0.00.083.687 I llm_load_print_meta: pooling type     = 0
0.00.083.687 I llm_load_print_meta: rope type        = 2
0.00.083.687 I llm_load_print_meta: rope scaling     = linear
0.00.083.688 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.688 I llm_load_print_meta: freq_scale_train = 1
0.00.083.688 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.688 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.689 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.689 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.689 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.689 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.689 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.700 I llm_load_print_meta: model type       = 1.4B
0.00.083.700 I llm_load_print_meta: model ftype      = Q5_0
0.00.083.701 I llm_load_print_meta: model params     = 1.41 B
0.00.083.701 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.083.702 I llm_load_print_meta: general.name     = 1.4B
0.00.083.702 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.702 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.703 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.703 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.703 I llm_load_print_meta: LF token         = 128 ''
0.00.083.703 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.704 I llm_load_print_meta: max token length = 1024
0.00.086.413 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.414 I llm_load_tensors: offloading output layer to GPU
0.00.086.414 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.425 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.086.427 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.087.768 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.769 I llama_new_context_with_model: n_ctx         = 2048
0.00.087.770 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.087.770 I llama_new_context_with_model: n_batch       = 2048
0.00.087.770 I llama_new_context_with_model: n_ubatch      = 512
0.00.087.771 I llama_new_context_with_model: flash_attn    = 0
0.00.087.771 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.771 I llama_new_context_with_model: freq_scale    = 1
0.00.087.772 I ggml_metal_init: allocating
0.00.087.776 I ggml_metal_init: found device: Apple M4
0.00.087.779 I ggml_metal_init: picking default device: Apple M4
0.00.088.646 I ggml_metal_init: using embedded metal library
0.00.092.357 I ggml_metal_init: GPU name:   Apple M4
0.00.092.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.360 I ggml_metal_init: simdgroup reduction   = true
0.00.092.360 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.361 I ggml_metal_init: has bfloat            = true
0.00.092.361 I ggml_metal_init: use bfloat            = true
0.00.092.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.362 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.126.736 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.742 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.764 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.847 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.848 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.848 I llama_new_context_with_model: graph nodes  = 967
0.00.127.849 I llama_new_context_with_model: graph splits = 2
0.00.127.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.685 I main: llama threadpool init, n_threads = 4
0.00.896.777 I 
0.00.896.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.896.845 I 
0.00.897.253 I sampler seed: 1234
0.00.897.258 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.897.314 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.897.314 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.897.314 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.686.734 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49408.49 tokens per second)
0.01.686.735 I llama_perf_context_print:        load time =     886.18 ms
0.01.686.736 I llama_perf_context_print: prompt eval time =      43.34 ms /     7 tokens (    6.19 ms per token,   161.50 tokens per second)
0.01.686.737 I llama_perf_context_print:        eval time =     743.54 ms /    63 runs   (   11.80 ms per token,    84.73 tokens per second)
0.01.686.737 I llama_perf_context_print:       total time =     790.05 ms /    70 tokens
0.01.686.936 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.140s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.561 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.512 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.520 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.363 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.311 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.312 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.314 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.314 I llama_model_loader: - type  f32:  194 tensors
0.00.024.314 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.315 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.369 I llm_load_vocab: special tokens cache size = 25
0.00.051.449 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.452 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.453 I llm_load_print_meta: arch             = gptneox
0.00.051.453 I llm_load_print_meta: vocab type       = BPE
0.00.051.453 I llm_load_print_meta: n_vocab          = 50304
0.00.051.453 I llm_load_print_meta: n_merges         = 50009
0.00.051.454 I llm_load_print_meta: vocab_only       = 0
0.00.051.454 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.454 I llm_load_print_meta: n_embd           = 2048
0.00.051.454 I llm_load_print_meta: n_layer          = 24
0.00.051.461 I llm_load_print_meta: n_head           = 16
0.00.051.462 I llm_load_print_meta: n_head_kv        = 16
0.00.051.462 I llm_load_print_meta: n_rot            = 32
0.00.051.462 I llm_load_print_meta: n_swa            = 0
0.00.051.462 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.462 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.463 I llm_load_print_meta: n_gqa            = 1
0.00.051.464 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.464 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.465 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.465 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.466 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.466 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.466 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.467 I llm_load_print_meta: n_ff             = 8192
0.00.051.467 I llm_load_print_meta: n_expert         = 0
0.00.051.467 I llm_load_print_meta: n_expert_used    = 0
0.00.051.467 I llm_load_print_meta: causal attn      = 1
0.00.051.467 I llm_load_print_meta: pooling type     = 0
0.00.051.467 I llm_load_print_meta: rope type        = 2
0.00.051.468 I llm_load_print_meta: rope scaling     = linear
0.00.051.468 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.469 I llm_load_print_meta: freq_scale_train = 1
0.00.051.469 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.469 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.469 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.469 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.472 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.472 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.472 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.476 I llm_load_print_meta: model type       = 1.4B
0.00.051.476 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.476 I llm_load_print_meta: model params     = 1.41 B
0.00.051.477 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.477 I llm_load_print_meta: general.name     = 1.4B
0.00.051.478 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: LF token         = 128 ''
0.00.051.479 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: max token length = 1024
0.00.053.229 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.229 I llm_load_tensors: offloading output layer to GPU
0.00.053.230 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.235 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.236 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.082 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.083 I llama_new_context_with_model: n_ctx         = 128
0.00.054.083 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.083 I llama_new_context_with_model: n_batch       = 128
0.00.054.083 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.083 I llama_new_context_with_model: flash_attn    = 0
0.00.054.084 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.084 I llama_new_context_with_model: freq_scale    = 1
0.00.054.084 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.085 I ggml_metal_init: allocating
0.00.054.087 I ggml_metal_init: found device: Apple M4
0.00.054.089 I ggml_metal_init: picking default device: Apple M4
0.00.054.633 I ggml_metal_init: using embedded metal library
0.00.056.925 I ggml_metal_init: GPU name:   Apple M4
0.00.056.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.928 I ggml_metal_init: simdgroup reduction   = true
0.00.056.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.928 I ggml_metal_init: has bfloat            = true
0.00.056.928 I ggml_metal_init: use bfloat            = true
0.00.056.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.550 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.555 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.574 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.445 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.447 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.447 I llama_new_context_with_model: graph nodes  = 967
0.00.069.447 I llama_new_context_with_model: graph splits = 2
0.00.069.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.150 I 
0.00.721.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.204 I perplexity: tokenizing the input ..
0.00.729.589 I perplexity: tokenization took 8.384 ms
0.00.729.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.951 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.866.212 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.866.235 I llama_perf_context_print:        load time =     711.58 ms
0.00.866.236 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.29 tokens per second)
0.00.866.237 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.238 I llama_perf_context_print:       total time =     145.09 ms /   129 tokens
0.00.866.748 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.080s
sys	0m0.119s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.111 I llama_model_loader: - type  f32:  194 tensors
0.00.024.111 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.111 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.559 I llm_load_vocab: special tokens cache size = 25
0.00.050.577 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.580 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.581 I llm_load_print_meta: arch             = gptneox
0.00.050.581 I llm_load_print_meta: vocab type       = BPE
0.00.050.581 I llm_load_print_meta: n_vocab          = 50304
0.00.050.581 I llm_load_print_meta: n_merges         = 50009
0.00.050.582 I llm_load_print_meta: vocab_only       = 0
0.00.050.582 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.582 I llm_load_print_meta: n_embd           = 2048
0.00.050.582 I llm_load_print_meta: n_layer          = 24
0.00.050.597 I llm_load_print_meta: n_head           = 16
0.00.050.598 I llm_load_print_meta: n_head_kv        = 16
0.00.050.598 I llm_load_print_meta: n_rot            = 32
0.00.050.598 I llm_load_print_meta: n_swa            = 0
0.00.050.598 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.598 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.599 I llm_load_print_meta: n_gqa            = 1
0.00.050.600 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.602 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.603 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.603 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.603 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.603 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.604 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.604 I llm_load_print_meta: n_ff             = 8192
0.00.050.604 I llm_load_print_meta: n_expert         = 0
0.00.050.604 I llm_load_print_meta: n_expert_used    = 0
0.00.050.606 I llm_load_print_meta: causal attn      = 1
0.00.050.608 I llm_load_print_meta: pooling type     = 0
0.00.050.608 I llm_load_print_meta: rope type        = 2
0.00.050.608 I llm_load_print_meta: rope scaling     = linear
0.00.050.608 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.608 I llm_load_print_meta: freq_scale_train = 1
0.00.050.609 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.609 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.609 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.609 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.609 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.609 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.609 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.619 I llm_load_print_meta: model type       = 1.4B
0.00.050.619 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.619 I llm_load_print_meta: model params     = 1.41 B
0.00.050.620 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.620 I llm_load_print_meta: general.name     = 1.4B
0.00.050.620 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.620 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.621 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.621 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.621 I llm_load_print_meta: LF token         = 128 ''
0.00.050.621 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.621 I llm_load_print_meta: max token length = 1024
0.00.052.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.655 I llm_load_tensors: offloading output layer to GPU
0.00.052.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.666 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.667 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.566 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.566 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.566 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.567 I llama_new_context_with_model: n_batch       = 2048
0.00.053.567 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.567 I llama_new_context_with_model: flash_attn    = 0
0.00.053.567 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.568 I llama_new_context_with_model: freq_scale    = 1
0.00.053.568 I ggml_metal_init: allocating
0.00.053.575 I ggml_metal_init: found device: Apple M4
0.00.053.577 I ggml_metal_init: picking default device: Apple M4
0.00.054.185 I ggml_metal_init: using embedded metal library
0.00.056.506 I ggml_metal_init: GPU name:   Apple M4
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.509 I ggml_metal_init: simdgroup reduction   = true
0.00.056.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.510 I ggml_metal_init: has bfloat            = true
0.00.056.511 I ggml_metal_init: use bfloat            = true
0.00.056.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.449 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.456 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.476 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.520 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.521 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.521 I llama_new_context_with_model: graph nodes  = 967
0.00.086.522 I llama_new_context_with_model: graph splits = 2
0.00.086.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.595 I main: llama threadpool init, n_threads = 4
0.00.718.639 I 
0.00.718.669 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.670 I 
0.00.718.914 I sampler seed: 1234
0.00.718.919 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.718.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.718.962 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.718.962 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.559.770 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.559.771 I llama_perf_context_print:        load time =     709.84 ms
0.01.559.772 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.55 tokens per second)
0.01.559.773 I llama_perf_context_print:        eval time =     795.45 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.559.773 I llama_perf_context_print:       total time =     841.18 ms /    70 tokens
0.01.559.966 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.653 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.659 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.662 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.482 I llama_model_loader: - type  f32:  194 tensors
0.00.023.482 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.482 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.805 I llm_load_vocab: special tokens cache size = 25
0.00.049.757 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.760 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.760 I llm_load_print_meta: arch             = gptneox
0.00.049.761 I llm_load_print_meta: vocab type       = BPE
0.00.049.761 I llm_load_print_meta: n_vocab          = 50304
0.00.049.761 I llm_load_print_meta: n_merges         = 50009
0.00.049.761 I llm_load_print_meta: vocab_only       = 0
0.00.049.762 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.762 I llm_load_print_meta: n_embd           = 2048
0.00.049.762 I llm_load_print_meta: n_layer          = 24
0.00.049.776 I llm_load_print_meta: n_head           = 16
0.00.049.777 I llm_load_print_meta: n_head_kv        = 16
0.00.049.777 I llm_load_print_meta: n_rot            = 32
0.00.049.779 I llm_load_print_meta: n_swa            = 0
0.00.049.779 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.779 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.780 I llm_load_print_meta: n_gqa            = 1
0.00.049.781 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.782 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.782 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.782 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.783 I llm_load_print_meta: n_ff             = 8192
0.00.049.784 I llm_load_print_meta: n_expert         = 0
0.00.049.784 I llm_load_print_meta: n_expert_used    = 0
0.00.049.784 I llm_load_print_meta: causal attn      = 1
0.00.049.784 I llm_load_print_meta: pooling type     = 0
0.00.049.784 I llm_load_print_meta: rope type        = 2
0.00.049.784 I llm_load_print_meta: rope scaling     = linear
0.00.049.786 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.787 I llm_load_print_meta: freq_scale_train = 1
0.00.049.787 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.787 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.788 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.798 I llm_load_print_meta: model type       = 1.4B
0.00.049.798 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.799 I llm_load_print_meta: model params     = 1.41 B
0.00.049.799 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.799 I llm_load_print_meta: general.name     = 1.4B
0.00.049.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.800 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.800 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.800 I llm_load_print_meta: LF token         = 128 ''
0.00.049.801 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.801 I llm_load_print_meta: max token length = 1024
0.00.051.840 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.840 I llm_load_tensors: offloading output layer to GPU
0.00.051.841 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.851 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.852 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.806 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.807 I llama_new_context_with_model: n_ctx         = 128
0.00.052.807 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.807 I llama_new_context_with_model: n_batch       = 128
0.00.052.808 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.808 I llama_new_context_with_model: flash_attn    = 0
0.00.052.808 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.808 I llama_new_context_with_model: freq_scale    = 1
0.00.052.809 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.809 I ggml_metal_init: allocating
0.00.052.815 I ggml_metal_init: found device: Apple M4
0.00.052.817 I ggml_metal_init: picking default device: Apple M4
0.00.053.387 I ggml_metal_init: using embedded metal library
0.00.055.712 I ggml_metal_init: GPU name:   Apple M4
0.00.055.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.714 I ggml_metal_init: simdgroup reduction   = true
0.00.055.715 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.715 I ggml_metal_init: has bfloat            = true
0.00.055.715 I ggml_metal_init: use bfloat            = true
0.00.055.717 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.718 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.383 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.390 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.404 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.265 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.266 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.266 I llama_new_context_with_model: graph nodes  = 967
0.00.067.267 I llama_new_context_with_model: graph splits = 2
0.00.067.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.763 I 
0.00.631.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.801 I perplexity: tokenizing the input ..
0.00.639.841 I perplexity: tokenization took 8.038 ms
0.00.639.855 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.862 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.776.117 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.776.134 I llama_perf_context_print:        load time =     623.00 ms
0.00.776.135 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.68 tokens per second)
0.00.776.136 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.136 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.776.614 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.077s
sys	0m0.118s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.653 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.204 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.204 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.205 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.205 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.206 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.208 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.134 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.036 I llama_model_loader: - type  f32:  194 tensors
0.00.024.036 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.037 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.467 I llm_load_vocab: special tokens cache size = 25
0.00.050.482 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.485 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.485 I llm_load_print_meta: arch             = gptneox
0.00.050.486 I llm_load_print_meta: vocab type       = BPE
0.00.050.486 I llm_load_print_meta: n_vocab          = 50304
0.00.050.486 I llm_load_print_meta: n_merges         = 50009
0.00.050.486 I llm_load_print_meta: vocab_only       = 0
0.00.050.487 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.487 I llm_load_print_meta: n_embd           = 2048
0.00.050.487 I llm_load_print_meta: n_layer          = 24
0.00.050.502 I llm_load_print_meta: n_head           = 16
0.00.050.502 I llm_load_print_meta: n_head_kv        = 16
0.00.050.502 I llm_load_print_meta: n_rot            = 32
0.00.050.503 I llm_load_print_meta: n_swa            = 0
0.00.050.503 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.503 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.505 I llm_load_print_meta: n_gqa            = 1
0.00.050.506 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.507 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.507 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.507 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.508 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.508 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.509 I llm_load_print_meta: n_ff             = 8192
0.00.050.509 I llm_load_print_meta: n_expert         = 0
0.00.050.509 I llm_load_print_meta: n_expert_used    = 0
0.00.050.509 I llm_load_print_meta: causal attn      = 1
0.00.050.509 I llm_load_print_meta: pooling type     = 0
0.00.050.511 I llm_load_print_meta: rope type        = 2
0.00.050.511 I llm_load_print_meta: rope scaling     = linear
0.00.050.511 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.512 I llm_load_print_meta: freq_scale_train = 1
0.00.050.512 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.512 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.512 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.513 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.522 I llm_load_print_meta: model type       = 1.4B
0.00.050.523 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.523 I llm_load_print_meta: model params     = 1.41 B
0.00.050.524 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.524 I llm_load_print_meta: general.name     = 1.4B
0.00.050.524 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.525 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.525 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.525 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.525 I llm_load_print_meta: LF token         = 128 ''
0.00.050.526 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.526 I llm_load_print_meta: max token length = 1024
0.00.052.379 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.379 I llm_load_tensors: offloading output layer to GPU
0.00.052.379 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.389 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.391 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.334 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.334 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.335 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.335 I llama_new_context_with_model: n_batch       = 2048
0.00.053.335 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.335 I llama_new_context_with_model: flash_attn    = 0
0.00.053.336 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.336 I llama_new_context_with_model: freq_scale    = 1
0.00.053.337 I ggml_metal_init: allocating
0.00.053.343 I ggml_metal_init: found device: Apple M4
0.00.053.345 I ggml_metal_init: picking default device: Apple M4
0.00.053.920 I ggml_metal_init: using embedded metal library
0.00.056.205 I ggml_metal_init: GPU name:   Apple M4
0.00.056.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.207 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.207 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.207 I ggml_metal_init: simdgroup reduction   = true
0.00.056.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.207 I ggml_metal_init: has bfloat            = true
0.00.056.207 I ggml_metal_init: use bfloat            = true
0.00.056.208 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.105 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.113 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.131 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.213 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.214 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.215 I llama_new_context_with_model: graph nodes  = 967
0.00.086.215 I llama_new_context_with_model: graph splits = 2
0.00.086.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.151 I main: llama threadpool init, n_threads = 4
0.00.442.195 I 
0.00.442.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.222 I 
0.00.442.373 I sampler seed: 1234
0.00.442.377 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.442.416 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.442.418 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.442.418 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.122.292 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62335.38 tokens per second)
0.01.122.292 I llama_perf_context_print:        load time =     432.49 ms
0.01.122.293 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.84 tokens per second)
0.01.122.294 I llama_perf_context_print:        eval time =     641.15 ms /    63 runs   (   10.18 ms per token,    98.26 tokens per second)
0.01.122.294 I llama_perf_context_print:       total time =     680.14 ms /    70 tokens
0.01.122.489 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.110s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.568 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.573 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.576 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.579 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.580 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.580 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.580 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.581 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.585 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.488 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.489 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.489 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.490 I llama_model_loader: - type  f32:  194 tensors
0.00.024.491 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.491 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.491 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.419 I llm_load_vocab: special tokens cache size = 25
0.00.051.437 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.440 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.441 I llm_load_print_meta: arch             = gptneox
0.00.051.441 I llm_load_print_meta: vocab type       = BPE
0.00.051.441 I llm_load_print_meta: n_vocab          = 50304
0.00.051.441 I llm_load_print_meta: n_merges         = 50009
0.00.051.442 I llm_load_print_meta: vocab_only       = 0
0.00.051.442 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.442 I llm_load_print_meta: n_embd           = 2048
0.00.051.442 I llm_load_print_meta: n_layer          = 24
0.00.051.457 I llm_load_print_meta: n_head           = 16
0.00.051.458 I llm_load_print_meta: n_head_kv        = 16
0.00.051.458 I llm_load_print_meta: n_rot            = 32
0.00.051.459 I llm_load_print_meta: n_swa            = 0
0.00.051.459 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.459 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.460 I llm_load_print_meta: n_gqa            = 1
0.00.051.461 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.461 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.462 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.462 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.462 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.462 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.462 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.463 I llm_load_print_meta: n_ff             = 8192
0.00.051.463 I llm_load_print_meta: n_expert         = 0
0.00.051.463 I llm_load_print_meta: n_expert_used    = 0
0.00.051.463 I llm_load_print_meta: causal attn      = 1
0.00.051.464 I llm_load_print_meta: pooling type     = 0
0.00.051.464 I llm_load_print_meta: rope type        = 2
0.00.051.464 I llm_load_print_meta: rope scaling     = linear
0.00.051.465 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.465 I llm_load_print_meta: freq_scale_train = 1
0.00.051.465 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.465 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.465 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.465 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.466 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.467 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.467 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.476 I llm_load_print_meta: model type       = 1.4B
0.00.051.477 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.478 I llm_load_print_meta: model params     = 1.41 B
0.00.051.478 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.478 I llm_load_print_meta: general.name     = 1.4B
0.00.051.478 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: LF token         = 128 ''
0.00.051.480 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.480 I llm_load_print_meta: max token length = 1024
0.00.053.393 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.393 I llm_load_tensors: offloading output layer to GPU
0.00.053.394 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.404 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.405 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.330 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.331 I llama_new_context_with_model: n_ctx         = 128
0.00.054.331 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.331 I llama_new_context_with_model: n_batch       = 128
0.00.054.331 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.331 I llama_new_context_with_model: flash_attn    = 0
0.00.054.332 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.332 I llama_new_context_with_model: freq_scale    = 1
0.00.054.332 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.333 I ggml_metal_init: allocating
0.00.054.336 I ggml_metal_init: found device: Apple M4
0.00.054.338 I ggml_metal_init: picking default device: Apple M4
0.00.054.910 I ggml_metal_init: using embedded metal library
0.00.057.234 I ggml_metal_init: GPU name:   Apple M4
0.00.057.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.236 I ggml_metal_init: simdgroup reduction   = true
0.00.057.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.237 I ggml_metal_init: has bfloat            = true
0.00.057.237 I ggml_metal_init: use bfloat            = true
0.00.057.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.236 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.240 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.255 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.161 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.162 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.163 I llama_new_context_with_model: graph nodes  = 967
0.00.069.163 I llama_new_context_with_model: graph splits = 2
0.00.069.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.394.842 I 
0.00.394.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.394.889 I perplexity: tokenizing the input ..
0.00.402.725 I perplexity: tokenization took 7.834 ms
0.00.402.736 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.535.355 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.536.515 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.536.548 I llama_perf_context_print:        load time =     385.02 ms
0.00.536.549 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.86 tokens per second)
0.00.536.550 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.536.550 I llama_perf_context_print:       total time =     141.71 ms /   129 tokens
0.00.537.080 I ggml_metal_free: deallocating

real	0m0.552s
user	0m0.079s
sys	0m0.073s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.011.450 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.868 I llama_model_loader: - type  f32:  194 tensors
0.00.025.869 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.869 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.869 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.363 I llm_load_vocab: special tokens cache size = 25
0.00.052.331 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.333 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.334 I llm_load_print_meta: arch             = gptneox
0.00.052.334 I llm_load_print_meta: vocab type       = BPE
0.00.052.334 I llm_load_print_meta: n_vocab          = 50304
0.00.052.334 I llm_load_print_meta: n_merges         = 50009
0.00.052.335 I llm_load_print_meta: vocab_only       = 0
0.00.052.335 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.335 I llm_load_print_meta: n_embd           = 2048
0.00.052.335 I llm_load_print_meta: n_layer          = 24
0.00.052.350 I llm_load_print_meta: n_head           = 16
0.00.052.352 I llm_load_print_meta: n_head_kv        = 16
0.00.052.352 I llm_load_print_meta: n_rot            = 32
0.00.052.352 I llm_load_print_meta: n_swa            = 0
0.00.052.352 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.352 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.353 I llm_load_print_meta: n_gqa            = 1
0.00.052.354 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.354 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.355 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.355 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.356 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.356 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.356 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.357 I llm_load_print_meta: n_ff             = 8192
0.00.052.358 I llm_load_print_meta: n_expert         = 0
0.00.052.360 I llm_load_print_meta: n_expert_used    = 0
0.00.052.360 I llm_load_print_meta: causal attn      = 1
0.00.052.360 I llm_load_print_meta: pooling type     = 0
0.00.052.360 I llm_load_print_meta: rope type        = 2
0.00.052.361 I llm_load_print_meta: rope scaling     = linear
0.00.052.361 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.361 I llm_load_print_meta: freq_scale_train = 1
0.00.052.361 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.361 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.362 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.362 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.362 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.362 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.362 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.371 I llm_load_print_meta: model type       = 1.4B
0.00.052.372 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.372 I llm_load_print_meta: model params     = 1.41 B
0.00.052.373 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.373 I llm_load_print_meta: general.name     = 1.4B
0.00.052.373 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.373 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.373 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.373 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.374 I llm_load_print_meta: LF token         = 128 ''
0.00.052.374 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.374 I llm_load_print_meta: max token length = 1024
0.00.054.345 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.345 I llm_load_tensors: offloading output layer to GPU
0.00.054.345 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.356 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.357 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.386 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.387 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.387 I llama_new_context_with_model: n_batch       = 2048
0.00.055.387 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.387 I llama_new_context_with_model: flash_attn    = 0
0.00.055.388 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.388 I llama_new_context_with_model: freq_scale    = 1
0.00.055.388 I ggml_metal_init: allocating
0.00.055.392 I ggml_metal_init: found device: Apple M4
0.00.055.394 I ggml_metal_init: picking default device: Apple M4
0.00.056.015 I ggml_metal_init: using embedded metal library
0.00.058.339 I ggml_metal_init: GPU name:   Apple M4
0.00.058.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.343 I ggml_metal_init: simdgroup reduction   = true
0.00.058.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.343 I ggml_metal_init: has bfloat            = true
0.00.058.343 I ggml_metal_init: use bfloat            = true
0.00.058.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.861 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.867 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.884 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.927 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.929 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.929 I llama_new_context_with_model: graph nodes  = 967
0.00.089.929 I llama_new_context_with_model: graph splits = 2
0.00.089.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.611 I main: llama threadpool init, n_threads = 4
0.00.540.653 I 
0.00.540.683 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.683 I 
0.00.540.908 I sampler seed: 1234
0.00.540.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.934 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.934 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.934 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.286.345 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.286.346 I llama_perf_context_print:        load time =     529.16 ms
0.01.286.346 I llama_perf_context_print: prompt eval time =      40.50 ms /     7 tokens (    5.79 ms per token,   172.83 tokens per second)
0.01.286.348 I llama_perf_context_print:        eval time =     702.07 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.286.349 I llama_perf_context_print:       total time =     745.74 ms /    70 tokens
0.01.286.558 I ggml_metal_free: deallocating

real	0m1.302s
user	0m0.109s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.579 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.295 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.980 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.982 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.983 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.983 I llama_model_loader: - type  f32:  194 tensors
0.00.022.984 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.984 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.984 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.984 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.184 I llm_load_vocab: special tokens cache size = 25
0.00.049.201 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.204 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.204 I llm_load_print_meta: arch             = gptneox
0.00.049.204 I llm_load_print_meta: vocab type       = BPE
0.00.049.204 I llm_load_print_meta: n_vocab          = 50304
0.00.049.205 I llm_load_print_meta: n_merges         = 50009
0.00.049.205 I llm_load_print_meta: vocab_only       = 0
0.00.049.205 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.205 I llm_load_print_meta: n_embd           = 2048
0.00.049.205 I llm_load_print_meta: n_layer          = 24
0.00.049.219 I llm_load_print_meta: n_head           = 16
0.00.049.220 I llm_load_print_meta: n_head_kv        = 16
0.00.049.220 I llm_load_print_meta: n_rot            = 32
0.00.049.221 I llm_load_print_meta: n_swa            = 0
0.00.049.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.222 I llm_load_print_meta: n_gqa            = 1
0.00.049.222 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.225 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.226 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.226 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.227 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.227 I llm_load_print_meta: n_ff             = 8192
0.00.049.227 I llm_load_print_meta: n_expert         = 0
0.00.049.227 I llm_load_print_meta: n_expert_used    = 0
0.00.049.228 I llm_load_print_meta: causal attn      = 1
0.00.049.228 I llm_load_print_meta: pooling type     = 0
0.00.049.229 I llm_load_print_meta: rope type        = 2
0.00.049.229 I llm_load_print_meta: rope scaling     = linear
0.00.049.229 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.230 I llm_load_print_meta: freq_scale_train = 1
0.00.049.230 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.230 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.231 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.231 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.231 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.231 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.231 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.241 I llm_load_print_meta: model type       = 1.4B
0.00.049.241 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.241 I llm_load_print_meta: model params     = 1.41 B
0.00.049.242 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.242 I llm_load_print_meta: general.name     = 1.4B
0.00.049.242 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.242 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.242 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.243 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.243 I llm_load_print_meta: LF token         = 128 ''
0.00.049.244 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.244 I llm_load_print_meta: max token length = 1024
0.00.051.116 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.117 I llm_load_tensors: offloading output layer to GPU
0.00.051.117 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.127 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.129 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.025 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.026 I llama_new_context_with_model: n_ctx         = 128
0.00.052.026 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.026 I llama_new_context_with_model: n_batch       = 128
0.00.052.026 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.026 I llama_new_context_with_model: flash_attn    = 0
0.00.052.027 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.027 I llama_new_context_with_model: freq_scale    = 1
0.00.052.027 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.028 I ggml_metal_init: allocating
0.00.052.034 I ggml_metal_init: found device: Apple M4
0.00.052.037 I ggml_metal_init: picking default device: Apple M4
0.00.052.638 I ggml_metal_init: using embedded metal library
0.00.054.952 I ggml_metal_init: GPU name:   Apple M4
0.00.054.954 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.955 I ggml_metal_init: simdgroup reduction   = true
0.00.054.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.955 I ggml_metal_init: has bfloat            = true
0.00.054.955 I ggml_metal_init: use bfloat            = true
0.00.054.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.883 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.887 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.901 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.823 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.824 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.824 I llama_new_context_with_model: graph nodes  = 967
0.00.066.824 I llama_new_context_with_model: graph splits = 2
0.00.066.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.974 I 
0.00.502.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.060 I perplexity: tokenizing the input ..
0.00.510.515 I perplexity: tokenization took 8.453 ms
0.00.510.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.642.580 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.643.774 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.643.788 I llama_perf_context_print:        load time =     493.39 ms
0.00.643.789 I llama_perf_context_print: prompt eval time =     131.82 ms /   128 tokens (    1.03 ms per token,   970.98 tokens per second)
0.00.643.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.643.791 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.644.139 I ggml_metal_free: deallocating

real	0m0.658s
user	0m0.079s
sys	0m0.097s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.669 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.439 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.440 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.443 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.568 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.706 I llama_model_loader: - type  f32:  194 tensors
0.00.023.706 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.707 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.707 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.597 I llm_load_vocab: special tokens cache size = 25
0.00.051.695 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.700 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.700 I llm_load_print_meta: arch             = gptneox
0.00.051.700 I llm_load_print_meta: vocab type       = BPE
0.00.051.701 I llm_load_print_meta: n_vocab          = 50304
0.00.051.701 I llm_load_print_meta: n_merges         = 50009
0.00.051.701 I llm_load_print_meta: vocab_only       = 0
0.00.051.703 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.703 I llm_load_print_meta: n_embd           = 2048
0.00.051.703 I llm_load_print_meta: n_layer          = 24
0.00.051.720 I llm_load_print_meta: n_head           = 16
0.00.051.722 I llm_load_print_meta: n_head_kv        = 16
0.00.051.722 I llm_load_print_meta: n_rot            = 32
0.00.051.722 I llm_load_print_meta: n_swa            = 0
0.00.051.722 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.723 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.723 I llm_load_print_meta: n_gqa            = 1
0.00.051.724 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.724 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.725 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.725 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.726 I llm_load_print_meta: n_ff             = 8192
0.00.051.727 I llm_load_print_meta: n_expert         = 0
0.00.051.727 I llm_load_print_meta: n_expert_used    = 0
0.00.051.727 I llm_load_print_meta: causal attn      = 1
0.00.051.727 I llm_load_print_meta: pooling type     = 0
0.00.051.727 I llm_load_print_meta: rope type        = 2
0.00.051.727 I llm_load_print_meta: rope scaling     = linear
0.00.051.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.741 I llm_load_print_meta: freq_scale_train = 1
0.00.051.741 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.742 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.742 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.743 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.753 I llm_load_print_meta: model type       = 1.4B
0.00.051.753 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.754 I llm_load_print_meta: model params     = 1.41 B
0.00.051.754 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.754 I llm_load_print_meta: general.name     = 1.4B
0.00.051.755 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.755 I llm_load_print_meta: LF token         = 128 ''
0.00.051.756 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.756 I llm_load_print_meta: max token length = 1024
0.00.053.758 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.758 I llm_load_tensors: offloading output layer to GPU
0.00.053.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.769 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.770 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.706 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.707 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.707 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.708 I llama_new_context_with_model: n_batch       = 2048
0.00.054.708 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.708 I llama_new_context_with_model: flash_attn    = 0
0.00.054.708 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.709 I llama_new_context_with_model: freq_scale    = 1
0.00.054.710 I ggml_metal_init: allocating
0.00.054.717 I ggml_metal_init: found device: Apple M4
0.00.054.719 I ggml_metal_init: picking default device: Apple M4
0.00.055.320 I ggml_metal_init: using embedded metal library
0.00.057.795 I ggml_metal_init: GPU name:   Apple M4
0.00.057.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.799 I ggml_metal_init: simdgroup reduction   = true
0.00.057.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.800 I ggml_metal_init: has bfloat            = true
0.00.057.800 I ggml_metal_init: use bfloat            = true
0.00.057.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.803 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.250 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.258 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.281 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.254 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.255 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.256 I llama_new_context_with_model: graph nodes  = 967
0.00.089.256 I llama_new_context_with_model: graph splits = 2
0.00.089.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.006 I main: llama threadpool init, n_threads = 4
0.00.612.045 I 
0.00.612.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.100 I 
0.00.612.339 I sampler seed: 1234
0.00.612.344 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.397 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.408 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.408 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.372.193 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.372.194 I llama_perf_context_print:        load time =     603.33 ms
0.01.372.194 I llama_perf_context_print: prompt eval time =      47.05 ms /     7 tokens (    6.72 ms per token,   148.77 tokens per second)
0.01.372.195 I llama_perf_context_print:        eval time =     709.76 ms /    63 runs   (   11.27 ms per token,    88.76 tokens per second)
0.01.372.195 I llama_perf_context_print:       total time =     760.19 ms /    70 tokens
0.01.372.413 I ggml_metal_free: deallocating

real	0m1.390s
user	0m0.111s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.252 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.949 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.955 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.962 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.899 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.899 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.900 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.900 I llama_model_loader: - type  f32:  194 tensors
0.00.023.901 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.901 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.901 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.883 I llm_load_vocab: special tokens cache size = 25
0.00.050.827 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.829 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.830 I llm_load_print_meta: arch             = gptneox
0.00.050.830 I llm_load_print_meta: vocab type       = BPE
0.00.050.830 I llm_load_print_meta: n_vocab          = 50304
0.00.050.831 I llm_load_print_meta: n_merges         = 50009
0.00.050.831 I llm_load_print_meta: vocab_only       = 0
0.00.050.831 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.831 I llm_load_print_meta: n_embd           = 2048
0.00.050.831 I llm_load_print_meta: n_layer          = 24
0.00.050.846 I llm_load_print_meta: n_head           = 16
0.00.050.848 I llm_load_print_meta: n_head_kv        = 16
0.00.050.848 I llm_load_print_meta: n_rot            = 32
0.00.050.848 I llm_load_print_meta: n_swa            = 0
0.00.050.848 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.848 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.849 I llm_load_print_meta: n_gqa            = 1
0.00.050.851 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.852 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.853 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.853 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.853 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.853 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.854 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.854 I llm_load_print_meta: n_ff             = 8192
0.00.050.854 I llm_load_print_meta: n_expert         = 0
0.00.050.855 I llm_load_print_meta: n_expert_used    = 0
0.00.050.855 I llm_load_print_meta: causal attn      = 1
0.00.050.855 I llm_load_print_meta: pooling type     = 0
0.00.050.855 I llm_load_print_meta: rope type        = 2
0.00.050.855 I llm_load_print_meta: rope scaling     = linear
0.00.050.856 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.856 I llm_load_print_meta: freq_scale_train = 1
0.00.050.856 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.856 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.858 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.858 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.858 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.858 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.858 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.868 I llm_load_print_meta: model type       = 1.4B
0.00.050.868 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.868 I llm_load_print_meta: model params     = 1.41 B
0.00.050.869 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.869 I llm_load_print_meta: general.name     = 1.4B
0.00.050.869 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.869 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: LF token         = 128 ''
0.00.050.870 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.870 I llm_load_print_meta: max token length = 1024
0.00.052.847 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.847 I llm_load_tensors: offloading output layer to GPU
0.00.052.848 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.858 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.859 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.818 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.819 I llama_new_context_with_model: n_ctx         = 128
0.00.053.819 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.819 I llama_new_context_with_model: n_batch       = 128
0.00.053.820 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.820 I llama_new_context_with_model: flash_attn    = 0
0.00.053.820 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.820 I llama_new_context_with_model: freq_scale    = 1
0.00.053.821 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.821 I ggml_metal_init: allocating
0.00.053.825 I ggml_metal_init: found device: Apple M4
0.00.053.827 I ggml_metal_init: picking default device: Apple M4
0.00.054.396 I ggml_metal_init: using embedded metal library
0.00.056.717 I ggml_metal_init: GPU name:   Apple M4
0.00.056.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.719 I ggml_metal_init: simdgroup reduction   = true
0.00.056.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.719 I ggml_metal_init: has bfloat            = true
0.00.056.720 I ggml_metal_init: use bfloat            = true
0.00.056.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.676 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.678 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.691 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.619 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.620 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.621 I llama_new_context_with_model: graph nodes  = 967
0.00.068.621 I llama_new_context_with_model: graph splits = 2
0.00.068.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.845 I 
0.00.574.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.890 I perplexity: tokenizing the input ..
0.00.582.780 I perplexity: tokenization took 7.889 ms
0.00.582.791 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.258 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.529 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.569 I llama_perf_context_print:        load time =     565.59 ms
0.00.718.570 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.64 tokens per second)
0.00.718.571 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.571 I llama_perf_context_print:       total time =     143.73 ms /   129 tokens
0.00.718.973 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.079s
sys	0m0.110s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.122 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.656 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.666 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.667 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.619 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.588 I llama_model_loader: - type  f32:  194 tensors
0.00.024.588 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.588 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.945 I llm_load_vocab: special tokens cache size = 25
0.00.050.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.907 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.907 I llm_load_print_meta: arch             = gptneox
0.00.050.907 I llm_load_print_meta: vocab type       = BPE
0.00.050.908 I llm_load_print_meta: n_vocab          = 50304
0.00.050.908 I llm_load_print_meta: n_merges         = 50009
0.00.050.908 I llm_load_print_meta: vocab_only       = 0
0.00.050.908 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.908 I llm_load_print_meta: n_embd           = 2048
0.00.050.908 I llm_load_print_meta: n_layer          = 24
0.00.050.923 I llm_load_print_meta: n_head           = 16
0.00.050.924 I llm_load_print_meta: n_head_kv        = 16
0.00.050.924 I llm_load_print_meta: n_rot            = 32
0.00.050.924 I llm_load_print_meta: n_swa            = 0
0.00.050.925 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.925 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.925 I llm_load_print_meta: n_gqa            = 1
0.00.050.926 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.927 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.927 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.928 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.928 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.928 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.928 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.929 I llm_load_print_meta: n_ff             = 8192
0.00.050.929 I llm_load_print_meta: n_expert         = 0
0.00.050.929 I llm_load_print_meta: n_expert_used    = 0
0.00.050.929 I llm_load_print_meta: causal attn      = 1
0.00.050.929 I llm_load_print_meta: pooling type     = 0
0.00.050.929 I llm_load_print_meta: rope type        = 2
0.00.050.930 I llm_load_print_meta: rope scaling     = linear
0.00.050.930 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.930 I llm_load_print_meta: freq_scale_train = 1
0.00.050.930 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.930 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.932 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.932 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.933 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.934 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.943 I llm_load_print_meta: model type       = 1.4B
0.00.050.943 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.944 I llm_load_print_meta: model params     = 1.41 B
0.00.050.944 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.944 I llm_load_print_meta: general.name     = 1.4B
0.00.050.944 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.945 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.945 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.945 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.946 I llm_load_print_meta: LF token         = 128 ''
0.00.050.946 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.947 I llm_load_print_meta: max token length = 1024
0.00.052.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.962 I llm_load_tensors: offloading output layer to GPU
0.00.052.962 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.973 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.974 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.836 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.837 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.837 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.837 I llama_new_context_with_model: n_batch       = 2048
0.00.053.837 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.838 I llama_new_context_with_model: flash_attn    = 0
0.00.053.838 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.838 I llama_new_context_with_model: freq_scale    = 1
0.00.053.839 I ggml_metal_init: allocating
0.00.053.845 I ggml_metal_init: found device: Apple M4
0.00.053.847 I ggml_metal_init: picking default device: Apple M4
0.00.054.431 I ggml_metal_init: using embedded metal library
0.00.056.803 I ggml_metal_init: GPU name:   Apple M4
0.00.056.805 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.806 I ggml_metal_init: simdgroup reduction   = true
0.00.056.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.806 I ggml_metal_init: has bfloat            = true
0.00.056.806 I ggml_metal_init: use bfloat            = true
0.00.056.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.574 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.579 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.597 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.661 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.663 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.663 I llama_new_context_with_model: graph nodes  = 967
0.00.086.663 I llama_new_context_with_model: graph splits = 2
0.00.086.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.906 I main: llama threadpool init, n_threads = 4
0.00.711.949 I 
0.00.711.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.983 I 
0.00.712.224 I sampler seed: 1234
0.00.712.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.240 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.240 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.574.481 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.01.574.482 I llama_perf_context_print:        load time =     702.78 ms
0.01.574.484 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.38 ms per token,   135.59 tokens per second)
0.01.574.485 I llama_perf_context_print:        eval time =     807.69 ms /    63 runs   (   12.82 ms per token,    78.00 tokens per second)
0.01.574.485 I llama_perf_context_print:       total time =     862.58 ms /    70 tokens
0.01.574.689 I ggml_metal_free: deallocating

real	0m1.592s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.527 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.262 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.238 I llama_model_loader: - type  f32:  194 tensors
0.00.023.238 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.238 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.063 I llm_load_vocab: special tokens cache size = 25
0.00.050.244 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.247 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.247 I llm_load_print_meta: arch             = gptneox
0.00.050.248 I llm_load_print_meta: vocab type       = BPE
0.00.050.248 I llm_load_print_meta: n_vocab          = 50304
0.00.050.248 I llm_load_print_meta: n_merges         = 50009
0.00.050.248 I llm_load_print_meta: vocab_only       = 0
0.00.050.248 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.249 I llm_load_print_meta: n_embd           = 2048
0.00.050.249 I llm_load_print_meta: n_layer          = 24
0.00.050.263 I llm_load_print_meta: n_head           = 16
0.00.050.264 I llm_load_print_meta: n_head_kv        = 16
0.00.050.264 I llm_load_print_meta: n_rot            = 32
0.00.050.264 I llm_load_print_meta: n_swa            = 0
0.00.050.264 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.264 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.265 I llm_load_print_meta: n_gqa            = 1
0.00.050.266 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.268 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.269 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.269 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.269 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.270 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.270 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.270 I llm_load_print_meta: n_ff             = 8192
0.00.050.270 I llm_load_print_meta: n_expert         = 0
0.00.050.272 I llm_load_print_meta: n_expert_used    = 0
0.00.050.272 I llm_load_print_meta: causal attn      = 1
0.00.050.272 I llm_load_print_meta: pooling type     = 0
0.00.050.272 I llm_load_print_meta: rope type        = 2
0.00.050.272 I llm_load_print_meta: rope scaling     = linear
0.00.050.272 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.274 I llm_load_print_meta: freq_scale_train = 1
0.00.050.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.275 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.275 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.275 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.275 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.275 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.285 I llm_load_print_meta: model type       = 1.4B
0.00.050.285 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.285 I llm_load_print_meta: model params     = 1.41 B
0.00.050.286 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.286 I llm_load_print_meta: general.name     = 1.4B
0.00.050.286 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.286 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.287 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.287 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.287 I llm_load_print_meta: LF token         = 128 ''
0.00.050.287 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.287 I llm_load_print_meta: max token length = 1024
0.00.052.353 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.354 I llm_load_tensors: offloading output layer to GPU
0.00.052.354 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.364 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.366 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.255 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.256 I llama_new_context_with_model: n_ctx         = 128
0.00.053.256 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.256 I llama_new_context_with_model: n_batch       = 128
0.00.053.257 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.257 I llama_new_context_with_model: flash_attn    = 0
0.00.053.257 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.258 I llama_new_context_with_model: freq_scale    = 1
0.00.053.258 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.258 I ggml_metal_init: allocating
0.00.053.262 I ggml_metal_init: found device: Apple M4
0.00.053.264 I ggml_metal_init: picking default device: Apple M4
0.00.053.833 I ggml_metal_init: using embedded metal library
0.00.056.156 I ggml_metal_init: GPU name:   Apple M4
0.00.056.157 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.158 I ggml_metal_init: simdgroup reduction   = true
0.00.056.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.159 I ggml_metal_init: has bfloat            = true
0.00.056.159 I ggml_metal_init: use bfloat            = true
0.00.056.159 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.160 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.307 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.325 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.244 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.245 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.246 I llama_new_context_with_model: graph nodes  = 967
0.00.068.246 I llama_new_context_with_model: graph splits = 2
0.00.068.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.048 I 
0.00.636.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.093 I perplexity: tokenizing the input ..
0.00.643.329 I perplexity: tokenization took 7.235 ms
0.00.643.342 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.100 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.348 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.364 I llama_perf_context_print:        load time =     627.35 ms
0.00.785.365 I llama_perf_context_print: prompt eval time =     140.54 ms /   128 tokens (    1.10 ms per token,   910.79 tokens per second)
0.00.785.366 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.367 I llama_perf_context_print:       total time =     149.32 ms /   129 tokens
0.00.785.778 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.078s
sys	0m0.109s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.127 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.823 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.834 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.921 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.865 I llama_model_loader: - type  f32:  194 tensors
0.00.024.866 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.259 I llm_load_vocab: special tokens cache size = 25
0.00.051.272 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.275 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.275 I llm_load_print_meta: arch             = gptneox
0.00.051.275 I llm_load_print_meta: vocab type       = BPE
0.00.051.276 I llm_load_print_meta: n_vocab          = 50304
0.00.051.276 I llm_load_print_meta: n_merges         = 50009
0.00.051.276 I llm_load_print_meta: vocab_only       = 0
0.00.051.276 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.276 I llm_load_print_meta: n_embd           = 2048
0.00.051.276 I llm_load_print_meta: n_layer          = 24
0.00.051.291 I llm_load_print_meta: n_head           = 16
0.00.051.293 I llm_load_print_meta: n_head_kv        = 16
0.00.051.293 I llm_load_print_meta: n_rot            = 32
0.00.051.293 I llm_load_print_meta: n_swa            = 0
0.00.051.294 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.294 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.294 I llm_load_print_meta: n_gqa            = 1
0.00.051.295 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.296 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.296 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.297 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.297 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.297 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.297 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.298 I llm_load_print_meta: n_ff             = 8192
0.00.051.298 I llm_load_print_meta: n_expert         = 0
0.00.051.298 I llm_load_print_meta: n_expert_used    = 0
0.00.051.298 I llm_load_print_meta: causal attn      = 1
0.00.051.298 I llm_load_print_meta: pooling type     = 0
0.00.051.299 I llm_load_print_meta: rope type        = 2
0.00.051.299 I llm_load_print_meta: rope scaling     = linear
0.00.051.299 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.299 I llm_load_print_meta: freq_scale_train = 1
0.00.051.299 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.300 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.300 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.300 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.300 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.302 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.302 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.311 I llm_load_print_meta: model type       = 1.4B
0.00.051.311 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.311 I llm_load_print_meta: model params     = 1.41 B
0.00.051.312 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.312 I llm_load_print_meta: general.name     = 1.4B
0.00.051.312 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.314 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.314 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.314 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.314 I llm_load_print_meta: LF token         = 128 ''
0.00.051.315 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.315 I llm_load_print_meta: max token length = 1024
0.00.052.934 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.934 I llm_load_tensors: offloading output layer to GPU
0.00.052.934 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.944 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.945 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.801 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.802 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.802 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.802 I llama_new_context_with_model: n_batch       = 2048
0.00.053.802 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.802 I llama_new_context_with_model: flash_attn    = 0
0.00.053.803 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.803 I llama_new_context_with_model: freq_scale    = 1
0.00.053.803 I ggml_metal_init: allocating
0.00.053.807 I ggml_metal_init: found device: Apple M4
0.00.053.809 I ggml_metal_init: picking default device: Apple M4
0.00.054.397 I ggml_metal_init: using embedded metal library
0.00.056.722 I ggml_metal_init: GPU name:   Apple M4
0.00.056.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.724 I ggml_metal_init: simdgroup reduction   = true
0.00.056.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.724 I ggml_metal_init: has bfloat            = true
0.00.056.725 I ggml_metal_init: use bfloat            = true
0.00.056.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.282 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.288 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.309 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.309 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.310 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.311 I llama_new_context_with_model: graph nodes  = 967
0.00.086.311 I llama_new_context_with_model: graph splits = 2
0.00.086.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.220 I main: llama threadpool init, n_threads = 4
0.00.764.264 I 
0.00.764.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.321 I 
0.00.764.492 I sampler seed: 1234
0.00.764.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.522 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.523 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.687.784 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.687.785 I llama_perf_context_print:        load time =     755.09 ms
0.01.687.786 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.50 tokens per second)
0.01.687.787 I llama_perf_context_print:        eval time =     865.79 ms /    63 runs   (   13.74 ms per token,    72.77 tokens per second)
0.01.687.787 I llama_perf_context_print:       total time =     923.57 ms /    70 tokens
0.01.687.992 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4294 (c37fb4cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.527 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.275 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.201 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.194 I llama_model_loader: - type  f32:  194 tensors
0.00.025.195 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.440 I llm_load_vocab: special tokens cache size = 25
0.00.051.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.394 I llm_load_print_meta: arch             = gptneox
0.00.051.395 I llm_load_print_meta: vocab type       = BPE
0.00.051.395 I llm_load_print_meta: n_vocab          = 50304
0.00.051.395 I llm_load_print_meta: n_merges         = 50009
0.00.051.395 I llm_load_print_meta: vocab_only       = 0
0.00.051.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.395 I llm_load_print_meta: n_embd           = 2048
0.00.051.396 I llm_load_print_meta: n_layer          = 24
0.00.051.410 I llm_load_print_meta: n_head           = 16
0.00.051.411 I llm_load_print_meta: n_head_kv        = 16
0.00.051.411 I llm_load_print_meta: n_rot            = 32
0.00.051.411 I llm_load_print_meta: n_swa            = 0
0.00.051.411 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.413 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.413 I llm_load_print_meta: n_gqa            = 1
0.00.051.414 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.415 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.416 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.416 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.416 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.417 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.418 I llm_load_print_meta: n_ff             = 8192
0.00.051.418 I llm_load_print_meta: n_expert         = 0
0.00.051.418 I llm_load_print_meta: n_expert_used    = 0
0.00.051.418 I llm_load_print_meta: causal attn      = 1
0.00.051.419 I llm_load_print_meta: pooling type     = 0
0.00.051.419 I llm_load_print_meta: rope type        = 2
0.00.051.419 I llm_load_print_meta: rope scaling     = linear
0.00.051.419 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.420 I llm_load_print_meta: freq_scale_train = 1
0.00.051.420 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.420 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.420 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.420 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.420 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.420 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.421 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.430 I llm_load_print_meta: model type       = 1.4B
0.00.051.430 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.431 I llm_load_print_meta: model params     = 1.41 B
0.00.051.431 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.431 I llm_load_print_meta: general.name     = 1.4B
0.00.051.431 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.432 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.432 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.432 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.432 I llm_load_print_meta: LF token         = 128 ''
0.00.051.433 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.434 I llm_load_print_meta: max token length = 1024
0.00.053.482 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.482 I llm_load_tensors: offloading output layer to GPU
0.00.053.482 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.493 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.494 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.414 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.415 I llama_new_context_with_model: n_ctx         = 128
0.00.054.415 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.415 I llama_new_context_with_model: n_batch       = 128
0.00.054.415 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.416 I llama_new_context_with_model: flash_attn    = 0
0.00.054.416 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.416 I llama_new_context_with_model: freq_scale    = 1
0.00.054.417 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.417 I ggml_metal_init: allocating
0.00.054.423 I ggml_metal_init: found device: Apple M4
0.00.054.425 I ggml_metal_init: picking default device: Apple M4
0.00.054.994 I ggml_metal_init: using embedded metal library
0.00.057.330 I ggml_metal_init: GPU name:   Apple M4
0.00.057.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.333 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.333 I ggml_metal_init: simdgroup reduction   = true
0.00.057.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.333 I ggml_metal_init: has bfloat            = true
0.00.057.333 I ggml_metal_init: use bfloat            = true
0.00.057.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.083 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.098 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.001 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.002 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.002 I llama_new_context_with_model: graph nodes  = 967
0.00.069.003 I llama_new_context_with_model: graph splits = 2
0.00.069.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.259.280 I 
0.00.259.343 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.259.360 I perplexity: tokenizing the input ..
0.00.266.899 I perplexity: tokenization took 7.537 ms
0.00.266.910 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.405.827 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.407.171 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.407.180 I llama_perf_context_print:        load time =     248.74 ms
0.00.407.182 I llama_perf_context_print: prompt eval time =     138.68 ms /   128 tokens (    1.08 ms per token,   922.97 tokens per second)
0.00.407.182 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.407.183 I llama_perf_context_print:       total time =     147.91 ms /   129 tokens
0.00.407.580 I ggml_metal_free: deallocating

real	0m0.423s
user	0m0.078s
sys	0m0.055s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4294 (c37fb4cf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e20a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e20a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e20af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e20b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e20baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e20c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e20c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e20cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e20d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e20d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e20db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e20e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e20eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e20f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e20fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e210260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e210980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e2110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e2117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e211f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e2126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e212dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e2134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e213d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e2144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e214770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e214d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e2159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e215f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e2161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e216690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e216950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e2171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e217720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e2179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e217e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e218320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e2187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e218c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e219100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e2195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e219a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e219ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e21a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e21a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e21ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e21b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e21bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e21c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e21c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e21cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e21d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e21d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e21dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e21e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e21ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e21f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e21f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e21f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e2201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e220490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e220930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e220dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e221270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e221710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e221bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e222050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e2224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e222990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e222e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e2232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e223770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e223c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e224160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e2246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e224c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e225150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e2256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e225bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e226140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e226690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e226be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e227130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e227680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e227bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e228120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e228670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e228bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e229110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e229660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e229bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e22a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e22a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e22aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e22b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e22b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e22bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e21b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e22c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e22c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e22cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e22d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e22d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e22dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e22e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e22e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e22ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e22f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e22f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e22fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e230220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e230770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e230cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e231160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e231600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e231aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e231f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e2323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e232880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e232d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e2331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e233660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e233b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e233fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e234440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e2348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e234d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e235220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e2356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e235b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e236000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e2364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e236940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e236de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e237280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e237720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e237bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e238060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e238500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e2389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e238e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e2392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e239780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e239c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e23a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e23a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e23aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e23aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e23b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e23b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e23bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e23c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e23c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e23ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e23cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e23d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e23d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e23dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e23e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e23e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e23eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e23ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e23f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e23f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e23fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e2401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e240680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e240b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e240fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e241460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e241900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e241da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e242240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e2426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e242b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e243020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e2434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e243960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e243e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e2442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e244740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e244be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e245080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e245520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e2459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e245e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e246300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e2467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e246c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e2470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e247580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e247a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e247ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e248410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e248960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e248eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e249400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e2496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e249cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e24a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e24a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e24b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e24b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e24b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e24be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e24c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e24cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e24d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e24d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e24da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e24e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e24e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e24ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e24f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e24f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e24fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e2501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e250710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e250c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e2511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e251700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e251c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e2521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e2526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e252c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e253190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e2536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e253c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e254180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e2546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e254c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e255170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e2556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e255c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e256160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e2566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e256c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e257150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e2576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e257bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e258140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e258690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e258be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e259130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e259680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e259bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e25a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e25a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e25abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e25b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e25b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e25bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e25c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e25c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e25cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e25d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e25d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e25db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e25e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e25e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e25eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e25f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e25f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e25fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e2600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e260610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e260b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e261000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e2614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e261940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e261de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e262280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e262720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e262bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e263060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e263500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e2639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e263e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e2642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e264780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e264c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e2650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e265610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e265d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e266450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e266b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e267290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e267550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e267d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e268000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e268610 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.148.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e2253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e225840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e225cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e226120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e226590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e226a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e226e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e2272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e227750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e227bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e228030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e228610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e228f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e229680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e229e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e22a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e22ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e22b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e22ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e22c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e22ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e22d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e22d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e22df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e22e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e22eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e22ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e22f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e22f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e22fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e2300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e230560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e2309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e230c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e231100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e231570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e2319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e231e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e2322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e232730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e232ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e233010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e233480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e2338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e233d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e2341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e234640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e234ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e234f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e235390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e235800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e235c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e2360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e236550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e2369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e236e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e2372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e237710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e237b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e237ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e238460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e2388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e238d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e2391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e239620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e239a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e239f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e23a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e23a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e23ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e23b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e23b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e23b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e23be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e23c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e23c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e23cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e23cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e23d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e23d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e23dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e23e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e23e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e23ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e23eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e23f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e23f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e23fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e2400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e240510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e240980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e240df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e241260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e2416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e241b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e241fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e242420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e242890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e242d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e243170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e2435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e243a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e243ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e244330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e2447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e244c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e245080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e2454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e245960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e245dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e246240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e2466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e246b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e246f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e247400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e247870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e247ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e248150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e2485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e248a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e248ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e249310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e249780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e249bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e24a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e24a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e24a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e24adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e24b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e24b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e24bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e24bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e24c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e24c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e24ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e24d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e24d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e24da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e24de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e24e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e24e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e24ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e24f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e24f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e24f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e24fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e250200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e250670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e250ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e250f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e2513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e251830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e251ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e252110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e252580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e2529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e252e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e2532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e253740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e253bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e254020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e254490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e254900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e254d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e2551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e255650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e255ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e255f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e2563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e256810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e256c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e2570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e257560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e2579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e257e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e2582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e258720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e258b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e259000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e259470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e2598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e259d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e25a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e25a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e25aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e25af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e25b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e25b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e25bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e25c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e25c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e25c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e25ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e25d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e25d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e25db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e25dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e25e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e25e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e25ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e25f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e25f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e25fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e25fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e260360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e2607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e260c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e2610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e261520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e261990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e262110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e262580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e2629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e262e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e2632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e263740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e263bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e264020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e264490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e264900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e264d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e2651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e265650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e265ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e265f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e2663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e266810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e266c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e2670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e267560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e2679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e267e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e2682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e268720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e20af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e20b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e20baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e20bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e20c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e20c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e20cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e20d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e20a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e217710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e2179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e217e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e2182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e218720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e218b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e219000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e219470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e2198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e219d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e21a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e21a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e21aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e21af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e21b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e21b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e21bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e21c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e21c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e21c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e21ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e21d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e21d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e21db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e21dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e21e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e21e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e21ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e21f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e21f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e21fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e21fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e220360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e2207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e220c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e2210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e221520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e221990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e221e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e222270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e222960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e223050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e223740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e223e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e2242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e215f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e2161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e216650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128a044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128a04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128a04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128a05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128a056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128a05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128a05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128a063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128a06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128a06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128a07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128a07870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128a08390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128a08b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128a09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128a09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128a0a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128a0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128a0afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128a0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128a0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128a0c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128a0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128a0d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128a0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128a0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128a0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128a0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128a0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128a0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128a0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128a0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128a0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128a0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128a102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128a10720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128a10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128a11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128a11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128a118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128a11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128a121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128a12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128a12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128a12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128a13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128a137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128a13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128a140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128a14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128a149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128a14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128a15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128a15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128a15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128a15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128a16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128a16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128a16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128a17330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128a177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128a17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128a18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128a184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128a18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128a18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128a19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128a196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128a19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128a19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128a1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128a1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128a1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128a1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128a1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128a1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128a1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128a1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128a1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128a1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128a1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128a1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128a1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128a1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128a1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128a1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128a1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128a1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128a1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128a1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128a1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128a20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128a205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128a20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128a20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128a212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128a21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128a21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128a22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128a224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128a22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128a22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128a23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128a23670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128a23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128a23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128a243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128a24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128a24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128a25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128a25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128a259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128a25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128a262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128a26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128a26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128a27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128a27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128a27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128a27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128a281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128a28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128a28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128a28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128a293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128a29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128a29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128a2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128a2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128a2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128a2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128a2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128a2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128a2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128a2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128a2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128a2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128a2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128a2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128a2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128a2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128a2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128a2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128a2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128a2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128a2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128a2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128a2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128a2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128a30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128a30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128a30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128a30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128a31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128a318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128a31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128a321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128a32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128a32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128a32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128a33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128a337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128a33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128a340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128a34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128a34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128a34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128a35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128a356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128a35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128a35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128a36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128a368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128a36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128a37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128a375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128a37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128a37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128a38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128a387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128a38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128a39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128a39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128a39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128a39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128a3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128a3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128a3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128a3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128a3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128a3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128a3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128a3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128a3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128a3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128a3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128a3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128a3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128a3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128a3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128a3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128a3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128a3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128a3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128a3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128a3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128a3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128a40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128a40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128a40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128a41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128a41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128a41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128a42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128a427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128a42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128a43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128a434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128a43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128a43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128a44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128a446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128a44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128a44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128a45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128a45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128a45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128a46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128a465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128a46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128a47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128a47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128a47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128a48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128a484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128a48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128a48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128a49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128a49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128a49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128a49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128a4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128a4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128a4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128a4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128a4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128a4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128a4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128a4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128a4cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128a4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128a4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128a4d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128a4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128a4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128a4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128a4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128a4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128a4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128a4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128a4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128a4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128a50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128a508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128a50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128a511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128a51620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128a51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128a51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128a52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128a527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128a52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128a530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128a53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128a539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128a53e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128a54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128a546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128a54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128a54fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128a55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128a558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128a56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128a56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128a57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128a57880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128a57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128a57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128a585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128a58bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.821s
user	0m0.293s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4294 (c37fb4cf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c7102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c7109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c711530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c711ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c712090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c7131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c7136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c713ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c7140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c714bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c715370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c715b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c7162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c7169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c7170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c7186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c718e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c71a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c71a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c71adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c71ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c71c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c71c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c71d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c71d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c71da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c71dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c71e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c71e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c71eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c71f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c71f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c71fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c71ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c7203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c720c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c7212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c7221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c7227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c723400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c723a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c725150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c725a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c726210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c7264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c726970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c7272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c727750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c727bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c728090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c728530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c7289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c728e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c7297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c72a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c72a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c72ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c72b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c72b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c72bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c72c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c72c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c72cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c72d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c72d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c72dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c72e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c72e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c72ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c72f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c72f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c72fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c730140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c730690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c730be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c731130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c731680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c731bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c7218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c732040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c7327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c732d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c733290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c7337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c733d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c734280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c7347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c7357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c735d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c736260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c7367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c7371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c737f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c7388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c738d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c739200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c7396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c739b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c73a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c73a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c73adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c73b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c73b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c73bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c73c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c73c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c73c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c73ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c73d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c73d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c73dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c73e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c73e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c73e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c73ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c73f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c73f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c73fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c7405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c740ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c742160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c742600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c742aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c7433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c743d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c7441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c744fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c745440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c7458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c745d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c7466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c746b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c747000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c7474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c747de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c748280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c748bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c749060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c749500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c7499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c749e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c74a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c74a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c74ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c74b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c74b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c74ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c74bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c74c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c74c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c74cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c74d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c74d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c74da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c74df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c74e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c74e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c74eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c74f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c74f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c74fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c750930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c751120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c7515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c751e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c7524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c753130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c7535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c754220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c754770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c754cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c755210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c755760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c755cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c756200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c756750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c756ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c7571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c757740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c757c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c7581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c758730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c758c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c7591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c759720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c759c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c75a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c75a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c75ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c75b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c75b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c75bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c75c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c75c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c75cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c75d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c75d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c75dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c75e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c75e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c75ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c75f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c75f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c75fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c760160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c7606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c760c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c761150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c7616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c761bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c762140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c762690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c762be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c763130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c763680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c763bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c764120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c764bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c765110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c765bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c766100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c766650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c766ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c767040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c767980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c767e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c7682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c768760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c768c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c7690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c769540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c7699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c769e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c76a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c76a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c76ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c76b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c76b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c76bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c76c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c76cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c76d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c76d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c76dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c76e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c76e650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e004b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e005000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e005470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e0058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e005d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e0061c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e006630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e006aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e006f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e007380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e0077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e007ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e008a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e0091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e0099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e00a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e00a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e00af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e00b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e00bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e00c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e00cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e00d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e00d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e00e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e00e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e00e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e00eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e00ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e00f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e00f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e00fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e0101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e0104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e010920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e010d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e011200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e011670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e011ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e011f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e0123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e012830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e012ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e013110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e013580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e0139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e013e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e0142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e014740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e014bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e015020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e015490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e015900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e015d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e0161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e016650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e016bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e0170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e017530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e0179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e017e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e018280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e0186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e018b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e018fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e019440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e0198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e019d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e01a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e01a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e01aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e01aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e01b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e01b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e01bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e01c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e01c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e01c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e01cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e01d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e01d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e01db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e01dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e01e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e01e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e01ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e01f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e01f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e01fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e01fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e020330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e0207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e020c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e021080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e0214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e021960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e021dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e022240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e0226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e022b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e022f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e023400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e023870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e023ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e024150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e0245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e024a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e024ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e025310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e025780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e025bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e026060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e0264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e026940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e026db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e027220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e027690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e027b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e027f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e0283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e028850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e028cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e029130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e0295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e029a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e029e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e02a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e02a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e02abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e02b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e02b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e02b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e02bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e02c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e02c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e02cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e02cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e02d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e02d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e02dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e02e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e02e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e02e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e02ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e02f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e02f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e02fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e030020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e030490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e030900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e030d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e0311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e031650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e031ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e031f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e0323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e032810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e032c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e0330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e033560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e0339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e033e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e0342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e034720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e034b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e035000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e035470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e0358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e035d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e0361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e036630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e036aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e036f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e037380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e0377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e037c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e0380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e038540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e0389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e038e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e039290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e039700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e039b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e039fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e03a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e03a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e03ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e03b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e03b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e03ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e03bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e03c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e03c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e03cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e03d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e03d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e03d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e03de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e03e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e03e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e03eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e03efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e03f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e03f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e03fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e040180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e0405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e040b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e040ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e041460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e041fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e042270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e042530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e0429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e042e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e043280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e0436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e043b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e043fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e044440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e0448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e044d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e045190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e045600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e045a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e045ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e046350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e0467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e046c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e0470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e047510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e047980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e047df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e048260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e0486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e048b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e048fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e049420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e049890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e049d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e04a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e04a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e04aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e04aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e04b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e04b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e04bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e04c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e04c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e04c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e04cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e04d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e04d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e04db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e04df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e04e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e04e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e04ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e04f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e04f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e04fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e04fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e050780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e050bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e051060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e0514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e051940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e051db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e052220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e052690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e052b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e052f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e0533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e053850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e053cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e054130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e0545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e054a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e054e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e0552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e055760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e055bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e056640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e056d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e057480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e057ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e057e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e0582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e0588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e058ee0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a6046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a604b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a604fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a605430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a6058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a605d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a606180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a6065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a606a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a606ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a607340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a607a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a608580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a608d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a609540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a609c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a60a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a60aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a60b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a60b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a60c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a60c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a60ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a60d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a60dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a60df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a60e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a60e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a60eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a60ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a60f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a60f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a60fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a610030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a6104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a610d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a6111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a611660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a611ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a611f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a6123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a612820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a612c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a613100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a613570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a6139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a6142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a614730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a614ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a615010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a615480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a6158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a615d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a6161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a616c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a6170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a617520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a617990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a617e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a618270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a6186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a618b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a619430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a6198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a619d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a61a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a61a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a61aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a61aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a61b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a61b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a61bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a61c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a61c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a61c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a61cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a61d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a61d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a61db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a61dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a61e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a61e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a61ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a61f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a61f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a61fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a61feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a620320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a620790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a620c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a621070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a6214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a621950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a622230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a6226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a622b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a622f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a6233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a623860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a623cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a624140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a6245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a624a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a624e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a625300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a625770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a625be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a626050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a6264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a626930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a626da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a627210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a627af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a6283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a628840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a628cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a629120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a629590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a629a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a629e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a62a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a62a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a62abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a62b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a62b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a62b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a62bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a62c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a62c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a62cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a62cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a62d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a62d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a62dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a62e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a62e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a62e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a62ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a62f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a62f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a62fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a630010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a630480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a6308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a630d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a6311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a631640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a631ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a631f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a632800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a632c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a6330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a633550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a6339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a633e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a6342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a634710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a634b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a634ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a635460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a6358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a635d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a6361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a636620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a636f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a637370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a6377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a6380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a638530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a6389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a638e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a639280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a6396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a639b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a639fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a63a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a63a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a63ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a63b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a63b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a63ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a63bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a63c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a63c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a63cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a63d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a63d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a63d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a63ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a63e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a63e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a63eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a63efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a63f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a63f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a63fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a640700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a640b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a641b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a6420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a642520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a642990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a643270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a6436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a643b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a643fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a644430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a6448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a644d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a645180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a6455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a645a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a645ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a646340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a6467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a646c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a647500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a647970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a647de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a648250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a6486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a648fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a649880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a649cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a64a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a64a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a64aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a64b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a64b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a64bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a64bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a64c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a64c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a64cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a64d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a64d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a64d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a64de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a64e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a64e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a64ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a64f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a64f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a64f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a64fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a6501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a650650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a650ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a650f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a6513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a651810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a651c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a6520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a652560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a6529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a652e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a6532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a653720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a653b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a654000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a654470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a6548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a6551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a655630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a655aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a656510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a656c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a657350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a657a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a657d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a6581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a6587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a658db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.245s
sys	0m0.147s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.24 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.15 user         0.04 sys
```
