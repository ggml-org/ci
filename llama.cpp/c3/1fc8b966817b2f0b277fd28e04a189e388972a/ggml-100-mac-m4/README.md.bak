### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.01 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.51 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  176.11 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.69 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 218.50 sec*proc (28 tests)

Total Test time (real) = 218.51 sec

real	3m38.514s
user	7m27.525s
sys	0m6.677s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.30 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.38 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.35 sec*proc (28 tests)

Total Test time (real) =  51.36 sec

real	0m51.369s
user	1m11.745s
sys	0m5.587s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.168 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.391 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.655 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.665 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.666 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.667 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.668 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.669 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.670 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.671 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.671 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.672 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.675 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.676 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.677 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.677 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.678 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.678 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.679 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.100 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.102 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.103 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.104 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.104 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.105 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.105 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.106 I llama_model_loader: - type  f32:  124 tensors
0.00.030.106 I llama_model_loader: - type  f16:   73 tensors
0.00.035.004 I llm_load_vocab: special tokens cache size = 5
0.00.037.361 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.365 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.365 I llm_load_print_meta: arch             = bert
0.00.037.366 I llm_load_print_meta: vocab type       = WPM
0.00.037.366 I llm_load_print_meta: n_vocab          = 30522
0.00.037.366 I llm_load_print_meta: n_merges         = 0
0.00.037.366 I llm_load_print_meta: vocab_only       = 0
0.00.037.367 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.367 I llm_load_print_meta: n_embd           = 384
0.00.037.367 I llm_load_print_meta: n_layer          = 12
0.00.037.371 I llm_load_print_meta: n_head           = 12
0.00.037.372 I llm_load_print_meta: n_head_kv        = 12
0.00.037.372 I llm_load_print_meta: n_rot            = 32
0.00.037.372 I llm_load_print_meta: n_swa            = 0
0.00.037.372 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.372 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.373 I llm_load_print_meta: n_gqa            = 1
0.00.037.374 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.377 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.378 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.379 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.379 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.380 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.380 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.381 I llm_load_print_meta: n_ff             = 1536
0.00.037.383 I llm_load_print_meta: n_expert         = 0
0.00.037.383 I llm_load_print_meta: n_expert_used    = 0
0.00.037.384 I llm_load_print_meta: causal attn      = 0
0.00.037.384 I llm_load_print_meta: pooling type     = 2
0.00.037.384 I llm_load_print_meta: rope type        = 2
0.00.037.384 I llm_load_print_meta: rope scaling     = linear
0.00.037.385 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.385 I llm_load_print_meta: freq_scale_train = 1
0.00.037.386 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.386 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.386 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.386 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.387 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.388 I llm_load_print_meta: model type       = 33M
0.00.037.395 I llm_load_print_meta: model ftype      = F16
0.00.037.396 I llm_load_print_meta: model params     = 33.21 M
0.00.037.397 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.397 I llm_load_print_meta: general.name     = Bge Small
0.00.037.398 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.399 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.400 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.400 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.400 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.401 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.401 I llm_load_print_meta: max token length = 21
0.00.039.371 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.372 I llm_load_tensors: offloading output layer to GPU
0.00.039.372 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.399 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.400 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.989 I llama_new_context_with_model: n_seq_max     = 1
0.00.039.991 I llama_new_context_with_model: n_ctx         = 512
0.00.039.991 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.039.991 I llama_new_context_with_model: n_batch       = 2048
0.00.039.992 I llama_new_context_with_model: n_ubatch      = 2048
0.00.039.992 I llama_new_context_with_model: flash_attn    = 0
0.00.039.993 I llama_new_context_with_model: freq_base     = 10000.0
0.00.039.993 I llama_new_context_with_model: freq_scale    = 1
0.00.039.994 I ggml_metal_init: allocating
0.00.040.004 I ggml_metal_init: found device: Apple M4
0.00.040.008 I ggml_metal_init: picking default device: Apple M4
0.00.040.890 I ggml_metal_init: using embedded metal library
0.00.045.357 I ggml_metal_init: GPU name:   Apple M4
0.00.045.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.361 I ggml_metal_init: simdgroup reduction   = true
0.00.045.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.362 I ggml_metal_init: has bfloat            = true
0.00.045.362 I ggml_metal_init: use bfloat            = true
0.00.045.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.905 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.607 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.610 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.611 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.497 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.498 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.499 I llama_new_context_with_model: graph nodes  = 429
0.00.059.499 I llama_new_context_with_model: graph splits = 2
0.00.059.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.212 I 
0.00.066.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.908 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.760 I llama_perf_context_print:        load time =      46.81 ms
0.00.071.761 I llama_perf_context_print: prompt eval time =       4.71 ms /     9 tokens (    0.52 ms per token,  1911.23 tokens per second)
0.00.071.762 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.762 I llama_perf_context_print:       total time =       5.55 ms /    10 tokens
0.00.071.942 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.051s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.042 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.085 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.389 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.393 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.395 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.397 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.398 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.398 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.399 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.399 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.400 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.400 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.400 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.403 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.403 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.404 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.404 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.404 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.406 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.406 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.776 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.777 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.778 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.778 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.778 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.779 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.779 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.780 I llama_model_loader: - type  f32:  124 tensors
0.00.015.780 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.456 I llm_load_vocab: special tokens cache size = 5
0.00.019.882 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.885 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.885 I llm_load_print_meta: arch             = bert
0.00.019.885 I llm_load_print_meta: vocab type       = WPM
0.00.019.886 I llm_load_print_meta: n_vocab          = 30522
0.00.019.886 I llm_load_print_meta: n_merges         = 0
0.00.019.886 I llm_load_print_meta: vocab_only       = 0
0.00.019.886 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.886 I llm_load_print_meta: n_embd           = 384
0.00.019.886 I llm_load_print_meta: n_layer          = 12
0.00.019.890 I llm_load_print_meta: n_head           = 12
0.00.019.891 I llm_load_print_meta: n_head_kv        = 12
0.00.019.891 I llm_load_print_meta: n_rot            = 32
0.00.019.891 I llm_load_print_meta: n_swa            = 0
0.00.019.892 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.892 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.892 I llm_load_print_meta: n_gqa            = 1
0.00.019.894 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.895 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.895 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.898 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.898 I llm_load_print_meta: n_ff             = 1536
0.00.019.898 I llm_load_print_meta: n_expert         = 0
0.00.019.898 I llm_load_print_meta: n_expert_used    = 0
0.00.019.899 I llm_load_print_meta: causal attn      = 0
0.00.019.899 I llm_load_print_meta: pooling type     = 2
0.00.019.899 I llm_load_print_meta: rope type        = 2
0.00.019.899 I llm_load_print_meta: rope scaling     = linear
0.00.019.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.900 I llm_load_print_meta: freq_scale_train = 1
0.00.019.900 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.900 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.901 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.901 I llm_load_print_meta: model type       = 33M
0.00.019.901 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.902 I llm_load_print_meta: model params     = 33.21 M
0.00.019.906 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.906 I llm_load_print_meta: general.name     = Bge Small
0.00.019.906 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.906 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.906 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.906 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.907 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.907 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.907 I llm_load_print_meta: max token length = 21
0.00.021.219 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.021.219 I llm_load_tensors: offloading output layer to GPU
0.00.021.219 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.021.227 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.228 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.583 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.584 I llama_new_context_with_model: n_ctx         = 512
0.00.021.584 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.584 I llama_new_context_with_model: n_batch       = 2048
0.00.021.584 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.585 I llama_new_context_with_model: flash_attn    = 0
0.00.021.585 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.585 I llama_new_context_with_model: freq_scale    = 1
0.00.021.586 I ggml_metal_init: allocating
0.00.021.588 I ggml_metal_init: found device: Apple M4
0.00.021.592 I ggml_metal_init: picking default device: Apple M4
0.00.022.235 I ggml_metal_init: using embedded metal library
0.00.024.856 I ggml_metal_init: GPU name:   Apple M4
0.00.024.858 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.858 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.859 I ggml_metal_init: simdgroup reduction   = true
0.00.024.859 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.859 I ggml_metal_init: has bfloat            = true
0.00.024.860 I ggml_metal_init: use bfloat            = true
0.00.024.860 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.861 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.267 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.778 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.780 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.782 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.036.398 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.036.399 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.036.400 I llama_new_context_with_model: graph nodes  = 429
0.00.036.400 I llama_new_context_with_model: graph splits = 2
0.00.036.402 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.179 I 
0.00.041.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.755 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.989 I llama_perf_context_print:        load time =      31.09 ms
0.00.044.990 I llama_perf_context_print: prompt eval time =       3.10 ms /     9 tokens (    0.34 ms per token,  2903.23 tokens per second)
0.00.044.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.991 I llama_perf_context_print:       total time =       3.81 ms /    10 tokens
0.00.045.157 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.032s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.191 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.964 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.463 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.471 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.481 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.482 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.483 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.487 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.488 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.488 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.494 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.495 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.499 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.500 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.501 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.320 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.178 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.178 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.179 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.179 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.179 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.180 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.180 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.181 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.181 I llama_model_loader: - type  f32:   40 tensors
0.00.051.183 I llama_model_loader: - type  f16:   30 tensors
0.00.069.675 W llm_load_vocab: empty token at index 5
0.00.074.245 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.514 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.546 I llm_load_vocab: special tokens cache size = 5
0.00.328.179 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.328.186 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.328.186 I llm_load_print_meta: arch             = jina-bert-v2
0.00.328.193 I llm_load_print_meta: vocab type       = BPE
0.00.328.193 I llm_load_print_meta: n_vocab          = 61056
0.00.328.193 I llm_load_print_meta: n_merges         = 39382
0.00.328.194 I llm_load_print_meta: vocab_only       = 0
0.00.328.195 I llm_load_print_meta: n_ctx_train      = 8192
0.00.328.195 I llm_load_print_meta: n_embd           = 384
0.00.328.196 I llm_load_print_meta: n_layer          = 4
0.00.328.201 I llm_load_print_meta: n_head           = 12
0.00.328.202 I llm_load_print_meta: n_head_kv        = 12
0.00.328.202 I llm_load_print_meta: n_rot            = 32
0.00.328.202 I llm_load_print_meta: n_swa            = 0
0.00.328.202 I llm_load_print_meta: n_embd_head_k    = 32
0.00.328.203 I llm_load_print_meta: n_embd_head_v    = 32
0.00.328.203 I llm_load_print_meta: n_gqa            = 1
0.00.328.209 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.328.209 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.328.210 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.328.211 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.328.211 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.328.212 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.328.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.328.213 I llm_load_print_meta: n_ff             = 1536
0.00.328.213 I llm_load_print_meta: n_expert         = 0
0.00.328.213 I llm_load_print_meta: n_expert_used    = 0
0.00.328.213 I llm_load_print_meta: causal attn      = 0
0.00.328.213 I llm_load_print_meta: pooling type     = -1
0.00.328.214 I llm_load_print_meta: rope type        = -1
0.00.328.214 I llm_load_print_meta: rope scaling     = linear
0.00.328.214 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.328.215 I llm_load_print_meta: freq_scale_train = 1
0.00.328.215 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.328.215 I llm_load_print_meta: rope_finetuned   = unknown
0.00.328.217 I llm_load_print_meta: ssm_d_conv       = 0
0.00.328.217 I llm_load_print_meta: ssm_d_inner      = 0
0.00.328.217 I llm_load_print_meta: ssm_d_state      = 0
0.00.328.217 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.328.218 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.328.218 I llm_load_print_meta: model type       = 33M
0.00.328.219 I llm_load_print_meta: model ftype      = F16
0.00.328.219 I llm_load_print_meta: model params     = 32.90 M
0.00.328.220 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.328.220 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.328.220 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.328.220 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.328.221 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.328.221 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.328.221 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.328.222 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.328.222 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.328.222 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.328.222 I llm_load_print_meta: max token length = 45
0.00.329.396 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.329.396 I llm_load_tensors: offloading output layer to GPU
0.00.329.396 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.329.417 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.419 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.330.252 I llama_new_context_with_model: n_seq_max     = 1
0.00.330.253 I llama_new_context_with_model: n_ctx         = 8192
0.00.330.253 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.330.254 I llama_new_context_with_model: n_batch       = 2048
0.00.330.254 I llama_new_context_with_model: n_ubatch      = 2048
0.00.330.254 I llama_new_context_with_model: flash_attn    = 0
0.00.330.254 I llama_new_context_with_model: freq_base     = 10000.0
0.00.330.254 I llama_new_context_with_model: freq_scale    = 1
0.00.330.255 I ggml_metal_init: allocating
0.00.330.258 I ggml_metal_init: found device: Apple M4
0.00.330.260 I ggml_metal_init: picking default device: Apple M4
0.00.331.014 I ggml_metal_init: using embedded metal library
0.00.333.878 I ggml_metal_init: GPU name:   Apple M4
0.00.333.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.333.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.333.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.333.881 I ggml_metal_init: simdgroup reduction   = true
0.00.333.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.333.881 I ggml_metal_init: has bfloat            = true
0.00.333.881 I ggml_metal_init: use bfloat            = true
0.00.333.882 I ggml_metal_init: hasUnifiedMemory      = true
0.00.333.883 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.341 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.345.770 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.345.772 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.345.773 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.346.379 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.346.380 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.346.381 I llama_new_context_with_model: graph nodes  = 154
0.00.346.381 I llama_new_context_with_model: graph splits = 2
0.00.346.382 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.346.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.264 I 
0.00.361.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.442 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.443 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.445 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.446 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.449 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.450 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.988 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.626 I llama_perf_context_print:        load time =     337.29 ms
0.00.364.627 I llama_perf_context_print: prompt eval time =       2.63 ms /    62 tokens (    0.04 ms per token, 23574.14 tokens per second)
0.00.364.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.629 I llama_perf_context_print:       total time =       3.36 ms /    63 tokens
0.00.364.889 I ggml_metal_free: deallocating

real	0m1.087s
user	0m0.347s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.174 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.290 I main: llama backend init
0.00.000.297 I main: load the model and apply lora adapter, if any
0.00.081.938 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.092.676 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.092.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.092.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.092.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.092.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.092.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.092.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.092.704 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.092.705 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.092.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.092.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.092.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.092.707 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.092.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.092.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.092.713 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.092.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.099.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.101.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.108.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.108.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.108.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.108.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.108.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.108.825 I llama_model_loader: - type  f32:  194 tensors
0.00.108.826 I llama_model_loader: - type  f16:   98 tensors
0.00.147.509 I llm_load_vocab: special tokens cache size = 25
0.00.155.020 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.155.024 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.155.024 I llm_load_print_meta: arch             = gptneox
0.00.155.024 I llm_load_print_meta: vocab type       = BPE
0.00.155.024 I llm_load_print_meta: n_vocab          = 50304
0.00.155.025 I llm_load_print_meta: n_merges         = 50009
0.00.155.025 I llm_load_print_meta: vocab_only       = 0
0.00.155.025 I llm_load_print_meta: n_ctx_train      = 2048
0.00.155.025 I llm_load_print_meta: n_embd           = 2048
0.00.155.027 I llm_load_print_meta: n_layer          = 24
0.00.155.031 I llm_load_print_meta: n_head           = 16
0.00.155.032 I llm_load_print_meta: n_head_kv        = 16
0.00.155.032 I llm_load_print_meta: n_rot            = 32
0.00.155.033 I llm_load_print_meta: n_swa            = 0
0.00.155.033 I llm_load_print_meta: n_embd_head_k    = 128
0.00.155.035 I llm_load_print_meta: n_embd_head_v    = 128
0.00.155.036 I llm_load_print_meta: n_gqa            = 1
0.00.155.036 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.155.037 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.155.037 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.155.038 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.155.039 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.155.039 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.155.039 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.155.040 I llm_load_print_meta: n_ff             = 8192
0.00.155.040 I llm_load_print_meta: n_expert         = 0
0.00.155.042 I llm_load_print_meta: n_expert_used    = 0
0.00.155.042 I llm_load_print_meta: causal attn      = 1
0.00.155.042 I llm_load_print_meta: pooling type     = 0
0.00.155.042 I llm_load_print_meta: rope type        = 2
0.00.155.042 I llm_load_print_meta: rope scaling     = linear
0.00.155.043 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.155.043 I llm_load_print_meta: freq_scale_train = 1
0.00.155.043 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.155.044 I llm_load_print_meta: rope_finetuned   = unknown
0.00.155.044 I llm_load_print_meta: ssm_d_conv       = 0
0.00.155.044 I llm_load_print_meta: ssm_d_inner      = 0
0.00.155.044 I llm_load_print_meta: ssm_d_state      = 0
0.00.155.044 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.155.044 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.155.044 I llm_load_print_meta: model type       = 1.4B
0.00.155.045 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.155.045 I llm_load_print_meta: model params     = 1.41 B
0.00.155.046 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.155.046 I llm_load_print_meta: general.name     = 1.4B
0.00.155.048 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.155.048 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.155.048 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.155.048 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.155.049 I llm_load_print_meta: LF token         = 128 ''
0.00.155.049 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.155.049 I llm_load_print_meta: max token length = 1024
0.00.157.773 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.157.773 I llm_load_tensors: offloading output layer to GPU
0.00.157.773 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.157.792 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.157.794 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.158.814 I llama_new_context_with_model: n_seq_max     = 1
0.00.158.814 I llama_new_context_with_model: n_ctx         = 2048
0.00.158.815 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.158.815 I llama_new_context_with_model: n_batch       = 2048
0.00.158.815 I llama_new_context_with_model: n_ubatch      = 512
0.00.158.815 I llama_new_context_with_model: flash_attn    = 0
0.00.158.816 I llama_new_context_with_model: freq_base     = 10000.0
0.00.158.816 I llama_new_context_with_model: freq_scale    = 1
0.00.158.817 I ggml_metal_init: allocating
0.00.158.820 I ggml_metal_init: found device: Apple M4
0.00.158.822 I ggml_metal_init: picking default device: Apple M4
0.00.159.527 I ggml_metal_init: using embedded metal library
0.00.169.620 I ggml_metal_init: GPU name:   Apple M4
0.00.169.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.169.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.169.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.169.624 I ggml_metal_init: simdgroup reduction   = true
0.00.169.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.169.624 I ggml_metal_init: has bfloat            = true
0.00.169.624 I ggml_metal_init: use bfloat            = true
0.00.169.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.169.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.198.854 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.221.443 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.221.448 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.221.466 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.222.503 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.222.504 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.222.505 I llama_new_context_with_model: graph nodes  = 967
0.00.222.505 I llama_new_context_with_model: graph splits = 2
0.00.222.509 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.222.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.222.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.308.874 I main: llama threadpool init, n_threads = 4
0.00.308.923 I 
0.00.308.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.308.956 I 
0.00.309.194 I sampler seed: 1234
0.00.309.200 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.309.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.309.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.309.225 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.143.057 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.02.143.058 I llama_perf_context_print:        load time =     226.92 ms
0.02.143.058 I llama_perf_context_print: prompt eval time =      44.07 ms /     7 tokens (    6.30 ms per token,   158.82 tokens per second)
0.02.143.059 I llama_perf_context_print:        eval time =    1786.89 ms /    63 runs   (   28.36 ms per token,    35.26 tokens per second)
0.02.143.060 I llama_perf_context_print:       total time =    1834.19 ms /    70 tokens
0.02.143.283 I ggml_metal_free: deallocating

real	0m2.446s
user	0m0.150s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.959 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.656 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.674 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.682 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.640 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.641 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.642 I llama_model_loader: - type  f32:  194 tensors
0.00.041.642 I llama_model_loader: - type  f16:   98 tensors
0.00.062.269 I llm_load_vocab: special tokens cache size = 25
0.00.068.244 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.247 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.248 I llm_load_print_meta: arch             = gptneox
0.00.068.248 I llm_load_print_meta: vocab type       = BPE
0.00.068.248 I llm_load_print_meta: n_vocab          = 50304
0.00.068.249 I llm_load_print_meta: n_merges         = 50009
0.00.068.249 I llm_load_print_meta: vocab_only       = 0
0.00.068.249 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.249 I llm_load_print_meta: n_embd           = 2048
0.00.068.249 I llm_load_print_meta: n_layer          = 24
0.00.068.253 I llm_load_print_meta: n_head           = 16
0.00.068.254 I llm_load_print_meta: n_head_kv        = 16
0.00.068.255 I llm_load_print_meta: n_rot            = 32
0.00.068.255 I llm_load_print_meta: n_swa            = 0
0.00.068.255 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.255 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.256 I llm_load_print_meta: n_gqa            = 1
0.00.068.257 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.257 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.258 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.258 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.258 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.258 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.259 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.259 I llm_load_print_meta: n_ff             = 8192
0.00.068.259 I llm_load_print_meta: n_expert         = 0
0.00.068.259 I llm_load_print_meta: n_expert_used    = 0
0.00.068.259 I llm_load_print_meta: causal attn      = 1
0.00.068.260 I llm_load_print_meta: pooling type     = 0
0.00.068.260 I llm_load_print_meta: rope type        = 2
0.00.068.260 I llm_load_print_meta: rope scaling     = linear
0.00.068.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.261 I llm_load_print_meta: freq_scale_train = 1
0.00.068.261 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.261 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.261 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.261 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.263 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.264 I llm_load_print_meta: model type       = 1.4B
0.00.068.265 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.068.265 I llm_load_print_meta: model params     = 1.41 B
0.00.068.265 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.068.265 I llm_load_print_meta: general.name     = 1.4B
0.00.068.266 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.266 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.266 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.266 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.266 I llm_load_print_meta: LF token         = 128 ''
0.00.068.271 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.271 I llm_load_print_meta: max token length = 1024
0.00.069.885 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.885 I llm_load_tensors: offloading output layer to GPU
0.00.069.886 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.896 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.069.897 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.070.705 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.706 I llama_new_context_with_model: n_ctx         = 128
0.00.070.706 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.707 I llama_new_context_with_model: n_batch       = 128
0.00.070.707 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.707 I llama_new_context_with_model: flash_attn    = 0
0.00.070.707 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.708 I llama_new_context_with_model: freq_scale    = 1
0.00.070.708 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.708 I ggml_metal_init: allocating
0.00.070.716 I ggml_metal_init: found device: Apple M4
0.00.070.718 I ggml_metal_init: picking default device: Apple M4
0.00.071.326 I ggml_metal_init: using embedded metal library
0.00.073.700 I ggml_metal_init: GPU name:   Apple M4
0.00.073.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.703 I ggml_metal_init: simdgroup reduction   = true
0.00.073.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.703 I ggml_metal_init: has bfloat            = true
0.00.073.703 I ggml_metal_init: use bfloat            = true
0.00.073.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.913 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.201 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.204 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.219 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.100 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.102 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.102 I llama_new_context_with_model: graph nodes  = 967
0.00.086.102 I llama_new_context_with_model: graph splits = 2
0.00.086.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.446.676 I 
0.01.446.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.446.763 I perplexity: tokenizing the input ..
0.01.458.539 I perplexity: tokenization took 11.774 ms
0.01.458.546 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.579.674 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.581.362 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.581.385 I llama_perf_context_print:        load time =    1428.92 ms
0.01.581.386 I llama_perf_context_print: prompt eval time =     120.75 ms /   128 tokens (    0.94 ms per token,  1060.04 tokens per second)
0.01.581.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.581.388 I llama_perf_context_print:       total time =     134.71 ms /   129 tokens
0.01.582.113 I ggml_metal_free: deallocating

real	0m1.781s
user	0m0.104s
sys	0m0.246s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.013.649 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.838 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.849 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.850 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.851 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.852 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.852 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.852 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.856 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.856 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.540 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.566 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.542 I llama_model_loader: - type  f32:  194 tensors
0.00.037.542 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.353 I llm_load_vocab: special tokens cache size = 25
0.00.067.925 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.930 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.930 I llm_load_print_meta: arch             = gptneox
0.00.067.931 I llm_load_print_meta: vocab type       = BPE
0.00.067.931 I llm_load_print_meta: n_vocab          = 50304
0.00.067.931 I llm_load_print_meta: n_merges         = 50009
0.00.067.931 I llm_load_print_meta: vocab_only       = 0
0.00.067.931 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.932 I llm_load_print_meta: n_embd           = 2048
0.00.067.932 I llm_load_print_meta: n_layer          = 24
0.00.067.936 I llm_load_print_meta: n_head           = 16
0.00.067.937 I llm_load_print_meta: n_head_kv        = 16
0.00.067.940 I llm_load_print_meta: n_rot            = 32
0.00.067.941 I llm_load_print_meta: n_swa            = 0
0.00.067.941 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.941 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.941 I llm_load_print_meta: n_gqa            = 1
0.00.067.942 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.943 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.943 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.944 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.945 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.945 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.945 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.946 I llm_load_print_meta: n_ff             = 8192
0.00.067.946 I llm_load_print_meta: n_expert         = 0
0.00.067.946 I llm_load_print_meta: n_expert_used    = 0
0.00.067.946 I llm_load_print_meta: causal attn      = 1
0.00.067.947 I llm_load_print_meta: pooling type     = 0
0.00.067.947 I llm_load_print_meta: rope type        = 2
0.00.067.947 I llm_load_print_meta: rope scaling     = linear
0.00.067.947 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.948 I llm_load_print_meta: freq_scale_train = 1
0.00.067.948 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.948 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.948 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.948 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.949 I llm_load_print_meta: model type       = 1.4B
0.00.067.949 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.950 I llm_load_print_meta: model params     = 1.41 B
0.00.067.950 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.950 I llm_load_print_meta: general.name     = 1.4B
0.00.067.950 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.950 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.951 I llm_load_print_meta: LF token         = 128 ''
0.00.067.951 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.951 I llm_load_print_meta: max token length = 1024
0.00.070.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.467 I llm_load_tensors: offloading output layer to GPU
0.00.070.467 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.478 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.480 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.477 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.478 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.479 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.479 I llama_new_context_with_model: n_batch       = 2048
0.00.071.479 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.479 I llama_new_context_with_model: flash_attn    = 0
0.00.071.480 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.480 I llama_new_context_with_model: freq_scale    = 1
0.00.071.480 I ggml_metal_init: allocating
0.00.071.484 I ggml_metal_init: found device: Apple M4
0.00.071.486 I ggml_metal_init: picking default device: Apple M4
0.00.072.238 I ggml_metal_init: using embedded metal library
0.00.075.009 I ggml_metal_init: GPU name:   Apple M4
0.00.075.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.011 I ggml_metal_init: simdgroup reduction   = true
0.00.075.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.012 I ggml_metal_init: has bfloat            = true
0.00.075.012 I ggml_metal_init: use bfloat            = true
0.00.075.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.721 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.111.675 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.685 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.710 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.871 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.112.874 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.112.875 I llama_new_context_with_model: graph nodes  = 967
0.00.112.875 I llama_new_context_with_model: graph splits = 2
0.00.112.879 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.113.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.335.531 I main: llama threadpool init, n_threads = 4
0.01.335.565 I 
0.01.335.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.335.587 I 
0.01.335.820 I sampler seed: 1234
0.01.335.825 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.335.855 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.335.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.335.857 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.423.342 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.02.423.343 I llama_perf_context_print:        load time =    1321.88 ms
0.02.423.344 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   159.99 tokens per second)
0.02.423.344 I llama_perf_context_print:        eval time =    1040.82 ms /    63 runs   (   16.52 ms per token,    60.53 tokens per second)
0.02.423.345 I llama_perf_context_print:       total time =    1087.81 ms /    70 tokens
0.02.423.533 I ggml_metal_free: deallocating

real	0m2.442s
user	0m0.116s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.336 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.944 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.823 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.823 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.824 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.824 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.152 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.712 I llama_model_loader: - type  f32:  194 tensors
0.00.038.713 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.978 I llm_load_vocab: special tokens cache size = 25
0.00.075.221 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.226 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.227 I llm_load_print_meta: arch             = gptneox
0.00.075.227 I llm_load_print_meta: vocab type       = BPE
0.00.075.227 I llm_load_print_meta: n_vocab          = 50304
0.00.075.227 I llm_load_print_meta: n_merges         = 50009
0.00.075.228 I llm_load_print_meta: vocab_only       = 0
0.00.075.228 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.228 I llm_load_print_meta: n_embd           = 2048
0.00.075.228 I llm_load_print_meta: n_layer          = 24
0.00.075.232 I llm_load_print_meta: n_head           = 16
0.00.075.233 I llm_load_print_meta: n_head_kv        = 16
0.00.075.233 I llm_load_print_meta: n_rot            = 32
0.00.075.234 I llm_load_print_meta: n_swa            = 0
0.00.075.234 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.236 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.237 I llm_load_print_meta: n_gqa            = 1
0.00.075.238 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.238 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.239 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.240 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.240 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.241 I llm_load_print_meta: n_ff             = 8192
0.00.075.241 I llm_load_print_meta: n_expert         = 0
0.00.075.241 I llm_load_print_meta: n_expert_used    = 0
0.00.075.241 I llm_load_print_meta: causal attn      = 1
0.00.075.241 I llm_load_print_meta: pooling type     = 0
0.00.075.241 I llm_load_print_meta: rope type        = 2
0.00.075.241 I llm_load_print_meta: rope scaling     = linear
0.00.075.242 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.242 I llm_load_print_meta: freq_scale_train = 1
0.00.075.243 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.243 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.245 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.245 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.245 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.245 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.246 I llm_load_print_meta: model type       = 1.4B
0.00.075.246 I llm_load_print_meta: model ftype      = Q8_0
0.00.075.247 I llm_load_print_meta: model params     = 1.41 B
0.00.075.247 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.075.247 I llm_load_print_meta: general.name     = 1.4B
0.00.075.247 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.249 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.249 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.249 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.250 I llm_load_print_meta: LF token         = 128 ''
0.00.075.250 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.250 I llm_load_print_meta: max token length = 1024
0.00.077.608 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.608 I llm_load_tensors: offloading output layer to GPU
0.00.077.608 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.620 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.077.621 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.078.605 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.605 I llama_new_context_with_model: n_ctx         = 128
0.00.078.606 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.078.606 I llama_new_context_with_model: n_batch       = 128
0.00.078.606 I llama_new_context_with_model: n_ubatch      = 128
0.00.078.606 I llama_new_context_with_model: flash_attn    = 0
0.00.078.607 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.607 I llama_new_context_with_model: freq_scale    = 1
0.00.078.607 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.078.608 I ggml_metal_init: allocating
0.00.078.612 I ggml_metal_init: found device: Apple M4
0.00.078.614 I ggml_metal_init: picking default device: Apple M4
0.00.079.276 I ggml_metal_init: using embedded metal library
0.00.083.368 I ggml_metal_init: GPU name:   Apple M4
0.00.083.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.370 I ggml_metal_init: simdgroup reduction   = true
0.00.083.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.371 I ggml_metal_init: has bfloat            = true
0.00.083.371 I ggml_metal_init: use bfloat            = true
0.00.083.371 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.372 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.262 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.710 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.094.713 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.094.728 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.695 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.095.696 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.095.697 I llama_new_context_with_model: graph nodes  = 967
0.00.095.697 I llama_new_context_with_model: graph splits = 2
0.00.095.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.095.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.019.869 I 
0.01.019.898 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.019.932 I perplexity: tokenizing the input ..
0.01.027.794 I perplexity: tokenization took 7.859 ms
0.01.027.797 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.152.203 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.153.385 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.153.395 I llama_perf_context_print:        load time =    1005.92 ms
0.01.153.396 I llama_perf_context_print: prompt eval time =     124.18 ms /   128 tokens (    0.97 ms per token,  1030.74 tokens per second)
0.01.153.400 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.153.401 I llama_perf_context_print:       total time =     133.53 ms /   129 tokens
0.01.153.716 I ggml_metal_free: deallocating

real	0m1.175s
user	0m0.102s
sys	0m0.176s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.015.859 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.743 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.744 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.744 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.745 I llama_model_loader: - type  f32:  194 tensors
0.00.041.745 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.746 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.697 I llm_load_vocab: special tokens cache size = 25
0.00.076.664 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.669 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.669 I llm_load_print_meta: arch             = gptneox
0.00.076.670 I llm_load_print_meta: vocab type       = BPE
0.00.076.670 I llm_load_print_meta: n_vocab          = 50304
0.00.076.670 I llm_load_print_meta: n_merges         = 50009
0.00.076.671 I llm_load_print_meta: vocab_only       = 0
0.00.076.671 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.671 I llm_load_print_meta: n_embd           = 2048
0.00.076.671 I llm_load_print_meta: n_layer          = 24
0.00.076.676 I llm_load_print_meta: n_head           = 16
0.00.076.677 I llm_load_print_meta: n_head_kv        = 16
0.00.076.678 I llm_load_print_meta: n_rot            = 32
0.00.076.678 I llm_load_print_meta: n_swa            = 0
0.00.076.678 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.679 I llm_load_print_meta: n_gqa            = 1
0.00.076.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.682 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.684 I llm_load_print_meta: n_ff             = 8192
0.00.076.684 I llm_load_print_meta: n_expert         = 0
0.00.076.684 I llm_load_print_meta: n_expert_used    = 0
0.00.076.684 I llm_load_print_meta: causal attn      = 1
0.00.076.684 I llm_load_print_meta: pooling type     = 0
0.00.076.685 I llm_load_print_meta: rope type        = 2
0.00.076.685 I llm_load_print_meta: rope scaling     = linear
0.00.076.686 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.686 I llm_load_print_meta: freq_scale_train = 1
0.00.076.686 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.686 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.687 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.690 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.690 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.691 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.691 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.691 I llm_load_print_meta: model type       = 1.4B
0.00.076.692 I llm_load_print_meta: model ftype      = Q4_0
0.00.076.692 I llm_load_print_meta: model params     = 1.41 B
0.00.076.693 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.076.693 I llm_load_print_meta: general.name     = 1.4B
0.00.076.694 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.694 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.694 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.694 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.695 I llm_load_print_meta: LF token         = 128 ''
0.00.076.695 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.695 I llm_load_print_meta: max token length = 1024
0.00.079.508 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.508 I llm_load_tensors: offloading output layer to GPU
0.00.079.508 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.520 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.079.522 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.080.903 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.905 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.905 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.905 I llama_new_context_with_model: n_batch       = 2048
0.00.080.906 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.906 I llama_new_context_with_model: flash_attn    = 0
0.00.080.906 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.907 I llama_new_context_with_model: freq_scale    = 1
0.00.080.907 I ggml_metal_init: allocating
0.00.080.912 I ggml_metal_init: found device: Apple M4
0.00.080.914 I ggml_metal_init: picking default device: Apple M4
0.00.081.847 I ggml_metal_init: using embedded metal library
0.00.085.921 I ggml_metal_init: GPU name:   Apple M4
0.00.085.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.925 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.925 I ggml_metal_init: simdgroup reduction   = true
0.00.085.926 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.926 I ggml_metal_init: has bfloat            = true
0.00.085.926 I ggml_metal_init: use bfloat            = true
0.00.085.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.427 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.129.350 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.357 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.382 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.130.551 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.130.554 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.130.554 I llama_new_context_with_model: graph nodes  = 967
0.00.130.554 I llama_new_context_with_model: graph splits = 2
0.00.130.558 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.703 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.735 I main: llama threadpool init, n_threads = 4
0.00.758.781 I 
0.00.758.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.809 I 
0.00.759.050 I sampler seed: 1234
0.00.759.056 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.074 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.436.001 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.436.001 I llama_perf_context_print:        load time =     742.87 ms
0.01.436.002 I llama_perf_context_print: prompt eval time =      39.88 ms /     7 tokens (    5.70 ms per token,   175.54 tokens per second)
0.01.436.003 I llama_perf_context_print:        eval time =     633.94 ms /    63 runs   (   10.06 ms per token,    99.38 tokens per second)
0.01.436.003 I llama_perf_context_print:       total time =     677.27 ms /    70 tokens
0.01.436.225 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.127s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.266 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.401 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.013 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.015 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.022 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.022 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.658 I llama_model_loader: - type  f32:  194 tensors
0.00.024.659 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.659 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.414 I llm_load_vocab: special tokens cache size = 25
0.00.051.211 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.214 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.214 I llm_load_print_meta: arch             = gptneox
0.00.051.215 I llm_load_print_meta: vocab type       = BPE
0.00.051.215 I llm_load_print_meta: n_vocab          = 50304
0.00.051.215 I llm_load_print_meta: n_merges         = 50009
0.00.051.215 I llm_load_print_meta: vocab_only       = 0
0.00.051.215 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.216 I llm_load_print_meta: n_embd           = 2048
0.00.051.216 I llm_load_print_meta: n_layer          = 24
0.00.051.219 I llm_load_print_meta: n_head           = 16
0.00.051.219 I llm_load_print_meta: n_head_kv        = 16
0.00.051.221 I llm_load_print_meta: n_rot            = 32
0.00.051.221 I llm_load_print_meta: n_swa            = 0
0.00.051.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.222 I llm_load_print_meta: n_gqa            = 1
0.00.051.223 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.225 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.225 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.231 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.235 I llm_load_print_meta: n_ff             = 8192
0.00.051.236 I llm_load_print_meta: n_expert         = 0
0.00.051.236 I llm_load_print_meta: n_expert_used    = 0
0.00.051.236 I llm_load_print_meta: causal attn      = 1
0.00.051.236 I llm_load_print_meta: pooling type     = 0
0.00.051.236 I llm_load_print_meta: rope type        = 2
0.00.051.237 I llm_load_print_meta: rope scaling     = linear
0.00.051.237 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.237 I llm_load_print_meta: freq_scale_train = 1
0.00.051.237 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.238 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.238 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.238 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.238 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.238 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.238 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.239 I llm_load_print_meta: model type       = 1.4B
0.00.051.239 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.239 I llm_load_print_meta: model params     = 1.41 B
0.00.051.240 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.241 I llm_load_print_meta: general.name     = 1.4B
0.00.051.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: LF token         = 128 ''
0.00.051.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: max token length = 1024
0.00.053.136 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.136 I llm_load_tensors: offloading output layer to GPU
0.00.053.136 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.147 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.148 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.044 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.045 I llama_new_context_with_model: n_ctx         = 128
0.00.054.045 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.045 I llama_new_context_with_model: n_batch       = 128
0.00.054.045 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.045 I llama_new_context_with_model: flash_attn    = 0
0.00.054.046 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.046 I llama_new_context_with_model: freq_scale    = 1
0.00.054.046 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.047 I ggml_metal_init: allocating
0.00.054.050 I ggml_metal_init: found device: Apple M4
0.00.054.052 I ggml_metal_init: picking default device: Apple M4
0.00.054.623 I ggml_metal_init: using embedded metal library
0.00.056.957 I ggml_metal_init: GPU name:   Apple M4
0.00.056.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.959 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.960 I ggml_metal_init: simdgroup reduction   = true
0.00.056.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.960 I ggml_metal_init: has bfloat            = true
0.00.056.960 I ggml_metal_init: use bfloat            = true
0.00.056.961 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.565 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.567 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.580 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.416 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.417 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.417 I llama_new_context_with_model: graph nodes  = 967
0.00.071.418 I llama_new_context_with_model: graph splits = 2
0.00.071.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.487 I 
0.00.651.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.529 I perplexity: tokenizing the input ..
0.00.659.230 I perplexity: tokenization took 7.699 ms
0.00.659.233 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.503 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.783.719 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.783.750 I llama_perf_context_print:        load time =     641.08 ms
0.00.783.752 I llama_perf_context_print: prompt eval time =     123.03 ms /   128 tokens (    0.96 ms per token,  1040.44 tokens per second)
0.00.783.753 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.755 I llama_perf_context_print:       total time =     132.26 ms /   129 tokens
0.00.784.324 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.077s
sys	0m0.105s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.710 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.279 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.285 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.286 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.287 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.288 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.288 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.288 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.293 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.293 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.149 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.027 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.028 I llama_model_loader: - type  f32:  194 tensors
0.00.034.028 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.029 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.012 I llm_load_vocab: special tokens cache size = 25
0.00.059.774 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.777 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.777 I llm_load_print_meta: arch             = gptneox
0.00.059.777 I llm_load_print_meta: vocab type       = BPE
0.00.059.777 I llm_load_print_meta: n_vocab          = 50304
0.00.059.778 I llm_load_print_meta: n_merges         = 50009
0.00.059.778 I llm_load_print_meta: vocab_only       = 0
0.00.059.778 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.778 I llm_load_print_meta: n_embd           = 2048
0.00.059.778 I llm_load_print_meta: n_layer          = 24
0.00.059.781 I llm_load_print_meta: n_head           = 16
0.00.059.782 I llm_load_print_meta: n_head_kv        = 16
0.00.059.782 I llm_load_print_meta: n_rot            = 32
0.00.059.782 I llm_load_print_meta: n_swa            = 0
0.00.059.782 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.784 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.785 I llm_load_print_meta: n_gqa            = 1
0.00.059.785 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.786 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.786 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.787 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.787 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.787 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.787 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.788 I llm_load_print_meta: n_ff             = 8192
0.00.059.788 I llm_load_print_meta: n_expert         = 0
0.00.059.788 I llm_load_print_meta: n_expert_used    = 0
0.00.059.790 I llm_load_print_meta: causal attn      = 1
0.00.059.792 I llm_load_print_meta: pooling type     = 0
0.00.059.792 I llm_load_print_meta: rope type        = 2
0.00.059.792 I llm_load_print_meta: rope scaling     = linear
0.00.059.792 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.793 I llm_load_print_meta: freq_scale_train = 1
0.00.059.793 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.793 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.793 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.794 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.794 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.794 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.794 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.794 I llm_load_print_meta: model type       = 1.4B
0.00.059.795 I llm_load_print_meta: model ftype      = Q4_1
0.00.059.795 I llm_load_print_meta: model params     = 1.41 B
0.00.059.796 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.059.796 I llm_load_print_meta: general.name     = 1.4B
0.00.059.797 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.798 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.798 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.798 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.798 I llm_load_print_meta: LF token         = 128 ''
0.00.059.799 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.799 I llm_load_print_meta: max token length = 1024
0.00.061.735 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.735 I llm_load_tensors: offloading output layer to GPU
0.00.061.735 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.746 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.747 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.062.663 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.664 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.664 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.664 I llama_new_context_with_model: n_batch       = 2048
0.00.062.665 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.665 I llama_new_context_with_model: flash_attn    = 0
0.00.062.665 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.665 I llama_new_context_with_model: freq_scale    = 1
0.00.062.666 I ggml_metal_init: allocating
0.00.062.669 I ggml_metal_init: found device: Apple M4
0.00.062.671 I ggml_metal_init: picking default device: Apple M4
0.00.063.265 I ggml_metal_init: using embedded metal library
0.00.065.579 I ggml_metal_init: GPU name:   Apple M4
0.00.065.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.581 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.581 I ggml_metal_init: simdgroup reduction   = true
0.00.065.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.581 I ggml_metal_init: has bfloat            = true
0.00.065.583 I ggml_metal_init: use bfloat            = true
0.00.065.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.037 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.301 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.306 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.324 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.385 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.386 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.387 I llama_new_context_with_model: graph nodes  = 967
0.00.096.387 I llama_new_context_with_model: graph splits = 2
0.00.096.390 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.092.704 I main: llama threadpool init, n_threads = 4
0.01.092.749 I 
0.01.092.771 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.092.771 I 
0.01.093.013 I sampler seed: 1234
0.01.093.017 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.093.060 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.093.063 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.093.063 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.834.623 I llama_perf_sampler_print:    sampling time =       1.60 ms /    71 runs   (    0.02 ms per token, 44291.95 tokens per second)
0.01.834.623 I llama_perf_context_print:        load time =    1083.99 ms
0.01.834.624 I llama_perf_context_print: prompt eval time =      49.76 ms /     7 tokens (    7.11 ms per token,   140.67 tokens per second)
0.01.834.626 I llama_perf_context_print:        eval time =     688.56 ms /    63 runs   (   10.93 ms per token,    91.49 tokens per second)
0.01.834.626 I llama_perf_context_print:       total time =     741.92 ms /    70 tokens
0.01.834.878 I ggml_metal_free: deallocating

real	0m1.856s
user	0m0.117s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.838 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.328 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.330 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.331 I llama_model_loader: - type  f32:  194 tensors
0.00.023.331 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.331 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.808 I llm_load_vocab: special tokens cache size = 25
0.00.049.729 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.732 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.732 I llm_load_print_meta: arch             = gptneox
0.00.049.732 I llm_load_print_meta: vocab type       = BPE
0.00.049.733 I llm_load_print_meta: n_vocab          = 50304
0.00.049.733 I llm_load_print_meta: n_merges         = 50009
0.00.049.733 I llm_load_print_meta: vocab_only       = 0
0.00.049.733 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.733 I llm_load_print_meta: n_embd           = 2048
0.00.049.734 I llm_load_print_meta: n_layer          = 24
0.00.049.737 I llm_load_print_meta: n_head           = 16
0.00.049.738 I llm_load_print_meta: n_head_kv        = 16
0.00.049.738 I llm_load_print_meta: n_rot            = 32
0.00.049.738 I llm_load_print_meta: n_swa            = 0
0.00.049.738 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.738 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.739 I llm_load_print_meta: n_gqa            = 1
0.00.049.740 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.743 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.745 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.745 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.745 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.745 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.746 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.746 I llm_load_print_meta: n_ff             = 8192
0.00.049.746 I llm_load_print_meta: n_expert         = 0
0.00.049.747 I llm_load_print_meta: n_expert_used    = 0
0.00.049.747 I llm_load_print_meta: causal attn      = 1
0.00.049.747 I llm_load_print_meta: pooling type     = 0
0.00.049.747 I llm_load_print_meta: rope type        = 2
0.00.049.747 I llm_load_print_meta: rope scaling     = linear
0.00.049.748 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.748 I llm_load_print_meta: freq_scale_train = 1
0.00.049.748 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.748 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.749 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.749 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.749 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.749 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.749 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.749 I llm_load_print_meta: model type       = 1.4B
0.00.049.750 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.750 I llm_load_print_meta: model params     = 1.41 B
0.00.049.751 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.751 I llm_load_print_meta: general.name     = 1.4B
0.00.049.751 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.751 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.752 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.752 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.752 I llm_load_print_meta: LF token         = 128 ''
0.00.049.756 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.756 I llm_load_print_meta: max token length = 1024
0.00.051.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.689 I llm_load_tensors: offloading output layer to GPU
0.00.051.689 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.700 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.701 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.612 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.613 I llama_new_context_with_model: n_ctx         = 128
0.00.052.613 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.613 I llama_new_context_with_model: n_batch       = 128
0.00.052.613 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.614 I llama_new_context_with_model: flash_attn    = 0
0.00.052.614 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.614 I llama_new_context_with_model: freq_scale    = 1
0.00.052.615 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.615 I ggml_metal_init: allocating
0.00.052.621 I ggml_metal_init: found device: Apple M4
0.00.052.624 I ggml_metal_init: picking default device: Apple M4
0.00.053.215 I ggml_metal_init: using embedded metal library
0.00.055.541 I ggml_metal_init: GPU name:   Apple M4
0.00.055.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.543 I ggml_metal_init: simdgroup reduction   = true
0.00.055.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.543 I ggml_metal_init: has bfloat            = true
0.00.055.543 I ggml_metal_init: use bfloat            = true
0.00.055.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.545 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.043 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.306 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.325 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.219 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.220 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.221 I llama_new_context_with_model: graph nodes  = 967
0.00.067.221 I llama_new_context_with_model: graph splits = 2
0.00.067.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.222 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.111 I 
0.00.668.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.166 I perplexity: tokenizing the input ..
0.00.676.162 I perplexity: tokenization took 7.994 ms
0.00.676.165 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.949 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.800.112 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.800.133 I llama_perf_context_print:        load time =     659.27 ms
0.00.800.134 I llama_perf_context_print: prompt eval time =     122.56 ms /   128 tokens (    0.96 ms per token,  1044.39 tokens per second)
0.00.800.135 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.136 I llama_perf_context_print:       total time =     132.02 ms /   129 tokens
0.00.800.700 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.078s
sys	0m0.100s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.547 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.616 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.440 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.441 I llama_model_loader: - type  f32:  194 tensors
0.00.027.441 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.442 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.556 I llm_load_vocab: special tokens cache size = 25
0.00.053.547 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.550 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.551 I llm_load_print_meta: arch             = gptneox
0.00.053.551 I llm_load_print_meta: vocab type       = BPE
0.00.053.551 I llm_load_print_meta: n_vocab          = 50304
0.00.053.551 I llm_load_print_meta: n_merges         = 50009
0.00.053.552 I llm_load_print_meta: vocab_only       = 0
0.00.053.552 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.552 I llm_load_print_meta: n_embd           = 2048
0.00.053.552 I llm_load_print_meta: n_layer          = 24
0.00.053.555 I llm_load_print_meta: n_head           = 16
0.00.053.556 I llm_load_print_meta: n_head_kv        = 16
0.00.053.556 I llm_load_print_meta: n_rot            = 32
0.00.053.558 I llm_load_print_meta: n_swa            = 0
0.00.053.558 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.558 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.559 I llm_load_print_meta: n_gqa            = 1
0.00.053.559 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.560 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.561 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.561 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.561 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.562 I llm_load_print_meta: n_ff             = 8192
0.00.053.562 I llm_load_print_meta: n_expert         = 0
0.00.053.562 I llm_load_print_meta: n_expert_used    = 0
0.00.053.563 I llm_load_print_meta: causal attn      = 1
0.00.053.563 I llm_load_print_meta: pooling type     = 0
0.00.053.563 I llm_load_print_meta: rope type        = 2
0.00.053.563 I llm_load_print_meta: rope scaling     = linear
0.00.053.564 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.565 I llm_load_print_meta: freq_scale_train = 1
0.00.053.565 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.565 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.565 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.565 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.566 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.567 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.568 I llm_load_print_meta: model type       = 1.4B
0.00.053.568 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.568 I llm_load_print_meta: model params     = 1.41 B
0.00.053.569 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.569 I llm_load_print_meta: general.name     = 1.4B
0.00.053.570 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.570 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.570 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.570 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.570 I llm_load_print_meta: LF token         = 128 ''
0.00.053.571 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.572 I llm_load_print_meta: max token length = 1024
0.00.055.555 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.556 I llm_load_tensors: offloading output layer to GPU
0.00.055.556 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.566 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.568 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.556 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.557 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.557 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.557 I llama_new_context_with_model: n_batch       = 2048
0.00.056.558 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.558 I llama_new_context_with_model: flash_attn    = 0
0.00.056.558 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.558 I llama_new_context_with_model: freq_scale    = 1
0.00.056.559 I ggml_metal_init: allocating
0.00.056.562 I ggml_metal_init: found device: Apple M4
0.00.056.564 I ggml_metal_init: picking default device: Apple M4
0.00.057.156 I ggml_metal_init: using embedded metal library
0.00.059.468 I ggml_metal_init: GPU name:   Apple M4
0.00.059.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.470 I ggml_metal_init: simdgroup reduction   = true
0.00.059.470 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.471 I ggml_metal_init: has bfloat            = true
0.00.059.471 I ggml_metal_init: use bfloat            = true
0.00.059.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.924 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.071 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.081 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.101 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.263 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.264 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.265 I llama_new_context_with_model: graph nodes  = 967
0.00.091.265 I llama_new_context_with_model: graph splits = 2
0.00.091.269 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.577 I main: llama threadpool init, n_threads = 4
0.00.784.663 I 
0.00.784.683 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.683 I 
0.00.784.912 I sampler seed: 1234
0.00.784.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.948 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.950 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.950 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.576.345 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.576.346 I llama_perf_context_print:        load time =     772.66 ms
0.01.576.347 I llama_perf_context_print: prompt eval time =      45.55 ms /     7 tokens (    6.51 ms per token,   153.68 tokens per second)
0.01.576.347 I llama_perf_context_print:        eval time =     742.84 ms /    63 runs   (   11.79 ms per token,    84.81 tokens per second)
0.01.576.351 I llama_perf_context_print:       total time =     791.77 ms /    70 tokens
0.01.576.577 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.132 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.569 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.093 I llama_model_loader: - type  f32:  194 tensors
0.00.024.093 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.093 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.836 I llm_load_vocab: special tokens cache size = 25
0.00.049.736 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.738 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.739 I llm_load_print_meta: arch             = gptneox
0.00.049.739 I llm_load_print_meta: vocab type       = BPE
0.00.049.739 I llm_load_print_meta: n_vocab          = 50304
0.00.049.740 I llm_load_print_meta: n_merges         = 50009
0.00.049.740 I llm_load_print_meta: vocab_only       = 0
0.00.049.740 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.740 I llm_load_print_meta: n_embd           = 2048
0.00.049.740 I llm_load_print_meta: n_layer          = 24
0.00.049.743 I llm_load_print_meta: n_head           = 16
0.00.049.743 I llm_load_print_meta: n_head_kv        = 16
0.00.049.744 I llm_load_print_meta: n_rot            = 32
0.00.049.744 I llm_load_print_meta: n_swa            = 0
0.00.049.744 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.744 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.745 I llm_load_print_meta: n_gqa            = 1
0.00.049.746 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.746 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.747 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.747 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.747 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.748 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.748 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.749 I llm_load_print_meta: n_ff             = 8192
0.00.049.749 I llm_load_print_meta: n_expert         = 0
0.00.049.750 I llm_load_print_meta: n_expert_used    = 0
0.00.049.750 I llm_load_print_meta: causal attn      = 1
0.00.049.750 I llm_load_print_meta: pooling type     = 0
0.00.049.750 I llm_load_print_meta: rope type        = 2
0.00.049.750 I llm_load_print_meta: rope scaling     = linear
0.00.049.752 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.753 I llm_load_print_meta: freq_scale_train = 1
0.00.049.753 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.753 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.753 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.753 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.753 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.753 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.754 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.754 I llm_load_print_meta: model type       = 1.4B
0.00.049.754 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.755 I llm_load_print_meta: model params     = 1.41 B
0.00.049.755 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.756 I llm_load_print_meta: general.name     = 1.4B
0.00.049.756 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.761 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.761 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.761 I llm_load_print_meta: LF token         = 128 ''
0.00.049.761 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.761 I llm_load_print_meta: max token length = 1024
0.00.051.675 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.675 I llm_load_tensors: offloading output layer to GPU
0.00.051.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.686 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.687 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.543 I llama_new_context_with_model: n_ctx         = 128
0.00.052.543 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.543 I llama_new_context_with_model: n_batch       = 128
0.00.052.543 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.543 I llama_new_context_with_model: flash_attn    = 0
0.00.052.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.544 I llama_new_context_with_model: freq_scale    = 1
0.00.052.544 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.545 I ggml_metal_init: allocating
0.00.052.551 I ggml_metal_init: found device: Apple M4
0.00.052.553 I ggml_metal_init: picking default device: Apple M4
0.00.053.124 I ggml_metal_init: using embedded metal library
0.00.055.462 I ggml_metal_init: GPU name:   Apple M4
0.00.055.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.465 I ggml_metal_init: simdgroup reduction   = true
0.00.055.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.465 I ggml_metal_init: has bfloat            = true
0.00.055.465 I ggml_metal_init: use bfloat            = true
0.00.055.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.845 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.079 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.085 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.099 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.983 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.984 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.985 I llama_new_context_with_model: graph nodes  = 967
0.00.066.985 I llama_new_context_with_model: graph splits = 2
0.00.066.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.513 I 
0.00.731.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.564 I perplexity: tokenizing the input ..
0.00.739.754 I perplexity: tokenization took 8.189 ms
0.00.739.762 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.979 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.876.168 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.876.179 I llama_perf_context_print:        load time =     721.38 ms
0.00.876.180 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.21 tokens per second)
0.00.876.180 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.181 I llama_perf_context_print:       total time =     144.67 ms /   129 tokens
0.00.876.521 I ggml_metal_free: deallocating

real	0m0.891s
user	0m0.077s
sys	0m0.119s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.823 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.823 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.824 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.824 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.459 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.460 I llama_model_loader: - type  f32:  194 tensors
0.00.023.460 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.460 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.205 I llm_load_vocab: special tokens cache size = 25
0.00.050.095 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.098 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.098 I llm_load_print_meta: arch             = gptneox
0.00.050.099 I llm_load_print_meta: vocab type       = BPE
0.00.050.099 I llm_load_print_meta: n_vocab          = 50304
0.00.050.099 I llm_load_print_meta: n_merges         = 50009
0.00.050.099 I llm_load_print_meta: vocab_only       = 0
0.00.050.099 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.100 I llm_load_print_meta: n_embd           = 2048
0.00.050.100 I llm_load_print_meta: n_layer          = 24
0.00.050.102 I llm_load_print_meta: n_head           = 16
0.00.050.103 I llm_load_print_meta: n_head_kv        = 16
0.00.050.103 I llm_load_print_meta: n_rot            = 32
0.00.050.104 I llm_load_print_meta: n_swa            = 0
0.00.050.104 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.104 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.107 I llm_load_print_meta: n_gqa            = 1
0.00.050.108 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.109 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.109 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.110 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.112 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.112 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.113 I llm_load_print_meta: n_ff             = 8192
0.00.050.113 I llm_load_print_meta: n_expert         = 0
0.00.050.113 I llm_load_print_meta: n_expert_used    = 0
0.00.050.113 I llm_load_print_meta: causal attn      = 1
0.00.050.113 I llm_load_print_meta: pooling type     = 0
0.00.050.114 I llm_load_print_meta: rope type        = 2
0.00.050.114 I llm_load_print_meta: rope scaling     = linear
0.00.050.114 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.115 I llm_load_print_meta: freq_scale_train = 1
0.00.050.115 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.115 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.115 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.115 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.116 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.116 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.116 I llm_load_print_meta: model type       = 1.4B
0.00.050.116 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.117 I llm_load_print_meta: model params     = 1.41 B
0.00.050.117 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.118 I llm_load_print_meta: general.name     = 1.4B
0.00.050.118 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.119 I llm_load_print_meta: LF token         = 128 ''
0.00.050.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.119 I llm_load_print_meta: max token length = 1024
0.00.052.111 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.111 I llm_load_tensors: offloading output layer to GPU
0.00.052.111 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.122 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.123 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.065 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.065 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.066 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.066 I llama_new_context_with_model: n_batch       = 2048
0.00.053.066 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.066 I llama_new_context_with_model: flash_attn    = 0
0.00.053.067 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.067 I llama_new_context_with_model: freq_scale    = 1
0.00.053.068 I ggml_metal_init: allocating
0.00.053.071 I ggml_metal_init: found device: Apple M4
0.00.053.073 I ggml_metal_init: picking default device: Apple M4
0.00.053.682 I ggml_metal_init: using embedded metal library
0.00.056.027 I ggml_metal_init: GPU name:   Apple M4
0.00.056.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.030 I ggml_metal_init: simdgroup reduction   = true
0.00.056.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.030 I ggml_metal_init: has bfloat            = true
0.00.056.031 I ggml_metal_init: use bfloat            = true
0.00.056.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.879 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.301 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.351 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.352 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.352 I llama_new_context_with_model: graph nodes  = 967
0.00.087.353 I llama_new_context_with_model: graph splits = 2
0.00.087.355 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.497 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.561 I main: llama threadpool init, n_threads = 4
0.00.738.601 I 
0.00.738.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.624 I 
0.00.738.850 I sampler seed: 1234
0.00.738.855 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.876 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.876 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.876 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.577.364 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.577.364 I llama_perf_context_print:        load time =     729.76 ms
0.01.577.365 I llama_perf_context_print: prompt eval time =      44.96 ms /     7 tokens (    6.42 ms per token,   155.71 tokens per second)
0.01.577.366 I llama_perf_context_print:        eval time =     790.35 ms /    63 runs   (   12.55 ms per token,    79.71 tokens per second)
0.01.577.366 I llama_perf_context_print:       total time =     838.80 ms /    70 tokens
0.01.577.592 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.220 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.597 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.041 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.043 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.043 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.044 I llama_model_loader: - type  f32:  194 tensors
0.00.023.044 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.044 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.613 I llm_load_vocab: special tokens cache size = 25
0.00.048.489 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.491 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.492 I llm_load_print_meta: arch             = gptneox
0.00.048.492 I llm_load_print_meta: vocab type       = BPE
0.00.048.492 I llm_load_print_meta: n_vocab          = 50304
0.00.048.493 I llm_load_print_meta: n_merges         = 50009
0.00.048.493 I llm_load_print_meta: vocab_only       = 0
0.00.048.493 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.493 I llm_load_print_meta: n_embd           = 2048
0.00.048.493 I llm_load_print_meta: n_layer          = 24
0.00.048.496 I llm_load_print_meta: n_head           = 16
0.00.048.497 I llm_load_print_meta: n_head_kv        = 16
0.00.048.498 I llm_load_print_meta: n_rot            = 32
0.00.048.498 I llm_load_print_meta: n_swa            = 0
0.00.048.498 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.498 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.499 I llm_load_print_meta: n_gqa            = 1
0.00.048.500 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.500 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.501 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.501 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.501 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.503 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.504 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.504 I llm_load_print_meta: n_ff             = 8192
0.00.048.504 I llm_load_print_meta: n_expert         = 0
0.00.048.504 I llm_load_print_meta: n_expert_used    = 0
0.00.048.505 I llm_load_print_meta: causal attn      = 1
0.00.048.505 I llm_load_print_meta: pooling type     = 0
0.00.048.505 I llm_load_print_meta: rope type        = 2
0.00.048.505 I llm_load_print_meta: rope scaling     = linear
0.00.048.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.507 I llm_load_print_meta: freq_scale_train = 1
0.00.048.507 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.508 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.508 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.508 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.508 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.508 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.509 I llm_load_print_meta: model type       = 1.4B
0.00.048.509 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.513 I llm_load_print_meta: model params     = 1.41 B
0.00.048.514 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.514 I llm_load_print_meta: general.name     = 1.4B
0.00.048.514 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.514 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.514 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.514 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.515 I llm_load_print_meta: LF token         = 128 ''
0.00.048.515 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.518 I llm_load_print_meta: max token length = 1024
0.00.050.475 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.475 I llm_load_tensors: offloading output layer to GPU
0.00.050.476 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.486 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.487 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.386 I llama_new_context_with_model: n_ctx         = 128
0.00.051.386 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.386 I llama_new_context_with_model: n_batch       = 128
0.00.051.386 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.387 I llama_new_context_with_model: flash_attn    = 0
0.00.051.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.387 I llama_new_context_with_model: freq_scale    = 1
0.00.051.388 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.388 I ggml_metal_init: allocating
0.00.051.391 I ggml_metal_init: found device: Apple M4
0.00.051.393 I ggml_metal_init: picking default device: Apple M4
0.00.051.932 I ggml_metal_init: using embedded metal library
0.00.054.233 I ggml_metal_init: GPU name:   Apple M4
0.00.054.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.234 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.235 I ggml_metal_init: simdgroup reduction   = true
0.00.054.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.235 I ggml_metal_init: has bfloat            = true
0.00.054.235 I ggml_metal_init: use bfloat            = true
0.00.054.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.542 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.794 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.797 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.692 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.693 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.693 I llama_new_context_with_model: graph nodes  = 967
0.00.065.694 I llama_new_context_with_model: graph splits = 2
0.00.065.695 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.124 I 
0.00.632.156 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.170 I perplexity: tokenizing the input ..
0.00.639.954 I perplexity: tokenization took 7.782 ms
0.00.639.957 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.211 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.776.440 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.776.454 I llama_perf_context_print:        load time =     622.90 ms
0.00.776.456 I llama_perf_context_print: prompt eval time =     135.02 ms /   128 tokens (    1.05 ms per token,   948.00 tokens per second)
0.00.776.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.457 I llama_perf_context_print:       total time =     144.33 ms /   129 tokens
0.00.776.808 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.076s
sys	0m0.110s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.037 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.917 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.919 I llama_model_loader: - type  f32:  194 tensors
0.00.023.919 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.920 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.842 I llm_load_vocab: special tokens cache size = 25
0.00.049.731 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.733 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.734 I llm_load_print_meta: arch             = gptneox
0.00.049.734 I llm_load_print_meta: vocab type       = BPE
0.00.049.734 I llm_load_print_meta: n_vocab          = 50304
0.00.049.735 I llm_load_print_meta: n_merges         = 50009
0.00.049.735 I llm_load_print_meta: vocab_only       = 0
0.00.049.735 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.735 I llm_load_print_meta: n_embd           = 2048
0.00.049.735 I llm_load_print_meta: n_layer          = 24
0.00.049.738 I llm_load_print_meta: n_head           = 16
0.00.049.739 I llm_load_print_meta: n_head_kv        = 16
0.00.049.739 I llm_load_print_meta: n_rot            = 32
0.00.049.739 I llm_load_print_meta: n_swa            = 0
0.00.049.739 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.739 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.740 I llm_load_print_meta: n_gqa            = 1
0.00.049.741 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.743 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.744 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.744 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.745 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.745 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.745 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.746 I llm_load_print_meta: n_ff             = 8192
0.00.049.746 I llm_load_print_meta: n_expert         = 0
0.00.049.746 I llm_load_print_meta: n_expert_used    = 0
0.00.049.746 I llm_load_print_meta: causal attn      = 1
0.00.049.746 I llm_load_print_meta: pooling type     = 0
0.00.049.747 I llm_load_print_meta: rope type        = 2
0.00.049.747 I llm_load_print_meta: rope scaling     = linear
0.00.049.748 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.748 I llm_load_print_meta: freq_scale_train = 1
0.00.049.748 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.749 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.749 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.749 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.749 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.751 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.751 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.751 I llm_load_print_meta: model type       = 1.4B
0.00.049.752 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.752 I llm_load_print_meta: model params     = 1.41 B
0.00.049.753 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.753 I llm_load_print_meta: general.name     = 1.4B
0.00.049.753 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.753 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.753 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.754 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.757 I llm_load_print_meta: LF token         = 128 ''
0.00.049.758 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.759 I llm_load_print_meta: max token length = 1024
0.00.051.611 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.612 I llm_load_tensors: offloading output layer to GPU
0.00.051.612 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.623 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.624 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.573 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.574 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.574 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.574 I llama_new_context_with_model: n_batch       = 2048
0.00.052.575 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.575 I llama_new_context_with_model: flash_attn    = 0
0.00.052.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.575 I llama_new_context_with_model: freq_scale    = 1
0.00.052.576 I ggml_metal_init: allocating
0.00.052.579 I ggml_metal_init: found device: Apple M4
0.00.052.581 I ggml_metal_init: picking default device: Apple M4
0.00.053.145 I ggml_metal_init: using embedded metal library
0.00.055.440 I ggml_metal_init: GPU name:   Apple M4
0.00.055.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.442 I ggml_metal_init: simdgroup reduction   = true
0.00.055.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.443 I ggml_metal_init: has bfloat            = true
0.00.055.443 I ggml_metal_init: use bfloat            = true
0.00.055.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.898 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.244 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.160 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.161 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.161 I llama_new_context_with_model: graph nodes  = 967
0.00.084.162 I llama_new_context_with_model: graph splits = 2
0.00.084.164 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.320 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.350 I main: llama threadpool init, n_threads = 4
0.00.496.396 I 
0.00.496.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.422 I 
0.00.496.657 I sampler seed: 1234
0.00.496.664 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.712 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.712 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.172.341 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.172.341 I llama_perf_context_print:        load time =     486.31 ms
0.01.172.342 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.49 tokens per second)
0.01.172.346 I llama_perf_context_print:        eval time =     636.89 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.172.346 I llama_perf_context_print:       total time =     676.00 ms /    70 tokens
0.01.172.573 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.794 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.583 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.352 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.179 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.180 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.181 I llama_model_loader: - type  f32:  194 tensors
0.00.025.182 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.182 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.182 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.896 I llm_load_vocab: special tokens cache size = 25
0.00.051.781 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.786 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.786 I llm_load_print_meta: arch             = gptneox
0.00.051.786 I llm_load_print_meta: vocab type       = BPE
0.00.051.787 I llm_load_print_meta: n_vocab          = 50304
0.00.051.788 I llm_load_print_meta: n_merges         = 50009
0.00.051.789 I llm_load_print_meta: vocab_only       = 0
0.00.051.789 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.789 I llm_load_print_meta: n_embd           = 2048
0.00.051.789 I llm_load_print_meta: n_layer          = 24
0.00.051.792 I llm_load_print_meta: n_head           = 16
0.00.051.795 I llm_load_print_meta: n_head_kv        = 16
0.00.051.795 I llm_load_print_meta: n_rot            = 32
0.00.051.795 I llm_load_print_meta: n_swa            = 0
0.00.051.795 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.795 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.796 I llm_load_print_meta: n_gqa            = 1
0.00.051.797 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.798 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.798 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.799 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.800 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.800 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.800 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.801 I llm_load_print_meta: n_ff             = 8192
0.00.051.801 I llm_load_print_meta: n_expert         = 0
0.00.051.801 I llm_load_print_meta: n_expert_used    = 0
0.00.051.801 I llm_load_print_meta: causal attn      = 1
0.00.051.801 I llm_load_print_meta: pooling type     = 0
0.00.051.802 I llm_load_print_meta: rope type        = 2
0.00.051.802 I llm_load_print_meta: rope scaling     = linear
0.00.051.802 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.803 I llm_load_print_meta: freq_scale_train = 1
0.00.051.803 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.807 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.807 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.808 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.808 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.808 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.809 I llm_load_print_meta: model type       = 1.4B
0.00.051.809 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.809 I llm_load_print_meta: model params     = 1.41 B
0.00.051.810 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.810 I llm_load_print_meta: general.name     = 1.4B
0.00.051.810 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.813 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.813 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.813 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.814 I llm_load_print_meta: LF token         = 128 ''
0.00.051.814 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.814 I llm_load_print_meta: max token length = 1024
0.00.053.739 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.739 I llm_load_tensors: offloading output layer to GPU
0.00.053.739 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.750 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.752 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.738 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.739 I llama_new_context_with_model: n_ctx         = 128
0.00.054.739 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.739 I llama_new_context_with_model: n_batch       = 128
0.00.054.739 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.739 I llama_new_context_with_model: flash_attn    = 0
0.00.054.740 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.740 I llama_new_context_with_model: freq_scale    = 1
0.00.054.740 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.741 I ggml_metal_init: allocating
0.00.054.744 I ggml_metal_init: found device: Apple M4
0.00.054.746 I ggml_metal_init: picking default device: Apple M4
0.00.055.311 I ggml_metal_init: using embedded metal library
0.00.057.637 I ggml_metal_init: GPU name:   Apple M4
0.00.057.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.639 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.639 I ggml_metal_init: simdgroup reduction   = true
0.00.057.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.640 I ggml_metal_init: has bfloat            = true
0.00.057.640 I ggml_metal_init: use bfloat            = true
0.00.057.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.248 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.494 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.499 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.515 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.405 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.406 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.407 I llama_new_context_with_model: graph nodes  = 967
0.00.069.407 I llama_new_context_with_model: graph splits = 2
0.00.069.408 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.375.686 I 
0.00.375.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.375.734 I perplexity: tokenizing the input ..
0.00.383.230 I perplexity: tokenization took 7.495 ms
0.00.383.233 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.515.785 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.516.987 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.517.004 I llama_perf_context_print:        load time =     364.89 ms
0.00.517.005 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.30 tokens per second)
0.00.517.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.517.006 I llama_perf_context_print:       total time =     141.32 ms /   129 tokens
0.00.517.509 I ggml_metal_free: deallocating

real	0m0.532s
user	0m0.078s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.141 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.559 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.092 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.093 I llama_model_loader: - type  f32:  194 tensors
0.00.025.093 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.093 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.094 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.910 I llm_load_vocab: special tokens cache size = 25
0.00.051.109 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.113 I llm_load_print_meta: arch             = gptneox
0.00.051.113 I llm_load_print_meta: vocab type       = BPE
0.00.051.113 I llm_load_print_meta: n_vocab          = 50304
0.00.051.113 I llm_load_print_meta: n_merges         = 50009
0.00.051.114 I llm_load_print_meta: vocab_only       = 0
0.00.051.114 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.114 I llm_load_print_meta: n_embd           = 2048
0.00.051.114 I llm_load_print_meta: n_layer          = 24
0.00.051.117 I llm_load_print_meta: n_head           = 16
0.00.051.117 I llm_load_print_meta: n_head_kv        = 16
0.00.051.118 I llm_load_print_meta: n_rot            = 32
0.00.051.118 I llm_load_print_meta: n_swa            = 0
0.00.051.118 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.118 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.119 I llm_load_print_meta: n_gqa            = 1
0.00.051.120 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.122 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.124 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.124 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.124 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.124 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.125 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.125 I llm_load_print_meta: n_ff             = 8192
0.00.051.126 I llm_load_print_meta: n_expert         = 0
0.00.051.126 I llm_load_print_meta: n_expert_used    = 0
0.00.051.126 I llm_load_print_meta: causal attn      = 1
0.00.051.126 I llm_load_print_meta: pooling type     = 0
0.00.051.126 I llm_load_print_meta: rope type        = 2
0.00.051.126 I llm_load_print_meta: rope scaling     = linear
0.00.051.127 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.127 I llm_load_print_meta: freq_scale_train = 1
0.00.051.127 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.128 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.128 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.128 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.128 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.128 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.129 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.129 I llm_load_print_meta: model type       = 1.4B
0.00.051.131 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.131 I llm_load_print_meta: model params     = 1.41 B
0.00.051.132 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.132 I llm_load_print_meta: general.name     = 1.4B
0.00.051.132 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.133 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: LF token         = 128 ''
0.00.051.134 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.135 I llm_load_print_meta: max token length = 1024
0.00.052.880 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.880 I llm_load_tensors: offloading output layer to GPU
0.00.052.880 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.886 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.888 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.780 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.780 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.780 I llama_new_context_with_model: n_batch       = 2048
0.00.053.781 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.781 I llama_new_context_with_model: flash_attn    = 0
0.00.053.782 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.783 I llama_new_context_with_model: freq_scale    = 1
0.00.053.783 I ggml_metal_init: allocating
0.00.053.791 I ggml_metal_init: found device: Apple M4
0.00.053.800 I ggml_metal_init: picking default device: Apple M4
0.00.054.393 I ggml_metal_init: using embedded metal library
0.00.056.983 I ggml_metal_init: GPU name:   Apple M4
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.985 I ggml_metal_init: simdgroup reduction   = true
0.00.056.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.985 I ggml_metal_init: has bfloat            = true
0.00.056.985 I ggml_metal_init: use bfloat            = true
0.00.056.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.351 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.161 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.169 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.190 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.166 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.167 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.168 I llama_new_context_with_model: graph nodes  = 967
0.00.087.168 I llama_new_context_with_model: graph splits = 2
0.00.087.174 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.807 I main: llama threadpool init, n_threads = 4
0.00.571.842 I 
0.00.571.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.862 I 
0.00.572.092 I sampler seed: 1234
0.00.572.095 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.572.106 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.572.107 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.572.107 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.322.830 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.322.831 I llama_perf_context_print:        load time =     561.66 ms
0.01.322.832 I llama_perf_context_print: prompt eval time =      40.40 ms /     7 tokens (    5.77 ms per token,   173.28 tokens per second)
0.01.322.832 I llama_perf_context_print:        eval time =     707.36 ms /    63 runs   (   11.23 ms per token,    89.06 tokens per second)
0.01.322.836 I llama_perf_context_print:       total time =     751.03 ms /    70 tokens
0.01.323.045 I ggml_metal_free: deallocating

real	0m1.339s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.773 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.415 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.416 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.837 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.837 I llama_model_loader: - type  f32:  194 tensors
0.00.022.838 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.838 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.838 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.838 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.427 I llm_load_vocab: special tokens cache size = 25
0.00.048.380 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.382 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.382 I llm_load_print_meta: arch             = gptneox
0.00.048.383 I llm_load_print_meta: vocab type       = BPE
0.00.048.383 I llm_load_print_meta: n_vocab          = 50304
0.00.048.383 I llm_load_print_meta: n_merges         = 50009
0.00.048.383 I llm_load_print_meta: vocab_only       = 0
0.00.048.383 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.383 I llm_load_print_meta: n_embd           = 2048
0.00.048.384 I llm_load_print_meta: n_layer          = 24
0.00.048.386 I llm_load_print_meta: n_head           = 16
0.00.048.387 I llm_load_print_meta: n_head_kv        = 16
0.00.048.387 I llm_load_print_meta: n_rot            = 32
0.00.048.387 I llm_load_print_meta: n_swa            = 0
0.00.048.388 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.388 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.388 I llm_load_print_meta: n_gqa            = 1
0.00.048.389 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.390 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.390 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.391 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.391 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.391 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.391 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.392 I llm_load_print_meta: n_ff             = 8192
0.00.048.392 I llm_load_print_meta: n_expert         = 0
0.00.048.392 I llm_load_print_meta: n_expert_used    = 0
0.00.048.392 I llm_load_print_meta: causal attn      = 1
0.00.048.393 I llm_load_print_meta: pooling type     = 0
0.00.048.393 I llm_load_print_meta: rope type        = 2
0.00.048.393 I llm_load_print_meta: rope scaling     = linear
0.00.048.393 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.394 I llm_load_print_meta: freq_scale_train = 1
0.00.048.394 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.394 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.394 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.394 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.395 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.395 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.395 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.395 I llm_load_print_meta: model type       = 1.4B
0.00.048.396 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.396 I llm_load_print_meta: model params     = 1.41 B
0.00.048.397 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.397 I llm_load_print_meta: general.name     = 1.4B
0.00.048.397 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.397 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.397 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.398 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.398 I llm_load_print_meta: LF token         = 128 ''
0.00.048.398 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.398 I llm_load_print_meta: max token length = 1024
0.00.050.224 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.224 I llm_load_tensors: offloading output layer to GPU
0.00.050.225 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.235 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.236 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.144 I llama_new_context_with_model: n_ctx         = 128
0.00.051.144 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.145 I llama_new_context_with_model: n_batch       = 128
0.00.051.145 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.145 I llama_new_context_with_model: flash_attn    = 0
0.00.051.145 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.145 I llama_new_context_with_model: freq_scale    = 1
0.00.051.146 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.146 I ggml_metal_init: allocating
0.00.051.150 I ggml_metal_init: found device: Apple M4
0.00.051.152 I ggml_metal_init: picking default device: Apple M4
0.00.051.708 I ggml_metal_init: using embedded metal library
0.00.053.989 I ggml_metal_init: GPU name:   Apple M4
0.00.053.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.992 I ggml_metal_init: simdgroup reduction   = true
0.00.053.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.992 I ggml_metal_init: has bfloat            = true
0.00.053.992 I ggml_metal_init: use bfloat            = true
0.00.053.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.570 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.574 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.588 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.509 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.510 I llama_new_context_with_model: graph nodes  = 967
0.00.065.510 I llama_new_context_with_model: graph splits = 2
0.00.065.511 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.511 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.469.084 I 
0.00.469.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.469.193 I perplexity: tokenizing the input ..
0.00.477.410 I perplexity: tokenization took 8.216 ms
0.00.477.418 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.609.820 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.611.063 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.611.081 I llama_perf_context_print:        load time =     460.30 ms
0.00.611.082 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.41 tokens per second)
0.00.611.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.083 I llama_perf_context_print:       total time =     142.00 ms /   129 tokens
0.00.611.587 I ggml_metal_free: deallocating

real	0m0.626s
user	0m0.077s
sys	0m0.080s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.135 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.891 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.905 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.905 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.905 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.535 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.536 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.536 I llama_model_loader: - type  f32:  194 tensors
0.00.024.537 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.537 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.537 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.220 I llm_load_vocab: special tokens cache size = 25
0.00.051.249 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.251 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.252 I llm_load_print_meta: arch             = gptneox
0.00.051.252 I llm_load_print_meta: vocab type       = BPE
0.00.051.253 I llm_load_print_meta: n_vocab          = 50304
0.00.051.253 I llm_load_print_meta: n_merges         = 50009
0.00.051.253 I llm_load_print_meta: vocab_only       = 0
0.00.051.253 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.253 I llm_load_print_meta: n_embd           = 2048
0.00.051.253 I llm_load_print_meta: n_layer          = 24
0.00.051.256 I llm_load_print_meta: n_head           = 16
0.00.051.259 I llm_load_print_meta: n_head_kv        = 16
0.00.051.259 I llm_load_print_meta: n_rot            = 32
0.00.051.259 I llm_load_print_meta: n_swa            = 0
0.00.051.260 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.260 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.260 I llm_load_print_meta: n_gqa            = 1
0.00.051.261 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.262 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.262 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.263 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.263 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.263 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.263 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.264 I llm_load_print_meta: n_ff             = 8192
0.00.051.269 I llm_load_print_meta: n_expert         = 0
0.00.051.271 I llm_load_print_meta: n_expert_used    = 0
0.00.051.273 I llm_load_print_meta: causal attn      = 1
0.00.051.273 I llm_load_print_meta: pooling type     = 0
0.00.051.273 I llm_load_print_meta: rope type        = 2
0.00.051.273 I llm_load_print_meta: rope scaling     = linear
0.00.051.274 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.275 I llm_load_print_meta: freq_scale_train = 1
0.00.051.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.276 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.277 I llm_load_print_meta: model type       = 1.4B
0.00.051.278 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.278 I llm_load_print_meta: model params     = 1.41 B
0.00.051.279 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.279 I llm_load_print_meta: general.name     = 1.4B
0.00.051.279 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.280 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.280 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.280 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.280 I llm_load_print_meta: LF token         = 128 ''
0.00.051.283 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.283 I llm_load_print_meta: max token length = 1024
0.00.052.888 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.889 I llm_load_tensors: offloading output layer to GPU
0.00.052.889 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.899 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.900 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.725 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.726 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.726 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.726 I llama_new_context_with_model: n_batch       = 2048
0.00.053.727 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.727 I llama_new_context_with_model: flash_attn    = 0
0.00.053.727 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.727 I llama_new_context_with_model: freq_scale    = 1
0.00.053.728 I ggml_metal_init: allocating
0.00.053.734 I ggml_metal_init: found device: Apple M4
0.00.053.736 I ggml_metal_init: picking default device: Apple M4
0.00.054.351 I ggml_metal_init: using embedded metal library
0.00.056.660 I ggml_metal_init: GPU name:   Apple M4
0.00.056.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.664 I ggml_metal_init: simdgroup reduction   = true
0.00.056.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.664 I ggml_metal_init: has bfloat            = true
0.00.056.664 I ggml_metal_init: use bfloat            = true
0.00.056.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.042 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.480 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.489 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.509 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.424 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.425 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.425 I llama_new_context_with_model: graph nodes  = 967
0.00.086.425 I llama_new_context_with_model: graph splits = 2
0.00.086.428 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.100 I main: llama threadpool init, n_threads = 4
0.00.638.146 I 
0.00.638.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.167 I 
0.00.638.343 I sampler seed: 1234
0.00.638.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.638.370 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.638.372 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.638.372 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.429.470 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.429.470 I llama_perf_context_print:        load time =     628.96 ms
0.01.429.471 I llama_perf_context_print: prompt eval time =      47.20 ms /     7 tokens (    6.74 ms per token,   148.31 tokens per second)
0.01.429.472 I llama_perf_context_print:        eval time =     740.71 ms /    63 runs   (   11.76 ms per token,    85.05 tokens per second)
0.01.429.472 I llama_perf_context_print:       total time =     791.37 ms /    70 tokens
0.01.429.682 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.110s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.196 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.599 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.307 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.096 I llama_model_loader: - type  f32:  194 tensors
0.00.024.097 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.097 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.097 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.765 I llm_load_vocab: special tokens cache size = 25
0.00.050.719 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.722 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.722 I llm_load_print_meta: arch             = gptneox
0.00.050.722 I llm_load_print_meta: vocab type       = BPE
0.00.050.723 I llm_load_print_meta: n_vocab          = 50304
0.00.050.723 I llm_load_print_meta: n_merges         = 50009
0.00.050.723 I llm_load_print_meta: vocab_only       = 0
0.00.050.723 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.723 I llm_load_print_meta: n_embd           = 2048
0.00.050.724 I llm_load_print_meta: n_layer          = 24
0.00.050.727 I llm_load_print_meta: n_head           = 16
0.00.050.727 I llm_load_print_meta: n_head_kv        = 16
0.00.050.728 I llm_load_print_meta: n_rot            = 32
0.00.050.728 I llm_load_print_meta: n_swa            = 0
0.00.050.729 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.729 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.730 I llm_load_print_meta: n_gqa            = 1
0.00.050.730 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.731 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.732 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.732 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.734 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.734 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.734 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.735 I llm_load_print_meta: n_ff             = 8192
0.00.050.735 I llm_load_print_meta: n_expert         = 0
0.00.050.735 I llm_load_print_meta: n_expert_used    = 0
0.00.050.735 I llm_load_print_meta: causal attn      = 1
0.00.050.736 I llm_load_print_meta: pooling type     = 0
0.00.050.736 I llm_load_print_meta: rope type        = 2
0.00.050.737 I llm_load_print_meta: rope scaling     = linear
0.00.050.737 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.737 I llm_load_print_meta: freq_scale_train = 1
0.00.050.738 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.738 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.738 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.738 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.738 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.738 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.739 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.739 I llm_load_print_meta: model type       = 1.4B
0.00.050.740 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.740 I llm_load_print_meta: model params     = 1.41 B
0.00.050.741 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.741 I llm_load_print_meta: general.name     = 1.4B
0.00.050.741 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.741 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.742 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.743 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.743 I llm_load_print_meta: LF token         = 128 ''
0.00.050.743 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.743 I llm_load_print_meta: max token length = 1024
0.00.052.721 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.721 I llm_load_tensors: offloading output layer to GPU
0.00.052.721 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.732 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.733 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.640 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.641 I llama_new_context_with_model: n_ctx         = 128
0.00.053.641 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.641 I llama_new_context_with_model: n_batch       = 128
0.00.053.641 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.641 I llama_new_context_with_model: flash_attn    = 0
0.00.053.642 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.642 I llama_new_context_with_model: freq_scale    = 1
0.00.053.642 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.643 I ggml_metal_init: allocating
0.00.053.646 I ggml_metal_init: found device: Apple M4
0.00.053.648 I ggml_metal_init: picking default device: Apple M4
0.00.054.203 I ggml_metal_init: using embedded metal library
0.00.056.534 I ggml_metal_init: GPU name:   Apple M4
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.536 I ggml_metal_init: simdgroup reduction   = true
0.00.056.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.536 I ggml_metal_init: has bfloat            = true
0.00.056.537 I ggml_metal_init: use bfloat            = true
0.00.056.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.166 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.439 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.442 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.457 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.337 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.338 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.339 I llama_new_context_with_model: graph nodes  = 967
0.00.068.339 I llama_new_context_with_model: graph splits = 2
0.00.068.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.648 I 
0.00.549.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.686 I perplexity: tokenizing the input ..
0.00.557.591 I perplexity: tokenization took 7.904 ms
0.00.557.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.691.387 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.692.539 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.692.555 I llama_perf_context_print:        load time =     540.45 ms
0.00.692.555 I llama_perf_context_print: prompt eval time =     133.55 ms /   128 tokens (    1.04 ms per token,   958.46 tokens per second)
0.00.692.556 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.560 I llama_perf_context_print:       total time =     142.91 ms /   129 tokens
0.00.692.989 I ggml_metal_free: deallocating

real	0m0.706s
user	0m0.078s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.153 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.696 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.697 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.697 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.698 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.698 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.698 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.699 I llama_model_loader: - type  f32:  194 tensors
0.00.025.699 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.699 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.355 I llm_load_vocab: special tokens cache size = 25
0.00.052.509 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.511 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.512 I llm_load_print_meta: arch             = gptneox
0.00.052.512 I llm_load_print_meta: vocab type       = BPE
0.00.052.512 I llm_load_print_meta: n_vocab          = 50304
0.00.052.512 I llm_load_print_meta: n_merges         = 50009
0.00.052.513 I llm_load_print_meta: vocab_only       = 0
0.00.052.513 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.513 I llm_load_print_meta: n_embd           = 2048
0.00.052.513 I llm_load_print_meta: n_layer          = 24
0.00.052.516 I llm_load_print_meta: n_head           = 16
0.00.052.519 I llm_load_print_meta: n_head_kv        = 16
0.00.052.519 I llm_load_print_meta: n_rot            = 32
0.00.052.519 I llm_load_print_meta: n_swa            = 0
0.00.052.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.519 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.520 I llm_load_print_meta: n_gqa            = 1
0.00.052.526 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.527 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.527 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.528 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.528 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.528 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.528 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.529 I llm_load_print_meta: n_ff             = 8192
0.00.052.529 I llm_load_print_meta: n_expert         = 0
0.00.052.529 I llm_load_print_meta: n_expert_used    = 0
0.00.052.531 I llm_load_print_meta: causal attn      = 1
0.00.052.532 I llm_load_print_meta: pooling type     = 0
0.00.052.532 I llm_load_print_meta: rope type        = 2
0.00.052.533 I llm_load_print_meta: rope scaling     = linear
0.00.052.533 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.533 I llm_load_print_meta: freq_scale_train = 1
0.00.052.534 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.534 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.534 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.534 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.534 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.534 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.535 I llm_load_print_meta: model type       = 1.4B
0.00.052.536 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.536 I llm_load_print_meta: model params     = 1.41 B
0.00.052.536 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.537 I llm_load_print_meta: general.name     = 1.4B
0.00.052.537 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.537 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.537 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.537 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.538 I llm_load_print_meta: LF token         = 128 ''
0.00.052.538 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.538 I llm_load_print_meta: max token length = 1024
0.00.054.212 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.212 I llm_load_tensors: offloading output layer to GPU
0.00.054.212 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.222 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.223 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.067 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.068 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.068 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.068 I llama_new_context_with_model: n_batch       = 2048
0.00.055.069 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.069 I llama_new_context_with_model: flash_attn    = 0
0.00.055.069 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.069 I llama_new_context_with_model: freq_scale    = 1
0.00.055.070 I ggml_metal_init: allocating
0.00.055.074 I ggml_metal_init: found device: Apple M4
0.00.055.076 I ggml_metal_init: picking default device: Apple M4
0.00.055.677 I ggml_metal_init: using embedded metal library
0.00.057.997 I ggml_metal_init: GPU name:   Apple M4
0.00.057.998 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.001 I ggml_metal_init: simdgroup reduction   = true
0.00.058.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.001 I ggml_metal_init: has bfloat            = true
0.00.058.001 I ggml_metal_init: use bfloat            = true
0.00.058.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.002 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.276 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.282 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.302 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.374 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.376 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.376 I llama_new_context_with_model: graph nodes  = 967
0.00.088.376 I llama_new_context_with_model: graph splits = 2
0.00.088.379 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.836 I main: llama threadpool init, n_threads = 4
0.00.697.883 I 
0.00.697.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.903 I 
0.00.698.071 I sampler seed: 1234
0.00.698.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.095 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.096 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.568.685 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.568.686 I llama_perf_context_print:        load time =     687.48 ms
0.01.568.686 I llama_perf_context_print: prompt eval time =      51.64 ms /     7 tokens (    7.38 ms per token,   135.55 tokens per second)
0.01.568.687 I llama_perf_context_print:        eval time =     815.94 ms /    63 runs   (   12.95 ms per token,    77.21 tokens per second)
0.01.568.687 I llama_perf_context_print:       total time =     870.85 ms /    70 tokens
0.01.568.916 I ggml_metal_free: deallocating

real	0m1.586s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.067 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.531 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.541 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.541 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.064 I llama_model_loader: - type  f32:  194 tensors
0.00.024.064 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.064 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.573 I llm_load_vocab: special tokens cache size = 25
0.00.050.501 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.504 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.504 I llm_load_print_meta: arch             = gptneox
0.00.050.505 I llm_load_print_meta: vocab type       = BPE
0.00.050.505 I llm_load_print_meta: n_vocab          = 50304
0.00.050.505 I llm_load_print_meta: n_merges         = 50009
0.00.050.505 I llm_load_print_meta: vocab_only       = 0
0.00.050.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.505 I llm_load_print_meta: n_embd           = 2048
0.00.050.506 I llm_load_print_meta: n_layer          = 24
0.00.050.509 I llm_load_print_meta: n_head           = 16
0.00.050.509 I llm_load_print_meta: n_head_kv        = 16
0.00.050.510 I llm_load_print_meta: n_rot            = 32
0.00.050.510 I llm_load_print_meta: n_swa            = 0
0.00.050.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.510 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.511 I llm_load_print_meta: n_gqa            = 1
0.00.050.512 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.513 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.515 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.515 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.516 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.516 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.516 I llm_load_print_meta: n_ff             = 8192
0.00.050.516 I llm_load_print_meta: n_expert         = 0
0.00.050.518 I llm_load_print_meta: n_expert_used    = 0
0.00.050.518 I llm_load_print_meta: causal attn      = 1
0.00.050.518 I llm_load_print_meta: pooling type     = 0
0.00.050.518 I llm_load_print_meta: rope type        = 2
0.00.050.518 I llm_load_print_meta: rope scaling     = linear
0.00.050.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.519 I llm_load_print_meta: freq_scale_train = 1
0.00.050.519 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.520 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.520 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.520 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.525 I llm_load_print_meta: model type       = 1.4B
0.00.050.525 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.526 I llm_load_print_meta: model params     = 1.41 B
0.00.050.526 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.526 I llm_load_print_meta: general.name     = 1.4B
0.00.050.527 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: LF token         = 128 ''
0.00.050.528 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: max token length = 1024
0.00.052.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I llm_load_tensors: offloading output layer to GPU
0.00.052.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.505 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.506 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.389 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.390 I llama_new_context_with_model: n_ctx         = 128
0.00.053.390 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.390 I llama_new_context_with_model: n_batch       = 128
0.00.053.390 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.390 I llama_new_context_with_model: flash_attn    = 0
0.00.053.391 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.391 I llama_new_context_with_model: freq_scale    = 1
0.00.053.391 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.392 I ggml_metal_init: allocating
0.00.053.395 I ggml_metal_init: found device: Apple M4
0.00.053.397 I ggml_metal_init: picking default device: Apple M4
0.00.053.964 I ggml_metal_init: using embedded metal library
0.00.056.265 I ggml_metal_init: GPU name:   Apple M4
0.00.056.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.267 I ggml_metal_init: simdgroup reduction   = true
0.00.056.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.268 I ggml_metal_init: has bfloat            = true
0.00.056.268 I ggml_metal_init: use bfloat            = true
0.00.056.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.802 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.158 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.172 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.057 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.058 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.059 I llama_new_context_with_model: graph nodes  = 967
0.00.068.059 I llama_new_context_with_model: graph splits = 2
0.00.068.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.156 I 
0.00.441.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.218 I perplexity: tokenizing the input ..
0.00.448.880 I perplexity: tokenization took 7.661 ms
0.00.448.884 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.590.045 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.591.287 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.591.304 I llama_perf_context_print:        load time =     431.08 ms
0.00.591.305 I llama_perf_context_print: prompt eval time =     140.93 ms /   128 tokens (    1.10 ms per token,   908.27 tokens per second)
0.00.591.313 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.591.314 I llama_perf_context_print:       total time =     150.15 ms /   129 tokens
0.00.591.808 I ggml_metal_free: deallocating

real	0m0.608s
user	0m0.077s
sys	0m0.086s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.321 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.330 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.331 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.333 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.064 I llama_model_loader: - type  f32:  194 tensors
0.00.025.064 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.919 I llm_load_vocab: special tokens cache size = 25
0.00.051.722 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.724 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.725 I llm_load_print_meta: arch             = gptneox
0.00.051.725 I llm_load_print_meta: vocab type       = BPE
0.00.051.726 I llm_load_print_meta: n_vocab          = 50304
0.00.051.726 I llm_load_print_meta: n_merges         = 50009
0.00.051.726 I llm_load_print_meta: vocab_only       = 0
0.00.051.726 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.726 I llm_load_print_meta: n_embd           = 2048
0.00.051.726 I llm_load_print_meta: n_layer          = 24
0.00.051.729 I llm_load_print_meta: n_head           = 16
0.00.051.730 I llm_load_print_meta: n_head_kv        = 16
0.00.051.730 I llm_load_print_meta: n_rot            = 32
0.00.051.731 I llm_load_print_meta: n_swa            = 0
0.00.051.731 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.731 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.732 I llm_load_print_meta: n_gqa            = 1
0.00.051.732 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.733 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.734 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.734 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.734 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.735 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.735 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.735 I llm_load_print_meta: n_ff             = 8192
0.00.051.736 I llm_load_print_meta: n_expert         = 0
0.00.051.736 I llm_load_print_meta: n_expert_used    = 0
0.00.051.736 I llm_load_print_meta: causal attn      = 1
0.00.051.738 I llm_load_print_meta: pooling type     = 0
0.00.051.739 I llm_load_print_meta: rope type        = 2
0.00.051.739 I llm_load_print_meta: rope scaling     = linear
0.00.051.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.740 I llm_load_print_meta: freq_scale_train = 1
0.00.051.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.740 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.740 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.741 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.741 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.741 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.741 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.741 I llm_load_print_meta: model type       = 1.4B
0.00.051.742 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.742 I llm_load_print_meta: model params     = 1.41 B
0.00.051.747 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.747 I llm_load_print_meta: general.name     = 1.4B
0.00.051.747 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.748 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.748 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.749 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.749 I llm_load_print_meta: LF token         = 128 ''
0.00.051.749 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.749 I llm_load_print_meta: max token length = 1024
0.00.053.813 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.813 I llm_load_tensors: offloading output layer to GPU
0.00.053.813 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.823 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.825 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.727 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.728 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.728 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.728 I llama_new_context_with_model: n_batch       = 2048
0.00.054.728 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.728 I llama_new_context_with_model: flash_attn    = 0
0.00.054.729 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.729 I llama_new_context_with_model: freq_scale    = 1
0.00.054.730 I ggml_metal_init: allocating
0.00.054.733 I ggml_metal_init: found device: Apple M4
0.00.054.735 I ggml_metal_init: picking default device: Apple M4
0.00.055.345 I ggml_metal_init: using embedded metal library
0.00.057.692 I ggml_metal_init: GPU name:   Apple M4
0.00.057.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.694 I ggml_metal_init: simdgroup reduction   = true
0.00.057.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.696 I ggml_metal_init: has bfloat            = true
0.00.057.696 I ggml_metal_init: use bfloat            = true
0.00.057.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.510 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.976 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.981 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.999 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.058 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.059 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.060 I llama_new_context_with_model: graph nodes  = 967
0.00.088.060 I llama_new_context_with_model: graph splits = 2
0.00.088.063 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.161 I main: llama threadpool init, n_threads = 4
0.00.740.197 I 
0.00.740.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.218 I 
0.00.740.468 I sampler seed: 1234
0.00.740.472 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.513 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.517 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.517 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.624.296 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.624.297 I llama_perf_context_print:        load time =     731.41 ms
0.01.624.298 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.53 tokens per second)
0.01.624.299 I llama_perf_context_print:        eval time =     826.51 ms /    63 runs   (   13.12 ms per token,    76.22 tokens per second)
0.01.624.299 I llama_perf_context_print:       total time =     884.14 ms /    70 tokens
0.01.624.585 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4411 (c31fc8b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.507 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.278 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.988 I llama_model_loader: - type  f32:  194 tensors
0.00.022.988 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.715 I llm_load_vocab: special tokens cache size = 25
0.00.048.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.627 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.628 I llm_load_print_meta: arch             = gptneox
0.00.048.628 I llm_load_print_meta: vocab type       = BPE
0.00.048.628 I llm_load_print_meta: n_vocab          = 50304
0.00.048.629 I llm_load_print_meta: n_merges         = 50009
0.00.048.629 I llm_load_print_meta: vocab_only       = 0
0.00.048.629 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.629 I llm_load_print_meta: n_embd           = 2048
0.00.048.629 I llm_load_print_meta: n_layer          = 24
0.00.048.632 I llm_load_print_meta: n_head           = 16
0.00.048.633 I llm_load_print_meta: n_head_kv        = 16
0.00.048.633 I llm_load_print_meta: n_rot            = 32
0.00.048.634 I llm_load_print_meta: n_swa            = 0
0.00.048.636 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.636 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.637 I llm_load_print_meta: n_gqa            = 1
0.00.048.638 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.638 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.639 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.639 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.639 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.640 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.640 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.640 I llm_load_print_meta: n_ff             = 8192
0.00.048.641 I llm_load_print_meta: n_expert         = 0
0.00.048.641 I llm_load_print_meta: n_expert_used    = 0
0.00.048.641 I llm_load_print_meta: causal attn      = 1
0.00.048.641 I llm_load_print_meta: pooling type     = 0
0.00.048.641 I llm_load_print_meta: rope type        = 2
0.00.048.641 I llm_load_print_meta: rope scaling     = linear
0.00.048.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.644 I llm_load_print_meta: freq_scale_train = 1
0.00.048.644 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.644 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.644 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.644 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.644 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.645 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.645 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.645 I llm_load_print_meta: model type       = 1.4B
0.00.048.645 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.646 I llm_load_print_meta: model params     = 1.41 B
0.00.048.650 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.650 I llm_load_print_meta: general.name     = 1.4B
0.00.048.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: LF token         = 128 ''
0.00.048.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.652 I llm_load_print_meta: max token length = 1024
0.00.050.650 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.650 I llm_load_tensors: offloading output layer to GPU
0.00.050.650 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.661 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.662 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.604 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.605 I llama_new_context_with_model: n_ctx         = 128
0.00.051.605 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.606 I llama_new_context_with_model: n_batch       = 128
0.00.051.606 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.606 I llama_new_context_with_model: flash_attn    = 0
0.00.051.606 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.607 I llama_new_context_with_model: freq_scale    = 1
0.00.051.607 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.607 I ggml_metal_init: allocating
0.00.051.611 I ggml_metal_init: found device: Apple M4
0.00.051.612 I ggml_metal_init: picking default device: Apple M4
0.00.052.172 I ggml_metal_init: using embedded metal library
0.00.054.455 I ggml_metal_init: GPU name:   Apple M4
0.00.054.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.457 I ggml_metal_init: simdgroup reduction   = true
0.00.054.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.457 I ggml_metal_init: has bfloat            = true
0.00.054.458 I ggml_metal_init: use bfloat            = true
0.00.054.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.793 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.093 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.096 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.112 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.010 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.011 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.012 I llama_new_context_with_model: graph nodes  = 967
0.00.066.012 I llama_new_context_with_model: graph splits = 2
0.00.066.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.145 I 
0.00.637.188 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.200 I perplexity: tokenizing the input ..
0.00.644.386 I perplexity: tokenization took 7.184 ms
0.00.644.389 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.587 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.784.922 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.784.936 I llama_perf_context_print:        load time =     628.25 ms
0.00.784.937 I llama_perf_context_print: prompt eval time =     138.97 ms /   128 tokens (    1.09 ms per token,   921.04 tokens per second)
0.00.784.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.938 I llama_perf_context_print:       total time =     147.79 ms /   129 tokens
0.00.785.286 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.077s
sys	0m0.103s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4411 (c31fc8b9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a0a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a0b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a0baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a0c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a0cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a0d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a0db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a0e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a0fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a10260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a12dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a16950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134a1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134a1f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134a1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134a1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134a201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134a20490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134a20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134a20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134a21270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134a21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134a21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134a22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134a224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134a22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134a22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134a232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134a23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134a23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134a24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134a246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134a24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134a25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134a256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134a25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134a26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134a26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134a26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134a27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134a27680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134a27bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134a28120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134a28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134a28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134a29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134a29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134a29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134a2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134a2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134a2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134a2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134a2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134a2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134a1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134a2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134a2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134a2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134a2d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134a2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134a2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134a2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134a2e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134a2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134a2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134a2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134a2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134a30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134a30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134a30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134a31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134a31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134a31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134a31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134a323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134a32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134a32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134a331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134a33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134a33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134a33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134a34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134a348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134a34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134a35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134a356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134a35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134a36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134a364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134a36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134a36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134a37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134a37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134a37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134a38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134a38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134a389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134a38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134a392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134a39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134a39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134a3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134a3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134a3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134a3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134a3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134a3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134a3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134a3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134a3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134a3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134a3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134a3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134a3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134a3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134a3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134a3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134a3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134a3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134a3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134a3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134a401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134a40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134a40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134a40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134a41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134a41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134a41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134a42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134a426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134a42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134a43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134a434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134a43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134a43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134a442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134a44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134a44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134a45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134a45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134a459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134a45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134a46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134a467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134a46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134a470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134a47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134a47a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134a47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134a48410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134a48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134a48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134a49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134a496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134a49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134a4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134a4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134a4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134a4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134a4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134a4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134a4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134a4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134a4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134a4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134a4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134a4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134a4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134a4ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134a4f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134a4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134a4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134a501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134a50710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134a50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134a511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134a51700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134a51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134a521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134a526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134a52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134a53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134a536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134a53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134a54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134a546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134a54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134a55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134a556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134a55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134a56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134a566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134a56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134a57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134a576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134a57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134a58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134a58690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134a58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134a59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134a59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134a59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134a5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134a5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134a5abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134a5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134a5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134a5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134a5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134a5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134a5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134a5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134a5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134a5db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134a5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134a5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134a5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134a5f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134a5f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134a5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134a600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134a60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134a60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134a61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134a614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134a61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134a61de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134a62280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134a62720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134a62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134a63060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134a63500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134a639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134a63e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134a642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134a64780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134a64c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134a650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134a65610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134a65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134a66450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134a66b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134a67290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134a67550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134a67d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134a68000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134a68610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.158.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a49980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a1d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a1f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a1be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a1c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a1a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a2c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a67810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a16c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a15040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a68a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a68d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a68ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a69570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a69af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a69db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a6a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a6a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a6a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a6a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a6ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a6ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a6b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a6b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a6b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a6b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a6bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a6beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a6c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a6c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a6c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a6c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a6cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a6cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a6d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a6d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134a6d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134a6da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134a6dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134a6dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134a6e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134a6e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134a6e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134a6eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134a6ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134a6f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134a6f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134a6f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134a6f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134a6fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134a6fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134a700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134a70370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134a70630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134a708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134a70bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134a70e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134a71130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134a713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134a716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134a71970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134a71c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134a71ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134a721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134a72470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134a72730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134a729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134a72cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134a72f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134a73230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134a734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134a737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134a73a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134a73d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134a73ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134a742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134a74570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134a74830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134a74af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134a74db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134a75070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134a75330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134a755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134a758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134a75b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134a75e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134a760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134a763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134a76670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134a76930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134a76bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134a76eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134a77170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134a77430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134a776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134a779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134a77c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134a77f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134a781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134a784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134a78770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134a78a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134a78cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134a78fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134a79270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134a79530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134a797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134a79ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134a79d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134a7a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134a7a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134a7a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134a7a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134a7ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134a7adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134a7b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134a7b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134a7b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134a7b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134a7bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134a7be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134a7c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134a7c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134a7c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134a7c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134a7cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134a7cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134a7d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134a7d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134a7d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134a7d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134a7dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134a7df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134a7e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134a7e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134a7e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134a7ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134a7ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134a7eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134a7f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134a7f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134a7f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134a7faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134a7fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134a80070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134a80330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134a805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134a808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134a80b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134a80e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134a810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134a813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134a81670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134a81930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134a81bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134a81eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134a82170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134a82430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134a826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134a829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134a82c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134a82f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134a831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134a834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134a83770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134a83a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134a83cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134a83fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134a84270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134a84530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134a847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134a84ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134a84d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134a85030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134a852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134a855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134a85870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134a85b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134a85df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134a860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134a86370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134a86630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134a868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134a86bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134a86e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134a87130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134a873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134a876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134a87970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134a87c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134a87ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134a884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134a88780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134a88a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134a88d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134a88fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134a89280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134a89540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134a89800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134a89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134a89d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134a8a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134a8a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134a8a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134a8a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134a8ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134a8ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134a8b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134a8b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134a8b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134a8b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134a8bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134a8be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134a8c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134a8c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134a8c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134a8c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134a8cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134a8cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134a8d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134a8d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134a8d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134a8dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134a8e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134a8e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134a8ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134a8f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134a8f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134a8fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134a901c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134a90710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134a90c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134a911b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134a91700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134a91c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134a921a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134a926f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134a92c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134a93190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134a936e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134a93c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134a94180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134a946d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134a94c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134a95170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134a956c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134a95c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134a96160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134a96420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134a966e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134a96be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134a970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134a975e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134a97ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134a97fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134a984e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134a989e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134a98ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134a993e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134a998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134a99de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134a9a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134a9a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134a9ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134a9b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134a9be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134a9c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134a9cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134a9cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134a9d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134a9d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134a9dfd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a9dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a9d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a9e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a9e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a9e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a9ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a9ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a9f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a9f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a9f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a9fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134aa0000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134aa05d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134aa0c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134aa0ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134aa1180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134aa1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134aa1700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134aa19c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134aa1c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134aa1f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134aa2200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134aa24c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134aa2780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134aa2a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134aa2d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134aa2fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134aa3280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134aa3540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134aa3800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134aa3ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134aa3d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134aa4040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134aa4300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134aa45c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134aa4880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134aa4b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134aa4e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134aa50c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134aa5380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134aa5640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134aa5900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134aa5bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134aa5e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134aa6140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134aa6400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134aa66c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134aa6980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134aa6c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134aa6f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134aa71c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134aa7480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134aa7740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134aa7a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134aa7cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134aa7f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134aa8240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134aa8500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134aa87c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134aa8a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134aa8d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134aa9000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134aa92c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134aa9580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134aa9840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134aa9b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134aa9dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134aaa080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134aaa340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134aaa600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134aaa8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134aaab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134aaae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134aab100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134aab3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134aab680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134aab940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134aabc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134aabec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134aac180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134aac440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134aac700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134aac9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134aacc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134aacf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134aad200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134aad4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134aad780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134aada40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134aadd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134aadfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134aae280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134aae540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134aae800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134aaeac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134aaed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134aaf040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134aaf300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134aaf5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134aaf880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134aafb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134aafe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134ab00c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134ab0380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134ab0640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134ab0900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134ab0bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134ab0e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134ab1140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134ab1400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134ab16c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134ab1980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134ab1c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134ab1f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134ab21c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134ab2480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134ab2740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134ab2a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134ab2cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134ab2f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134ab3240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134ab3500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134ab37c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134ab3a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134ab3d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134ab4000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134ab42c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134ab4580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134ab4840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134ab4b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134ab4dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134ab5080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134ab5340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134ab5600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134ab58c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134ab5b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134ab5e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134ab6100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134ab63c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134ab6680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134ab6940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134ab6c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134ab6ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134ab7180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134ab7440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134ab7700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134ab79c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134ab7c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134ab7f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134ab8200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134ab84c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134ab8780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134ab8a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134ab8d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134ab8fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134ab9280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134ab9540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134ab9800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134ab9ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134ab9d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134aba040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134aba300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134aba5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134aba880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134abab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134abae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134abb0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134abb380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134abb640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134abb900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134abbbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134abbe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134abc140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134abc400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134abc6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134abc980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134abcc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134abcf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134abd1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134abd480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134abd740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134abda00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134abdcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134abdf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134abe240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134abe500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134abe7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134abea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134abed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134abf000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134abf2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134abf580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134abf840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134abfb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134abfdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134ac0080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134ac0340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134ac0600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134ac08c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134ac0b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134ac0e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134ac1100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134ac13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134ac1680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134ac1940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x102004300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x102008320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1020085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1020088a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x102009400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1020096c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x102009980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x102009df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10200a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10200a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10200ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10200afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10200b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10200b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10200bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10200c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10200c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10200ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10200cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10200d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10200d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10200dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10200e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10200e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10200e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10200edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10200f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10200f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10200fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10200ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x102010400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x102010870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x102010ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x102011150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1020115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x102011a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x102011ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x102012310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x102012780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x102012bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x102013060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1020134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x102013940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x102013db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x102014220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x102014690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x102014b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x102014f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1020153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x102015850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x102015cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x102016130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1020165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x102016a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x102016e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1020172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x102017760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x102017bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x102018040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1020184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x102018920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x102018d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x102019200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x102019670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x102019ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x102019f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10201a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10201a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10201aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10201b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10201b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10201b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10201be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10201c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10201c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10201cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10201d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10201da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10201e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10201e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10201eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10201f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10201f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10201fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x102020330 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.823s
user	0m0.294s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4411 (c31fc8b9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e00a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e00a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e00aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e00b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e00ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e00bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e00c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e00cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e00d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e00d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e00db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e00e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e00eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e00f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e00fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e010200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e010920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e011760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e012650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e012d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e013490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e013d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e014450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e014710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e014d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e015990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e015ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e016190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e0168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e017180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e0176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e017e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e0182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e018760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e018c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e0190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e019540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e0199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e019e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e01a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e01a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e01abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e01b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e01bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e01c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e01c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e01cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e01d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e01d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e01df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e01e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e01ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e01f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e01f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e01f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e020430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e0208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e020d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e021210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e0216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e021b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e021ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e022490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e022930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e023270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e023710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e023bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e024100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e024650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e024ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e0250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e025640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e025b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e0260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e026630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e026b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e0270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e027620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e027b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e0280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e028610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e028b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e0290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e029600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e029b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e02a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e02a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e02ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e02b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e02b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e02bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e01b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e02bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e02c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e02cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e02d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e02d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e02dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e02e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e02e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e02ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e02f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e02f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e02fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e0301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e030710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e030c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e031100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e0315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e031a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e031ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e032820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e032cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e033160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e033600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e033aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e033f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e0343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e034880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e034d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e0351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e035660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e035b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e035fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e036440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e0368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e036d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e037220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e0376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e037b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e038000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e0384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e038940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e038de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e039280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e039720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e039bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e03a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e03a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e03ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e03b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e03b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e03bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e03c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e03c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e03ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e03cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e03d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e03d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e03dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e03e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e03e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e03ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e03ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e03f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e03f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e03fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e040180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e040620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e040ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e040f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e041400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e0418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e041d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e0421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e042680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e042b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e042fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e043460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e043900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e043da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e044240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e0446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e044b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e045020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e0454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e045960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e045e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e0462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e046740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e046be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e047080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e047520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e0479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e047e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e0483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e048900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e048e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e0493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e049660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e049c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e04a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e04a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e04b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e04b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e04b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e04bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e04c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e04cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e04d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e04d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e04d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e04e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e04e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e04ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e04f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e04f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e04fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e050160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e0506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e050c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e051150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e0516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e051bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e052140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e052690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e052be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e053130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e053680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e053bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e054120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e054670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e054bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e055110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e055660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e055bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e056100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e056650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e056ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e0570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e057640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e057b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e0580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e058630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e058b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e0590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e059620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e059b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e05a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e05a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e05ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e05b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e05b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e05bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e05c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e05c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e05cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e05d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e05d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e05db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e05e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e05e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e05eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e05f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e05f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e05fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e060060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e0605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e060b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e060fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e061440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e0618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e061d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e062220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e0626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e062b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e063000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e0634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e063940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e063de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e064280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e064720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e064bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e065060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e0655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e065cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e0663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e066b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e067230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e0674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e067ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e067fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e0685b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.098.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ce07f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ce083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ce08860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ce08cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ce09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ce095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ce09a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ce09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ce0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ce0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ce0abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ce0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ce0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ce0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ce0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ce0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ce0dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ce0e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ce0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ce0f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ce0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ce0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ce106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ce10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ce11530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ce117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ce11ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ce11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ce12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ce12800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ce12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ce131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ce13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ce138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ce13d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ce141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ce14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ce14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ce14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ce15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ce157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ce15c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ce160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ce16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ce169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ce16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ce17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ce176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ce17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ce17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ce18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ce188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ce18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ce19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ce19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ce19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ce19fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ce1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ce1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ce1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ce1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ce1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ce1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ce1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ce1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ce1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ce1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ce1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ce1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ce1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ce1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ce1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ce1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ce1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ce1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ce1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ce1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ce1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ce20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ce20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ce20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ce20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ce213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ce21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ce21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ce22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ce22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ce22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ce22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ce232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ce23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ce23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ce24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ce244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ce24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ce24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ce251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ce25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ce25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ce25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ce263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ce26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ce26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ce27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ce27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ce279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ce27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ce282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ce28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ce28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ce29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ce29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ce298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ce29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ce2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ce2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ce2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ce2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ce2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ce2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ce2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ce2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ce2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ce2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ce2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ce2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ce2d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ce2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ce2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ce2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ce2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ce2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ce2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ce2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ce2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ce2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ce30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ce307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ce30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ce310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ce31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ce319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ce31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ce32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ce326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ce32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ce32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ce33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ce338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ce33d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ce34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ce34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ce34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ce34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ce35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ce357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ce35c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ce360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ce36510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ce36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ce36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ce37260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ce376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ce37b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ce37fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ce38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ce39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ce39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ce395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ce39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ce39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ce3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ce3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ce3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ce3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ce3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ce3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ce3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ce3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ce3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ce3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ce3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ce3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ce3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ce3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ce3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ce3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ce3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ce3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ce3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ce3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ce3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ce40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ce404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ce40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ce40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ce41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ce41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ce41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ce41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ce423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ce42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ce42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ce432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ce43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ce43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ce44000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ce44470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ce44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ce44ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ce45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ce45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ce46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ce46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ce46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ce473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ce47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ce47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ce48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ce48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ce49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ce49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ce49c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ce4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ce4a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ce4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ce4b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ce4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ce4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ce4c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ce4ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ce4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ce4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ce4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ce4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ce4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ce4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ce4f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ce4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ce4fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ce50390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ce50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ce50f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ce514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ce51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ce52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ce52610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ce52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ce53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ce53750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ce53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ce542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ce54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ce54e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ce55410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ce559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ce55f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ce56550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ce56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ce570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ce57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ce57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ce58210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ce587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ce58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ce59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ce59910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ce59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ce5a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ce5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ce5add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ce5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ce5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ce5bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ce5c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ce5c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ce5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ce5d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ce5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ce5dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ce5dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ce5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ce5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ce5f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ce5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ce60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ce60940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ce60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ce613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ce616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ce61cc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ce5ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ce4fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ce4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ce4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ce48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ce584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ce55c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ce53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ce51790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ce49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ce470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ce4c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ce4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ce528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ce4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ce57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ce49ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ce52310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ce4ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ce45f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ce50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ce4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ce56250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ce511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ce46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ce487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ce59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ce4e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ce56810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ce4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ce4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ce52e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ce4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ce4a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ce54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ce49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ce57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ce55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ce50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ce59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ce48210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ce59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ce47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ce57f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ce51d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ce53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ce56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ce556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ce4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ce45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ce60ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ce0ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ce623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ce62660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ce62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ce62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ce62ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ce63160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ce63420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ce636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ce639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ce63c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ce63f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ce641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ce644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ce64760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ce64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ce64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ce64fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ce65260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ce65520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ce657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ce65aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ce65d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ce66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ce662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ce665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ce66860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ce66b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ce66de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ce670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ce67360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ce67620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ce678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ce67ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ce67e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ce68120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ce683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ce686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ce68960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ce68c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ce68ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ce691a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ce69460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ce69720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ce699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ce69ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ce69f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ce6a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ce6a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ce6a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ce6aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ce6ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ce6afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ce6b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ce6b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ce6b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ce6bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ce6bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ce6c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ce6c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ce6c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ce6c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ce6cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ce6ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ce6d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ce6d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ce6d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ce6d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ce6dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ce6dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ce6e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ce6e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ce6e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ce6e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ce6ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ce6ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ce6f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ce6f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ce6f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ce6fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ce6fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ce6ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ce70260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ce70520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ce707e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ce70aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ce70d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ce71020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ce712e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ce715a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ce71860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ce71b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ce71de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ce720a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ce72360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ce72620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ce728e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ce72ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ce72e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ce73120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ce733e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ce736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ce73960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ce73c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ce73ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ce741a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ce74460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ce74720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ce749e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ce74ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ce74f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ce75220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ce754e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ce757a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ce75a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ce75d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ce75fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ce762a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ce76560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ce76820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ce76ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ce76da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ce77060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ce77320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ce775e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ce778a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ce77b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ce77e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ce780e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ce783a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ce78660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ce78920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ce78be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ce78ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ce79160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ce79420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ce796e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ce799a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ce79c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ce79f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ce7a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ce7a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ce7a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ce7aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ce7ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ce7afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ce7b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ce7b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ce7b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ce7baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ce7bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ce7c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ce7c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ce7c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ce7c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ce7cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ce7cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ce7d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ce7d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ce7d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ce7dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ce7deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ce7e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ce7e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ce7e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ce7e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ce7ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ce7ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ce7f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ce7f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ce7fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ce801e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ce80730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ce80c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ce811d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ce81720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ce81c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ce821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ce82710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ce82c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ce831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ce83700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ce83c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ce841a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ce846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ce84c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ce85190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ce856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ce85c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ce86180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ce866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ce86c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ce87170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ce876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ce87c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ce88160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ce886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ce88c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ce89150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ce896a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ce89bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ce8a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ce8a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ce8abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ce8b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ce8b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ce8bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ce8c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ce8c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ce8cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ce8d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ce8d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ce8dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ce8e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ce8e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ce8eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ce8ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ce8f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ce8f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ce8f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ce8fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ce90130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ce905a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ce90a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ce90e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ce912f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ce91760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ce91bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ce92040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ce924b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ce92920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ce92d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ce93200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ce93ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ce94610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ce94d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ce94ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ce95460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ce95a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ce96070 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.927s
user	0m0.243s
sys	0m0.137s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.17 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.15 user         0.04 sys
```
