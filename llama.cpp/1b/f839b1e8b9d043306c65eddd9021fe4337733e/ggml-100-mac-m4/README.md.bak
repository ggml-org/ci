### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.81 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.46 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.01 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.26 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.95 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.65 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.65 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.77 sec*proc (28 tests)

Total Test time (real) = 221.78 sec

real	3m41.887s
user	7m42.177s
sys	0m6.348s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.94 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.62 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.18 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.51 sec*proc (28 tests)

Total Test time (real) =  51.52 sec

real	0m51.534s
user	1m11.623s
sys	0m5.706s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.128 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.904 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.694 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.705 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.030.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.706 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.030.707 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.030.707 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.030.709 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.030.710 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.030.710 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.030.711 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.030.712 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.030.715 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.716 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.716 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.030.717 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.030.718 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.718 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.030.719 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.035.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.036.630 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.632 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.036.632 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.036.632 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.036.633 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.036.633 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.036.633 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.036.634 I llama_model_loader: - type  f32:  124 tensors
0.00.036.634 I llama_model_loader: - type  f16:   73 tensors
0.00.040.523 I llm_load_vocab: special tokens cache size = 5
0.00.042.955 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.042.960 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.042.960 I llm_load_print_meta: arch             = bert
0.00.042.961 I llm_load_print_meta: vocab type       = WPM
0.00.042.961 I llm_load_print_meta: n_vocab          = 30522
0.00.042.961 I llm_load_print_meta: n_merges         = 0
0.00.042.962 I llm_load_print_meta: vocab_only       = 0
0.00.042.962 I llm_load_print_meta: n_ctx_train      = 512
0.00.042.962 I llm_load_print_meta: n_embd           = 384
0.00.042.963 I llm_load_print_meta: n_layer          = 12
0.00.042.966 I llm_load_print_meta: n_head           = 12
0.00.042.967 I llm_load_print_meta: n_head_kv        = 12
0.00.042.968 I llm_load_print_meta: n_rot            = 32
0.00.042.968 I llm_load_print_meta: n_swa            = 0
0.00.042.968 I llm_load_print_meta: n_embd_head_k    = 32
0.00.042.969 I llm_load_print_meta: n_embd_head_v    = 32
0.00.042.969 I llm_load_print_meta: n_gqa            = 1
0.00.042.971 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.042.971 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.042.972 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.042.973 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.042.973 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.042.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.042.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.042.974 I llm_load_print_meta: n_ff             = 1536
0.00.042.975 I llm_load_print_meta: n_expert         = 0
0.00.042.975 I llm_load_print_meta: n_expert_used    = 0
0.00.042.975 I llm_load_print_meta: causal attn      = 0
0.00.042.975 I llm_load_print_meta: pooling type     = 2
0.00.042.976 I llm_load_print_meta: rope type        = 2
0.00.042.976 I llm_load_print_meta: rope scaling     = linear
0.00.042.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.042.977 I llm_load_print_meta: freq_scale_train = 1
0.00.042.977 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.042.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.042.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.042.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.042.979 I llm_load_print_meta: ssm_d_state      = 0
0.00.042.979 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.042.979 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.042.979 I llm_load_print_meta: model type       = 33M
0.00.042.980 I llm_load_print_meta: model ftype      = F16
0.00.042.980 I llm_load_print_meta: model params     = 33.21 M
0.00.042.982 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.042.985 I llm_load_print_meta: general.name     = Bge Small
0.00.042.986 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.042.986 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.042.986 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.042.986 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.042.987 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.042.987 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.042.988 I llm_load_print_meta: max token length = 21
0.00.044.849 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.044.849 I llm_load_tensors: offloading output layer to GPU
0.00.044.853 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.044.873 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.874 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.045.070 I llama_new_context_with_model: n_seq_max     = 1
0.00.045.071 I llama_new_context_with_model: n_ctx         = 512
0.00.045.071 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.045.072 I llama_new_context_with_model: n_batch       = 2048
0.00.045.072 I llama_new_context_with_model: n_ubatch      = 2048
0.00.045.072 I llama_new_context_with_model: flash_attn    = 0
0.00.045.073 I llama_new_context_with_model: freq_base     = 10000.0
0.00.045.073 I llama_new_context_with_model: freq_scale    = 1
0.00.045.074 I ggml_metal_init: allocating
0.00.045.079 I ggml_metal_init: found device: Apple M4
0.00.045.084 I ggml_metal_init: picking default device: Apple M4
0.00.045.997 I ggml_metal_init: using embedded metal library
0.00.049.404 I ggml_metal_init: GPU name:   Apple M4
0.00.049.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.408 I ggml_metal_init: simdgroup reduction   = true
0.00.049.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.408 I ggml_metal_init: has bfloat            = true
0.00.049.408 I ggml_metal_init: use bfloat            = true
0.00.049.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.360 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.870 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.872 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.873 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.060.528 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.060.529 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.060.529 I llama_new_context_with_model: graph nodes  = 429
0.00.060.529 I llama_new_context_with_model: graph splits = 2
0.00.060.531 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.712 I 
0.00.065.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.319 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.070.470 I llama_perf_context_print:        load time =      40.80 ms
0.00.070.471 I llama_perf_context_print: prompt eval time =       4.02 ms /     9 tokens (    0.45 ms per token,  2239.36 tokens per second)
0.00.070.471 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.070.472 I llama_perf_context_print:       total time =       4.76 ms /    10 tokens
0.00.070.687 I ggml_metal_free: deallocating

real	0m0.244s
user	0m0.047s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.974 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.354 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.359 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.360 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.360 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.361 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.361 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.362 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.362 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.363 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.363 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.363 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.366 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.366 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.367 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.367 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.367 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.368 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.473 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.068 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.069 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.069 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.070 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.070 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.070 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.071 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.014.071 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.014.072 I llama_model_loader: - type  f32:  124 tensors
0.00.014.072 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.307 I llm_load_vocab: special tokens cache size = 5
0.00.017.446 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.449 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.449 I llm_load_print_meta: arch             = bert
0.00.017.450 I llm_load_print_meta: vocab type       = WPM
0.00.017.450 I llm_load_print_meta: n_vocab          = 30522
0.00.017.450 I llm_load_print_meta: n_merges         = 0
0.00.017.450 I llm_load_print_meta: vocab_only       = 0
0.00.017.451 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.451 I llm_load_print_meta: n_embd           = 384
0.00.017.451 I llm_load_print_meta: n_layer          = 12
0.00.017.454 I llm_load_print_meta: n_head           = 12
0.00.017.455 I llm_load_print_meta: n_head_kv        = 12
0.00.017.455 I llm_load_print_meta: n_rot            = 32
0.00.017.455 I llm_load_print_meta: n_swa            = 0
0.00.017.455 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.455 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.456 I llm_load_print_meta: n_gqa            = 1
0.00.017.457 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.457 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.458 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.458 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.458 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.459 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.459 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.460 I llm_load_print_meta: n_ff             = 1536
0.00.017.460 I llm_load_print_meta: n_expert         = 0
0.00.017.460 I llm_load_print_meta: n_expert_used    = 0
0.00.017.460 I llm_load_print_meta: causal attn      = 0
0.00.017.460 I llm_load_print_meta: pooling type     = 2
0.00.017.461 I llm_load_print_meta: rope type        = 2
0.00.017.461 I llm_load_print_meta: rope scaling     = linear
0.00.017.461 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.462 I llm_load_print_meta: freq_scale_train = 1
0.00.017.462 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.464 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.467 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.467 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.468 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.468 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.468 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.468 I llm_load_print_meta: model type       = 33M
0.00.017.469 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.469 I llm_load_print_meta: model params     = 33.21 M
0.00.017.469 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.470 I llm_load_print_meta: general.name     = Bge Small
0.00.017.470 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.476 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.477 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.477 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.478 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.478 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.478 I llm_load_print_meta: max token length = 21
0.00.018.591 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.591 I llm_load_tensors: offloading output layer to GPU
0.00.018.591 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.598 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.599 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.780 I llama_new_context_with_model: n_ctx         = 512
0.00.018.780 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.780 I llama_new_context_with_model: n_batch       = 2048
0.00.018.780 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.780 I llama_new_context_with_model: flash_attn    = 0
0.00.018.781 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.781 I llama_new_context_with_model: freq_scale    = 1
0.00.018.782 I ggml_metal_init: allocating
0.00.018.788 I ggml_metal_init: found device: Apple M4
0.00.018.791 I ggml_metal_init: picking default device: Apple M4
0.00.019.387 I ggml_metal_init: using embedded metal library
0.00.021.763 I ggml_metal_init: GPU name:   Apple M4
0.00.021.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.766 I ggml_metal_init: simdgroup reduction   = true
0.00.021.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.766 I ggml_metal_init: has bfloat            = true
0.00.021.767 I ggml_metal_init: use bfloat            = true
0.00.021.767 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.768 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.045 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.546 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.550 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.552 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.171 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.172 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.172 I llama_new_context_with_model: graph nodes  = 429
0.00.033.172 I llama_new_context_with_model: graph splits = 2
0.00.033.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.199 I 
0.00.038.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.736 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.180 I llama_perf_context_print:        load time =      29.22 ms
0.00.043.181 I llama_perf_context_print: prompt eval time =       4.32 ms /     9 tokens (    0.48 ms per token,  2083.82 tokens per second)
0.00.043.182 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.182 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.043.387 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.202 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.834 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.699 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.707 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.709 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.710 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.711 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.712 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.713 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.713 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.714 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.714 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.718 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.718 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.719 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.311 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.314 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.315 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.315 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.315 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.316 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.316 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.316 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.317 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.317 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.318 I llama_model_loader: - type  f32:   40 tensors
0.00.048.318 I llama_model_loader: - type  f16:   30 tensors
0.00.065.399 W llm_load_vocab: empty token at index 5
0.00.069.781 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.014 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.046 I llm_load_vocab: special tokens cache size = 5
0.00.329.994 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.330.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.330.002 I llm_load_print_meta: arch             = jina-bert-v2
0.00.330.002 I llm_load_print_meta: vocab type       = BPE
0.00.330.002 I llm_load_print_meta: n_vocab          = 61056
0.00.330.003 I llm_load_print_meta: n_merges         = 39382
0.00.330.003 I llm_load_print_meta: vocab_only       = 0
0.00.330.003 I llm_load_print_meta: n_ctx_train      = 8192
0.00.330.003 I llm_load_print_meta: n_embd           = 384
0.00.330.004 I llm_load_print_meta: n_layer          = 4
0.00.330.017 I llm_load_print_meta: n_head           = 12
0.00.330.018 I llm_load_print_meta: n_head_kv        = 12
0.00.330.018 I llm_load_print_meta: n_rot            = 32
0.00.330.018 I llm_load_print_meta: n_swa            = 0
0.00.330.018 I llm_load_print_meta: n_embd_head_k    = 32
0.00.330.019 I llm_load_print_meta: n_embd_head_v    = 32
0.00.330.021 I llm_load_print_meta: n_gqa            = 1
0.00.330.023 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.330.024 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.330.026 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.330.027 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.330.028 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.330.028 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.330.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.330.029 I llm_load_print_meta: n_ff             = 1536
0.00.330.029 I llm_load_print_meta: n_expert         = 0
0.00.330.030 I llm_load_print_meta: n_expert_used    = 0
0.00.330.030 I llm_load_print_meta: causal attn      = 0
0.00.330.030 I llm_load_print_meta: pooling type     = -1
0.00.330.030 I llm_load_print_meta: rope type        = -1
0.00.330.030 I llm_load_print_meta: rope scaling     = linear
0.00.330.031 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.330.031 I llm_load_print_meta: freq_scale_train = 1
0.00.330.031 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.330.033 I llm_load_print_meta: rope_finetuned   = unknown
0.00.330.033 I llm_load_print_meta: ssm_d_conv       = 0
0.00.330.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.330.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.330.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.330.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.330.040 I llm_load_print_meta: model type       = 33M
0.00.330.043 I llm_load_print_meta: model ftype      = F16
0.00.330.044 I llm_load_print_meta: model params     = 32.90 M
0.00.330.045 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.330.045 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.330.045 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.330.046 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.330.046 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.330.046 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.330.048 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.330.048 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.330.048 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.330.048 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.330.049 I llm_load_print_meta: max token length = 45
0.00.331.216 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.331.216 I llm_load_tensors: offloading output layer to GPU
0.00.331.219 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.331.244 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.245 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.331.657 I llama_new_context_with_model: n_seq_max     = 1
0.00.331.658 I llama_new_context_with_model: n_ctx         = 8192
0.00.331.658 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.331.659 I llama_new_context_with_model: n_batch       = 2048
0.00.331.659 I llama_new_context_with_model: n_ubatch      = 2048
0.00.331.659 I llama_new_context_with_model: flash_attn    = 0
0.00.331.659 I llama_new_context_with_model: freq_base     = 10000.0
0.00.331.660 I llama_new_context_with_model: freq_scale    = 1
0.00.331.660 I ggml_metal_init: allocating
0.00.331.663 I ggml_metal_init: found device: Apple M4
0.00.331.666 I ggml_metal_init: picking default device: Apple M4
0.00.332.872 I ggml_metal_init: using embedded metal library
0.00.335.842 I ggml_metal_init: GPU name:   Apple M4
0.00.335.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.844 I ggml_metal_init: simdgroup reduction   = true
0.00.335.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.845 I ggml_metal_init: has bfloat            = true
0.00.335.845 I ggml_metal_init: use bfloat            = true
0.00.335.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.258 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.347.844 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.846 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.848 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.348.402 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.348.403 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.348.404 I llama_new_context_with_model: graph nodes  = 154
0.00.348.404 I llama_new_context_with_model: graph splits = 2
0.00.348.405 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.071 I 
0.00.360.092 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.306 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.306 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.315 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.315 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.320 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.320 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.872 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.665 I llama_perf_context_print:        load time =     338.23 ms
0.00.364.666 I llama_perf_context_print: prompt eval time =       3.79 ms /    62 tokens (    0.06 ms per token, 16371.80 tokens per second)
0.00.364.668 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.668 I llama_perf_context_print:       total time =       4.59 ms /    63 tokens
0.00.364.897 I ggml_metal_free: deallocating

real	0m1.076s
user	0m0.338s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.105 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.194 I main: llama backend init
0.00.000.200 I main: load the model and apply lora adapter, if any
0.00.024.530 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.480 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.503 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.509 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.640 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.016 I llama_model_loader: - type  f32:  194 tensors
0.00.050.016 I llama_model_loader: - type  f16:   98 tensors
0.00.071.303 I llm_load_vocab: special tokens cache size = 25
0.00.077.372 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.377 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.377 I llm_load_print_meta: arch             = gptneox
0.00.077.377 I llm_load_print_meta: vocab type       = BPE
0.00.077.378 I llm_load_print_meta: n_vocab          = 50304
0.00.077.378 I llm_load_print_meta: n_merges         = 50009
0.00.077.378 I llm_load_print_meta: vocab_only       = 0
0.00.077.378 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.378 I llm_load_print_meta: n_embd           = 2048
0.00.077.382 I llm_load_print_meta: n_layer          = 24
0.00.077.386 I llm_load_print_meta: n_head           = 16
0.00.077.386 I llm_load_print_meta: n_head_kv        = 16
0.00.077.387 I llm_load_print_meta: n_rot            = 32
0.00.077.387 I llm_load_print_meta: n_swa            = 0
0.00.077.387 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.388 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.389 I llm_load_print_meta: n_gqa            = 1
0.00.077.390 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.391 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.391 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.392 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.392 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.392 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.392 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.393 I llm_load_print_meta: n_ff             = 8192
0.00.077.393 I llm_load_print_meta: n_expert         = 0
0.00.077.393 I llm_load_print_meta: n_expert_used    = 0
0.00.077.393 I llm_load_print_meta: causal attn      = 1
0.00.077.393 I llm_load_print_meta: pooling type     = 0
0.00.077.393 I llm_load_print_meta: rope type        = 2
0.00.077.394 I llm_load_print_meta: rope scaling     = linear
0.00.077.394 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.394 I llm_load_print_meta: freq_scale_train = 1
0.00.077.395 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.395 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.396 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.396 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.396 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.396 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.396 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.396 I llm_load_print_meta: model type       = 1.4B
0.00.077.397 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.077.397 I llm_load_print_meta: model params     = 1.41 B
0.00.077.398 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.077.398 I llm_load_print_meta: general.name     = 1.4B
0.00.077.398 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.400 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.400 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.400 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.400 I llm_load_print_meta: LF token         = 128 ''
0.00.077.400 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.400 I llm_load_print_meta: max token length = 1024
0.00.079.926 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.926 I llm_load_tensors: offloading output layer to GPU
0.00.079.926 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.946 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.079.948 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.080.294 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.295 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.295 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.295 I llama_new_context_with_model: n_batch       = 2048
0.00.080.296 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.296 I llama_new_context_with_model: flash_attn    = 0
0.00.080.296 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.297 I llama_new_context_with_model: freq_scale    = 1
0.00.080.297 I ggml_metal_init: allocating
0.00.080.308 I ggml_metal_init: found device: Apple M4
0.00.080.311 I ggml_metal_init: picking default device: Apple M4
0.00.080.966 I ggml_metal_init: using embedded metal library
0.00.089.580 I ggml_metal_init: GPU name:   Apple M4
0.00.089.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.585 I ggml_metal_init: simdgroup reduction   = true
0.00.089.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.585 I ggml_metal_init: has bfloat            = true
0.00.089.585 I ggml_metal_init: use bfloat            = true
0.00.089.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.167.446 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.186.942 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.186.947 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.186.968 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.187.972 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.187.973 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.187.974 I llama_new_context_with_model: graph nodes  = 967
0.00.187.974 I llama_new_context_with_model: graph splits = 2
0.00.187.977 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.188.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.188.123 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.762 I main: llama threadpool init, n_threads = 4
0.00.268.810 I 
0.00.268.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.832 I 
0.00.268.896 I sampler seed: 1234
0.00.268.901 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.936 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.937 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.937 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.115.068 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.02.115.068 I llama_perf_context_print:        load time =     244.22 ms
0.02.115.069 I llama_perf_context_print: prompt eval time =      54.18 ms /     7 tokens (    7.74 ms per token,   129.21 tokens per second)
0.02.115.070 I llama_perf_context_print:        eval time =    1789.17 ms /    63 runs   (   28.40 ms per token,    35.21 tokens per second)
0.02.115.070 I llama_perf_context_print:       total time =    1846.31 ms /    70 tokens
0.02.115.280 I ggml_metal_free: deallocating

real	0m2.433s
user	0m0.127s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.602 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.099 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.065 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.074 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.075 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.077 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.473 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.839 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.840 I llama_model_loader: - type  f32:  194 tensors
0.00.051.840 I llama_model_loader: - type  f16:   98 tensors
0.00.078.727 I llm_load_vocab: special tokens cache size = 25
0.00.084.748 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.752 I llm_load_print_meta: arch             = gptneox
0.00.084.752 I llm_load_print_meta: vocab type       = BPE
0.00.084.752 I llm_load_print_meta: n_vocab          = 50304
0.00.084.752 I llm_load_print_meta: n_merges         = 50009
0.00.084.752 I llm_load_print_meta: vocab_only       = 0
0.00.084.753 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.753 I llm_load_print_meta: n_embd           = 2048
0.00.084.753 I llm_load_print_meta: n_layer          = 24
0.00.084.756 I llm_load_print_meta: n_head           = 16
0.00.084.757 I llm_load_print_meta: n_head_kv        = 16
0.00.084.757 I llm_load_print_meta: n_rot            = 32
0.00.084.757 I llm_load_print_meta: n_swa            = 0
0.00.084.757 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.757 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.758 I llm_load_print_meta: n_gqa            = 1
0.00.084.759 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.759 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.760 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.762 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.763 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.763 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.763 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.763 I llm_load_print_meta: n_ff             = 8192
0.00.084.764 I llm_load_print_meta: n_expert         = 0
0.00.084.764 I llm_load_print_meta: n_expert_used    = 0
0.00.084.764 I llm_load_print_meta: causal attn      = 1
0.00.084.764 I llm_load_print_meta: pooling type     = 0
0.00.084.764 I llm_load_print_meta: rope type        = 2
0.00.084.764 I llm_load_print_meta: rope scaling     = linear
0.00.084.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.765 I llm_load_print_meta: freq_scale_train = 1
0.00.084.765 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.765 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.766 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.766 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.766 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.766 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.766 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.766 I llm_load_print_meta: model type       = 1.4B
0.00.084.767 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.767 I llm_load_print_meta: model params     = 1.41 B
0.00.084.768 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.768 I llm_load_print_meta: general.name     = 1.4B
0.00.084.768 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.768 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.768 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.769 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.769 I llm_load_print_meta: LF token         = 128 ''
0.00.084.770 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.770 I llm_load_print_meta: max token length = 1024
0.00.087.464 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.464 I llm_load_tensors: offloading output layer to GPU
0.00.087.464 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.476 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.477 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.832 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.833 I llama_new_context_with_model: n_ctx         = 128
0.00.087.833 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.833 I llama_new_context_with_model: n_batch       = 128
0.00.087.833 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.833 I llama_new_context_with_model: flash_attn    = 0
0.00.087.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.834 I llama_new_context_with_model: freq_scale    = 1
0.00.087.835 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.835 I ggml_metal_init: allocating
0.00.087.845 I ggml_metal_init: found device: Apple M4
0.00.087.849 I ggml_metal_init: picking default device: Apple M4
0.00.088.469 I ggml_metal_init: using embedded metal library
0.00.091.074 I ggml_metal_init: GPU name:   Apple M4
0.00.091.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.076 I ggml_metal_init: simdgroup reduction   = true
0.00.091.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.076 I ggml_metal_init: has bfloat            = true
0.00.091.077 I ggml_metal_init: use bfloat            = true
0.00.091.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.047 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.245 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.251 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.265 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.112 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.113 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.114 I llama_new_context_with_model: graph nodes  = 967
0.00.103.114 I llama_new_context_with_model: graph splits = 2
0.00.103.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.982 I 
0.00.770.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.032 I perplexity: tokenizing the input ..
0.00.782.267 I perplexity: tokenization took 12.233 ms
0.00.782.280 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.904.242 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.906.100 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.906.158 I llama_perf_context_print:        load time =     748.88 ms
0.00.906.160 I llama_perf_context_print: prompt eval time =     121.57 ms /   128 tokens (    0.95 ms per token,  1052.88 tokens per second)
0.00.906.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.163 I llama_perf_context_print:       total time =     136.17 ms /   129 tokens
0.00.906.828 I ggml_metal_free: deallocating

real	0m1.098s
user	0m0.119s
sys	0m0.176s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.843 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.789 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.797 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.798 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.798 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.539 I llama_model_loader: - type  f32:  194 tensors
0.00.026.539 I llama_model_loader: - type q8_0:   98 tensors
0.00.048.143 I llm_load_vocab: special tokens cache size = 25
0.00.054.600 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.605 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.605 I llm_load_print_meta: arch             = gptneox
0.00.054.606 I llm_load_print_meta: vocab type       = BPE
0.00.054.606 I llm_load_print_meta: n_vocab          = 50304
0.00.054.606 I llm_load_print_meta: n_merges         = 50009
0.00.054.607 I llm_load_print_meta: vocab_only       = 0
0.00.054.607 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.607 I llm_load_print_meta: n_embd           = 2048
0.00.054.607 I llm_load_print_meta: n_layer          = 24
0.00.054.613 I llm_load_print_meta: n_head           = 16
0.00.054.614 I llm_load_print_meta: n_head_kv        = 16
0.00.054.614 I llm_load_print_meta: n_rot            = 32
0.00.054.614 I llm_load_print_meta: n_swa            = 0
0.00.054.615 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.615 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.615 I llm_load_print_meta: n_gqa            = 1
0.00.054.616 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.617 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.618 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.618 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.618 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.618 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.618 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.619 I llm_load_print_meta: n_ff             = 8192
0.00.054.619 I llm_load_print_meta: n_expert         = 0
0.00.054.620 I llm_load_print_meta: n_expert_used    = 0
0.00.054.620 I llm_load_print_meta: causal attn      = 1
0.00.054.621 I llm_load_print_meta: pooling type     = 0
0.00.054.622 I llm_load_print_meta: rope type        = 2
0.00.054.624 I llm_load_print_meta: rope scaling     = linear
0.00.054.624 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.624 I llm_load_print_meta: freq_scale_train = 1
0.00.054.625 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.625 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.625 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.625 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.625 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.625 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.625 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.626 I llm_load_print_meta: model type       = 1.4B
0.00.054.626 I llm_load_print_meta: model ftype      = Q8_0
0.00.054.627 I llm_load_print_meta: model params     = 1.41 B
0.00.054.627 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.054.627 I llm_load_print_meta: general.name     = 1.4B
0.00.054.627 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.628 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.632 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.632 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.632 I llm_load_print_meta: LF token         = 128 ''
0.00.054.632 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.632 I llm_load_print_meta: max token length = 1024
0.00.057.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.150 I llm_load_tensors: offloading output layer to GPU
0.00.057.150 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.162 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.163 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.057.555 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.556 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.556 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.556 I llama_new_context_with_model: n_batch       = 2048
0.00.057.557 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.557 I llama_new_context_with_model: flash_attn    = 0
0.00.057.557 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.558 I llama_new_context_with_model: freq_scale    = 1
0.00.057.558 I ggml_metal_init: allocating
0.00.057.567 I ggml_metal_init: found device: Apple M4
0.00.057.569 I ggml_metal_init: picking default device: Apple M4
0.00.058.323 I ggml_metal_init: using embedded metal library
0.00.060.864 I ggml_metal_init: GPU name:   Apple M4
0.00.060.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.866 I ggml_metal_init: simdgroup reduction   = true
0.00.060.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.867 I ggml_metal_init: has bfloat            = true
0.00.060.867 I ggml_metal_init: use bfloat            = true
0.00.060.867 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.310 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.261 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.273 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.301 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.380 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.382 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.382 I llama_new_context_with_model: graph nodes  = 967
0.00.095.382 I llama_new_context_with_model: graph splits = 2
0.00.095.386 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.521 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.986.748 I main: llama threadpool init, n_threads = 4
0.00.986.783 I 
0.00.986.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.986.808 I 
0.00.987.044 I sampler seed: 1234
0.00.987.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.987.072 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.987.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.987.074 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.070.804 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.02.070.805 I llama_perf_context_print:        load time =     976.90 ms
0.02.070.806 I llama_perf_context_print: prompt eval time =      43.76 ms /     7 tokens (    6.25 ms per token,   159.98 tokens per second)
0.02.070.807 I llama_perf_context_print:        eval time =    1036.92 ms /    63 runs   (   16.46 ms per token,    60.76 tokens per second)
0.02.070.807 I llama_perf_context_print:       total time =    1084.06 ms /    70 tokens
0.02.071.083 I ggml_metal_free: deallocating

real	0m2.090s
user	0m0.115s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.625 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.947 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.949 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.950 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.399 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.400 I llama_model_loader: - type  f32:  194 tensors
0.00.032.400 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.611 I llm_load_vocab: special tokens cache size = 25
0.00.064.879 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.882 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.882 I llm_load_print_meta: arch             = gptneox
0.00.064.882 I llm_load_print_meta: vocab type       = BPE
0.00.064.883 I llm_load_print_meta: n_vocab          = 50304
0.00.064.883 I llm_load_print_meta: n_merges         = 50009
0.00.064.883 I llm_load_print_meta: vocab_only       = 0
0.00.064.883 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.883 I llm_load_print_meta: n_embd           = 2048
0.00.064.883 I llm_load_print_meta: n_layer          = 24
0.00.064.887 I llm_load_print_meta: n_head           = 16
0.00.064.887 I llm_load_print_meta: n_head_kv        = 16
0.00.064.888 I llm_load_print_meta: n_rot            = 32
0.00.064.889 I llm_load_print_meta: n_swa            = 0
0.00.064.889 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.889 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.890 I llm_load_print_meta: n_gqa            = 1
0.00.064.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.891 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.892 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.892 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.894 I llm_load_print_meta: n_ff             = 8192
0.00.064.896 I llm_load_print_meta: n_expert         = 0
0.00.064.896 I llm_load_print_meta: n_expert_used    = 0
0.00.064.896 I llm_load_print_meta: causal attn      = 1
0.00.064.896 I llm_load_print_meta: pooling type     = 0
0.00.064.896 I llm_load_print_meta: rope type        = 2
0.00.064.896 I llm_load_print_meta: rope scaling     = linear
0.00.064.897 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.897 I llm_load_print_meta: freq_scale_train = 1
0.00.064.897 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.897 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.898 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.898 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.898 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.902 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.902 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.902 I llm_load_print_meta: model type       = 1.4B
0.00.064.903 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.903 I llm_load_print_meta: model params     = 1.41 B
0.00.064.904 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.905 I llm_load_print_meta: general.name     = 1.4B
0.00.064.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.905 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.905 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.905 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.906 I llm_load_print_meta: LF token         = 128 ''
0.00.064.906 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.906 I llm_load_print_meta: max token length = 1024
0.00.067.251 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.251 I llm_load_tensors: offloading output layer to GPU
0.00.067.251 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.263 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.264 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.639 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.639 I llama_new_context_with_model: n_ctx         = 128
0.00.067.640 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.640 I llama_new_context_with_model: n_batch       = 128
0.00.067.640 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.640 I llama_new_context_with_model: flash_attn    = 0
0.00.067.640 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.641 I llama_new_context_with_model: freq_scale    = 1
0.00.067.641 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.642 I ggml_metal_init: allocating
0.00.067.648 I ggml_metal_init: found device: Apple M4
0.00.067.651 I ggml_metal_init: picking default device: Apple M4
0.00.068.291 I ggml_metal_init: using embedded metal library
0.00.070.882 I ggml_metal_init: GPU name:   Apple M4
0.00.070.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.884 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.885 I ggml_metal_init: simdgroup reduction   = true
0.00.070.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.885 I ggml_metal_init: has bfloat            = true
0.00.070.885 I ggml_metal_init: use bfloat            = true
0.00.070.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.139 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.478 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.480 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.498 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.397 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.399 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.399 I llama_new_context_with_model: graph nodes  = 967
0.00.083.399 I llama_new_context_with_model: graph splits = 2
0.00.083.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.849.453 I 
0.00.849.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.849.517 I perplexity: tokenizing the input ..
0.00.857.677 I perplexity: tokenization took 8.157 ms
0.00.857.686 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.982.164 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.983.339 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.983.364 I llama_perf_context_print:        load time =     837.82 ms
0.00.983.365 I llama_perf_context_print: prompt eval time =     124.22 ms /   128 tokens (    0.97 ms per token,  1030.43 tokens per second)
0.00.983.366 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.983.367 I llama_perf_context_print:       total time =     133.91 ms /   129 tokens
0.00.983.814 I ggml_metal_free: deallocating

real	0m1.002s
user	0m0.094s
sys	0m0.160s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.012.086 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.811 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.818 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.818 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.819 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.819 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.820 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.821 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.521 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.522 I llama_model_loader: - type  f32:  194 tensors
0.00.028.522 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.522 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.291 I llm_load_vocab: special tokens cache size = 25
0.00.055.326 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.329 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.330 I llm_load_print_meta: arch             = gptneox
0.00.055.330 I llm_load_print_meta: vocab type       = BPE
0.00.055.330 I llm_load_print_meta: n_vocab          = 50304
0.00.055.330 I llm_load_print_meta: n_merges         = 50009
0.00.055.331 I llm_load_print_meta: vocab_only       = 0
0.00.055.331 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.331 I llm_load_print_meta: n_embd           = 2048
0.00.055.331 I llm_load_print_meta: n_layer          = 24
0.00.055.335 I llm_load_print_meta: n_head           = 16
0.00.055.336 I llm_load_print_meta: n_head_kv        = 16
0.00.055.337 I llm_load_print_meta: n_rot            = 32
0.00.055.337 I llm_load_print_meta: n_swa            = 0
0.00.055.337 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.340 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.340 I llm_load_print_meta: n_gqa            = 1
0.00.055.341 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.342 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.342 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.343 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.343 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.343 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.343 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.344 I llm_load_print_meta: n_ff             = 8192
0.00.055.344 I llm_load_print_meta: n_expert         = 0
0.00.055.344 I llm_load_print_meta: n_expert_used    = 0
0.00.055.344 I llm_load_print_meta: causal attn      = 1
0.00.055.345 I llm_load_print_meta: pooling type     = 0
0.00.055.345 I llm_load_print_meta: rope type        = 2
0.00.055.345 I llm_load_print_meta: rope scaling     = linear
0.00.055.345 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.346 I llm_load_print_meta: freq_scale_train = 1
0.00.055.346 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.346 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.346 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.346 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.347 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.347 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.347 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.348 I llm_load_print_meta: model type       = 1.4B
0.00.055.349 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.349 I llm_load_print_meta: model params     = 1.41 B
0.00.055.350 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.350 I llm_load_print_meta: general.name     = 1.4B
0.00.055.350 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.351 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.351 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.351 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.351 I llm_load_print_meta: LF token         = 128 ''
0.00.055.351 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.352 I llm_load_print_meta: max token length = 1024
0.00.057.668 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.668 I llm_load_tensors: offloading output layer to GPU
0.00.057.668 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.680 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.682 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.054 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.055 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.056 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.056 I llama_new_context_with_model: n_batch       = 2048
0.00.058.056 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.056 I llama_new_context_with_model: flash_attn    = 0
0.00.058.056 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.057 I llama_new_context_with_model: freq_scale    = 1
0.00.058.057 I ggml_metal_init: allocating
0.00.058.060 I ggml_metal_init: found device: Apple M4
0.00.058.062 I ggml_metal_init: picking default device: Apple M4
0.00.058.793 I ggml_metal_init: using embedded metal library
0.00.061.364 I ggml_metal_init: GPU name:   Apple M4
0.00.061.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.374 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.374 I ggml_metal_init: simdgroup reduction   = true
0.00.061.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.374 I ggml_metal_init: has bfloat            = true
0.00.061.374 I ggml_metal_init: use bfloat            = true
0.00.061.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.655 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.891 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.900 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.924 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.189 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.191 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.191 I llama_new_context_with_model: graph nodes  = 967
0.00.098.192 I llama_new_context_with_model: graph splits = 2
0.00.098.195 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.585 I main: llama threadpool init, n_threads = 4
0.00.655.627 I 
0.00.655.650 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.650 I 
0.00.655.884 I sampler seed: 1234
0.00.655.890 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.655.906 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.655.906 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.655.906 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.334.898 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.334.898 I llama_perf_context_print:        load time =     643.49 ms
0.01.334.899 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   159.99 tokens per second)
0.01.334.899 I llama_perf_context_print:        eval time =     632.22 ms /    63 runs   (   10.04 ms per token,    99.65 tokens per second)
0.01.334.901 I llama_perf_context_print:       total time =     679.32 ms /    70 tokens
0.01.335.135 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.112s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.577 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.720 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.720 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.727 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.195 I llama_model_loader: - type  f32:  194 tensors
0.00.025.195 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.196 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.071 I llm_load_vocab: special tokens cache size = 25
0.00.051.028 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.031 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.031 I llm_load_print_meta: arch             = gptneox
0.00.051.032 I llm_load_print_meta: vocab type       = BPE
0.00.051.032 I llm_load_print_meta: n_vocab          = 50304
0.00.051.032 I llm_load_print_meta: n_merges         = 50009
0.00.051.032 I llm_load_print_meta: vocab_only       = 0
0.00.051.032 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.033 I llm_load_print_meta: n_embd           = 2048
0.00.051.033 I llm_load_print_meta: n_layer          = 24
0.00.051.036 I llm_load_print_meta: n_head           = 16
0.00.051.037 I llm_load_print_meta: n_head_kv        = 16
0.00.051.037 I llm_load_print_meta: n_rot            = 32
0.00.051.037 I llm_load_print_meta: n_swa            = 0
0.00.051.037 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.038 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.038 I llm_load_print_meta: n_gqa            = 1
0.00.051.041 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.042 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.044 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.044 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.049 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.049 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.049 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.050 I llm_load_print_meta: n_ff             = 8192
0.00.051.051 I llm_load_print_meta: n_expert         = 0
0.00.051.052 I llm_load_print_meta: n_expert_used    = 0
0.00.051.052 I llm_load_print_meta: causal attn      = 1
0.00.051.052 I llm_load_print_meta: pooling type     = 0
0.00.051.052 I llm_load_print_meta: rope type        = 2
0.00.051.052 I llm_load_print_meta: rope scaling     = linear
0.00.051.053 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.053 I llm_load_print_meta: freq_scale_train = 1
0.00.051.054 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.055 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.055 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.055 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.055 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.055 I llm_load_print_meta: model type       = 1.4B
0.00.051.056 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.056 I llm_load_print_meta: model params     = 1.41 B
0.00.051.056 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.057 I llm_load_print_meta: general.name     = 1.4B
0.00.051.057 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.057 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.057 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.057 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: LF token         = 128 ''
0.00.051.060 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: max token length = 1024
0.00.052.955 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.955 I llm_load_tensors: offloading output layer to GPU
0.00.052.955 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.965 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.967 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.299 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.300 I llama_new_context_with_model: n_ctx         = 128
0.00.053.300 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.300 I llama_new_context_with_model: n_batch       = 128
0.00.053.300 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.301 I llama_new_context_with_model: flash_attn    = 0
0.00.053.301 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.301 I llama_new_context_with_model: freq_scale    = 1
0.00.053.302 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.302 I ggml_metal_init: allocating
0.00.053.305 I ggml_metal_init: found device: Apple M4
0.00.053.307 I ggml_metal_init: picking default device: Apple M4
0.00.053.876 I ggml_metal_init: using embedded metal library
0.00.056.230 I ggml_metal_init: GPU name:   Apple M4
0.00.056.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.233 I ggml_metal_init: simdgroup reduction   = true
0.00.056.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.233 I ggml_metal_init: has bfloat            = true
0.00.056.233 I ggml_metal_init: use bfloat            = true
0.00.056.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.778 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.029 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.031 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.949 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.950 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.950 I llama_new_context_with_model: graph nodes  = 967
0.00.067.951 I llama_new_context_with_model: graph splits = 2
0.00.067.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.750 I 
0.00.578.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.795 I perplexity: tokenizing the input ..
0.00.586.727 I perplexity: tokenization took 7.93 ms
0.00.586.730 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.041 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.710.253 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.710.279 I llama_perf_context_print:        load time =     569.16 ms
0.00.710.280 I llama_perf_context_print: prompt eval time =     122.08 ms /   128 tokens (    0.95 ms per token,  1048.47 tokens per second)
0.00.710.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.710.281 I llama_perf_context_print:       total time =     131.54 ms /   129 tokens
0.00.710.700 I ggml_metal_free: deallocating

real	0m0.724s
user	0m0.077s
sys	0m0.084s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.014.518 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.160 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.785 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.785 I llama_model_loader: - type  f32:  194 tensors
0.00.030.786 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.786 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.687 I llm_load_vocab: special tokens cache size = 25
0.00.057.785 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.788 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.788 I llm_load_print_meta: arch             = gptneox
0.00.057.789 I llm_load_print_meta: vocab type       = BPE
0.00.057.789 I llm_load_print_meta: n_vocab          = 50304
0.00.057.789 I llm_load_print_meta: n_merges         = 50009
0.00.057.789 I llm_load_print_meta: vocab_only       = 0
0.00.057.789 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.790 I llm_load_print_meta: n_embd           = 2048
0.00.057.790 I llm_load_print_meta: n_layer          = 24
0.00.057.793 I llm_load_print_meta: n_head           = 16
0.00.057.794 I llm_load_print_meta: n_head_kv        = 16
0.00.057.794 I llm_load_print_meta: n_rot            = 32
0.00.057.794 I llm_load_print_meta: n_swa            = 0
0.00.057.794 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.794 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.795 I llm_load_print_meta: n_gqa            = 1
0.00.057.797 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.797 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.798 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.798 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.799 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.799 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.799 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.801 I llm_load_print_meta: n_ff             = 8192
0.00.057.802 I llm_load_print_meta: n_expert         = 0
0.00.057.802 I llm_load_print_meta: n_expert_used    = 0
0.00.057.802 I llm_load_print_meta: causal attn      = 1
0.00.057.802 I llm_load_print_meta: pooling type     = 0
0.00.057.802 I llm_load_print_meta: rope type        = 2
0.00.057.802 I llm_load_print_meta: rope scaling     = linear
0.00.057.804 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.805 I llm_load_print_meta: freq_scale_train = 1
0.00.057.805 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.805 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.805 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.805 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.805 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.806 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.806 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.806 I llm_load_print_meta: model type       = 1.4B
0.00.057.806 I llm_load_print_meta: model ftype      = Q4_1
0.00.057.807 I llm_load_print_meta: model params     = 1.41 B
0.00.057.807 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.057.808 I llm_load_print_meta: general.name     = 1.4B
0.00.057.808 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.809 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.809 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.809 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.812 I llm_load_print_meta: LF token         = 128 ''
0.00.057.813 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.813 I llm_load_print_meta: max token length = 1024
0.00.059.836 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.836 I llm_load_tensors: offloading output layer to GPU
0.00.059.836 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.846 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.059.847 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.060.237 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.237 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.237 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.238 I llama_new_context_with_model: n_batch       = 2048
0.00.060.238 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.238 I llama_new_context_with_model: flash_attn    = 0
0.00.060.238 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.238 I llama_new_context_with_model: freq_scale    = 1
0.00.060.239 I ggml_metal_init: allocating
0.00.060.242 I ggml_metal_init: found device: Apple M4
0.00.060.244 I ggml_metal_init: picking default device: Apple M4
0.00.060.840 I ggml_metal_init: using embedded metal library
0.00.063.221 I ggml_metal_init: GPU name:   Apple M4
0.00.063.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.223 I ggml_metal_init: simdgroup reduction   = true
0.00.063.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.224 I ggml_metal_init: has bfloat            = true
0.00.063.224 I ggml_metal_init: use bfloat            = true
0.00.063.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.391 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.179 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.190 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.208 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.334 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.335 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.335 I llama_new_context_with_model: graph nodes  = 967
0.00.095.336 I llama_new_context_with_model: graph splits = 2
0.00.095.338 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.857 I main: llama threadpool init, n_threads = 4
0.00.705.899 I 
0.00.705.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.924 I 
0.00.706.149 I sampler seed: 1234
0.00.706.154 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.199 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.204 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.204 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.435.907 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62610.23 tokens per second)
0.01.435.909 I llama_perf_context_print:        load time =     691.34 ms
0.01.435.909 I llama_perf_context_print: prompt eval time =      43.57 ms /     7 tokens (    6.22 ms per token,   160.66 tokens per second)
0.01.435.912 I llama_perf_context_print:        eval time =     683.27 ms /    63 runs   (   10.85 ms per token,    92.20 tokens per second)
0.01.435.916 I llama_perf_context_print:       total time =     730.05 ms /    70 tokens
0.01.436.159 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.946 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.247 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.247 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.247 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.810 I llama_model_loader: - type  f32:  194 tensors
0.00.024.810 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.811 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.455 I llm_load_vocab: special tokens cache size = 25
0.00.051.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.499 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.500 I llm_load_print_meta: arch             = gptneox
0.00.051.500 I llm_load_print_meta: vocab type       = BPE
0.00.051.500 I llm_load_print_meta: n_vocab          = 50304
0.00.051.500 I llm_load_print_meta: n_merges         = 50009
0.00.051.501 I llm_load_print_meta: vocab_only       = 0
0.00.051.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.501 I llm_load_print_meta: n_embd           = 2048
0.00.051.501 I llm_load_print_meta: n_layer          = 24
0.00.051.503 I llm_load_print_meta: n_head           = 16
0.00.051.504 I llm_load_print_meta: n_head_kv        = 16
0.00.051.506 I llm_load_print_meta: n_rot            = 32
0.00.051.506 I llm_load_print_meta: n_swa            = 0
0.00.051.506 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.506 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.507 I llm_load_print_meta: n_gqa            = 1
0.00.051.508 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.509 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.509 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.510 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.510 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.510 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.510 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.511 I llm_load_print_meta: n_ff             = 8192
0.00.051.511 I llm_load_print_meta: n_expert         = 0
0.00.051.511 I llm_load_print_meta: n_expert_used    = 0
0.00.051.511 I llm_load_print_meta: causal attn      = 1
0.00.051.511 I llm_load_print_meta: pooling type     = 0
0.00.051.511 I llm_load_print_meta: rope type        = 2
0.00.051.512 I llm_load_print_meta: rope scaling     = linear
0.00.051.512 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.512 I llm_load_print_meta: freq_scale_train = 1
0.00.051.513 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.513 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.513 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.513 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.514 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.514 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.514 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.514 I llm_load_print_meta: model type       = 1.4B
0.00.051.515 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.515 I llm_load_print_meta: model params     = 1.41 B
0.00.051.516 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.516 I llm_load_print_meta: general.name     = 1.4B
0.00.051.516 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.516 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.517 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.517 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.517 I llm_load_print_meta: LF token         = 128 ''
0.00.051.519 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.519 I llm_load_print_meta: max token length = 1024
0.00.053.245 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.245 I llm_load_tensors: offloading output layer to GPU
0.00.053.245 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.251 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.252 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.616 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.617 I llama_new_context_with_model: n_ctx         = 128
0.00.053.617 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.617 I llama_new_context_with_model: n_batch       = 128
0.00.053.618 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.618 I llama_new_context_with_model: flash_attn    = 0
0.00.053.618 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.619 I llama_new_context_with_model: freq_scale    = 1
0.00.053.619 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.619 I ggml_metal_init: allocating
0.00.053.625 I ggml_metal_init: found device: Apple M4
0.00.053.628 I ggml_metal_init: picking default device: Apple M4
0.00.054.196 I ggml_metal_init: using embedded metal library
0.00.056.495 I ggml_metal_init: GPU name:   Apple M4
0.00.056.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.497 I ggml_metal_init: simdgroup reduction   = true
0.00.056.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.497 I ggml_metal_init: has bfloat            = true
0.00.056.498 I ggml_metal_init: use bfloat            = true
0.00.056.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.019 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.437 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.441 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.456 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.423 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.424 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.425 I llama_new_context_with_model: graph nodes  = 967
0.00.068.425 I llama_new_context_with_model: graph splits = 2
0.00.068.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.154 I 
0.00.662.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.255 I perplexity: tokenizing the input ..
0.00.670.380 I perplexity: tokenization took 8.122 ms
0.00.670.383 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.168 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.794.342 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.794.374 I llama_perf_context_print:        load time =     653.20 ms
0.00.794.378 I llama_perf_context_print: prompt eval time =     122.55 ms /   128 tokens (    0.96 ms per token,  1044.45 tokens per second)
0.00.794.379 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.379 I llama_perf_context_print:       total time =     132.23 ms /   129 tokens
0.00.794.849 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.079s
sys	0m0.106s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.919 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.418 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.418 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.419 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.419 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.419 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.420 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.421 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.357 I llama_model_loader: - type  f32:  194 tensors
0.00.027.357 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.357 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.710 I llm_load_vocab: special tokens cache size = 25
0.00.055.062 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.067 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.067 I llm_load_print_meta: arch             = gptneox
0.00.055.067 I llm_load_print_meta: vocab type       = BPE
0.00.055.068 I llm_load_print_meta: n_vocab          = 50304
0.00.055.068 I llm_load_print_meta: n_merges         = 50009
0.00.055.068 I llm_load_print_meta: vocab_only       = 0
0.00.055.070 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.070 I llm_load_print_meta: n_embd           = 2048
0.00.055.070 I llm_load_print_meta: n_layer          = 24
0.00.055.074 I llm_load_print_meta: n_head           = 16
0.00.055.075 I llm_load_print_meta: n_head_kv        = 16
0.00.055.075 I llm_load_print_meta: n_rot            = 32
0.00.055.075 I llm_load_print_meta: n_swa            = 0
0.00.055.076 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.078 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.078 I llm_load_print_meta: n_gqa            = 1
0.00.055.079 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.079 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.080 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.080 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.080 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.081 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.081 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.081 I llm_load_print_meta: n_ff             = 8192
0.00.055.081 I llm_load_print_meta: n_expert         = 0
0.00.055.081 I llm_load_print_meta: n_expert_used    = 0
0.00.055.082 I llm_load_print_meta: causal attn      = 1
0.00.055.082 I llm_load_print_meta: pooling type     = 0
0.00.055.082 I llm_load_print_meta: rope type        = 2
0.00.055.082 I llm_load_print_meta: rope scaling     = linear
0.00.055.083 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.083 I llm_load_print_meta: freq_scale_train = 1
0.00.055.083 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.084 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.084 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.084 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.085 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.085 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.085 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.085 I llm_load_print_meta: model type       = 1.4B
0.00.055.086 I llm_load_print_meta: model ftype      = Q5_0
0.00.055.086 I llm_load_print_meta: model params     = 1.41 B
0.00.055.087 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.055.087 I llm_load_print_meta: general.name     = 1.4B
0.00.055.087 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.087 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.087 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.087 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.088 I llm_load_print_meta: LF token         = 128 ''
0.00.055.088 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.088 I llm_load_print_meta: max token length = 1024
0.00.057.010 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.010 I llm_load_tensors: offloading output layer to GPU
0.00.057.011 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.022 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.023 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.057.358 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.359 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.359 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.359 I llama_new_context_with_model: n_batch       = 2048
0.00.057.359 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.359 I llama_new_context_with_model: flash_attn    = 0
0.00.057.360 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.360 I llama_new_context_with_model: freq_scale    = 1
0.00.057.361 I ggml_metal_init: allocating
0.00.057.368 I ggml_metal_init: found device: Apple M4
0.00.057.373 I ggml_metal_init: picking default device: Apple M4
0.00.058.001 I ggml_metal_init: using embedded metal library
0.00.060.436 I ggml_metal_init: GPU name:   Apple M4
0.00.060.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.439 I ggml_metal_init: simdgroup reduction   = true
0.00.060.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.439 I ggml_metal_init: has bfloat            = true
0.00.060.440 I ggml_metal_init: use bfloat            = true
0.00.060.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.441 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.509 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.605 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.632 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.539 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.541 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.541 I llama_new_context_with_model: graph nodes  = 967
0.00.090.541 I llama_new_context_with_model: graph splits = 2
0.00.090.544 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.677 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.133 I main: llama threadpool init, n_threads = 4
0.00.773.171 I 
0.00.773.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.200 I 
0.00.773.352 I sampler seed: 1234
0.00.773.357 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.371 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.374 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.560.939 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.560.940 I llama_perf_context_print:        load time =     764.21 ms
0.01.560.943 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.51 tokens per second)
0.01.560.944 I llama_perf_context_print:        eval time =     741.38 ms /    63 runs   (   11.77 ms per token,    84.98 tokens per second)
0.01.560.944 I llama_perf_context_print:       total time =     787.81 ms /    70 tokens
0.01.561.188 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.112s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.498 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.473 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.474 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.304 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.084 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.085 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.085 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.085 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.086 I llama_model_loader: - type  f32:  194 tensors
0.00.026.086 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.086 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.980 I llm_load_vocab: special tokens cache size = 25
0.00.052.063 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.065 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.066 I llm_load_print_meta: arch             = gptneox
0.00.052.066 I llm_load_print_meta: vocab type       = BPE
0.00.052.066 I llm_load_print_meta: n_vocab          = 50304
0.00.052.066 I llm_load_print_meta: n_merges         = 50009
0.00.052.067 I llm_load_print_meta: vocab_only       = 0
0.00.052.067 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.067 I llm_load_print_meta: n_embd           = 2048
0.00.052.067 I llm_load_print_meta: n_layer          = 24
0.00.052.069 I llm_load_print_meta: n_head           = 16
0.00.052.071 I llm_load_print_meta: n_head_kv        = 16
0.00.052.071 I llm_load_print_meta: n_rot            = 32
0.00.052.071 I llm_load_print_meta: n_swa            = 0
0.00.052.071 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.071 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.072 I llm_load_print_meta: n_gqa            = 1
0.00.052.073 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.074 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.074 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.074 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.075 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.075 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.075 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.076 I llm_load_print_meta: n_ff             = 8192
0.00.052.076 I llm_load_print_meta: n_expert         = 0
0.00.052.076 I llm_load_print_meta: n_expert_used    = 0
0.00.052.076 I llm_load_print_meta: causal attn      = 1
0.00.052.076 I llm_load_print_meta: pooling type     = 0
0.00.052.076 I llm_load_print_meta: rope type        = 2
0.00.052.077 I llm_load_print_meta: rope scaling     = linear
0.00.052.078 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.078 I llm_load_print_meta: freq_scale_train = 1
0.00.052.078 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.079 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.079 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.079 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.079 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.079 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.080 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.080 I llm_load_print_meta: model type       = 1.4B
0.00.052.080 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.081 I llm_load_print_meta: model params     = 1.41 B
0.00.052.081 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.081 I llm_load_print_meta: general.name     = 1.4B
0.00.052.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: LF token         = 128 ''
0.00.052.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: max token length = 1024
0.00.054.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.045 I llm_load_tensors: offloading output layer to GPU
0.00.054.046 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.056 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.057 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.408 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.409 I llama_new_context_with_model: n_ctx         = 128
0.00.054.409 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.409 I llama_new_context_with_model: n_batch       = 128
0.00.054.409 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.409 I llama_new_context_with_model: flash_attn    = 0
0.00.054.410 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.410 I llama_new_context_with_model: freq_scale    = 1
0.00.054.410 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.411 I ggml_metal_init: allocating
0.00.054.417 I ggml_metal_init: found device: Apple M4
0.00.054.420 I ggml_metal_init: picking default device: Apple M4
0.00.054.993 I ggml_metal_init: using embedded metal library
0.00.057.313 I ggml_metal_init: GPU name:   Apple M4
0.00.057.315 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.316 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.316 I ggml_metal_init: simdgroup reduction   = true
0.00.057.316 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.316 I ggml_metal_init: has bfloat            = true
0.00.057.316 I ggml_metal_init: use bfloat            = true
0.00.057.317 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.317 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.659 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.026 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.031 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.925 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.926 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.926 I llama_new_context_with_model: graph nodes  = 967
0.00.068.927 I llama_new_context_with_model: graph splits = 2
0.00.068.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.800 I 
0.00.712.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.840 I perplexity: tokenizing the input ..
0.00.720.759 I perplexity: tokenization took 7.917 ms
0.00.720.762 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.856.087 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.857.322 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.857.347 I llama_perf_context_print:        load time =     702.30 ms
0.00.857.348 I llama_perf_context_print: prompt eval time =     135.10 ms /   128 tokens (    1.06 ms per token,   947.47 tokens per second)
0.00.857.349 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.350 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.857.839 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.078s
sys	0m0.112s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.935 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.481 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.829 I llama_model_loader: - type  f32:  194 tensors
0.00.025.829 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.822 I llm_load_vocab: special tokens cache size = 25
0.00.051.847 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.850 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.850 I llm_load_print_meta: arch             = gptneox
0.00.051.851 I llm_load_print_meta: vocab type       = BPE
0.00.051.851 I llm_load_print_meta: n_vocab          = 50304
0.00.051.851 I llm_load_print_meta: n_merges         = 50009
0.00.051.851 I llm_load_print_meta: vocab_only       = 0
0.00.051.852 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.852 I llm_load_print_meta: n_embd           = 2048
0.00.051.852 I llm_load_print_meta: n_layer          = 24
0.00.051.854 I llm_load_print_meta: n_head           = 16
0.00.051.855 I llm_load_print_meta: n_head_kv        = 16
0.00.051.855 I llm_load_print_meta: n_rot            = 32
0.00.051.856 I llm_load_print_meta: n_swa            = 0
0.00.051.856 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.856 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.857 I llm_load_print_meta: n_gqa            = 1
0.00.051.857 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.858 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.858 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.859 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.859 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.859 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.859 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.860 I llm_load_print_meta: n_ff             = 8192
0.00.051.860 I llm_load_print_meta: n_expert         = 0
0.00.051.860 I llm_load_print_meta: n_expert_used    = 0
0.00.051.861 I llm_load_print_meta: causal attn      = 1
0.00.051.861 I llm_load_print_meta: pooling type     = 0
0.00.051.861 I llm_load_print_meta: rope type        = 2
0.00.051.861 I llm_load_print_meta: rope scaling     = linear
0.00.051.862 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.862 I llm_load_print_meta: freq_scale_train = 1
0.00.051.864 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.864 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.865 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.865 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.865 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.865 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.865 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.865 I llm_load_print_meta: model type       = 1.4B
0.00.051.866 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.866 I llm_load_print_meta: model params     = 1.41 B
0.00.051.867 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.867 I llm_load_print_meta: general.name     = 1.4B
0.00.051.867 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.867 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.868 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.868 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.868 I llm_load_print_meta: LF token         = 128 ''
0.00.051.868 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.868 I llm_load_print_meta: max token length = 1024
0.00.053.839 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.839 I llm_load_tensors: offloading output layer to GPU
0.00.053.839 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.850 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.852 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.181 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.182 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.182 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.182 I llama_new_context_with_model: n_batch       = 2048
0.00.054.182 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.183 I llama_new_context_with_model: flash_attn    = 0
0.00.054.183 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.183 I llama_new_context_with_model: freq_scale    = 1
0.00.054.184 I ggml_metal_init: allocating
0.00.054.186 I ggml_metal_init: found device: Apple M4
0.00.054.188 I ggml_metal_init: picking default device: Apple M4
0.00.054.765 I ggml_metal_init: using embedded metal library
0.00.057.090 I ggml_metal_init: GPU name:   Apple M4
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.093 I ggml_metal_init: simdgroup reduction   = true
0.00.057.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.093 I ggml_metal_init: has bfloat            = true
0.00.057.093 I ggml_metal_init: use bfloat            = true
0.00.057.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.872 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.437 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.443 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.461 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.547 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.548 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.549 I llama_new_context_with_model: graph nodes  = 967
0.00.087.549 I llama_new_context_with_model: graph splits = 2
0.00.087.552 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.695 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.359 I main: llama threadpool init, n_threads = 4
0.00.821.422 I 
0.00.821.444 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.444 I 
0.00.821.666 I sampler seed: 1234
0.00.821.671 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.687 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.687 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.651.060 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.651.061 I llama_perf_context_print:        load time =     811.42 ms
0.01.651.062 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.55 tokens per second)
0.01.651.063 I llama_perf_context_print:        eval time =     784.27 ms /    63 runs   (   12.45 ms per token,    80.33 tokens per second)
0.01.651.064 I llama_perf_context_print:       total time =     829.71 ms /    70 tokens
0.01.651.313 I ggml_metal_free: deallocating

real	0m1.670s
user	0m0.109s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.860 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.117 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.137 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.918 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.642 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.643 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.643 I llama_model_loader: - type  f32:  194 tensors
0.00.024.643 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.644 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.201 I llm_load_vocab: special tokens cache size = 25
0.00.051.093 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.096 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.096 I llm_load_print_meta: arch             = gptneox
0.00.051.097 I llm_load_print_meta: vocab type       = BPE
0.00.051.097 I llm_load_print_meta: n_vocab          = 50304
0.00.051.097 I llm_load_print_meta: n_merges         = 50009
0.00.051.097 I llm_load_print_meta: vocab_only       = 0
0.00.051.098 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.098 I llm_load_print_meta: n_embd           = 2048
0.00.051.098 I llm_load_print_meta: n_layer          = 24
0.00.051.100 I llm_load_print_meta: n_head           = 16
0.00.051.101 I llm_load_print_meta: n_head_kv        = 16
0.00.051.101 I llm_load_print_meta: n_rot            = 32
0.00.051.101 I llm_load_print_meta: n_swa            = 0
0.00.051.101 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.102 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.102 I llm_load_print_meta: n_gqa            = 1
0.00.051.103 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.104 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.104 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.107 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.107 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.107 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.107 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.108 I llm_load_print_meta: n_ff             = 8192
0.00.051.110 I llm_load_print_meta: n_expert         = 0
0.00.051.110 I llm_load_print_meta: n_expert_used    = 0
0.00.051.110 I llm_load_print_meta: causal attn      = 1
0.00.051.110 I llm_load_print_meta: pooling type     = 0
0.00.051.110 I llm_load_print_meta: rope type        = 2
0.00.051.111 I llm_load_print_meta: rope scaling     = linear
0.00.051.112 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.113 I llm_load_print_meta: freq_scale_train = 1
0.00.051.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.114 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.114 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.114 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.114 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.116 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.117 I llm_load_print_meta: model type       = 1.4B
0.00.051.117 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.117 I llm_load_print_meta: model params     = 1.41 B
0.00.051.121 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.121 I llm_load_print_meta: general.name     = 1.4B
0.00.051.122 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.122 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.124 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.124 I llm_load_print_meta: LF token         = 128 ''
0.00.051.124 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.124 I llm_load_print_meta: max token length = 1024
0.00.053.123 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.123 I llm_load_tensors: offloading output layer to GPU
0.00.053.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.134 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.135 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.552 I llama_new_context_with_model: n_ctx         = 128
0.00.053.552 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.553 I llama_new_context_with_model: n_batch       = 128
0.00.053.553 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.553 I llama_new_context_with_model: flash_attn    = 0
0.00.053.553 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.553 I llama_new_context_with_model: freq_scale    = 1
0.00.053.554 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.554 I ggml_metal_init: allocating
0.00.053.557 I ggml_metal_init: found device: Apple M4
0.00.053.559 I ggml_metal_init: picking default device: Apple M4
0.00.054.123 I ggml_metal_init: using embedded metal library
0.00.056.466 I ggml_metal_init: GPU name:   Apple M4
0.00.056.468 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.468 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.468 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.468 I ggml_metal_init: simdgroup reduction   = true
0.00.056.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.469 I ggml_metal_init: has bfloat            = true
0.00.056.469 I ggml_metal_init: use bfloat            = true
0.00.056.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.471 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.075 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.412 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.417 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.434 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.300 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.301 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.301 I llama_new_context_with_model: graph nodes  = 967
0.00.068.301 I llama_new_context_with_model: graph splits = 2
0.00.068.303 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.727 I 
0.00.745.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.764 I perplexity: tokenizing the input ..
0.00.753.684 I perplexity: tokenization took 7.919 ms
0.00.753.688 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.477 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.889.627 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.652 I llama_perf_context_print:        load time =     736.86 ms
0.00.889.653 I llama_perf_context_print: prompt eval time =     134.56 ms /   128 tokens (    1.05 ms per token,   951.25 tokens per second)
0.00.889.654 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.654 I llama_perf_context_print:       total time =     143.92 ms /   129 tokens
0.00.890.109 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.078s
sys	0m0.110s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.967 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.861 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.867 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.873 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.876 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.767 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.639 I llama_model_loader: - type  f32:  194 tensors
0.00.025.639 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.639 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.540 I llm_load_vocab: special tokens cache size = 25
0.00.052.351 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.354 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.354 I llm_load_print_meta: arch             = gptneox
0.00.052.354 I llm_load_print_meta: vocab type       = BPE
0.00.052.355 I llm_load_print_meta: n_vocab          = 50304
0.00.052.355 I llm_load_print_meta: n_merges         = 50009
0.00.052.355 I llm_load_print_meta: vocab_only       = 0
0.00.052.355 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.355 I llm_load_print_meta: n_embd           = 2048
0.00.052.356 I llm_load_print_meta: n_layer          = 24
0.00.052.358 I llm_load_print_meta: n_head           = 16
0.00.052.359 I llm_load_print_meta: n_head_kv        = 16
0.00.052.360 I llm_load_print_meta: n_rot            = 32
0.00.052.360 I llm_load_print_meta: n_swa            = 0
0.00.052.360 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.360 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.361 I llm_load_print_meta: n_gqa            = 1
0.00.052.361 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.362 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.363 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.363 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.364 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.364 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.365 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.365 I llm_load_print_meta: n_ff             = 8192
0.00.052.365 I llm_load_print_meta: n_expert         = 0
0.00.052.365 I llm_load_print_meta: n_expert_used    = 0
0.00.052.366 I llm_load_print_meta: causal attn      = 1
0.00.052.366 I llm_load_print_meta: pooling type     = 0
0.00.052.366 I llm_load_print_meta: rope type        = 2
0.00.052.366 I llm_load_print_meta: rope scaling     = linear
0.00.052.367 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.367 I llm_load_print_meta: freq_scale_train = 1
0.00.052.367 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.367 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.368 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.368 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.368 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.368 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.368 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.368 I llm_load_print_meta: model type       = 1.4B
0.00.052.370 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.371 I llm_load_print_meta: model params     = 1.41 B
0.00.052.371 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.371 I llm_load_print_meta: general.name     = 1.4B
0.00.052.372 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.372 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.372 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.372 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.372 I llm_load_print_meta: LF token         = 128 ''
0.00.052.376 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.377 I llm_load_print_meta: max token length = 1024
0.00.054.308 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.308 I llm_load_tensors: offloading output layer to GPU
0.00.054.308 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.319 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.320 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.756 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.757 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.757 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.757 I llama_new_context_with_model: n_batch       = 2048
0.00.054.757 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.757 I llama_new_context_with_model: flash_attn    = 0
0.00.054.758 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.758 I llama_new_context_with_model: freq_scale    = 1
0.00.054.759 I ggml_metal_init: allocating
0.00.054.764 I ggml_metal_init: found device: Apple M4
0.00.054.766 I ggml_metal_init: picking default device: Apple M4
0.00.055.350 I ggml_metal_init: using embedded metal library
0.00.057.701 I ggml_metal_init: GPU name:   Apple M4
0.00.057.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.703 I ggml_metal_init: simdgroup reduction   = true
0.00.057.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.704 I ggml_metal_init: has bfloat            = true
0.00.057.704 I ggml_metal_init: use bfloat            = true
0.00.057.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.391 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.508 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.514 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.532 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.586 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.587 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.588 I llama_new_context_with_model: graph nodes  = 967
0.00.088.588 I llama_new_context_with_model: graph splits = 2
0.00.088.593 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.482 I main: llama threadpool init, n_threads = 4
0.00.508.526 I 
0.00.508.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.551 I 
0.00.508.778 I sampler seed: 1234
0.00.508.784 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.508.816 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.508.818 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.508.818 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.186.476 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.01.186.477 I llama_perf_context_print:        load time =     498.51 ms
0.01.186.478 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.65 tokens per second)
0.01.186.479 I llama_perf_context_print:        eval time =     639.02 ms /    63 runs   (   10.14 ms per token,    98.59 tokens per second)
0.01.186.479 I llama_perf_context_print:       total time =     678.00 ms /    70 tokens
0.01.186.707 I ggml_metal_free: deallocating

real	0m1.205s
user	0m0.111s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.921 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.108 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.110 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.111 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.116 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.116 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.117 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.119 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.120 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.120 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.609 I llama_model_loader: - type  f32:  194 tensors
0.00.024.610 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.610 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.610 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.322 I llm_load_vocab: special tokens cache size = 25
0.00.050.287 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.290 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.290 I llm_load_print_meta: arch             = gptneox
0.00.050.290 I llm_load_print_meta: vocab type       = BPE
0.00.050.290 I llm_load_print_meta: n_vocab          = 50304
0.00.050.291 I llm_load_print_meta: n_merges         = 50009
0.00.050.291 I llm_load_print_meta: vocab_only       = 0
0.00.050.291 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.291 I llm_load_print_meta: n_embd           = 2048
0.00.050.291 I llm_load_print_meta: n_layer          = 24
0.00.050.294 I llm_load_print_meta: n_head           = 16
0.00.050.295 I llm_load_print_meta: n_head_kv        = 16
0.00.050.295 I llm_load_print_meta: n_rot            = 32
0.00.050.295 I llm_load_print_meta: n_swa            = 0
0.00.050.295 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.295 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.296 I llm_load_print_meta: n_gqa            = 1
0.00.050.297 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.297 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.298 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.300 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.301 I llm_load_print_meta: n_ff             = 8192
0.00.050.301 I llm_load_print_meta: n_expert         = 0
0.00.050.301 I llm_load_print_meta: n_expert_used    = 0
0.00.050.301 I llm_load_print_meta: causal attn      = 1
0.00.050.301 I llm_load_print_meta: pooling type     = 0
0.00.050.301 I llm_load_print_meta: rope type        = 2
0.00.050.302 I llm_load_print_meta: rope scaling     = linear
0.00.050.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.302 I llm_load_print_meta: freq_scale_train = 1
0.00.050.303 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.303 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.303 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.305 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.305 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.305 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.305 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.306 I llm_load_print_meta: model type       = 1.4B
0.00.050.306 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.306 I llm_load_print_meta: model params     = 1.41 B
0.00.050.307 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.307 I llm_load_print_meta: general.name     = 1.4B
0.00.050.307 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.307 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: LF token         = 128 ''
0.00.050.309 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.310 I llm_load_print_meta: max token length = 1024
0.00.052.145 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.145 I llm_load_tensors: offloading output layer to GPU
0.00.052.145 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.156 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.157 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.492 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.493 I llama_new_context_with_model: n_ctx         = 128
0.00.052.493 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.493 I llama_new_context_with_model: n_batch       = 128
0.00.052.493 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.493 I llama_new_context_with_model: flash_attn    = 0
0.00.052.494 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.494 I llama_new_context_with_model: freq_scale    = 1
0.00.052.494 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.495 I ggml_metal_init: allocating
0.00.052.497 I ggml_metal_init: found device: Apple M4
0.00.052.499 I ggml_metal_init: picking default device: Apple M4
0.00.053.068 I ggml_metal_init: using embedded metal library
0.00.055.354 I ggml_metal_init: GPU name:   Apple M4
0.00.055.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.356 I ggml_metal_init: simdgroup reduction   = true
0.00.055.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.356 I ggml_metal_init: has bfloat            = true
0.00.055.357 I ggml_metal_init: use bfloat            = true
0.00.055.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.685 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.129 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.132 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.145 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.057 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.058 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.059 I llama_new_context_with_model: graph nodes  = 967
0.00.067.059 I llama_new_context_with_model: graph splits = 2
0.00.067.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.918 I 
0.00.444.954 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.969 I perplexity: tokenizing the input ..
0.00.453.036 I perplexity: tokenization took 8.066 ms
0.00.453.039 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.781 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.587.034 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.587.064 I llama_perf_context_print:        load time =     434.99 ms
0.00.587.065 I llama_perf_context_print: prompt eval time =     132.50 ms /   128 tokens (    1.04 ms per token,   966.02 tokens per second)
0.00.587.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.587.066 I llama_perf_context_print:       total time =     142.15 ms /   129 tokens
0.00.587.555 I ggml_metal_free: deallocating

real	0m0.602s
user	0m0.077s
sys	0m0.067s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.516 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.117 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.127 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.005 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.813 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.815 I llama_model_loader: - type  f32:  194 tensors
0.00.026.815 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.815 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.816 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.846 I llm_load_vocab: special tokens cache size = 25
0.00.052.810 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.812 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.813 I llm_load_print_meta: arch             = gptneox
0.00.052.813 I llm_load_print_meta: vocab type       = BPE
0.00.052.813 I llm_load_print_meta: n_vocab          = 50304
0.00.052.813 I llm_load_print_meta: n_merges         = 50009
0.00.052.813 I llm_load_print_meta: vocab_only       = 0
0.00.052.814 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.814 I llm_load_print_meta: n_embd           = 2048
0.00.052.814 I llm_load_print_meta: n_layer          = 24
0.00.052.817 I llm_load_print_meta: n_head           = 16
0.00.052.820 I llm_load_print_meta: n_head_kv        = 16
0.00.052.820 I llm_load_print_meta: n_rot            = 32
0.00.052.820 I llm_load_print_meta: n_swa            = 0
0.00.052.820 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.820 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.821 I llm_load_print_meta: n_gqa            = 1
0.00.052.826 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.827 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.828 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.828 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.828 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.828 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.828 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.829 I llm_load_print_meta: n_ff             = 8192
0.00.052.831 I llm_load_print_meta: n_expert         = 0
0.00.052.832 I llm_load_print_meta: n_expert_used    = 0
0.00.052.832 I llm_load_print_meta: causal attn      = 1
0.00.052.833 I llm_load_print_meta: pooling type     = 0
0.00.052.833 I llm_load_print_meta: rope type        = 2
0.00.052.833 I llm_load_print_meta: rope scaling     = linear
0.00.052.833 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.834 I llm_load_print_meta: freq_scale_train = 1
0.00.052.834 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.835 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.835 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.835 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.835 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.835 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.835 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.836 I llm_load_print_meta: model type       = 1.4B
0.00.052.837 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.838 I llm_load_print_meta: model params     = 1.41 B
0.00.052.838 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.838 I llm_load_print_meta: general.name     = 1.4B
0.00.052.839 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.839 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.839 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.839 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.840 I llm_load_print_meta: LF token         = 128 ''
0.00.052.840 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.841 I llm_load_print_meta: max token length = 1024
0.00.054.783 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.783 I llm_load_tensors: offloading output layer to GPU
0.00.054.784 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.794 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.796 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.137 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.138 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.138 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.138 I llama_new_context_with_model: n_batch       = 2048
0.00.055.138 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.139 I llama_new_context_with_model: flash_attn    = 0
0.00.055.139 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.139 I llama_new_context_with_model: freq_scale    = 1
0.00.055.140 I ggml_metal_init: allocating
0.00.055.142 I ggml_metal_init: found device: Apple M4
0.00.055.144 I ggml_metal_init: picking default device: Apple M4
0.00.055.717 I ggml_metal_init: using embedded metal library
0.00.058.036 I ggml_metal_init: GPU name:   Apple M4
0.00.058.039 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.040 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.040 I ggml_metal_init: simdgroup reduction   = true
0.00.058.040 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.041 I ggml_metal_init: has bfloat            = true
0.00.058.041 I ggml_metal_init: use bfloat            = true
0.00.058.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.669 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.680 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.685 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.704 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.791 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.792 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.792 I llama_new_context_with_model: graph nodes  = 967
0.00.087.793 I llama_new_context_with_model: graph splits = 2
0.00.087.795 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.925 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.926 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.392 I main: llama threadpool init, n_threads = 4
0.00.616.431 I 
0.00.616.470 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.472 I 
0.00.616.706 I sampler seed: 1234
0.00.616.710 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.725 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.726 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.727 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.362.393 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.362.393 I llama_perf_context_print:        load time =     605.87 ms
0.01.362.394 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.77 tokens per second)
0.01.362.395 I llama_perf_context_print:        eval time =     702.10 ms /    63 runs   (   11.14 ms per token,    89.73 tokens per second)
0.01.362.395 I llama_perf_context_print:       total time =     746.00 ms /    70 tokens
0.01.362.641 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.740 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.311 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.312 I llama_model_loader: - type  f32:  194 tensors
0.00.026.313 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.313 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.313 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.313 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.978 I llm_load_vocab: special tokens cache size = 25
0.00.053.006 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.009 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.009 I llm_load_print_meta: arch             = gptneox
0.00.053.010 I llm_load_print_meta: vocab type       = BPE
0.00.053.010 I llm_load_print_meta: n_vocab          = 50304
0.00.053.010 I llm_load_print_meta: n_merges         = 50009
0.00.053.010 I llm_load_print_meta: vocab_only       = 0
0.00.053.010 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.011 I llm_load_print_meta: n_embd           = 2048
0.00.053.011 I llm_load_print_meta: n_layer          = 24
0.00.053.013 I llm_load_print_meta: n_head           = 16
0.00.053.016 I llm_load_print_meta: n_head_kv        = 16
0.00.053.016 I llm_load_print_meta: n_rot            = 32
0.00.053.017 I llm_load_print_meta: n_swa            = 0
0.00.053.017 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.018 I llm_load_print_meta: n_gqa            = 1
0.00.053.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.021 I llm_load_print_meta: n_ff             = 8192
0.00.053.022 I llm_load_print_meta: n_expert         = 0
0.00.053.022 I llm_load_print_meta: n_expert_used    = 0
0.00.053.022 I llm_load_print_meta: causal attn      = 1
0.00.053.022 I llm_load_print_meta: pooling type     = 0
0.00.053.022 I llm_load_print_meta: rope type        = 2
0.00.053.023 I llm_load_print_meta: rope scaling     = linear
0.00.053.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.023 I llm_load_print_meta: freq_scale_train = 1
0.00.053.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.024 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.024 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.024 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.024 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.025 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.025 I llm_load_print_meta: model type       = 1.4B
0.00.053.026 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.026 I llm_load_print_meta: model params     = 1.41 B
0.00.053.027 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.027 I llm_load_print_meta: general.name     = 1.4B
0.00.053.029 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.029 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.029 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.029 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.030 I llm_load_print_meta: LF token         = 128 ''
0.00.053.030 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.030 I llm_load_print_meta: max token length = 1024
0.00.054.842 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.842 I llm_load_tensors: offloading output layer to GPU
0.00.054.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.853 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.854 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.197 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.198 I llama_new_context_with_model: n_ctx         = 128
0.00.055.198 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.198 I llama_new_context_with_model: n_batch       = 128
0.00.055.199 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.199 I llama_new_context_with_model: flash_attn    = 0
0.00.055.199 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.199 I llama_new_context_with_model: freq_scale    = 1
0.00.055.200 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.200 I ggml_metal_init: allocating
0.00.055.203 I ggml_metal_init: found device: Apple M4
0.00.055.205 I ggml_metal_init: picking default device: Apple M4
0.00.055.758 I ggml_metal_init: using embedded metal library
0.00.058.339 I ggml_metal_init: GPU name:   Apple M4
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.342 I ggml_metal_init: simdgroup reduction   = true
0.00.058.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.342 I ggml_metal_init: has bfloat            = true
0.00.058.342 I ggml_metal_init: use bfloat            = true
0.00.058.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.090 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.328 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.331 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.344 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.232 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.233 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.233 I llama_new_context_with_model: graph nodes  = 967
0.00.070.234 I llama_new_context_with_model: graph splits = 2
0.00.070.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.235 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.184 I 
0.00.568.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.239 I perplexity: tokenizing the input ..
0.00.575.812 I perplexity: tokenization took 7.57 ms
0.00.575.818 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.009 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.708.251 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.708.273 I llama_perf_context_print:        load time =     557.44 ms
0.00.708.274 I llama_perf_context_print: prompt eval time =     130.95 ms /   128 tokens (    1.02 ms per token,   977.46 tokens per second)
0.00.708.275 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.708.276 I llama_perf_context_print:       total time =     140.09 ms /   129 tokens
0.00.708.656 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.078s
sys	0m0.071s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.875 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.206 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.207 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.207 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.207 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.790 I llama_model_loader: - type  f32:  194 tensors
0.00.026.791 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.791 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.791 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.773 I llm_load_vocab: special tokens cache size = 25
0.00.052.875 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.878 I llm_load_print_meta: arch             = gptneox
0.00.052.878 I llm_load_print_meta: vocab type       = BPE
0.00.052.878 I llm_load_print_meta: n_vocab          = 50304
0.00.052.878 I llm_load_print_meta: n_merges         = 50009
0.00.052.878 I llm_load_print_meta: vocab_only       = 0
0.00.052.879 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.879 I llm_load_print_meta: n_embd           = 2048
0.00.052.879 I llm_load_print_meta: n_layer          = 24
0.00.052.881 I llm_load_print_meta: n_head           = 16
0.00.052.883 I llm_load_print_meta: n_head_kv        = 16
0.00.052.884 I llm_load_print_meta: n_rot            = 32
0.00.052.884 I llm_load_print_meta: n_swa            = 0
0.00.052.884 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.884 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.885 I llm_load_print_meta: n_gqa            = 1
0.00.052.890 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.890 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.891 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.892 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.892 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.894 I llm_load_print_meta: n_ff             = 8192
0.00.052.894 I llm_load_print_meta: n_expert         = 0
0.00.052.895 I llm_load_print_meta: n_expert_used    = 0
0.00.052.897 I llm_load_print_meta: causal attn      = 1
0.00.052.897 I llm_load_print_meta: pooling type     = 0
0.00.052.897 I llm_load_print_meta: rope type        = 2
0.00.052.898 I llm_load_print_meta: rope scaling     = linear
0.00.052.898 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.898 I llm_load_print_meta: freq_scale_train = 1
0.00.052.899 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.899 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.899 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.899 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.899 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.905 I llm_load_print_meta: model type       = 1.4B
0.00.052.907 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.908 I llm_load_print_meta: model params     = 1.41 B
0.00.052.908 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.909 I llm_load_print_meta: general.name     = 1.4B
0.00.052.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.910 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.910 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.910 I llm_load_print_meta: LF token         = 128 ''
0.00.052.911 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.911 I llm_load_print_meta: max token length = 1024
0.00.054.831 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.831 I llm_load_tensors: offloading output layer to GPU
0.00.054.831 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.842 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.843 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.204 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.204 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.204 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.205 I llama_new_context_with_model: n_batch       = 2048
0.00.055.205 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.205 I llama_new_context_with_model: flash_attn    = 0
0.00.055.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.206 I llama_new_context_with_model: freq_scale    = 1
0.00.055.206 I ggml_metal_init: allocating
0.00.055.212 I ggml_metal_init: found device: Apple M4
0.00.055.216 I ggml_metal_init: picking default device: Apple M4
0.00.055.831 I ggml_metal_init: using embedded metal library
0.00.058.164 I ggml_metal_init: GPU name:   Apple M4
0.00.058.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.167 I ggml_metal_init: simdgroup reduction   = true
0.00.058.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.168 I ggml_metal_init: has bfloat            = true
0.00.058.168 I ggml_metal_init: use bfloat            = true
0.00.058.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.674 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.610 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.616 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.634 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.713 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.714 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.715 I llama_new_context_with_model: graph nodes  = 967
0.00.088.715 I llama_new_context_with_model: graph splits = 2
0.00.088.718 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.859 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.865 I main: llama threadpool init, n_threads = 4
0.00.668.904 I 
0.00.668.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.929 I 
0.00.669.154 I sampler seed: 1234
0.00.669.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.200 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.217 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.217 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.428.813 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.428.814 I llama_perf_context_print:        load time =     657.99 ms
0.01.428.816 I llama_perf_context_print: prompt eval time =      47.10 ms /     7 tokens (    6.73 ms per token,   148.62 tokens per second)
0.01.428.816 I llama_perf_context_print:        eval time =     709.44 ms /    63 runs   (   11.26 ms per token,    88.80 tokens per second)
0.01.428.817 I llama_perf_context_print:       total time =     759.95 ms /    70 tokens
0.01.429.043 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.110s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.728 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.883 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.884 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.884 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.884 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.885 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.886 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.886 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.887 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.887 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.888 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.888 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.405 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.405 I llama_model_loader: - type  f32:  194 tensors
0.00.025.406 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.406 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.406 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.365 I llm_load_vocab: special tokens cache size = 25
0.00.051.212 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.216 I llm_load_print_meta: arch             = gptneox
0.00.051.216 I llm_load_print_meta: vocab type       = BPE
0.00.051.216 I llm_load_print_meta: n_vocab          = 50304
0.00.051.217 I llm_load_print_meta: n_merges         = 50009
0.00.051.217 I llm_load_print_meta: vocab_only       = 0
0.00.051.217 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.217 I llm_load_print_meta: n_embd           = 2048
0.00.051.217 I llm_load_print_meta: n_layer          = 24
0.00.051.220 I llm_load_print_meta: n_head           = 16
0.00.051.222 I llm_load_print_meta: n_head_kv        = 16
0.00.051.222 I llm_load_print_meta: n_rot            = 32
0.00.051.222 I llm_load_print_meta: n_swa            = 0
0.00.051.222 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.223 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.225 I llm_load_print_meta: n_gqa            = 1
0.00.051.226 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.226 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.227 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.227 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.228 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.228 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.228 I llm_load_print_meta: n_ff             = 8192
0.00.051.229 I llm_load_print_meta: n_expert         = 0
0.00.051.229 I llm_load_print_meta: n_expert_used    = 0
0.00.051.229 I llm_load_print_meta: causal attn      = 1
0.00.051.230 I llm_load_print_meta: pooling type     = 0
0.00.051.231 I llm_load_print_meta: rope type        = 2
0.00.051.232 I llm_load_print_meta: rope scaling     = linear
0.00.051.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.233 I llm_load_print_meta: freq_scale_train = 1
0.00.051.233 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.233 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.233 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.234 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.234 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.234 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.238 I llm_load_print_meta: model type       = 1.4B
0.00.051.238 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.239 I llm_load_print_meta: model params     = 1.41 B
0.00.051.239 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.239 I llm_load_print_meta: general.name     = 1.4B
0.00.051.240 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.240 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.240 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.240 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: LF token         = 128 ''
0.00.051.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: max token length = 1024
0.00.053.244 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.244 I llm_load_tensors: offloading output layer to GPU
0.00.053.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.255 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.256 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.711 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.712 I llama_new_context_with_model: n_ctx         = 128
0.00.053.712 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.712 I llama_new_context_with_model: n_batch       = 128
0.00.053.713 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.713 I llama_new_context_with_model: flash_attn    = 0
0.00.053.713 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.714 I llama_new_context_with_model: freq_scale    = 1
0.00.053.714 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.714 I ggml_metal_init: allocating
0.00.053.721 I ggml_metal_init: found device: Apple M4
0.00.053.723 I ggml_metal_init: picking default device: Apple M4
0.00.054.310 I ggml_metal_init: using embedded metal library
0.00.056.690 I ggml_metal_init: GPU name:   Apple M4
0.00.056.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.692 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.693 I ggml_metal_init: simdgroup reduction   = true
0.00.056.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.693 I ggml_metal_init: has bfloat            = true
0.00.056.695 I ggml_metal_init: use bfloat            = true
0.00.056.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.293 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.625 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.630 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.646 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.666 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.667 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.667 I llama_new_context_with_model: graph nodes  = 967
0.00.068.668 I llama_new_context_with_model: graph splits = 2
0.00.068.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.895 I 
0.00.612.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.965 I perplexity: tokenizing the input ..
0.00.621.065 I perplexity: tokenization took 8.099 ms
0.00.621.070 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.755.233 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.756.406 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.756.436 I llama_perf_context_print:        load time =     603.16 ms
0.00.756.437 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.67 tokens per second)
0.00.756.437 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.756.438 I llama_perf_context_print:       total time =     143.54 ms /   129 tokens
0.00.756.939 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.078s
sys	0m0.109s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.388 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.568 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.349 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.352 I llama_model_loader: - type  f32:  194 tensors
0.00.025.352 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.352 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.341 I llm_load_vocab: special tokens cache size = 25
0.00.051.339 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.341 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.342 I llm_load_print_meta: arch             = gptneox
0.00.051.342 I llm_load_print_meta: vocab type       = BPE
0.00.051.342 I llm_load_print_meta: n_vocab          = 50304
0.00.051.342 I llm_load_print_meta: n_merges         = 50009
0.00.051.343 I llm_load_print_meta: vocab_only       = 0
0.00.051.343 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.343 I llm_load_print_meta: n_embd           = 2048
0.00.051.343 I llm_load_print_meta: n_layer          = 24
0.00.051.346 I llm_load_print_meta: n_head           = 16
0.00.051.347 I llm_load_print_meta: n_head_kv        = 16
0.00.051.347 I llm_load_print_meta: n_rot            = 32
0.00.051.347 I llm_load_print_meta: n_swa            = 0
0.00.051.347 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.348 I llm_load_print_meta: n_gqa            = 1
0.00.051.349 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.351 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.352 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.352 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.353 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.353 I llm_load_print_meta: n_ff             = 8192
0.00.051.355 I llm_load_print_meta: n_expert         = 0
0.00.051.355 I llm_load_print_meta: n_expert_used    = 0
0.00.051.357 I llm_load_print_meta: causal attn      = 1
0.00.051.357 I llm_load_print_meta: pooling type     = 0
0.00.051.357 I llm_load_print_meta: rope type        = 2
0.00.051.358 I llm_load_print_meta: rope scaling     = linear
0.00.051.358 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.358 I llm_load_print_meta: freq_scale_train = 1
0.00.051.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.359 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.359 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.359 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.359 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.360 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.360 I llm_load_print_meta: model type       = 1.4B
0.00.051.364 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.365 I llm_load_print_meta: model params     = 1.41 B
0.00.051.366 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.366 I llm_load_print_meta: general.name     = 1.4B
0.00.051.366 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.367 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.367 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.367 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.368 I llm_load_print_meta: LF token         = 128 ''
0.00.051.368 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.368 I llm_load_print_meta: max token length = 1024
0.00.053.332 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.332 I llm_load_tensors: offloading output layer to GPU
0.00.053.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.343 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.344 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.685 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.685 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.686 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.686 I llama_new_context_with_model: n_batch       = 2048
0.00.053.686 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.686 I llama_new_context_with_model: flash_attn    = 0
0.00.053.686 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.687 I llama_new_context_with_model: freq_scale    = 1
0.00.053.687 I ggml_metal_init: allocating
0.00.053.690 I ggml_metal_init: found device: Apple M4
0.00.053.692 I ggml_metal_init: picking default device: Apple M4
0.00.054.272 I ggml_metal_init: using embedded metal library
0.00.056.576 I ggml_metal_init: GPU name:   Apple M4
0.00.056.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.580 I ggml_metal_init: simdgroup reduction   = true
0.00.056.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.580 I ggml_metal_init: has bfloat            = true
0.00.056.580 I ggml_metal_init: use bfloat            = true
0.00.056.580 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.581 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.128 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.246 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.251 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.270 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.361 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.363 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.363 I llama_new_context_with_model: graph nodes  = 967
0.00.086.363 I llama_new_context_with_model: graph splits = 2
0.00.086.366 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.524 I main: llama threadpool init, n_threads = 4
0.00.677.561 I 
0.00.677.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.588 I 
0.00.677.823 I sampler seed: 1234
0.00.677.828 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.860 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.861 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.861 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.527.816 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.527.818 I llama_perf_context_print:        load time =     668.13 ms
0.01.527.819 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.63 tokens per second)
0.01.527.819 I llama_perf_context_print:        eval time =     795.84 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.527.820 I llama_perf_context_print:       total time =     850.29 ms /    70 tokens
0.01.528.091 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.800 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.412 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.891 I llama_model_loader: - type  f32:  194 tensors
0.00.024.892 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.892 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.599 I llm_load_vocab: special tokens cache size = 25
0.00.050.606 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.608 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.608 I llm_load_print_meta: arch             = gptneox
0.00.050.609 I llm_load_print_meta: vocab type       = BPE
0.00.050.609 I llm_load_print_meta: n_vocab          = 50304
0.00.050.609 I llm_load_print_meta: n_merges         = 50009
0.00.050.609 I llm_load_print_meta: vocab_only       = 0
0.00.050.609 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.610 I llm_load_print_meta: n_embd           = 2048
0.00.050.610 I llm_load_print_meta: n_layer          = 24
0.00.050.612 I llm_load_print_meta: n_head           = 16
0.00.050.613 I llm_load_print_meta: n_head_kv        = 16
0.00.050.613 I llm_load_print_meta: n_rot            = 32
0.00.050.613 I llm_load_print_meta: n_swa            = 0
0.00.050.613 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.614 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.614 I llm_load_print_meta: n_gqa            = 1
0.00.050.615 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.616 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.616 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.617 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.617 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.617 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.617 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.618 I llm_load_print_meta: n_ff             = 8192
0.00.050.618 I llm_load_print_meta: n_expert         = 0
0.00.050.618 I llm_load_print_meta: n_expert_used    = 0
0.00.050.618 I llm_load_print_meta: causal attn      = 1
0.00.050.619 I llm_load_print_meta: pooling type     = 0
0.00.050.619 I llm_load_print_meta: rope type        = 2
0.00.050.619 I llm_load_print_meta: rope scaling     = linear
0.00.050.619 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.620 I llm_load_print_meta: freq_scale_train = 1
0.00.050.620 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.620 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.620 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.621 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.622 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.622 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.622 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.622 I llm_load_print_meta: model type       = 1.4B
0.00.050.623 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.623 I llm_load_print_meta: model params     = 1.41 B
0.00.050.624 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.624 I llm_load_print_meta: general.name     = 1.4B
0.00.050.624 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.625 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: LF token         = 128 ''
0.00.050.627 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: max token length = 1024
0.00.052.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.544 I llm_load_tensors: offloading output layer to GPU
0.00.052.544 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.555 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.556 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.880 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.881 I llama_new_context_with_model: n_ctx         = 128
0.00.052.881 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.881 I llama_new_context_with_model: n_batch       = 128
0.00.052.881 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.882 I llama_new_context_with_model: flash_attn    = 0
0.00.052.882 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.882 I llama_new_context_with_model: freq_scale    = 1
0.00.052.883 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.883 I ggml_metal_init: allocating
0.00.052.886 I ggml_metal_init: found device: Apple M4
0.00.052.888 I ggml_metal_init: picking default device: Apple M4
0.00.053.447 I ggml_metal_init: using embedded metal library
0.00.055.783 I ggml_metal_init: GPU name:   Apple M4
0.00.055.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.786 I ggml_metal_init: simdgroup reduction   = true
0.00.055.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.786 I ggml_metal_init: has bfloat            = true
0.00.055.786 I ggml_metal_init: use bfloat            = true
0.00.055.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.787 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.150 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.359 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.361 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.375 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.366 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.368 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.368 I llama_new_context_with_model: graph nodes  = 967
0.00.067.368 I llama_new_context_with_model: graph splits = 2
0.00.067.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.120 I 
0.00.631.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.166 I perplexity: tokenizing the input ..
0.00.639.112 I perplexity: tokenization took 7.944 ms
0.00.639.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.221 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.781.462 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.781.487 I llama_perf_context_print:        load time =     621.32 ms
0.00.781.488 I llama_perf_context_print: prompt eval time =     140.88 ms /   128 tokens (    1.10 ms per token,   908.57 tokens per second)
0.00.781.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.489 I llama_perf_context_print:       total time =     150.37 ms /   129 tokens
0.00.781.993 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.077s
sys	0m0.109s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.013.786 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.075 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.024.081 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.084 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.084 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.085 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.087 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.093 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.096 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.096 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.097 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.138 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.033.006 I llama_model_loader: - type  f32:  194 tensors
0.00.033.007 I llama_model_loader: - type q6_K:   98 tensors
0.00.054.051 I llm_load_vocab: special tokens cache size = 25
0.00.060.202 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.206 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.207 I llm_load_print_meta: arch             = gptneox
0.00.060.207 I llm_load_print_meta: vocab type       = BPE
0.00.060.207 I llm_load_print_meta: n_vocab          = 50304
0.00.060.207 I llm_load_print_meta: n_merges         = 50009
0.00.060.208 I llm_load_print_meta: vocab_only       = 0
0.00.060.208 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.208 I llm_load_print_meta: n_embd           = 2048
0.00.060.208 I llm_load_print_meta: n_layer          = 24
0.00.060.212 I llm_load_print_meta: n_head           = 16
0.00.060.213 I llm_load_print_meta: n_head_kv        = 16
0.00.060.213 I llm_load_print_meta: n_rot            = 32
0.00.060.213 I llm_load_print_meta: n_swa            = 0
0.00.060.214 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.214 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.215 I llm_load_print_meta: n_gqa            = 1
0.00.060.215 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.216 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.217 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.217 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.219 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.219 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.219 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.220 I llm_load_print_meta: n_ff             = 8192
0.00.060.220 I llm_load_print_meta: n_expert         = 0
0.00.060.221 I llm_load_print_meta: n_expert_used    = 0
0.00.060.221 I llm_load_print_meta: causal attn      = 1
0.00.060.221 I llm_load_print_meta: pooling type     = 0
0.00.060.221 I llm_load_print_meta: rope type        = 2
0.00.060.222 I llm_load_print_meta: rope scaling     = linear
0.00.060.222 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.222 I llm_load_print_meta: freq_scale_train = 1
0.00.060.222 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.223 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.223 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.223 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.223 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.223 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.224 I llm_load_print_meta: model type       = 1.4B
0.00.060.224 I llm_load_print_meta: model ftype      = Q6_K
0.00.060.224 I llm_load_print_meta: model params     = 1.41 B
0.00.060.225 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.060.225 I llm_load_print_meta: general.name     = 1.4B
0.00.060.225 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.225 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.226 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.226 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.226 I llm_load_print_meta: LF token         = 128 ''
0.00.060.226 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.227 I llm_load_print_meta: max token length = 1024
0.00.062.200 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.200 I llm_load_tensors: offloading output layer to GPU
0.00.062.201 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.212 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.062.213 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.062.587 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.588 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.588 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.588 I llama_new_context_with_model: n_batch       = 2048
0.00.062.588 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.589 I llama_new_context_with_model: flash_attn    = 0
0.00.062.589 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.590 I llama_new_context_with_model: freq_scale    = 1
0.00.062.591 I ggml_metal_init: allocating
0.00.062.594 I ggml_metal_init: found device: Apple M4
0.00.062.597 I ggml_metal_init: picking default device: Apple M4
0.00.063.242 I ggml_metal_init: using embedded metal library
0.00.065.649 I ggml_metal_init: GPU name:   Apple M4
0.00.065.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.651 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.652 I ggml_metal_init: simdgroup reduction   = true
0.00.065.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.652 I ggml_metal_init: has bfloat            = true
0.00.065.652 I ggml_metal_init: use bfloat            = true
0.00.065.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.481 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.117 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.127 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.160 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.101 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.104 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.104 I llama_new_context_with_model: graph nodes  = 967
0.00.096.105 I llama_new_context_with_model: graph splits = 2
0.00.096.107 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.249 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.849 I main: llama threadpool init, n_threads = 4
0.00.765.885 I 
0.00.765.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.931 I 
0.00.766.153 I sampler seed: 1234
0.00.766.160 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.178 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.178 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.641.096 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.641.096 I llama_perf_context_print:        load time =     752.06 ms
0.01.641.097 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.48 tokens per second)
0.01.641.098 I llama_perf_context_print:        eval time =     817.50 ms /    63 runs   (   12.98 ms per token,    77.06 tokens per second)
0.01.641.098 I llama_perf_context_print:       total time =     875.25 ms /    70 tokens
0.01.641.310 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.112s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4448 (1bf839b1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.929 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.963 I llama_model_loader: - type  f32:  194 tensors
0.00.024.963 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.761 I llm_load_vocab: special tokens cache size = 25
0.00.050.695 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.698 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.698 I llm_load_print_meta: arch             = gptneox
0.00.050.699 I llm_load_print_meta: vocab type       = BPE
0.00.050.699 I llm_load_print_meta: n_vocab          = 50304
0.00.050.699 I llm_load_print_meta: n_merges         = 50009
0.00.050.699 I llm_load_print_meta: vocab_only       = 0
0.00.050.699 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.700 I llm_load_print_meta: n_embd           = 2048
0.00.050.700 I llm_load_print_meta: n_layer          = 24
0.00.050.702 I llm_load_print_meta: n_head           = 16
0.00.050.703 I llm_load_print_meta: n_head_kv        = 16
0.00.050.704 I llm_load_print_meta: n_rot            = 32
0.00.050.705 I llm_load_print_meta: n_swa            = 0
0.00.050.705 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.705 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.705 I llm_load_print_meta: n_gqa            = 1
0.00.050.706 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.707 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.713 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.714 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.714 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.714 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.714 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.715 I llm_load_print_meta: n_ff             = 8192
0.00.050.715 I llm_load_print_meta: n_expert         = 0
0.00.050.716 I llm_load_print_meta: n_expert_used    = 0
0.00.050.716 I llm_load_print_meta: causal attn      = 1
0.00.050.716 I llm_load_print_meta: pooling type     = 0
0.00.050.716 I llm_load_print_meta: rope type        = 2
0.00.050.716 I llm_load_print_meta: rope scaling     = linear
0.00.050.719 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.719 I llm_load_print_meta: freq_scale_train = 1
0.00.050.719 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.719 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.719 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.720 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.720 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.720 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.720 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.720 I llm_load_print_meta: model type       = 1.4B
0.00.050.721 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.721 I llm_load_print_meta: model params     = 1.41 B
0.00.050.721 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.721 I llm_load_print_meta: general.name     = 1.4B
0.00.050.722 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.723 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.723 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.723 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.724 I llm_load_print_meta: LF token         = 128 ''
0.00.050.724 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.724 I llm_load_print_meta: max token length = 1024
0.00.052.674 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.675 I llm_load_tensors: offloading output layer to GPU
0.00.052.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.685 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.687 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.040 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.041 I llama_new_context_with_model: n_ctx         = 128
0.00.053.041 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.041 I llama_new_context_with_model: n_batch       = 128
0.00.053.041 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.042 I llama_new_context_with_model: flash_attn    = 0
0.00.053.042 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.042 I llama_new_context_with_model: freq_scale    = 1
0.00.053.043 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.043 I ggml_metal_init: allocating
0.00.053.046 I ggml_metal_init: found device: Apple M4
0.00.053.047 I ggml_metal_init: picking default device: Apple M4
0.00.053.592 I ggml_metal_init: using embedded metal library
0.00.055.938 I ggml_metal_init: GPU name:   Apple M4
0.00.055.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.940 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.940 I ggml_metal_init: simdgroup reduction   = true
0.00.055.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.940 I ggml_metal_init: has bfloat            = true
0.00.055.940 I ggml_metal_init: use bfloat            = true
0.00.055.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.299 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.697 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.702 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.716 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.536 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.537 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.537 I llama_new_context_with_model: graph nodes  = 967
0.00.067.537 I llama_new_context_with_model: graph splits = 2
0.00.067.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.241.220 I 
0.00.241.251 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.241.262 I perplexity: tokenizing the input ..
0.00.248.261 I perplexity: tokenization took 6.998 ms
0.00.248.264 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.388.253 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.389.411 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.389.433 I llama_perf_context_print:        load time =     231.29 ms
0.00.389.434 I llama_perf_context_print: prompt eval time =     139.77 ms /   128 tokens (    1.09 ms per token,   915.80 tokens per second)
0.00.389.435 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.389.435 I llama_perf_context_print:       total time =     148.21 ms /   129 tokens
0.00.389.982 I ggml_metal_free: deallocating

real	0m0.405s
user	0m0.077s
sys	0m0.043s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4448 (1bf839b1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11a60a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11a60a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11a60af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11a60b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11a60bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11a60c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11a60c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11a60cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11a60d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11a60d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11a60db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11a60e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11a60eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11a60f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11a60fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11a610270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11a610990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11a6110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11a6117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11a611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11a6126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11a612de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11a613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11a613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11a6144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11a614780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11a614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11a615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11a615f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11a616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11a6166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11a616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11a6171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11a617730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11a6179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11a617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11a618330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11a6187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11a618c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11a619110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11a6195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11a619a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11a619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11a61a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11a61a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11a61ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11a61b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11a61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11a61c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11a61c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11a61cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11a61d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11a61d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11a61dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11a61e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11a61ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11a61f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11a61f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11a61f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11a6201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11a6204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11a620940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11a620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11a621280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11a621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11a621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11a622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11a622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11a6229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11a622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11a6232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11a623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11a623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11a624170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11a6246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11a624c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11a625160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11a6256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11a625c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11a626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11a6266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11a626bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11a627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11a627690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11a627be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11a628130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11a628680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11a628bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11a629120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11a629670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11a629bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11a62a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11a62a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11a62abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11a62b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11a62b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11a62bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11a61b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11a62c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11a62c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11a62cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11a62d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11a62d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11a62dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11a62e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11a62e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11a62ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11a62f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11a62f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11a62fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11a630230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11a630780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11a630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11a631170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11a631610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11a631ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11a631f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11a6323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11a632890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11a632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11a6331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11a633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11a633b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11a633fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11a634450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11a6348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11a634d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11a635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11a6356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11a635b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11a636010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11a6364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11a636950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11a636df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11a637290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11a637730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11a637bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11a638070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11a638510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11a6389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11a638e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11a6392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11a639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11a639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11a63a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11a63a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11a63aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11a63aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11a63b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11a63b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11a63bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11a63c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11a63c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11a63ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11a63cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11a63d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11a63d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11a63dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11a63e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11a63e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11a63ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11a63ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11a63f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11a63f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11a63fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11a6401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11a640690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11a640b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11a640fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11a641470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11a641910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11a641db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11a642250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11a6426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11a642b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11a643030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11a6434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11a643970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11a643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11a6442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11a644750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11a644bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11a645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11a645530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11a6459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11a645e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11a646310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11a6467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11a646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11a6470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11a647590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11a647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11a647ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11a648420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11a648970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11a648ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11a649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11a6496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11a649ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11a64a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11a64a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11a64b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11a64b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a64b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a64be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11a64c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a64cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a64d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a64d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a64da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a64e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a64e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a64ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a64f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a64f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a64fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a6501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a650720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a650c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a6511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a651c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a6521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a652700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a652c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a6531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a6536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a653c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a654190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a6546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a654c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a655180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a6556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a655c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a6566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a656c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a657160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a6576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a657c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a658150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a6586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a658bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a659140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a659690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a659be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a65a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a65a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a65abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a65b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a65b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a65bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a65c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a65c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a65cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a65d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a65d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a65dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a65e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a65e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a65eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a65f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a65f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a65fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a6600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a660620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a660b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11a661010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11a6614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a661950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a661df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a662290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a662730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a662bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a663070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a663510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a6639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a663e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a6642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a664790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a664c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a6650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a665620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a665d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a666460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a666b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a6672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a667560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11a667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a668010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a668620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.165.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.165.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11a6682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11a649fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11a649990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11a64a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11a61d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11a61d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11a61f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11a64c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11a614a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11a61b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11a61be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11a61c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11a61a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11a61ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11a613a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11a6098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11a61e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11a61fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11a62c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11a667820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11a616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11a616ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11a64c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11a64abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11a615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11a615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11a6155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11a668a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11a668d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11a669000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11a6692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11a669580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11a669840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11a669b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11a669dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11a66a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11a66a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11a66a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11a66a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11a66ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11a66ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11a66b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11a66b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11a66b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11a66b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11a66bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11a66bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11a66c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11a66c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11a66c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11a66c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11a66cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11a66cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11a66d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11a66d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11a66d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11a66da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11a66dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11a66dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11a66e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11a66e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11a66e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11a66eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11a66ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11a66f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11a66f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11a66f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11a66f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11a66fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11a66fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11a6700c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11a670380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11a670640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11a670900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11a670bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11a670e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11a671140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11a671400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11a6716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11a671980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11a671c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11a671f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11a6721c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11a672480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11a672740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11a672a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11a672cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11a672f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11a673240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11a673500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11a6737c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11a673a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11a673d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11a674000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11a6742c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11a674580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11a674840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11a674b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11a674dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11a675080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11a675340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11a675600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11a6758c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11a675b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11a675e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11a676100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11a6763c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11a676680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11a676940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11a676c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11a676ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11a677180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11a677440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11a677700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11a6779c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11a677c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11a677f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11a678200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11a6784c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11a678780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11a678a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11a678d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11a678fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11a679280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11a679540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11a679800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11a679ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11a679d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11a67a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11a67a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11a67a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11a67a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11a67ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11a67ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11a67b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11a67b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11a67b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11a67b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11a67bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11a67be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11a67c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11a67c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11a67c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11a67c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11a67cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11a67cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11a67d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11a67d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11a67d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11a67da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11a67dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11a67df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11a67e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11a67e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11a67e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11a67ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11a67ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11a67f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11a67f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11a67f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11a67f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11a67fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11a67fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11a680080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11a680340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11a680600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11a6808c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11a680b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11a680e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11a681100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11a6813c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11a681680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11a681940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11a681c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11a681ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11a682180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11a682440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11a682700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11a6829c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11a682c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11a682f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11a683200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11a6834c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11a683780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11a683a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11a683d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11a683fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11a684280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11a684540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11a684800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11a684ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11a684d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11a685040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11a685300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11a6855c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11a685880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11a685b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11a685e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11a6860c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11a686380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11a686640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11a686900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11a686bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a686e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a687140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11a687400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a6876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a687980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a687c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a687f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a6884d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a688790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a688a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a688d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a688fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a689290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a689550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a689810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a689ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a689d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a68a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a68a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a68a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a68a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a68ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a68ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a68b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a68b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a68b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a68b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a68bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a68be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a68c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a68c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a68c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a68c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a68cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a68cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a68d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a68d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a68d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a68dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a68e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a68e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a68ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a68f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a68f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a68fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a6901d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a690720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a690c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a6911c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a691710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a691c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a6921b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a692700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a692c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a6931a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a6936f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a693c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a694190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a6946e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a694c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a695180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a6956d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a695c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a696170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11a696430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11a6966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a696bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a6970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a6975f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a697af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a697ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a6984f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a6989f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a698ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a6993f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a6998f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a699df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a69a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a69a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a69acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a69b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a69be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a69c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a69cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a69cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11a69d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a69d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a69dfe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f5082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f506100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f508910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f508d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f5091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f5097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f509d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f50a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f50a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f50adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f50b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f50b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f50c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f50ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f50d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f50d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f50e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f50e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f50ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f50f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f50fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f510520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f510c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f511360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f511a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f511d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f512350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f512960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f512f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f513760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f513c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f513ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f514750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f514c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f514f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f5153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f515890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f515d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f5161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f516670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f516b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f516fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f517450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f5178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f517bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f5181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f5187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f518de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f5193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f519a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f51a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f51a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f51ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f51b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f51ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f51bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f51c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f51c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f51cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f51d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f51d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f51dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f51e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f51e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f51eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f51eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f51f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f51f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f51fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f520270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f520710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f520bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f521050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f5215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f521af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f522040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f522590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f522ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f523030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f523580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f523ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f524020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f524570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f524ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f525010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f525560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f525ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f526000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f526550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f526aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f526ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f527540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f527a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f527fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f528530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f528a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f528fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f529520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f529a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f529fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f52a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f52aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f52afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f52b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f52ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f52bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f52c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f52ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f52cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f52d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f52da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f52df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f52e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f52e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f52ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f52f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f52f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f52fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f530090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f530530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f5309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f530e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f531310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f5317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f531c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f5320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f532590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f532a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f532ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f533370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f533810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f533cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f534150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f5345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f534a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f534f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f5353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f535870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f535d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f5361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f536650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f536af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f536f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f537430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f5378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f537d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f538210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f5386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f538b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f538ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f539490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f539930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f539dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f53a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f53a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f53abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f53b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f53b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f53b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f53be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f53c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f53c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f53cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f53d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f53d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f53d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f53de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f53e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f53e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f53ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f53f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f53f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f53fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f53fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f540390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f540830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f540cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f541170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f541610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f541ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f541f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f5423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f542890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f542d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f5431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f543670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f543b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f543fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f544450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f5448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f544d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f545230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f5456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f545c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f546170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f5466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f546c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f546ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f5474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f547af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f548100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f5488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f548d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f549050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f549660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f549c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f54a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f54a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f54ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f54b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f54b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f54bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f54c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f54c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f54cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f54d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f54d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f54df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f54e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f54e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f54ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f54f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f54f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f54ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f550450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f5509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f550ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f551440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f551990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f551ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f552430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f552980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f552ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f553420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f553970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f553ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f554410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f554960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f554eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f555400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f555950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f555ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f5563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f556940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f556e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f5573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f557930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f557e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f5583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f558920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f558e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f5593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f559910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f559e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f55a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f55a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f55ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f55b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f55b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f55be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f55c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f55c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f55ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f55d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f55d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f55de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f55e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f55e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f55ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f55f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f55f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f55fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f55ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f5603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f560870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f560d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f5611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f561650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f561af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f561f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f562430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f5628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f562e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f563540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f563c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f564380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f564aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f564d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f565550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f565810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f565e20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.983s
user	0m0.316s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4448 (1bf839b1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d10c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d10cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d10d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d10d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d10dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d10e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d10e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d10ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d10f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d10f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d10fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d1102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d110dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d111580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d111d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d1124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d112bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d1132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d113a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d1141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d114900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d115020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d115740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d115fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d116700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d1169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d116fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d117c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d118180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d118440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d1188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d118ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d119430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d119970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d119c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d11a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d11a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d11aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d11aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d11b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d11b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d11bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d11c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d11c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d11c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d11cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d11d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d11ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d11e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d11e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d11f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d11f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d11fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d120230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d120a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d120ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d121360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d121620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d121c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d122420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d1226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d122b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d123020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d1234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d123960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d123e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d1242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d124740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d124be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d125080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d125520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d1259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d125e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d1263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d126900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d126e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d1273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d1278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d127e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d128390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d1288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d128e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d129380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d1298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d129e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d12a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d12a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d12ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d12b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d12b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d12be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d12c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d12c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d12cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d12d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d12d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d12dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d11dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d12e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d12ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d12ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d12f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d12f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d12ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d130490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d1309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d130f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d131480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d1319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d131f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d132470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d1329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d132f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d1333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d133850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d133cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d134190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d134630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d134ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d134f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d135410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d1358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d135d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d1361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d136690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d136b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d136fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d137470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d137910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d137db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d138250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d1386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d138b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d139030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d1394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d139970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d139e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d13a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d13a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d13abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d13b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d13b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d13b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d13be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d13c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d13c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d13cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d13d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d13d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d13da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d13ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d13e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d13e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d13ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d13f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d13f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d13fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d13ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d1403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d140870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d140d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d1411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d141650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d141af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d141f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d142430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d1428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d142d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d143210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d1436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d143b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d143ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d144490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d144930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d144dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d145270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d145710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d145bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d146050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d1464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d146990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d146e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d1472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d147770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d147c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d1480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d148550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d1489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d148e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d149330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d1497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d149c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d14a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d14a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d14abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d14b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d14b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d14b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d14bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d14c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d14cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d14d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d14d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d14da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d14e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d14e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d14eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d14f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d14f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d14fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d150430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d150980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d150ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d151420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d151970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d151ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d152410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d152960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d152eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d153400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d153950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d153ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d1543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d154940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d154e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d1553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d155930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d155e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d1563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d156920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d156e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d1573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d157910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d157e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d1583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d158900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d158e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d1593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d1598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d159e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d15a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d15a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d15ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d15b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d15b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d15be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d15c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d15c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d15ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d15d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d15d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d15de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d15e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d15e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d15edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d15f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d15f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d15fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d160330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d160880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d160dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d161320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d161870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d161dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d162310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d162860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d162db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d163250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d1636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d163b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d164030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d1644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d164970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d164e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d1652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d165750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d165bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d166090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d166530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d1669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d166e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d167310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d167860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d167f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d1686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d168dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d1694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d1697a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d169f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d16a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d16a860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.110.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d16a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d14c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d14bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d14c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d11f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d11f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d1218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d14e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d116c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d11d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d11e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d11e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d11cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d11ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d115c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d121ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d12e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d169a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d118e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d119120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d14e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d14ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d117290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d117550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d117810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d16acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d16af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d16b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d16b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d16b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d16ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d16bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d16c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d16c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d16c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d16c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d16cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d16cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d16d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d16d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d16d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d16d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d16db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d16de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d16e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d16e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d16e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d16e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d16ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d16eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d16f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d16f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d16f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d16f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d16fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d16ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d170200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d1704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d170780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d170a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d170d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d170fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d171280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d171540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d171800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d171ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d171d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d172040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d172300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d1725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d172880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d172b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d172e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d1730c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d173380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d173640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d173900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d173bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d173e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d174140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d174400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d1746c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d174980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d174c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d174f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d1751c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d175480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d175740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d175a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d175cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d175f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d176240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d176500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d1767c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d176a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d176d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d177000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d1772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d177580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d177840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d177b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d177dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d178080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d178340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d178600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d1788c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d178b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d178e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d179100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d1793c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d179680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d179940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d179c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d179ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d17a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d17a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d17a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d17a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d17ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d17af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d17b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d17b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d17b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d17ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d17bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d17bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d17c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d17c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d17c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d17cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d17cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d17d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d17d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d17d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d17d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d17db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d17de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d17e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d17e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d17e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d17e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d17ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d17ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d17f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d17f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d17f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d17f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d17fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d17ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d1801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d180480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d180740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d180a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d180cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d180f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d181240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d181500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d1817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d181a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d181d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d182000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d1822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d182580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d182840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d182b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d182dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d183080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d183340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d183600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d1838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d183b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d183e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d184100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d1843c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d184680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d184940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d184c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d184ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d185180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d185440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d185700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d1859c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d185c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d185f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d186200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d1864c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d186780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d186a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d186d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d186fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d187280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d187540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d187800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d187ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d187d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d188040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d188300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d1885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d188880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d188b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d188e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d1890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d189380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d189640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d189900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d189bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d189e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d18a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d18a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d18a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d18ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d18af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d18b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d18b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d18baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d18bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d18c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d18c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d18ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d18d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d18d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d18da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d18de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d18e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d18e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d18ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d18f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d18f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d18f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d18fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d1901f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d190660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d190ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d190f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d1913b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d191820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d191c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d192100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d192570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d1929e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d192e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d1932c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d193730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d193ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d194010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d194480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d1948f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d194d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d1951d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d195640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d195ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d195f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d196390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d196800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d196c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d1970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d197550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d1979c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d197e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d1982a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d198710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d198b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d198ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d199460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d1998d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d199d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d19a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d19a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d19aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d19af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d19b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d19b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d19bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d19c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d19c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d19c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d19ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d19d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d19d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d19db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d19dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d19e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d19e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d19f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d19fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d1a0160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d1a0880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d1a0b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d1a1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d1a15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d1a1c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d19eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d1a18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d1a0e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d1a2060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d1a2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d1a25e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d1a28a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d1a2b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d1a2e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d1a30e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d1a33a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d1a3660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d1a3c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d1a4200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d1a4830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d1a4af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d1a4db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d1a5070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d1a5330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d1a55f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d1a58b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d1a5b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d1a5e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d1a60f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d1a63b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d1a6670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d1a6930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d1a6bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d1a6eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d1a7170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d1a7430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d1a76f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d1a79b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d1a7c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d1a7f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d1a81f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d1a84b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d1a8770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d1a8a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d1a8cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d1a8fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d1a9270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d1a9530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d1a97f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d1a9ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d1a9d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d1aa030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d1aa2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d1aa5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d1aa870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d1aab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d1aadf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d1ab0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d1ab370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d1ab630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d1ab8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d1abbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d1abe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d1ac130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d1ac3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d1ac6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d1ac970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d1acc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d1acef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d1ad1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d1ad470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d1ad730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d1ad9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d1adcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d1adf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d1ae230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d1ae4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d1ae7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d1aea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d1aed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d1aeff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d1af2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d1af570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d1af830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d1afaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d1afdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d1b0070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d1b0330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d1b05f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d1b08b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d1b0b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d1b0e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d1b10f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d1b13b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d1b1670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d1b1930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d1b1bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d1b1eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d1b2170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d1b2430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d1b26f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d1b29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d1b2c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d1b2f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d1b31f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d1b34b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d1b3770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d1b3a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d1b3cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d1b3fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d1b4270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d1b4530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d1b47f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d1b4ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d1b4d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d1b5030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d1b52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d1b55b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d1b5870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d1b5b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d1b5df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d1b60b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d1b6370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d1b6630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d1b68f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d1b6bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d1b6e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d1b7130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d1b73f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d1b76b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d1b7970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d1b7c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d1b7ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d1b81b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d1b8470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d1b8730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d1b89f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d1b8cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d1b8f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d1b9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d1b94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d1b97b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d1b9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d1b9d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d1b9ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d1ba2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d1ba570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d1ba830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d1baaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d1badb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d1bb070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d1bb330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d1bb5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d1bb8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d1bbb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d1bbe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d1bc0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d1bc3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d1bc670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d1bc930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d1bcbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d1bceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d1bd170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d1bd430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d1bd6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d1bd9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d1bdc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d1bdf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d1be1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d1be4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d1be770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d1bea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d1becf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d1befb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d1bf270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d1bf530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d1bf7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d1bfab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d1bfd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d1c0030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d1c02f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d1c05b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d1c0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d1c0b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d1c0df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d1c10b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d1c1370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d1c1630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d1c18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d1c1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d1c1e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d1c2130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d1c23f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d1c26b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d1c2970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d1c2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d1c2ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d1c31b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d1c3470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d1c3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d1c39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d1c3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d1c3f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d1c4230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d1c44f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d1c47b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d1c4a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d1c4d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d1c4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d1c52b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d1c5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d1c5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d1c5af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d1c5db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d1c6070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d1c6640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d1c6900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d1c6bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d1c6e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d1c7140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d1c7400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d1c76c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d1c7980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d1c7c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d1c7f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d1c81c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d1c8480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d1c8740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d1c8a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d1c8cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d1c8f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d1c9240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d1c9500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d1c97c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d1c9a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d1c9d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d1ca000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d1ca2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d1ca580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d1ca840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d1cab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d1cadc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d1cb080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d1cb340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d1cb600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d1cb8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d1cbb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d1cbe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d1cc100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d1cc3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d1cc680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d1cc940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d1ccc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d1ccec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d1cd180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d1cd440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d1cd700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d1cd9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d1cdc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d1cdf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d1ce200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d1ce4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d1ce780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d1cea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d1ced00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d1cefc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d1cf280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d1cf540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d1cf800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d1cfac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d1cfd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d1d0040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d1d0300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d1d05c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d1d0880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d1d0b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d1d0e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d1d1200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d1d14c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d1d1780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d1d1bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d1d2060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d1d24d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d1d2940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d1d2db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d1d3220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d1d3690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d1d3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d1d4670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d1d4d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d1d54b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d1d5bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d1d5e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d1d6150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d1d6680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d1d6af0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.260s
sys	0m0.132s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.56 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.15 sec*proc (2 tests)

Total Test time (real) =   1.16 sec
        1.18 real         0.75 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
