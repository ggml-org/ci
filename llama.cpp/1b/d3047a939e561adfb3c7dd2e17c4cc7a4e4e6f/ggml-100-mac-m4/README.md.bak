### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   13.12 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.84 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.79 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.21 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.91 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.95 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 247.54 sec*proc (29 tests)

Total Test time (real) = 247.55 sec

real	4m7.648s
user	8m29.790s
sys	0m7.104s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.35 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.12 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.39 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.64 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.36 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  53.98 sec*proc (29 tests)

Total Test time (real) =  53.99 sec

real	0m54.004s
user	1m16.588s
sys	0m6.170s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.096 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.378 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.321 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.336 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.338 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.339 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.339 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.341 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.342 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.343 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.343 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.344 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.348 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.349 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.350 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.350 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.351 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.352 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.353 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.358 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.359 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.359 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.359 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.359 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.360 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.360 I llama_model_loader: - type  f32:  124 tensors
0.00.027.360 I llama_model_loader: - type  f16:   73 tensors
0.00.027.361 I print_info: file format = GGUF V3 (latest)
0.00.027.361 I print_info: file type   = F16
0.00.027.362 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.032.697 I load: special tokens cache size = 5
0.00.034.094 I load: token to piece cache size = 0.2032 MB
0.00.034.097 I print_info: arch             = bert
0.00.034.097 I print_info: vocab_only       = 0
0.00.034.098 I print_info: n_ctx_train      = 512
0.00.034.098 I print_info: n_embd           = 384
0.00.034.098 I print_info: n_layer          = 12
0.00.034.101 I print_info: n_head           = 12
0.00.034.101 I print_info: n_head_kv        = 12
0.00.034.102 I print_info: n_rot            = 32
0.00.034.102 I print_info: n_swa            = 0
0.00.034.102 I print_info: n_embd_head_k    = 32
0.00.034.102 I print_info: n_embd_head_v    = 32
0.00.034.103 I print_info: n_gqa            = 1
0.00.034.103 I print_info: n_embd_k_gqa     = 384
0.00.034.104 I print_info: n_embd_v_gqa     = 384
0.00.034.105 I print_info: f_norm_eps       = 1.0e-12
0.00.034.105 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.105 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.105 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.106 I print_info: f_logit_scale    = 0.0e+00
0.00.034.106 I print_info: n_ff             = 1536
0.00.034.107 I print_info: n_expert         = 0
0.00.034.107 I print_info: n_expert_used    = 0
0.00.034.107 I print_info: causal attn      = 0
0.00.034.107 I print_info: pooling type     = 2
0.00.034.107 I print_info: rope type        = 2
0.00.034.107 I print_info: rope scaling     = linear
0.00.034.108 I print_info: freq_base_train  = 10000.0
0.00.034.108 I print_info: freq_scale_train = 1
0.00.034.108 I print_info: n_ctx_orig_yarn  = 512
0.00.034.109 I print_info: rope_finetuned   = unknown
0.00.034.111 I print_info: ssm_d_conv       = 0
0.00.034.111 I print_info: ssm_d_inner      = 0
0.00.034.111 I print_info: ssm_d_state      = 0
0.00.034.112 I print_info: ssm_dt_rank      = 0
0.00.034.112 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.112 I print_info: model type       = 33M
0.00.034.112 I print_info: model params     = 33.21 M
0.00.034.112 I print_info: general.name     = Bge Small
0.00.034.113 I print_info: vocab type       = WPM
0.00.034.113 I print_info: n_vocab          = 30522
0.00.034.113 I print_info: n_merges         = 0
0.00.034.114 I print_info: BOS token        = 101 '[CLS]'
0.00.034.114 I print_info: UNK token        = 100 '[UNK]'
0.00.034.114 I print_info: SEP token        = 102 '[SEP]'
0.00.034.114 I print_info: PAD token        = 0 '[PAD]'
0.00.034.114 I print_info: MASK token       = 103 '[MASK]'
0.00.034.114 I print_info: LF token         = 0 '[PAD]'
0.00.034.116 I print_info: max token length = 21
0.00.036.263 I load_tensors: offloading 12 repeating layers to GPU
0.00.036.264 I load_tensors: offloading output layer to GPU
0.00.036.264 I load_tensors: offloaded 13/13 layers to GPU
0.00.036.284 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.285 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.036.451 I llama_init_from_model: n_seq_max     = 1
0.00.036.451 I llama_init_from_model: n_ctx         = 512
0.00.036.451 I llama_init_from_model: n_ctx_per_seq = 512
0.00.036.452 I llama_init_from_model: n_batch       = 2048
0.00.036.452 I llama_init_from_model: n_ubatch      = 2048
0.00.036.452 I llama_init_from_model: flash_attn    = 0
0.00.036.452 I llama_init_from_model: freq_base     = 10000.0
0.00.036.453 I llama_init_from_model: freq_scale    = 1
0.00.036.453 I ggml_metal_init: allocating
0.00.036.457 I ggml_metal_init: found device: Apple M4
0.00.036.461 I ggml_metal_init: picking default device: Apple M4
0.00.037.007 I ggml_metal_init: using embedded metal library
0.00.039.677 I ggml_metal_init: GPU name:   Apple M4
0.00.039.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.680 I ggml_metal_init: simdgroup reduction   = true
0.00.039.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.680 I ggml_metal_init: has residency sets    = true
0.00.039.680 I ggml_metal_init: has bfloat            = true
0.00.039.680 I ggml_metal_init: use bfloat            = true
0.00.039.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.049.964 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.050.553 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.050.555 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.050.556 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.051.585 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.051.586 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.051.587 I llama_init_from_model: graph nodes  = 429
0.00.051.587 I llama_init_from_model: graph splits = 2
0.00.051.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.051.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.056.003 I 
0.00.056.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.056.607 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.060.936 I llama_perf_context_print:        load time =      39.62 ms
0.00.060.937 I llama_perf_context_print: prompt eval time =       4.20 ms /     9 tokens (    0.47 ms per token,  2142.35 tokens per second)
0.00.060.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.060.938 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.061.141 I ggml_metal_free: deallocating

real	0m0.234s
user	0m0.043s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.296 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.301 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.302 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.303 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.305 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.306 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.307 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.307 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.307 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.308 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.308 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.311 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.311 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.311 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.312 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.312 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.313 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.462 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.070 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.071 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.071 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.072 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.072 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.072 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.073 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.073 I llama_model_loader: - type  f32:  124 tensors
0.00.014.073 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.074 I print_info: file format = GGUF V3 (latest)
0.00.014.075 I print_info: file type   = Q8_0
0.00.014.076 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.349 I load: special tokens cache size = 5
0.00.017.547 I load: token to piece cache size = 0.2032 MB
0.00.017.549 I print_info: arch             = bert
0.00.017.550 I print_info: vocab_only       = 0
0.00.017.550 I print_info: n_ctx_train      = 512
0.00.017.550 I print_info: n_embd           = 384
0.00.017.550 I print_info: n_layer          = 12
0.00.017.553 I print_info: n_head           = 12
0.00.017.554 I print_info: n_head_kv        = 12
0.00.017.554 I print_info: n_rot            = 32
0.00.017.554 I print_info: n_swa            = 0
0.00.017.554 I print_info: n_embd_head_k    = 32
0.00.017.555 I print_info: n_embd_head_v    = 32
0.00.017.555 I print_info: n_gqa            = 1
0.00.017.556 I print_info: n_embd_k_gqa     = 384
0.00.017.557 I print_info: n_embd_v_gqa     = 384
0.00.017.557 I print_info: f_norm_eps       = 1.0e-12
0.00.017.558 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.558 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.558 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.558 I print_info: f_logit_scale    = 0.0e+00
0.00.017.559 I print_info: n_ff             = 1536
0.00.017.559 I print_info: n_expert         = 0
0.00.017.559 I print_info: n_expert_used    = 0
0.00.017.559 I print_info: causal attn      = 0
0.00.017.559 I print_info: pooling type     = 2
0.00.017.560 I print_info: rope type        = 2
0.00.017.560 I print_info: rope scaling     = linear
0.00.017.560 I print_info: freq_base_train  = 10000.0
0.00.017.560 I print_info: freq_scale_train = 1
0.00.017.561 I print_info: n_ctx_orig_yarn  = 512
0.00.017.563 I print_info: rope_finetuned   = unknown
0.00.017.563 I print_info: ssm_d_conv       = 0
0.00.017.563 I print_info: ssm_d_inner      = 0
0.00.017.563 I print_info: ssm_d_state      = 0
0.00.017.563 I print_info: ssm_dt_rank      = 0
0.00.017.563 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.565 I print_info: model type       = 33M
0.00.017.565 I print_info: model params     = 33.21 M
0.00.017.565 I print_info: general.name     = Bge Small
0.00.017.566 I print_info: vocab type       = WPM
0.00.017.568 I print_info: n_vocab          = 30522
0.00.017.568 I print_info: n_merges         = 0
0.00.017.568 I print_info: BOS token        = 101 '[CLS]'
0.00.017.569 I print_info: UNK token        = 100 '[UNK]'
0.00.017.569 I print_info: SEP token        = 102 '[SEP]'
0.00.017.569 I print_info: PAD token        = 0 '[PAD]'
0.00.017.569 I print_info: MASK token       = 103 '[MASK]'
0.00.017.569 I print_info: LF token         = 0 '[PAD]'
0.00.017.570 I print_info: max token length = 21
0.00.019.234 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.235 I load_tensors: offloading output layer to GPU
0.00.019.235 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.241 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.241 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.383 I llama_init_from_model: n_seq_max     = 1
0.00.019.384 I llama_init_from_model: n_ctx         = 512
0.00.019.384 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.384 I llama_init_from_model: n_batch       = 2048
0.00.019.384 I llama_init_from_model: n_ubatch      = 2048
0.00.019.384 I llama_init_from_model: flash_attn    = 0
0.00.019.385 I llama_init_from_model: freq_base     = 10000.0
0.00.019.385 I llama_init_from_model: freq_scale    = 1
0.00.019.385 I ggml_metal_init: allocating
0.00.019.388 I ggml_metal_init: found device: Apple M4
0.00.019.391 I ggml_metal_init: picking default device: Apple M4
0.00.019.946 I ggml_metal_init: using embedded metal library
0.00.023.615 I ggml_metal_init: GPU name:   Apple M4
0.00.023.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.618 I ggml_metal_init: simdgroup reduction   = true
0.00.023.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.618 I ggml_metal_init: has residency sets    = true
0.00.023.618 I ggml_metal_init: has bfloat            = true
0.00.023.619 I ggml_metal_init: use bfloat            = true
0.00.023.619 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.620 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.037.141 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.754 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.756 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.758 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.038.753 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.038.754 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.038.755 I llama_init_from_model: graph nodes  = 429
0.00.038.755 I llama_init_from_model: graph splits = 2
0.00.038.757 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.038.757 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.780 I 
0.00.042.806 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.043.362 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.047.775 I llama_perf_context_print:        load time =      33.92 ms
0.00.047.776 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2095.95 tokens per second)
0.00.047.777 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.777 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.047.986 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.282 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.260 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.031 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.036 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.039 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.048 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.048 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.049 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.051 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.052 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.052 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.053 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.053 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.056 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.057 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.060 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.027 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.587 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.587 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.588 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.588 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.589 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.589 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.589 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.590 I llama_model_loader: - type  f32:   40 tensors
0.00.050.590 I llama_model_loader: - type  f16:   30 tensors
0.00.050.591 I print_info: file format = GGUF V3 (latest)
0.00.050.592 I print_info: file type   = F16
0.00.050.593 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.971 W load: empty token at index 5
0.00.060.067 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.575 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.609 I load: special tokens cache size = 5
0.00.317.233 I load: token to piece cache size = 1.5060 MB
0.00.317.238 I print_info: arch             = jina-bert-v2
0.00.317.239 I print_info: vocab_only       = 0
0.00.317.239 I print_info: n_ctx_train      = 8192
0.00.317.239 I print_info: n_embd           = 384
0.00.317.239 I print_info: n_layer          = 4
0.00.317.245 I print_info: n_head           = 12
0.00.317.245 I print_info: n_head_kv        = 12
0.00.317.245 I print_info: n_rot            = 32
0.00.317.245 I print_info: n_swa            = 0
0.00.317.246 I print_info: n_embd_head_k    = 32
0.00.317.246 I print_info: n_embd_head_v    = 32
0.00.317.247 I print_info: n_gqa            = 1
0.00.317.247 I print_info: n_embd_k_gqa     = 384
0.00.317.248 I print_info: n_embd_v_gqa     = 384
0.00.317.249 I print_info: f_norm_eps       = 1.0e-12
0.00.317.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.317.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.317.250 I print_info: f_max_alibi_bias = 8.0e+00
0.00.317.250 I print_info: f_logit_scale    = 0.0e+00
0.00.317.251 I print_info: n_ff             = 1536
0.00.317.251 I print_info: n_expert         = 0
0.00.317.251 I print_info: n_expert_used    = 0
0.00.317.251 I print_info: causal attn      = 0
0.00.317.252 I print_info: pooling type     = -1
0.00.317.252 I print_info: rope type        = -1
0.00.317.252 I print_info: rope scaling     = linear
0.00.317.255 I print_info: freq_base_train  = 10000.0
0.00.317.255 I print_info: freq_scale_train = 1
0.00.317.255 I print_info: n_ctx_orig_yarn  = 8192
0.00.317.255 I print_info: rope_finetuned   = unknown
0.00.317.256 I print_info: ssm_d_conv       = 0
0.00.317.256 I print_info: ssm_d_inner      = 0
0.00.317.256 I print_info: ssm_d_state      = 0
0.00.317.258 I print_info: ssm_dt_rank      = 0
0.00.317.258 I print_info: ssm_dt_b_c_rms   = 0
0.00.317.258 I print_info: model type       = 33M
0.00.317.258 I print_info: model params     = 32.90 M
0.00.317.258 I print_info: general.name     = Jina Bert Implementation
0.00.317.260 I print_info: vocab type       = BPE
0.00.317.260 I print_info: n_vocab          = 61056
0.00.317.260 I print_info: n_merges         = 39382
0.00.317.260 I print_info: BOS token        = 0 '<s>'
0.00.317.260 I print_info: EOS token        = 2 '</s>'
0.00.317.260 I print_info: UNK token        = 3 '<unk>'
0.00.317.261 I print_info: SEP token        = 2 '</s>'
0.00.317.266 I print_info: PAD token        = 1 '<pad>'
0.00.317.266 I print_info: MASK token       = 4 '<mask>'
0.00.317.266 I print_info: EOG token        = 2 '</s>'
0.00.317.266 I print_info: max token length = 45
0.00.319.008 I load_tensors: offloading 4 repeating layers to GPU
0.00.319.009 I load_tensors: offloading output layer to GPU
0.00.319.009 I load_tensors: offloaded 5/5 layers to GPU
0.00.319.033 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.319.034 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.319.287 I llama_init_from_model: n_seq_max     = 1
0.00.319.288 I llama_init_from_model: n_ctx         = 8192
0.00.319.288 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.319.288 I llama_init_from_model: n_batch       = 2048
0.00.319.288 I llama_init_from_model: n_ubatch      = 2048
0.00.319.289 I llama_init_from_model: flash_attn    = 0
0.00.319.289 I llama_init_from_model: freq_base     = 10000.0
0.00.319.289 I llama_init_from_model: freq_scale    = 1
0.00.319.290 I ggml_metal_init: allocating
0.00.319.293 I ggml_metal_init: found device: Apple M4
0.00.319.297 I ggml_metal_init: picking default device: Apple M4
0.00.319.964 I ggml_metal_init: using embedded metal library
0.00.322.897 I ggml_metal_init: GPU name:   Apple M4
0.00.322.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.322.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.322.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.322.900 I ggml_metal_init: simdgroup reduction   = true
0.00.322.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.322.900 I ggml_metal_init: has residency sets    = true
0.00.322.900 I ggml_metal_init: has bfloat            = true
0.00.322.901 I ggml_metal_init: use bfloat            = true
0.00.322.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.322.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.332.536 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.335.894 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.335.895 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.335.897 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.342.478 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.342.479 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.342.479 I llama_init_from_model: graph nodes  = 154
0.00.342.479 I llama_init_from_model: graph splits = 2
0.00.342.481 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.342.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.056 I 
0.00.351.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.351.200 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.351.201 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.351.204 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.351.204 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.351.207 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.351.207 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.351.761 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.355.181 I llama_perf_context_print:        load time =     326.79 ms
0.00.355.182 I llama_perf_context_print: prompt eval time =       3.41 ms /    62 tokens (    0.06 ms per token, 18171.16 tokens per second)
0.00.355.183 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.355.184 I llama_perf_context_print:       total time =       4.12 ms /    63 tokens
0.00.355.427 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.323s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.194 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.372 I main: llama backend init
0.00.000.379 I main: load the model and apply lora adapter, if any
0.00.045.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.058.782 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.058.800 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.058.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.058.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.058.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.058.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.058.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.058.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.058.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.058.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.058.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.058.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.058.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.058.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.058.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.058.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.058.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.903 I llama_model_loader: - type  f32:  194 tensors
0.00.077.904 I llama_model_loader: - type  f16:   98 tensors
0.00.077.907 I print_info: file format = GGUF V3 (latest)
0.00.077.909 I print_info: file type   = all F32 (guessed)
0.00.077.910 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.092.450 I load: special tokens cache size = 25
0.00.101.193 I load: token to piece cache size = 0.2984 MB
0.00.101.196 I print_info: arch             = gptneox
0.00.101.197 I print_info: vocab_only       = 0
0.00.101.197 I print_info: n_ctx_train      = 2048
0.00.101.197 I print_info: n_embd           = 2048
0.00.101.197 I print_info: n_layer          = 24
0.00.101.200 I print_info: n_head           = 16
0.00.101.202 I print_info: n_head_kv        = 16
0.00.101.202 I print_info: n_rot            = 32
0.00.101.204 I print_info: n_swa            = 0
0.00.101.204 I print_info: n_embd_head_k    = 128
0.00.101.204 I print_info: n_embd_head_v    = 128
0.00.101.205 I print_info: n_gqa            = 1
0.00.101.206 I print_info: n_embd_k_gqa     = 2048
0.00.101.207 I print_info: n_embd_v_gqa     = 2048
0.00.101.208 I print_info: f_norm_eps       = 1.0e-05
0.00.101.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.101.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.101.209 I print_info: f_max_alibi_bias = 0.0e+00
0.00.101.209 I print_info: f_logit_scale    = 0.0e+00
0.00.101.210 I print_info: n_ff             = 8192
0.00.101.210 I print_info: n_expert         = 0
0.00.101.212 I print_info: n_expert_used    = 0
0.00.101.212 I print_info: causal attn      = 1
0.00.101.212 I print_info: pooling type     = 0
0.00.101.212 I print_info: rope type        = 2
0.00.101.212 I print_info: rope scaling     = linear
0.00.101.213 I print_info: freq_base_train  = 10000.0
0.00.101.213 I print_info: freq_scale_train = 1
0.00.101.213 I print_info: n_ctx_orig_yarn  = 2048
0.00.101.214 I print_info: rope_finetuned   = unknown
0.00.101.214 I print_info: ssm_d_conv       = 0
0.00.101.214 I print_info: ssm_d_inner      = 0
0.00.101.214 I print_info: ssm_d_state      = 0
0.00.101.214 I print_info: ssm_dt_rank      = 0
0.00.101.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.101.215 I print_info: model type       = 1.4B
0.00.101.215 I print_info: model params     = 1.41 B
0.00.101.215 I print_info: general.name     = 1.4B
0.00.101.216 I print_info: vocab type       = BPE
0.00.101.216 I print_info: n_vocab          = 50304
0.00.101.216 I print_info: n_merges         = 50009
0.00.101.217 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.101.222 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.101.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.101.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.101.222 I print_info: LF token         = 187 'Ċ'
0.00.101.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.101.223 I print_info: max token length = 1024
0.00.140.073 I load_tensors: offloading 24 repeating layers to GPU
0.00.140.076 I load_tensors: offloading output layer to GPU
0.00.140.076 I load_tensors: offloaded 25/25 layers to GPU
0.00.140.098 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.140.100 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.140.512 I llama_init_from_model: n_seq_max     = 1
0.00.140.513 I llama_init_from_model: n_ctx         = 2048
0.00.140.514 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.140.514 I llama_init_from_model: n_batch       = 2048
0.00.140.514 I llama_init_from_model: n_ubatch      = 512
0.00.140.514 I llama_init_from_model: flash_attn    = 0
0.00.140.515 I llama_init_from_model: freq_base     = 10000.0
0.00.140.515 I llama_init_from_model: freq_scale    = 1
0.00.140.515 I ggml_metal_init: allocating
0.00.140.532 I ggml_metal_init: found device: Apple M4
0.00.140.537 I ggml_metal_init: picking default device: Apple M4
0.00.141.124 I ggml_metal_init: using embedded metal library
0.00.337.106 I ggml_metal_init: GPU name:   Apple M4
0.00.337.123 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.126 I ggml_metal_init: simdgroup reduction   = true
0.00.337.126 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.126 I ggml_metal_init: has residency sets    = true
0.00.337.126 I ggml_metal_init: has bfloat            = true
0.00.337.127 I ggml_metal_init: use bfloat            = true
0.00.337.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.783 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.410.499 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.410.507 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.410.530 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.415.061 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.415.064 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.415.065 I llama_init_from_model: graph nodes  = 967
0.00.415.065 I llama_init_from_model: graph splits = 2
0.00.415.072 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.415.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.415.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.951 I main: llama threadpool init, n_threads = 4
0.00.484.987 I 
0.00.485.021 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.485.021 I 
0.00.485.227 I sampler seed: 1234
0.00.485.232 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.485.260 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.485.262 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.485.262 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.311.562 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.02.311.563 I llama_perf_context_print:        load time =     437.90 ms
0.02.311.565 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.56 tokens per second)
0.02.311.565 I llama_perf_context_print:        eval time =    1779.45 ms /    63 runs   (   28.25 ms per token,    35.40 tokens per second)
0.02.311.566 I llama_perf_context_print:       total time =    1827.89 ms /    70 tokens
0.02.311.756 I ggml_metal_free: deallocating

real	0m2.634s
user	0m0.148s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.757 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.418 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.544 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.567 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.573 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.574 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.574 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.898 I llama_model_loader: - type  f32:  194 tensors
0.00.054.899 I llama_model_loader: - type  f16:   98 tensors
0.00.054.900 I print_info: file format = GGUF V3 (latest)
0.00.054.901 I print_info: file type   = all F32 (guessed)
0.00.054.902 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.520 I load: special tokens cache size = 25
0.00.076.901 I load: token to piece cache size = 0.2984 MB
0.00.076.905 I print_info: arch             = gptneox
0.00.076.905 I print_info: vocab_only       = 0
0.00.076.905 I print_info: n_ctx_train      = 2048
0.00.076.905 I print_info: n_embd           = 2048
0.00.076.905 I print_info: n_layer          = 24
0.00.076.909 I print_info: n_head           = 16
0.00.076.909 I print_info: n_head_kv        = 16
0.00.076.910 I print_info: n_rot            = 32
0.00.076.910 I print_info: n_swa            = 0
0.00.076.910 I print_info: n_embd_head_k    = 128
0.00.076.910 I print_info: n_embd_head_v    = 128
0.00.076.911 I print_info: n_gqa            = 1
0.00.076.912 I print_info: n_embd_k_gqa     = 2048
0.00.076.913 I print_info: n_embd_v_gqa     = 2048
0.00.076.913 I print_info: f_norm_eps       = 1.0e-05
0.00.076.915 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.915 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.916 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.916 I print_info: f_logit_scale    = 0.0e+00
0.00.076.917 I print_info: n_ff             = 8192
0.00.076.917 I print_info: n_expert         = 0
0.00.076.917 I print_info: n_expert_used    = 0
0.00.076.917 I print_info: causal attn      = 1
0.00.076.917 I print_info: pooling type     = 0
0.00.076.917 I print_info: rope type        = 2
0.00.076.918 I print_info: rope scaling     = linear
0.00.076.919 I print_info: freq_base_train  = 10000.0
0.00.076.921 I print_info: freq_scale_train = 1
0.00.076.921 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.921 I print_info: rope_finetuned   = unknown
0.00.076.921 I print_info: ssm_d_conv       = 0
0.00.076.921 I print_info: ssm_d_inner      = 0
0.00.076.921 I print_info: ssm_d_state      = 0
0.00.076.922 I print_info: ssm_dt_rank      = 0
0.00.076.922 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.922 I print_info: model type       = 1.4B
0.00.076.922 I print_info: model params     = 1.41 B
0.00.076.923 I print_info: general.name     = 1.4B
0.00.076.923 I print_info: vocab type       = BPE
0.00.076.927 I print_info: n_vocab          = 50304
0.00.076.927 I print_info: n_merges         = 50009
0.00.076.927 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.928 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.928 I print_info: LF token         = 187 'Ċ'
0.00.076.929 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.929 I print_info: max token length = 1024
0.01.444.798 I load_tensors: offloading 24 repeating layers to GPU
0.01.444.803 I load_tensors: offloading output layer to GPU
0.01.444.804 I load_tensors: offloaded 25/25 layers to GPU
0.01.444.828 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.444.830 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.445.866 I llama_init_from_model: n_seq_max     = 1
0.01.445.867 I llama_init_from_model: n_ctx         = 128
0.01.445.868 I llama_init_from_model: n_ctx_per_seq = 128
0.01.445.868 I llama_init_from_model: n_batch       = 128
0.01.445.868 I llama_init_from_model: n_ubatch      = 128
0.01.445.869 I llama_init_from_model: flash_attn    = 0
0.01.445.872 I llama_init_from_model: freq_base     = 10000.0
0.01.445.875 I llama_init_from_model: freq_scale    = 1
0.01.445.875 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.445.876 I ggml_metal_init: allocating
0.01.445.966 I ggml_metal_init: found device: Apple M4
0.01.445.972 I ggml_metal_init: picking default device: Apple M4
0.01.447.126 I ggml_metal_init: using embedded metal library
0.01.450.902 I ggml_metal_init: GPU name:   Apple M4
0.01.450.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.450.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.450.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.450.906 I ggml_metal_init: simdgroup reduction   = true
0.01.450.906 I ggml_metal_init: simdgroup matrix mul. = true
0.01.450.906 I ggml_metal_init: has residency sets    = true
0.01.450.907 I ggml_metal_init: has bfloat            = true
0.01.450.907 I ggml_metal_init: use bfloat            = true
0.01.450.907 I ggml_metal_init: hasUnifiedMemory      = true
0.01.450.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.461.663 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.463.355 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.463.359 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.463.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.464.951 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.464.952 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.464.953 I llama_init_from_model: graph nodes  = 967
0.01.464.953 I llama_init_from_model: graph splits = 2
0.01.464.954 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.464.954 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.499.710 I 
0.01.499.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.499.770 I perplexity: tokenizing the input ..
0.01.504.629 I perplexity: tokenization took 4.856 ms
0.01.504.633 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.623.528 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.625.712 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.625.745 I llama_perf_context_print:        load time =    1476.28 ms
0.01.625.746 I llama_perf_context_print: prompt eval time =     118.63 ms /   128 tokens (    0.93 ms per token,  1078.99 tokens per second)
0.01.625.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.625.747 I llama_perf_context_print:       total time =     126.04 ms /   129 tokens
0.01.626.157 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.102s
sys	0m0.267s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.013.301 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.032.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.485 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.485 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.486 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.654 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.559 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.560 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.561 I llama_model_loader: - type  f32:  194 tensors
0.00.041.561 I llama_model_loader: - type q8_0:   98 tensors
0.00.041.562 I print_info: file format = GGUF V3 (latest)
0.00.041.565 I print_info: file type   = Q8_0
0.00.041.566 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.050.667 I load: special tokens cache size = 25
0.00.057.870 I load: token to piece cache size = 0.2984 MB
0.00.057.874 I print_info: arch             = gptneox
0.00.057.875 I print_info: vocab_only       = 0
0.00.057.875 I print_info: n_ctx_train      = 2048
0.00.057.875 I print_info: n_embd           = 2048
0.00.057.875 I print_info: n_layer          = 24
0.00.057.881 I print_info: n_head           = 16
0.00.057.882 I print_info: n_head_kv        = 16
0.00.057.882 I print_info: n_rot            = 32
0.00.057.882 I print_info: n_swa            = 0
0.00.057.882 I print_info: n_embd_head_k    = 128
0.00.057.882 I print_info: n_embd_head_v    = 128
0.00.057.883 I print_info: n_gqa            = 1
0.00.057.885 I print_info: n_embd_k_gqa     = 2048
0.00.057.886 I print_info: n_embd_v_gqa     = 2048
0.00.057.886 I print_info: f_norm_eps       = 1.0e-05
0.00.057.887 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.887 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.889 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.889 I print_info: f_logit_scale    = 0.0e+00
0.00.057.890 I print_info: n_ff             = 8192
0.00.057.890 I print_info: n_expert         = 0
0.00.057.890 I print_info: n_expert_used    = 0
0.00.057.890 I print_info: causal attn      = 1
0.00.057.891 I print_info: pooling type     = 0
0.00.057.891 I print_info: rope type        = 2
0.00.057.891 I print_info: rope scaling     = linear
0.00.057.892 I print_info: freq_base_train  = 10000.0
0.00.057.892 I print_info: freq_scale_train = 1
0.00.057.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.892 I print_info: rope_finetuned   = unknown
0.00.057.892 I print_info: ssm_d_conv       = 0
0.00.057.893 I print_info: ssm_d_inner      = 0
0.00.057.893 I print_info: ssm_d_state      = 0
0.00.057.893 I print_info: ssm_dt_rank      = 0
0.00.057.893 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.893 I print_info: model type       = 1.4B
0.00.057.894 I print_info: model params     = 1.41 B
0.00.057.894 I print_info: general.name     = 1.4B
0.00.057.895 I print_info: vocab type       = BPE
0.00.057.899 I print_info: n_vocab          = 50304
0.00.057.899 I print_info: n_merges         = 50009
0.00.057.899 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.900 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.900 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.901 I print_info: LF token         = 187 'Ċ'
0.00.057.901 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.901 I print_info: max token length = 1024
0.01.265.930 I load_tensors: offloading 24 repeating layers to GPU
0.01.265.933 I load_tensors: offloading output layer to GPU
0.01.265.934 I load_tensors: offloaded 25/25 layers to GPU
0.01.265.954 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.265.956 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.266.984 I llama_init_from_model: n_seq_max     = 1
0.01.266.986 I llama_init_from_model: n_ctx         = 2048
0.01.266.986 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.266.986 I llama_init_from_model: n_batch       = 2048
0.01.266.987 I llama_init_from_model: n_ubatch      = 512
0.01.266.987 I llama_init_from_model: flash_attn    = 0
0.01.266.988 I llama_init_from_model: freq_base     = 10000.0
0.01.266.989 I llama_init_from_model: freq_scale    = 1
0.01.266.990 I ggml_metal_init: allocating
0.01.267.004 I ggml_metal_init: found device: Apple M4
0.01.267.011 I ggml_metal_init: picking default device: Apple M4
0.01.268.259 I ggml_metal_init: using embedded metal library
0.01.273.682 I ggml_metal_init: GPU name:   Apple M4
0.01.273.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.273.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.273.687 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.273.688 I ggml_metal_init: simdgroup reduction   = true
0.01.273.688 I ggml_metal_init: simdgroup matrix mul. = true
0.01.273.688 I ggml_metal_init: has residency sets    = true
0.01.273.688 I ggml_metal_init: has bfloat            = true
0.01.273.689 I ggml_metal_init: use bfloat            = true
0.01.273.689 I ggml_metal_init: hasUnifiedMemory      = true
0.01.273.690 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.289.454 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.339.663 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.339.669 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.339.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.344.074 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.344.076 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.344.076 I llama_init_from_model: graph nodes  = 967
0.01.344.076 I llama_init_from_model: graph splits = 2
0.01.344.082 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.344.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.344.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.400.582 I main: llama threadpool init, n_threads = 4
0.01.400.684 I 
0.01.400.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.400.708 I 
0.01.400.891 I sampler seed: 1234
0.01.400.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.400.940 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.400.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.400.943 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.488.955 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.02.488.956 I llama_perf_context_print:        load time =    1386.39 ms
0.02.488.957 I llama_perf_context_print: prompt eval time =      49.18 ms /     7 tokens (    7.03 ms per token,   142.33 tokens per second)
0.02.488.958 I llama_perf_context_print:        eval time =    1035.94 ms /    63 runs   (   16.44 ms per token,    60.81 tokens per second)
0.02.488.958 I llama_perf_context_print:       total time =    1089.26 ms /    70 tokens
0.02.489.222 I ggml_metal_free: deallocating

real	0m2.510s
user	0m0.110s
sys	0m0.275s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.142 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.321 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.323 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.329 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.330 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.331 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.811 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.811 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.812 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.813 I llama_model_loader: - type  f32:  194 tensors
0.00.024.813 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.814 I print_info: file format = GGUF V3 (latest)
0.00.024.815 I print_info: file type   = Q8_0
0.00.024.816 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.032.742 I load: special tokens cache size = 25
0.00.038.614 I load: token to piece cache size = 0.2984 MB
0.00.038.618 I print_info: arch             = gptneox
0.00.038.619 I print_info: vocab_only       = 0
0.00.038.619 I print_info: n_ctx_train      = 2048
0.00.038.619 I print_info: n_embd           = 2048
0.00.038.619 I print_info: n_layer          = 24
0.00.038.623 I print_info: n_head           = 16
0.00.038.624 I print_info: n_head_kv        = 16
0.00.038.624 I print_info: n_rot            = 32
0.00.038.624 I print_info: n_swa            = 0
0.00.038.624 I print_info: n_embd_head_k    = 128
0.00.038.627 I print_info: n_embd_head_v    = 128
0.00.038.628 I print_info: n_gqa            = 1
0.00.038.629 I print_info: n_embd_k_gqa     = 2048
0.00.038.629 I print_info: n_embd_v_gqa     = 2048
0.00.038.630 I print_info: f_norm_eps       = 1.0e-05
0.00.038.631 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.631 I print_info: f_logit_scale    = 0.0e+00
0.00.038.632 I print_info: n_ff             = 8192
0.00.038.632 I print_info: n_expert         = 0
0.00.038.632 I print_info: n_expert_used    = 0
0.00.038.632 I print_info: causal attn      = 1
0.00.038.633 I print_info: pooling type     = 0
0.00.038.633 I print_info: rope type        = 2
0.00.038.633 I print_info: rope scaling     = linear
0.00.038.633 I print_info: freq_base_train  = 10000.0
0.00.038.633 I print_info: freq_scale_train = 1
0.00.038.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.634 I print_info: rope_finetuned   = unknown
0.00.038.635 I print_info: ssm_d_conv       = 0
0.00.038.635 I print_info: ssm_d_inner      = 0
0.00.038.635 I print_info: ssm_d_state      = 0
0.00.038.635 I print_info: ssm_dt_rank      = 0
0.00.038.635 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.635 I print_info: model type       = 1.4B
0.00.038.636 I print_info: model params     = 1.41 B
0.00.038.636 I print_info: general.name     = 1.4B
0.00.038.636 I print_info: vocab type       = BPE
0.00.038.637 I print_info: n_vocab          = 50304
0.00.038.637 I print_info: n_merges         = 50009
0.00.038.638 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: LF token         = 187 'Ċ'
0.00.038.639 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: max token length = 1024
0.00.864.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.864.783 I load_tensors: offloading output layer to GPU
0.00.864.783 I load_tensors: offloaded 25/25 layers to GPU
0.00.864.809 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.864.811 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.866.156 I llama_init_from_model: n_seq_max     = 1
0.00.866.158 I llama_init_from_model: n_ctx         = 128
0.00.866.159 I llama_init_from_model: n_ctx_per_seq = 128
0.00.866.159 I llama_init_from_model: n_batch       = 128
0.00.866.159 I llama_init_from_model: n_ubatch      = 128
0.00.866.160 I llama_init_from_model: flash_attn    = 0
0.00.866.160 I llama_init_from_model: freq_base     = 10000.0
0.00.866.161 I llama_init_from_model: freq_scale    = 1
0.00.866.161 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.866.163 I ggml_metal_init: allocating
0.00.866.211 I ggml_metal_init: found device: Apple M4
0.00.866.220 I ggml_metal_init: picking default device: Apple M4
0.00.867.513 I ggml_metal_init: using embedded metal library
0.00.872.993 I ggml_metal_init: GPU name:   Apple M4
0.00.872.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.872.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.872.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.872.999 I ggml_metal_init: simdgroup reduction   = true
0.00.872.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.872.999 I ggml_metal_init: has residency sets    = true
0.00.873.000 I ggml_metal_init: has bfloat            = true
0.00.873.000 I ggml_metal_init: use bfloat            = true
0.00.873.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.873.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.888.135 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.891.444 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.891.447 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.891.471 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.894.623 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.894.624 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.894.625 I llama_init_from_model: graph nodes  = 967
0.00.894.625 I llama_init_from_model: graph splits = 2
0.00.894.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.894.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.923.029 I 
0.00.923.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.923.130 I perplexity: tokenizing the input ..
0.00.929.955 I perplexity: tokenization took 6.823 ms
0.00.929.960 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.067.682 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.091 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.120 I llama_perf_context_print:        load time =     913.88 ms
0.01.069.121 I llama_perf_context_print: prompt eval time =     137.43 ms /   128 tokens (    1.07 ms per token,   931.36 tokens per second)
0.01.069.122 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.122 I llama_perf_context_print:       total time =     146.09 ms /   129 tokens
0.01.069.474 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.074s
sys	0m0.170s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.061 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.100 I main: llama backend init
0.00.000.102 I main: load the model and apply lora adapter, if any
0.00.015.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.441 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.445 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.970 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.686 I llama_model_loader: - type  f32:  194 tensors
0.00.035.687 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.687 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.688 I print_info: file format = GGUF V3 (latest)
0.00.035.688 I print_info: file type   = Q4_0
0.00.035.689 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.817 I load: special tokens cache size = 25
0.00.053.775 I load: token to piece cache size = 0.2984 MB
0.00.053.779 I print_info: arch             = gptneox
0.00.053.779 I print_info: vocab_only       = 0
0.00.053.780 I print_info: n_ctx_train      = 2048
0.00.053.780 I print_info: n_embd           = 2048
0.00.053.780 I print_info: n_layer          = 24
0.00.053.784 I print_info: n_head           = 16
0.00.053.785 I print_info: n_head_kv        = 16
0.00.053.786 I print_info: n_rot            = 32
0.00.053.786 I print_info: n_swa            = 0
0.00.053.786 I print_info: n_embd_head_k    = 128
0.00.053.786 I print_info: n_embd_head_v    = 128
0.00.053.787 I print_info: n_gqa            = 1
0.00.053.791 I print_info: n_embd_k_gqa     = 2048
0.00.053.792 I print_info: n_embd_v_gqa     = 2048
0.00.053.795 I print_info: f_norm_eps       = 1.0e-05
0.00.053.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.796 I print_info: f_logit_scale    = 0.0e+00
0.00.053.797 I print_info: n_ff             = 8192
0.00.053.797 I print_info: n_expert         = 0
0.00.053.797 I print_info: n_expert_used    = 0
0.00.053.797 I print_info: causal attn      = 1
0.00.053.798 I print_info: pooling type     = 0
0.00.053.798 I print_info: rope type        = 2
0.00.053.798 I print_info: rope scaling     = linear
0.00.053.800 I print_info: freq_base_train  = 10000.0
0.00.053.800 I print_info: freq_scale_train = 1
0.00.053.800 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.800 I print_info: rope_finetuned   = unknown
0.00.053.800 I print_info: ssm_d_conv       = 0
0.00.053.801 I print_info: ssm_d_inner      = 0
0.00.053.801 I print_info: ssm_d_state      = 0
0.00.053.801 I print_info: ssm_dt_rank      = 0
0.00.053.801 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.801 I print_info: model type       = 1.4B
0.00.053.802 I print_info: model params     = 1.41 B
0.00.053.802 I print_info: general.name     = 1.4B
0.00.053.803 I print_info: vocab type       = BPE
0.00.053.803 I print_info: n_vocab          = 50304
0.00.053.803 I print_info: n_merges         = 50009
0.00.053.804 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.805 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.805 I print_info: LF token         = 187 'Ċ'
0.00.053.806 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.806 I print_info: max token length = 1024
0.00.616.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.369 I load_tensors: offloading output layer to GPU
0.00.616.370 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.403 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.616.404 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.617.854 I llama_init_from_model: n_seq_max     = 1
0.00.617.859 I llama_init_from_model: n_ctx         = 2048
0.00.617.860 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.617.860 I llama_init_from_model: n_batch       = 2048
0.00.617.861 I llama_init_from_model: n_ubatch      = 512
0.00.617.861 I llama_init_from_model: flash_attn    = 0
0.00.617.863 I llama_init_from_model: freq_base     = 10000.0
0.00.617.863 I llama_init_from_model: freq_scale    = 1
0.00.617.865 I ggml_metal_init: allocating
0.00.617.970 I ggml_metal_init: found device: Apple M4
0.00.617.985 I ggml_metal_init: picking default device: Apple M4
0.00.619.773 I ggml_metal_init: using embedded metal library
0.00.625.220 I ggml_metal_init: GPU name:   Apple M4
0.00.625.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.234 I ggml_metal_init: simdgroup reduction   = true
0.00.625.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.235 I ggml_metal_init: has residency sets    = true
0.00.625.235 I ggml_metal_init: has bfloat            = true
0.00.625.235 I ggml_metal_init: use bfloat            = true
0.00.625.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.022 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.508 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.513 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.539 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.954 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.956 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.956 I llama_init_from_model: graph nodes  = 967
0.00.703.956 I llama_init_from_model: graph splits = 2
0.00.703.962 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.209 I main: llama threadpool init, n_threads = 4
0.00.761.251 I 
0.00.761.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.274 I 
0.00.761.443 I sampler seed: 1234
0.00.761.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.497 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.500 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.500 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.443.569 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.443.570 I llama_perf_context_print:        load time =     744.45 ms
0.01.443.570 I llama_perf_context_print: prompt eval time =      49.33 ms /     7 tokens (    7.05 ms per token,   141.90 tokens per second)
0.01.443.571 I llama_perf_context_print:        eval time =     629.89 ms /    63 runs   (   10.00 ms per token,   100.02 tokens per second)
0.01.443.571 I llama_perf_context_print:       total time =     683.28 ms /    70 tokens
0.01.443.761 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.117s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.418 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.486 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.487 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.487 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.489 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.489 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.490 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.490 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.492 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.493 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.493 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.024 I llama_model_loader: - type  f32:  194 tensors
0.00.027.024 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.025 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.025 I print_info: file format = GGUF V3 (latest)
0.00.027.026 I print_info: file type   = Q4_0
0.00.027.027 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.155 I load: special tokens cache size = 25
0.00.041.365 I load: token to piece cache size = 0.2984 MB
0.00.041.368 I print_info: arch             = gptneox
0.00.041.369 I print_info: vocab_only       = 0
0.00.041.369 I print_info: n_ctx_train      = 2048
0.00.041.369 I print_info: n_embd           = 2048
0.00.041.369 I print_info: n_layer          = 24
0.00.041.372 I print_info: n_head           = 16
0.00.041.373 I print_info: n_head_kv        = 16
0.00.041.373 I print_info: n_rot            = 32
0.00.041.374 I print_info: n_swa            = 0
0.00.041.374 I print_info: n_embd_head_k    = 128
0.00.041.374 I print_info: n_embd_head_v    = 128
0.00.041.375 I print_info: n_gqa            = 1
0.00.041.376 I print_info: n_embd_k_gqa     = 2048
0.00.041.376 I print_info: n_embd_v_gqa     = 2048
0.00.041.377 I print_info: f_norm_eps       = 1.0e-05
0.00.041.377 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.377 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.378 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.378 I print_info: f_logit_scale    = 0.0e+00
0.00.041.379 I print_info: n_ff             = 8192
0.00.041.379 I print_info: n_expert         = 0
0.00.041.380 I print_info: n_expert_used    = 0
0.00.041.380 I print_info: causal attn      = 1
0.00.041.380 I print_info: pooling type     = 0
0.00.041.381 I print_info: rope type        = 2
0.00.041.381 I print_info: rope scaling     = linear
0.00.041.381 I print_info: freq_base_train  = 10000.0
0.00.041.382 I print_info: freq_scale_train = 1
0.00.041.382 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.382 I print_info: rope_finetuned   = unknown
0.00.041.382 I print_info: ssm_d_conv       = 0
0.00.041.382 I print_info: ssm_d_inner      = 0
0.00.041.383 I print_info: ssm_d_state      = 0
0.00.041.383 I print_info: ssm_dt_rank      = 0
0.00.041.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.385 I print_info: model type       = 1.4B
0.00.041.385 I print_info: model params     = 1.41 B
0.00.041.386 I print_info: general.name     = 1.4B
0.00.041.386 I print_info: vocab type       = BPE
0.00.041.386 I print_info: n_vocab          = 50304
0.00.041.386 I print_info: n_merges         = 50009
0.00.041.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.387 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.391 I print_info: LF token         = 187 'Ċ'
0.00.041.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.395 I print_info: max token length = 1024
0.00.603.550 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.560 I load_tensors: offloading output layer to GPU
0.00.603.560 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.593 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.603.595 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.605.183 I llama_init_from_model: n_seq_max     = 1
0.00.605.188 I llama_init_from_model: n_ctx         = 128
0.00.605.188 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.189 I llama_init_from_model: n_batch       = 128
0.00.605.189 I llama_init_from_model: n_ubatch      = 128
0.00.605.190 I llama_init_from_model: flash_attn    = 0
0.00.605.192 I llama_init_from_model: freq_base     = 10000.0
0.00.605.193 I llama_init_from_model: freq_scale    = 1
0.00.605.193 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.195 I ggml_metal_init: allocating
0.00.605.274 I ggml_metal_init: found device: Apple M4
0.00.605.287 I ggml_metal_init: picking default device: Apple M4
0.00.606.983 I ggml_metal_init: using embedded metal library
0.00.612.569 I ggml_metal_init: GPU name:   Apple M4
0.00.612.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.586 I ggml_metal_init: simdgroup reduction   = true
0.00.612.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.586 I ggml_metal_init: has residency sets    = true
0.00.612.586 I ggml_metal_init: has bfloat            = true
0.00.612.587 I ggml_metal_init: use bfloat            = true
0.00.612.589 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.993 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.635.997 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.636.028 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.229 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.639.231 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.639.231 I llama_init_from_model: graph nodes  = 967
0.00.639.232 I llama_init_from_model: graph splits = 2
0.00.639.234 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.639.235 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.739 I 
0.00.669.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.822 I perplexity: tokenizing the input ..
0.00.677.003 I perplexity: tokenization took 7.177 ms
0.00.677.015 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.716 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.810.048 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.810.075 I llama_perf_context_print:        load time =     659.31 ms
0.00.810.076 I llama_perf_context_print: prompt eval time =     130.74 ms /   128 tokens (    1.02 ms per token,   979.01 tokens per second)
0.00.810.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.077 I llama_perf_context_print:       total time =     140.34 ms /   129 tokens
0.00.810.461 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.080s
sys	0m0.139s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.255 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.882 I llama_model_loader: - type  f32:  194 tensors
0.00.023.882 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.883 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.883 I print_info: file format = GGUF V3 (latest)
0.00.023.884 I print_info: file type   = Q4_1
0.00.023.885 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.031.564 I load: special tokens cache size = 25
0.00.037.520 I load: token to piece cache size = 0.2984 MB
0.00.037.523 I print_info: arch             = gptneox
0.00.037.523 I print_info: vocab_only       = 0
0.00.037.523 I print_info: n_ctx_train      = 2048
0.00.037.523 I print_info: n_embd           = 2048
0.00.037.524 I print_info: n_layer          = 24
0.00.037.526 I print_info: n_head           = 16
0.00.037.526 I print_info: n_head_kv        = 16
0.00.037.527 I print_info: n_rot            = 32
0.00.037.527 I print_info: n_swa            = 0
0.00.037.528 I print_info: n_embd_head_k    = 128
0.00.037.528 I print_info: n_embd_head_v    = 128
0.00.037.529 I print_info: n_gqa            = 1
0.00.037.530 I print_info: n_embd_k_gqa     = 2048
0.00.037.530 I print_info: n_embd_v_gqa     = 2048
0.00.037.535 I print_info: f_norm_eps       = 1.0e-05
0.00.037.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.536 I print_info: f_logit_scale    = 0.0e+00
0.00.037.538 I print_info: n_ff             = 8192
0.00.037.538 I print_info: n_expert         = 0
0.00.037.538 I print_info: n_expert_used    = 0
0.00.037.538 I print_info: causal attn      = 1
0.00.037.538 I print_info: pooling type     = 0
0.00.037.538 I print_info: rope type        = 2
0.00.037.539 I print_info: rope scaling     = linear
0.00.037.539 I print_info: freq_base_train  = 10000.0
0.00.037.539 I print_info: freq_scale_train = 1
0.00.037.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.543 I print_info: rope_finetuned   = unknown
0.00.037.543 I print_info: ssm_d_conv       = 0
0.00.037.545 I print_info: ssm_d_inner      = 0
0.00.037.545 I print_info: ssm_d_state      = 0
0.00.037.545 I print_info: ssm_dt_rank      = 0
0.00.037.545 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.546 I print_info: model type       = 1.4B
0.00.037.546 I print_info: model params     = 1.41 B
0.00.037.546 I print_info: general.name     = 1.4B
0.00.037.547 I print_info: vocab type       = BPE
0.00.037.547 I print_info: n_vocab          = 50304
0.00.037.547 I print_info: n_merges         = 50009
0.00.037.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.548 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.548 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.548 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.548 I print_info: LF token         = 187 'Ċ'
0.00.037.548 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.549 I print_info: max token length = 1024
0.00.733.724 I load_tensors: offloading 24 repeating layers to GPU
0.00.733.741 I load_tensors: offloading output layer to GPU
0.00.733.741 I load_tensors: offloaded 25/25 layers to GPU
0.00.733.779 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.733.780 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.735.269 I llama_init_from_model: n_seq_max     = 1
0.00.735.274 I llama_init_from_model: n_ctx         = 2048
0.00.735.275 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.735.275 I llama_init_from_model: n_batch       = 2048
0.00.735.276 I llama_init_from_model: n_ubatch      = 512
0.00.735.276 I llama_init_from_model: flash_attn    = 0
0.00.735.278 I llama_init_from_model: freq_base     = 10000.0
0.00.735.278 I llama_init_from_model: freq_scale    = 1
0.00.735.280 I ggml_metal_init: allocating
0.00.735.382 I ggml_metal_init: found device: Apple M4
0.00.735.396 I ggml_metal_init: picking default device: Apple M4
0.00.737.269 I ggml_metal_init: using embedded metal library
0.00.743.833 I ggml_metal_init: GPU name:   Apple M4
0.00.743.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.743.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.743.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.743.839 I ggml_metal_init: simdgroup reduction   = true
0.00.743.839 I ggml_metal_init: simdgroup matrix mul. = true
0.00.743.840 I ggml_metal_init: has residency sets    = true
0.00.743.840 I ggml_metal_init: has bfloat            = true
0.00.743.840 I ggml_metal_init: use bfloat            = true
0.00.743.841 I ggml_metal_init: hasUnifiedMemory      = true
0.00.743.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.761.263 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.815.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.815.720 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.815.745 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.820.282 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.820.284 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.820.284 I llama_init_from_model: graph nodes  = 967
0.00.820.285 I llama_init_from_model: graph splits = 2
0.00.820.291 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.820.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.820.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.262 I main: llama threadpool init, n_threads = 4
0.00.874.312 I 
0.00.874.336 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.336 I 
0.00.874.517 I sampler seed: 1234
0.00.874.522 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.532 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.532 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.532 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.600.981 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.600.982 I llama_perf_context_print:        load time =     864.48 ms
0.01.600.982 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.41 tokens per second)
0.01.600.983 I llama_perf_context_print:        eval time =     674.88 ms /    63 runs   (   10.71 ms per token,    93.35 tokens per second)
0.01.600.983 I llama_perf_context_print:       total time =     727.63 ms /    70 tokens
0.01.601.283 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.402 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.312 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.593 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.594 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.595 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.595 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.595 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.596 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.596 I llama_model_loader: - type  f32:  194 tensors
0.00.024.597 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.597 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.598 I print_info: file format = GGUF V3 (latest)
0.00.024.598 I print_info: file type   = Q4_1
0.00.024.599 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.273 I load: special tokens cache size = 25
0.00.038.346 I load: token to piece cache size = 0.2984 MB
0.00.038.349 I print_info: arch             = gptneox
0.00.038.349 I print_info: vocab_only       = 0
0.00.038.349 I print_info: n_ctx_train      = 2048
0.00.038.350 I print_info: n_embd           = 2048
0.00.038.350 I print_info: n_layer          = 24
0.00.038.353 I print_info: n_head           = 16
0.00.038.358 I print_info: n_head_kv        = 16
0.00.038.358 I print_info: n_rot            = 32
0.00.038.358 I print_info: n_swa            = 0
0.00.038.358 I print_info: n_embd_head_k    = 128
0.00.038.358 I print_info: n_embd_head_v    = 128
0.00.038.359 I print_info: n_gqa            = 1
0.00.038.360 I print_info: n_embd_k_gqa     = 2048
0.00.038.360 I print_info: n_embd_v_gqa     = 2048
0.00.038.361 I print_info: f_norm_eps       = 1.0e-05
0.00.038.361 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.361 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.363 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.363 I print_info: f_logit_scale    = 0.0e+00
0.00.038.363 I print_info: n_ff             = 8192
0.00.038.364 I print_info: n_expert         = 0
0.00.038.364 I print_info: n_expert_used    = 0
0.00.038.364 I print_info: causal attn      = 1
0.00.038.364 I print_info: pooling type     = 0
0.00.038.364 I print_info: rope type        = 2
0.00.038.364 I print_info: rope scaling     = linear
0.00.038.365 I print_info: freq_base_train  = 10000.0
0.00.038.365 I print_info: freq_scale_train = 1
0.00.038.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.365 I print_info: rope_finetuned   = unknown
0.00.038.365 I print_info: ssm_d_conv       = 0
0.00.038.366 I print_info: ssm_d_inner      = 0
0.00.038.366 I print_info: ssm_d_state      = 0
0.00.038.367 I print_info: ssm_dt_rank      = 0
0.00.038.371 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.371 I print_info: model type       = 1.4B
0.00.038.371 I print_info: model params     = 1.41 B
0.00.038.373 I print_info: general.name     = 1.4B
0.00.038.374 I print_info: vocab type       = BPE
0.00.038.374 I print_info: n_vocab          = 50304
0.00.038.374 I print_info: n_merges         = 50009
0.00.038.374 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: LF token         = 187 'Ċ'
0.00.038.375 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: max token length = 1024
0.00.660.081 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.094 I load_tensors: offloading output layer to GPU
0.00.660.095 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.130 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.660.132 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.661.477 I llama_init_from_model: n_seq_max     = 1
0.00.661.482 I llama_init_from_model: n_ctx         = 128
0.00.661.482 I llama_init_from_model: n_ctx_per_seq = 128
0.00.661.486 I llama_init_from_model: n_batch       = 128
0.00.661.487 I llama_init_from_model: n_ubatch      = 128
0.00.661.487 I llama_init_from_model: flash_attn    = 0
0.00.661.489 I llama_init_from_model: freq_base     = 10000.0
0.00.661.495 I llama_init_from_model: freq_scale    = 1
0.00.661.495 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.661.498 I ggml_metal_init: allocating
0.00.661.582 I ggml_metal_init: found device: Apple M4
0.00.661.595 I ggml_metal_init: picking default device: Apple M4
0.00.663.402 I ggml_metal_init: using embedded metal library
0.00.668.986 I ggml_metal_init: GPU name:   Apple M4
0.00.668.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.993 I ggml_metal_init: simdgroup reduction   = true
0.00.668.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.994 I ggml_metal_init: has residency sets    = true
0.00.668.994 I ggml_metal_init: has bfloat            = true
0.00.668.995 I ggml_metal_init: use bfloat            = true
0.00.668.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.841 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.283 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.691.289 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.691.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.694.575 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.694.577 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.694.577 I llama_init_from_model: graph nodes  = 967
0.00.694.578 I llama_init_from_model: graph splits = 2
0.00.694.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.694.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.441 I 
0.00.718.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.545 I perplexity: tokenizing the input ..
0.00.725.036 I perplexity: tokenization took 6.489 ms
0.00.725.042 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.728 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.861.061 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.861.093 I llama_perf_context_print:        load time =     709.03 ms
0.00.861.094 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.41 tokens per second)
0.00.861.094 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.095 I llama_perf_context_print:       total time =     142.65 ms /   129 tokens
0.00.861.468 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.077s
sys	0m0.129s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.658 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.132 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.138 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.143 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.143 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.144 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.876 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.599 I llama_model_loader: - type  f32:  194 tensors
0.00.023.600 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.601 I print_info: file format = GGUF V3 (latest)
0.00.023.601 I print_info: file type   = Q5_0
0.00.023.604 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.031.216 I load: special tokens cache size = 25
0.00.037.415 I load: token to piece cache size = 0.2984 MB
0.00.037.418 I print_info: arch             = gptneox
0.00.037.418 I print_info: vocab_only       = 0
0.00.037.418 I print_info: n_ctx_train      = 2048
0.00.037.419 I print_info: n_embd           = 2048
0.00.037.419 I print_info: n_layer          = 24
0.00.037.422 I print_info: n_head           = 16
0.00.037.422 I print_info: n_head_kv        = 16
0.00.037.422 I print_info: n_rot            = 32
0.00.037.424 I print_info: n_swa            = 0
0.00.037.424 I print_info: n_embd_head_k    = 128
0.00.037.425 I print_info: n_embd_head_v    = 128
0.00.037.425 I print_info: n_gqa            = 1
0.00.037.426 I print_info: n_embd_k_gqa     = 2048
0.00.037.431 I print_info: n_embd_v_gqa     = 2048
0.00.037.431 I print_info: f_norm_eps       = 1.0e-05
0.00.037.432 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.432 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.432 I print_info: f_logit_scale    = 0.0e+00
0.00.037.434 I print_info: n_ff             = 8192
0.00.037.434 I print_info: n_expert         = 0
0.00.037.434 I print_info: n_expert_used    = 0
0.00.037.434 I print_info: causal attn      = 1
0.00.037.434 I print_info: pooling type     = 0
0.00.037.434 I print_info: rope type        = 2
0.00.037.435 I print_info: rope scaling     = linear
0.00.037.436 I print_info: freq_base_train  = 10000.0
0.00.037.437 I print_info: freq_scale_train = 1
0.00.037.437 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.437 I print_info: rope_finetuned   = unknown
0.00.037.437 I print_info: ssm_d_conv       = 0
0.00.037.437 I print_info: ssm_d_inner      = 0
0.00.037.437 I print_info: ssm_d_state      = 0
0.00.037.441 I print_info: ssm_dt_rank      = 0
0.00.037.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.441 I print_info: model type       = 1.4B
0.00.037.441 I print_info: model params     = 1.41 B
0.00.037.441 I print_info: general.name     = 1.4B
0.00.037.442 I print_info: vocab type       = BPE
0.00.037.442 I print_info: n_vocab          = 50304
0.00.037.442 I print_info: n_merges         = 50009
0.00.037.442 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.443 I print_info: LF token         = 187 'Ċ'
0.00.037.443 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.443 I print_info: max token length = 1024
0.00.711.393 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.407 I load_tensors: offloading output layer to GPU
0.00.711.408 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.440 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.711.441 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.712.822 I llama_init_from_model: n_seq_max     = 1
0.00.712.828 I llama_init_from_model: n_ctx         = 2048
0.00.712.828 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.712.829 I llama_init_from_model: n_batch       = 2048
0.00.712.829 I llama_init_from_model: n_ubatch      = 512
0.00.712.829 I llama_init_from_model: flash_attn    = 0
0.00.712.832 I llama_init_from_model: freq_base     = 10000.0
0.00.712.832 I llama_init_from_model: freq_scale    = 1
0.00.712.838 I ggml_metal_init: allocating
0.00.712.964 I ggml_metal_init: found device: Apple M4
0.00.712.978 I ggml_metal_init: picking default device: Apple M4
0.00.714.875 I ggml_metal_init: using embedded metal library
0.00.721.463 I ggml_metal_init: GPU name:   Apple M4
0.00.721.467 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.468 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.468 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.469 I ggml_metal_init: simdgroup reduction   = true
0.00.721.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.470 I ggml_metal_init: has residency sets    = true
0.00.721.470 I ggml_metal_init: has bfloat            = true
0.00.721.470 I ggml_metal_init: use bfloat            = true
0.00.721.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.739.514 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.792.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.792.150 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.792.215 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.797.017 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.797.019 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.797.020 I llama_init_from_model: graph nodes  = 967
0.00.797.020 I llama_init_from_model: graph splits = 2
0.00.797.026 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.797.160 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.797.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.678 I main: llama threadpool init, n_threads = 4
0.00.855.722 I 
0.00.855.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.748 I 
0.00.855.917 I sampler seed: 1234
0.00.855.921 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.932 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.932 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.932 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.644.274 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.644.274 I llama_perf_context_print:        load time =     846.13 ms
0.01.644.275 I llama_perf_context_print: prompt eval time =      52.91 ms /     7 tokens (    7.56 ms per token,   132.30 tokens per second)
0.01.644.276 I llama_perf_context_print:        eval time =     732.61 ms /    63 runs   (   11.63 ms per token,    85.99 tokens per second)
0.01.644.276 I llama_perf_context_print:       total time =     789.49 ms /    70 tokens
0.01.644.554 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.109s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.974 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.914 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.916 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.917 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.917 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.919 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.920 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.922 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.429 I llama_model_loader: - type  f32:  194 tensors
0.00.024.430 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.430 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.431 I print_info: file format = GGUF V3 (latest)
0.00.024.431 I print_info: file type   = Q5_0
0.00.024.432 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.651 I load: special tokens cache size = 25
0.00.038.739 I load: token to piece cache size = 0.2984 MB
0.00.038.742 I print_info: arch             = gptneox
0.00.038.742 I print_info: vocab_only       = 0
0.00.038.742 I print_info: n_ctx_train      = 2048
0.00.038.742 I print_info: n_embd           = 2048
0.00.038.743 I print_info: n_layer          = 24
0.00.038.746 I print_info: n_head           = 16
0.00.038.746 I print_info: n_head_kv        = 16
0.00.038.747 I print_info: n_rot            = 32
0.00.038.747 I print_info: n_swa            = 0
0.00.038.747 I print_info: n_embd_head_k    = 128
0.00.038.747 I print_info: n_embd_head_v    = 128
0.00.038.748 I print_info: n_gqa            = 1
0.00.038.749 I print_info: n_embd_k_gqa     = 2048
0.00.038.750 I print_info: n_embd_v_gqa     = 2048
0.00.038.750 I print_info: f_norm_eps       = 1.0e-05
0.00.038.752 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.752 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.753 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.753 I print_info: f_logit_scale    = 0.0e+00
0.00.038.753 I print_info: n_ff             = 8192
0.00.038.754 I print_info: n_expert         = 0
0.00.038.754 I print_info: n_expert_used    = 0
0.00.038.754 I print_info: causal attn      = 1
0.00.038.754 I print_info: pooling type     = 0
0.00.038.754 I print_info: rope type        = 2
0.00.038.755 I print_info: rope scaling     = linear
0.00.038.755 I print_info: freq_base_train  = 10000.0
0.00.038.755 I print_info: freq_scale_train = 1
0.00.038.756 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.756 I print_info: rope_finetuned   = unknown
0.00.038.756 I print_info: ssm_d_conv       = 0
0.00.038.756 I print_info: ssm_d_inner      = 0
0.00.038.756 I print_info: ssm_d_state      = 0
0.00.038.756 I print_info: ssm_dt_rank      = 0
0.00.038.757 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.758 I print_info: model type       = 1.4B
0.00.038.759 I print_info: model params     = 1.41 B
0.00.038.759 I print_info: general.name     = 1.4B
0.00.038.759 I print_info: vocab type       = BPE
0.00.038.760 I print_info: n_vocab          = 50304
0.00.038.760 I print_info: n_merges         = 50009
0.00.038.760 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.761 I print_info: LF token         = 187 'Ċ'
0.00.038.761 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.762 I print_info: max token length = 1024
0.00.700.244 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.258 I load_tensors: offloading output layer to GPU
0.00.700.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.288 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.700.289 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.701.709 I llama_init_from_model: n_seq_max     = 1
0.00.701.716 I llama_init_from_model: n_ctx         = 128
0.00.701.717 I llama_init_from_model: n_ctx_per_seq = 128
0.00.701.718 I llama_init_from_model: n_batch       = 128
0.00.701.718 I llama_init_from_model: n_ubatch      = 128
0.00.701.719 I llama_init_from_model: flash_attn    = 0
0.00.701.720 I llama_init_from_model: freq_base     = 10000.0
0.00.701.720 I llama_init_from_model: freq_scale    = 1
0.00.701.721 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.701.723 I ggml_metal_init: allocating
0.00.701.769 I ggml_metal_init: found device: Apple M4
0.00.701.781 I ggml_metal_init: picking default device: Apple M4
0.00.703.393 I ggml_metal_init: using embedded metal library
0.00.710.052 I ggml_metal_init: GPU name:   Apple M4
0.00.710.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.710.058 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.710.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.710.060 I ggml_metal_init: simdgroup reduction   = true
0.00.710.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.710.061 I ggml_metal_init: has residency sets    = true
0.00.710.061 I ggml_metal_init: has bfloat            = true
0.00.710.061 I ggml_metal_init: use bfloat            = true
0.00.710.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.710.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.876 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.731.368 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.731.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.680 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.734.682 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.734.683 I llama_init_from_model: graph nodes  = 967
0.00.734.683 I llama_init_from_model: graph splits = 2
0.00.734.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.734.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.497 I 
0.00.762.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.594 I perplexity: tokenizing the input ..
0.00.769.548 I perplexity: tokenization took 6.953 ms
0.00.769.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.380 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.906.789 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.906.811 I llama_perf_context_print:        load time =     753.51 ms
0.00.906.812 I llama_perf_context_print: prompt eval time =     134.76 ms /   128 tokens (    1.05 ms per token,   949.86 tokens per second)
0.00.906.812 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.812 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.907.139 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.078s
sys	0m0.151s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.488 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.495 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.049 I llama_model_loader: - type  f32:  194 tensors
0.00.025.049 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.049 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.050 I print_info: file format = GGUF V3 (latest)
0.00.025.051 I print_info: file type   = Q5_1
0.00.025.056 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.128 I load: special tokens cache size = 25
0.00.039.247 I load: token to piece cache size = 0.2984 MB
0.00.039.251 I print_info: arch             = gptneox
0.00.039.251 I print_info: vocab_only       = 0
0.00.039.251 I print_info: n_ctx_train      = 2048
0.00.039.251 I print_info: n_embd           = 2048
0.00.039.251 I print_info: n_layer          = 24
0.00.039.254 I print_info: n_head           = 16
0.00.039.255 I print_info: n_head_kv        = 16
0.00.039.255 I print_info: n_rot            = 32
0.00.039.255 I print_info: n_swa            = 0
0.00.039.255 I print_info: n_embd_head_k    = 128
0.00.039.255 I print_info: n_embd_head_v    = 128
0.00.039.257 I print_info: n_gqa            = 1
0.00.039.258 I print_info: n_embd_k_gqa     = 2048
0.00.039.259 I print_info: n_embd_v_gqa     = 2048
0.00.039.259 I print_info: f_norm_eps       = 1.0e-05
0.00.039.260 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.263 I print_info: f_logit_scale    = 0.0e+00
0.00.039.264 I print_info: n_ff             = 8192
0.00.039.264 I print_info: n_expert         = 0
0.00.039.264 I print_info: n_expert_used    = 0
0.00.039.264 I print_info: causal attn      = 1
0.00.039.264 I print_info: pooling type     = 0
0.00.039.264 I print_info: rope type        = 2
0.00.039.264 I print_info: rope scaling     = linear
0.00.039.265 I print_info: freq_base_train  = 10000.0
0.00.039.265 I print_info: freq_scale_train = 1
0.00.039.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.266 I print_info: rope_finetuned   = unknown
0.00.039.266 I print_info: ssm_d_conv       = 0
0.00.039.266 I print_info: ssm_d_inner      = 0
0.00.039.266 I print_info: ssm_d_state      = 0
0.00.039.266 I print_info: ssm_dt_rank      = 0
0.00.039.266 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.267 I print_info: model type       = 1.4B
0.00.039.267 I print_info: model params     = 1.41 B
0.00.039.267 I print_info: general.name     = 1.4B
0.00.039.268 I print_info: vocab type       = BPE
0.00.039.268 I print_info: n_vocab          = 50304
0.00.039.268 I print_info: n_merges         = 50009
0.00.039.268 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.270 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.271 I print_info: LF token         = 187 'Ċ'
0.00.039.271 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.271 I print_info: max token length = 1024
0.00.668.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.001 I load_tensors: offloading output layer to GPU
0.00.669.002 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.038 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.669.040 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.670.216 I llama_init_from_model: n_seq_max     = 1
0.00.670.222 I llama_init_from_model: n_ctx         = 2048
0.00.670.223 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.223 I llama_init_from_model: n_batch       = 2048
0.00.670.224 I llama_init_from_model: n_ubatch      = 512
0.00.670.224 I llama_init_from_model: flash_attn    = 0
0.00.670.225 I llama_init_from_model: freq_base     = 10000.0
0.00.670.226 I llama_init_from_model: freq_scale    = 1
0.00.670.231 I ggml_metal_init: allocating
0.00.670.313 I ggml_metal_init: found device: Apple M4
0.00.670.326 I ggml_metal_init: picking default device: Apple M4
0.00.671.671 I ggml_metal_init: using embedded metal library
0.00.675.969 I ggml_metal_init: GPU name:   Apple M4
0.00.675.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.974 I ggml_metal_init: simdgroup reduction   = true
0.00.675.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.975 I ggml_metal_init: has residency sets    = true
0.00.675.975 I ggml_metal_init: has bfloat            = true
0.00.675.975 I ggml_metal_init: use bfloat            = true
0.00.675.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.600 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.872 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.877 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.715.897 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.948 I llama_init_from_model: graph nodes  = 967
0.00.719.948 I llama_init_from_model: graph splits = 2
0.00.719.954 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.087 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.088 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.740 I main: llama threadpool init, n_threads = 4
0.00.777.796 I 
0.00.777.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.823 I 
0.00.777.998 I sampler seed: 1234
0.00.778.003 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.013 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.015 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.015 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.604.462 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.604.462 I llama_perf_context_print:        load time =     767.02 ms
0.01.604.463 I llama_perf_context_print: prompt eval time =      41.94 ms /     7 tokens (    5.99 ms per token,   166.91 tokens per second)
0.01.604.464 I llama_perf_context_print:        eval time =     781.62 ms /    63 runs   (   12.41 ms per token,    80.60 tokens per second)
0.01.604.464 I llama_perf_context_print:       total time =     827.63 ms /    70 tokens
0.01.604.687 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.099s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.589 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.725 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.726 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.727 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.727 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.729 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.381 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.382 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.383 I llama_model_loader: - type  f32:  194 tensors
0.00.027.383 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.383 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.384 I print_info: file format = GGUF V3 (latest)
0.00.027.384 I print_info: file type   = Q5_1
0.00.027.387 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.157 I load: special tokens cache size = 25
0.00.041.079 I load: token to piece cache size = 0.2984 MB
0.00.041.082 I print_info: arch             = gptneox
0.00.041.082 I print_info: vocab_only       = 0
0.00.041.082 I print_info: n_ctx_train      = 2048
0.00.041.083 I print_info: n_embd           = 2048
0.00.041.083 I print_info: n_layer          = 24
0.00.041.086 I print_info: n_head           = 16
0.00.041.087 I print_info: n_head_kv        = 16
0.00.041.088 I print_info: n_rot            = 32
0.00.041.088 I print_info: n_swa            = 0
0.00.041.089 I print_info: n_embd_head_k    = 128
0.00.041.091 I print_info: n_embd_head_v    = 128
0.00.041.091 I print_info: n_gqa            = 1
0.00.041.092 I print_info: n_embd_k_gqa     = 2048
0.00.041.093 I print_info: n_embd_v_gqa     = 2048
0.00.041.093 I print_info: f_norm_eps       = 1.0e-05
0.00.041.094 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.099 I print_info: f_logit_scale    = 0.0e+00
0.00.041.099 I print_info: n_ff             = 8192
0.00.041.100 I print_info: n_expert         = 0
0.00.041.100 I print_info: n_expert_used    = 0
0.00.041.100 I print_info: causal attn      = 1
0.00.041.100 I print_info: pooling type     = 0
0.00.041.100 I print_info: rope type        = 2
0.00.041.101 I print_info: rope scaling     = linear
0.00.041.101 I print_info: freq_base_train  = 10000.0
0.00.041.101 I print_info: freq_scale_train = 1
0.00.041.101 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.102 I print_info: rope_finetuned   = unknown
0.00.041.102 I print_info: ssm_d_conv       = 0
0.00.041.102 I print_info: ssm_d_inner      = 0
0.00.041.102 I print_info: ssm_d_state      = 0
0.00.041.102 I print_info: ssm_dt_rank      = 0
0.00.041.102 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.103 I print_info: model type       = 1.4B
0.00.041.103 I print_info: model params     = 1.41 B
0.00.041.103 I print_info: general.name     = 1.4B
0.00.041.104 I print_info: vocab type       = BPE
0.00.041.104 I print_info: n_vocab          = 50304
0.00.041.104 I print_info: n_merges         = 50009
0.00.041.104 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.105 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.105 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.106 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.106 I print_info: LF token         = 187 'Ċ'
0.00.041.106 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.107 I print_info: max token length = 1024
0.00.596.908 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.912 I load_tensors: offloading output layer to GPU
0.00.596.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.931 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.596.933 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.597.780 I llama_init_from_model: n_seq_max     = 1
0.00.597.786 I llama_init_from_model: n_ctx         = 128
0.00.597.786 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.787 I llama_init_from_model: n_batch       = 128
0.00.597.787 I llama_init_from_model: n_ubatch      = 128
0.00.597.787 I llama_init_from_model: flash_attn    = 0
0.00.597.789 I llama_init_from_model: freq_base     = 10000.0
0.00.597.789 I llama_init_from_model: freq_scale    = 1
0.00.597.790 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.791 I ggml_metal_init: allocating
0.00.597.828 I ggml_metal_init: found device: Apple M4
0.00.597.839 I ggml_metal_init: picking default device: Apple M4
0.00.598.814 I ggml_metal_init: using embedded metal library
0.00.602.903 I ggml_metal_init: GPU name:   Apple M4
0.00.602.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.913 I ggml_metal_init: simdgroup reduction   = true
0.00.602.914 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.914 I ggml_metal_init: has residency sets    = true
0.00.602.914 I ggml_metal_init: has bfloat            = true
0.00.602.914 I ggml_metal_init: use bfloat            = true
0.00.602.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.215 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.809 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.812 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.829 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.375 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.376 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.376 I llama_init_from_model: graph nodes  = 967
0.00.621.376 I llama_init_from_model: graph splits = 2
0.00.621.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.589 I 
0.00.648.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.628 I perplexity: tokenizing the input ..
0.00.652.461 I perplexity: tokenization took 3.832 ms
0.00.652.466 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.119 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.800.461 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.800.485 I llama_perf_context_print:        load time =     639.00 ms
0.00.800.486 I llama_perf_context_print: prompt eval time =     146.43 ms /   128 tokens (    1.14 ms per token,   874.15 tokens per second)
0.00.800.487 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.487 I llama_perf_context_print:       total time =     151.90 ms /   129 tokens
0.00.800.809 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.069s
sys	0m0.097s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.477 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.037 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.040 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.041 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.545 I llama_model_loader: - type  f32:  194 tensors
0.00.023.545 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.546 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.546 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.546 I print_info: file format = GGUF V3 (latest)
0.00.023.547 I print_info: file type   = Q2_K - Medium
0.00.023.548 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.209 I load: special tokens cache size = 25
0.00.037.145 I load: token to piece cache size = 0.2984 MB
0.00.037.148 I print_info: arch             = gptneox
0.00.037.148 I print_info: vocab_only       = 0
0.00.037.148 I print_info: n_ctx_train      = 2048
0.00.037.148 I print_info: n_embd           = 2048
0.00.037.149 I print_info: n_layer          = 24
0.00.037.152 I print_info: n_head           = 16
0.00.037.153 I print_info: n_head_kv        = 16
0.00.037.153 I print_info: n_rot            = 32
0.00.037.153 I print_info: n_swa            = 0
0.00.037.153 I print_info: n_embd_head_k    = 128
0.00.037.155 I print_info: n_embd_head_v    = 128
0.00.037.156 I print_info: n_gqa            = 1
0.00.037.157 I print_info: n_embd_k_gqa     = 2048
0.00.037.159 I print_info: n_embd_v_gqa     = 2048
0.00.037.159 I print_info: f_norm_eps       = 1.0e-05
0.00.037.160 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.160 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.160 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.160 I print_info: f_logit_scale    = 0.0e+00
0.00.037.161 I print_info: n_ff             = 8192
0.00.037.162 I print_info: n_expert         = 0
0.00.037.162 I print_info: n_expert_used    = 0
0.00.037.162 I print_info: causal attn      = 1
0.00.037.163 I print_info: pooling type     = 0
0.00.037.163 I print_info: rope type        = 2
0.00.037.163 I print_info: rope scaling     = linear
0.00.037.163 I print_info: freq_base_train  = 10000.0
0.00.037.164 I print_info: freq_scale_train = 1
0.00.037.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.164 I print_info: rope_finetuned   = unknown
0.00.037.164 I print_info: ssm_d_conv       = 0
0.00.037.164 I print_info: ssm_d_inner      = 0
0.00.037.164 I print_info: ssm_d_state      = 0
0.00.037.164 I print_info: ssm_dt_rank      = 0
0.00.037.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.165 I print_info: model type       = 1.4B
0.00.037.165 I print_info: model params     = 1.41 B
0.00.037.165 I print_info: general.name     = 1.4B
0.00.037.166 I print_info: vocab type       = BPE
0.00.037.166 I print_info: n_vocab          = 50304
0.00.037.167 I print_info: n_merges         = 50009
0.00.037.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.171 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.171 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.171 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.174 I print_info: LF token         = 187 'Ċ'
0.00.037.174 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.174 I print_info: max token length = 1024
0.00.362.307 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.315 I load_tensors: offloading output layer to GPU
0.00.362.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.362.352 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.362.354 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.363.862 I llama_init_from_model: n_seq_max     = 1
0.00.363.866 I llama_init_from_model: n_ctx         = 2048
0.00.363.867 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.363.867 I llama_init_from_model: n_batch       = 2048
0.00.363.868 I llama_init_from_model: n_ubatch      = 512
0.00.363.868 I llama_init_from_model: flash_attn    = 0
0.00.363.870 I llama_init_from_model: freq_base     = 10000.0
0.00.363.874 I llama_init_from_model: freq_scale    = 1
0.00.363.876 I ggml_metal_init: allocating
0.00.363.958 I ggml_metal_init: found device: Apple M4
0.00.363.972 I ggml_metal_init: picking default device: Apple M4
0.00.366.062 I ggml_metal_init: using embedded metal library
0.00.372.174 I ggml_metal_init: GPU name:   Apple M4
0.00.372.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.186 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.187 I ggml_metal_init: simdgroup reduction   = true
0.00.372.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.187 I ggml_metal_init: has residency sets    = true
0.00.372.188 I ggml_metal_init: has bfloat            = true
0.00.372.188 I ggml_metal_init: use bfloat            = true
0.00.372.196 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.185 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.448.668 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.448.677 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.448.700 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.453.149 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.453.151 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.453.151 I llama_init_from_model: graph nodes  = 967
0.00.453.151 I llama_init_from_model: graph splits = 2
0.00.453.158 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.453.291 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.453.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.701 I main: llama threadpool init, n_threads = 4
0.00.509.742 I 
0.00.509.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.765 I 
0.00.509.934 I sampler seed: 1234
0.00.509.938 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.509.966 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.509.968 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.509.968 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.181.152 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.181.152 I llama_perf_context_print:        load time =     500.31 ms
0.01.181.153 I llama_perf_context_print: prompt eval time =      35.51 ms /     7 tokens (    5.07 ms per token,   197.13 tokens per second)
0.01.181.154 I llama_perf_context_print:        eval time =     632.89 ms /    63 runs   (   10.05 ms per token,    99.54 tokens per second)
0.01.181.154 I llama_perf_context_print:       total time =     672.36 ms /    70 tokens
0.01.181.381 I ggml_metal_free: deallocating

real	0m1.197s
user	0m0.109s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.979 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.931 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.943 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.948 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.949 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.625 I llama_model_loader: - type  f32:  194 tensors
0.00.024.625 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.625 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.625 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.626 I print_info: file format = GGUF V3 (latest)
0.00.024.627 I print_info: file type   = Q2_K - Medium
0.00.024.631 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.897 I load: special tokens cache size = 25
0.00.038.940 I load: token to piece cache size = 0.2984 MB
0.00.038.946 I print_info: arch             = gptneox
0.00.038.946 I print_info: vocab_only       = 0
0.00.038.946 I print_info: n_ctx_train      = 2048
0.00.038.948 I print_info: n_embd           = 2048
0.00.038.948 I print_info: n_layer          = 24
0.00.038.951 I print_info: n_head           = 16
0.00.038.952 I print_info: n_head_kv        = 16
0.00.038.954 I print_info: n_rot            = 32
0.00.038.954 I print_info: n_swa            = 0
0.00.038.954 I print_info: n_embd_head_k    = 128
0.00.038.954 I print_info: n_embd_head_v    = 128
0.00.038.955 I print_info: n_gqa            = 1
0.00.038.955 I print_info: n_embd_k_gqa     = 2048
0.00.038.956 I print_info: n_embd_v_gqa     = 2048
0.00.038.956 I print_info: f_norm_eps       = 1.0e-05
0.00.038.960 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.962 I print_info: f_logit_scale    = 0.0e+00
0.00.038.963 I print_info: n_ff             = 8192
0.00.038.963 I print_info: n_expert         = 0
0.00.038.963 I print_info: n_expert_used    = 0
0.00.038.963 I print_info: causal attn      = 1
0.00.038.963 I print_info: pooling type     = 0
0.00.038.963 I print_info: rope type        = 2
0.00.038.963 I print_info: rope scaling     = linear
0.00.038.964 I print_info: freq_base_train  = 10000.0
0.00.038.964 I print_info: freq_scale_train = 1
0.00.038.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.964 I print_info: rope_finetuned   = unknown
0.00.038.965 I print_info: ssm_d_conv       = 0
0.00.038.965 I print_info: ssm_d_inner      = 0
0.00.038.965 I print_info: ssm_d_state      = 0
0.00.038.965 I print_info: ssm_dt_rank      = 0
0.00.038.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.965 I print_info: model type       = 1.4B
0.00.038.966 I print_info: model params     = 1.41 B
0.00.038.966 I print_info: general.name     = 1.4B
0.00.038.966 I print_info: vocab type       = BPE
0.00.038.966 I print_info: n_vocab          = 50304
0.00.038.967 I print_info: n_merges         = 50009
0.00.038.967 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: LF token         = 187 'Ċ'
0.00.038.968 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.968 I print_info: max token length = 1024
0.00.343.092 I load_tensors: offloading 24 repeating layers to GPU
0.00.343.104 I load_tensors: offloading output layer to GPU
0.00.343.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.343.134 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.343.135 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.344.543 I llama_init_from_model: n_seq_max     = 1
0.00.344.551 I llama_init_from_model: n_ctx         = 128
0.00.344.552 I llama_init_from_model: n_ctx_per_seq = 128
0.00.344.552 I llama_init_from_model: n_batch       = 128
0.00.344.552 I llama_init_from_model: n_ubatch      = 128
0.00.344.553 I llama_init_from_model: flash_attn    = 0
0.00.344.554 I llama_init_from_model: freq_base     = 10000.0
0.00.344.554 I llama_init_from_model: freq_scale    = 1
0.00.344.554 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.344.557 I ggml_metal_init: allocating
0.00.344.628 I ggml_metal_init: found device: Apple M4
0.00.344.641 I ggml_metal_init: picking default device: Apple M4
0.00.346.280 I ggml_metal_init: using embedded metal library
0.00.351.596 I ggml_metal_init: GPU name:   Apple M4
0.00.351.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.610 I ggml_metal_init: simdgroup reduction   = true
0.00.351.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.610 I ggml_metal_init: has residency sets    = true
0.00.351.611 I ggml_metal_init: has bfloat            = true
0.00.351.611 I ggml_metal_init: use bfloat            = true
0.00.351.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.150 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.370.496 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.370.504 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.370.526 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.372.849 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.372.851 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.372.851 I llama_init_from_model: graph nodes  = 967
0.00.372.852 I llama_init_from_model: graph splits = 2
0.00.372.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.372.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.551 I 
0.00.401.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.604 I perplexity: tokenizing the input ..
0.00.405.664 I perplexity: tokenization took 4.059 ms
0.00.405.668 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.545.024 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.546.879 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.546.899 I llama_perf_context_print:        load time =     392.57 ms
0.00.546.900 I llama_perf_context_print: prompt eval time =     139.09 ms /   128 tokens (    1.09 ms per token,   920.27 tokens per second)
0.00.546.900 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.546.901 I llama_perf_context_print:       total time =     145.35 ms /   129 tokens
0.00.547.268 I ggml_metal_free: deallocating

real	0m0.561s
user	0m0.073s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.740 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.108 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.111 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.113 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.113 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.114 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.576 I llama_model_loader: - type  f32:  194 tensors
0.00.023.576 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.576 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.576 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.576 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.577 I print_info: file format = GGUF V3 (latest)
0.00.023.577 I print_info: file type   = Q3_K - Medium
0.00.023.578 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.271 I load: special tokens cache size = 25
0.00.037.366 I load: token to piece cache size = 0.2984 MB
0.00.037.369 I print_info: arch             = gptneox
0.00.037.369 I print_info: vocab_only       = 0
0.00.037.369 I print_info: n_ctx_train      = 2048
0.00.037.370 I print_info: n_embd           = 2048
0.00.037.370 I print_info: n_layer          = 24
0.00.037.372 I print_info: n_head           = 16
0.00.037.373 I print_info: n_head_kv        = 16
0.00.037.373 I print_info: n_rot            = 32
0.00.037.374 I print_info: n_swa            = 0
0.00.037.374 I print_info: n_embd_head_k    = 128
0.00.037.374 I print_info: n_embd_head_v    = 128
0.00.037.375 I print_info: n_gqa            = 1
0.00.037.378 I print_info: n_embd_k_gqa     = 2048
0.00.037.378 I print_info: n_embd_v_gqa     = 2048
0.00.037.379 I print_info: f_norm_eps       = 1.0e-05
0.00.037.379 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.379 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.380 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.380 I print_info: f_logit_scale    = 0.0e+00
0.00.037.381 I print_info: n_ff             = 8192
0.00.037.381 I print_info: n_expert         = 0
0.00.037.381 I print_info: n_expert_used    = 0
0.00.037.381 I print_info: causal attn      = 1
0.00.037.383 I print_info: pooling type     = 0
0.00.037.383 I print_info: rope type        = 2
0.00.037.383 I print_info: rope scaling     = linear
0.00.037.384 I print_info: freq_base_train  = 10000.0
0.00.037.384 I print_info: freq_scale_train = 1
0.00.037.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.384 I print_info: rope_finetuned   = unknown
0.00.037.384 I print_info: ssm_d_conv       = 0
0.00.037.385 I print_info: ssm_d_inner      = 0
0.00.037.385 I print_info: ssm_d_state      = 0
0.00.037.385 I print_info: ssm_dt_rank      = 0
0.00.037.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.385 I print_info: model type       = 1.4B
0.00.037.386 I print_info: model params     = 1.41 B
0.00.037.386 I print_info: general.name     = 1.4B
0.00.037.386 I print_info: vocab type       = BPE
0.00.037.386 I print_info: n_vocab          = 50304
0.00.037.386 I print_info: n_merges         = 50009
0.00.037.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.387 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.391 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.391 I print_info: LF token         = 187 'Ċ'
0.00.037.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.392 I print_info: max token length = 1024
0.00.432.822 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.836 I load_tensors: offloading output layer to GPU
0.00.432.837 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.869 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.870 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.046 I llama_init_from_model: n_seq_max     = 1
0.00.434.050 I llama_init_from_model: n_ctx         = 2048
0.00.434.050 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.434.050 I llama_init_from_model: n_batch       = 2048
0.00.434.051 I llama_init_from_model: n_ubatch      = 512
0.00.434.051 I llama_init_from_model: flash_attn    = 0
0.00.434.054 I llama_init_from_model: freq_base     = 10000.0
0.00.434.058 I llama_init_from_model: freq_scale    = 1
0.00.434.060 I ggml_metal_init: allocating
0.00.434.124 I ggml_metal_init: found device: Apple M4
0.00.434.145 I ggml_metal_init: picking default device: Apple M4
0.00.436.060 I ggml_metal_init: using embedded metal library
0.00.442.018 I ggml_metal_init: GPU name:   Apple M4
0.00.442.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.031 I ggml_metal_init: simdgroup reduction   = true
0.00.442.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.032 I ggml_metal_init: has residency sets    = true
0.00.442.032 I ggml_metal_init: has bfloat            = true
0.00.442.032 I ggml_metal_init: use bfloat            = true
0.00.442.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.462.896 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.517.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.517.746 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.517.781 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.522.345 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.522.347 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.522.347 I llama_init_from_model: graph nodes  = 967
0.00.522.348 I llama_init_from_model: graph splits = 2
0.00.522.353 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.522.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.522.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.127 I main: llama threadpool init, n_threads = 4
0.00.577.170 I 
0.00.577.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.577.194 I 
0.00.577.360 I sampler seed: 1234
0.00.577.364 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.577.411 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.577.414 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.577.414 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.315.716 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47715.05 tokens per second)
0.01.315.717 I llama_perf_context_print:        load time =     567.48 ms
0.01.315.718 I llama_perf_context_print: prompt eval time =      40.57 ms /     7 tokens (    5.80 ms per token,   172.56 tokens per second)
0.01.315.718 I llama_perf_context_print:        eval time =     695.21 ms /    63 runs   (   11.04 ms per token,    90.62 tokens per second)
0.01.315.719 I llama_perf_context_print:       total time =     739.50 ms /    70 tokens
0.01.315.965 I ggml_metal_free: deallocating

real	0m1.335s
user	0m0.111s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.641 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.348 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.349 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.350 I llama_model_loader: - type  f32:  194 tensors
0.00.024.350 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.350 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.351 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.351 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.351 I print_info: file format = GGUF V3 (latest)
0.00.024.352 I print_info: file type   = Q3_K - Medium
0.00.024.353 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.495 I load: special tokens cache size = 25
0.00.038.721 I load: token to piece cache size = 0.2984 MB
0.00.038.724 I print_info: arch             = gptneox
0.00.038.725 I print_info: vocab_only       = 0
0.00.038.725 I print_info: n_ctx_train      = 2048
0.00.038.725 I print_info: n_embd           = 2048
0.00.038.725 I print_info: n_layer          = 24
0.00.038.729 I print_info: n_head           = 16
0.00.038.730 I print_info: n_head_kv        = 16
0.00.038.730 I print_info: n_rot            = 32
0.00.038.730 I print_info: n_swa            = 0
0.00.038.730 I print_info: n_embd_head_k    = 128
0.00.038.733 I print_info: n_embd_head_v    = 128
0.00.038.733 I print_info: n_gqa            = 1
0.00.038.734 I print_info: n_embd_k_gqa     = 2048
0.00.038.735 I print_info: n_embd_v_gqa     = 2048
0.00.038.735 I print_info: f_norm_eps       = 1.0e-05
0.00.038.736 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.736 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.736 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.736 I print_info: f_logit_scale    = 0.0e+00
0.00.038.737 I print_info: n_ff             = 8192
0.00.038.737 I print_info: n_expert         = 0
0.00.038.737 I print_info: n_expert_used    = 0
0.00.038.737 I print_info: causal attn      = 1
0.00.038.737 I print_info: pooling type     = 0
0.00.038.738 I print_info: rope type        = 2
0.00.038.739 I print_info: rope scaling     = linear
0.00.038.739 I print_info: freq_base_train  = 10000.0
0.00.038.739 I print_info: freq_scale_train = 1
0.00.038.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.741 I print_info: rope_finetuned   = unknown
0.00.038.741 I print_info: ssm_d_conv       = 0
0.00.038.741 I print_info: ssm_d_inner      = 0
0.00.038.741 I print_info: ssm_d_state      = 0
0.00.038.741 I print_info: ssm_dt_rank      = 0
0.00.038.742 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.742 I print_info: model type       = 1.4B
0.00.038.742 I print_info: model params     = 1.41 B
0.00.038.742 I print_info: general.name     = 1.4B
0.00.038.743 I print_info: vocab type       = BPE
0.00.038.743 I print_info: n_vocab          = 50304
0.00.038.743 I print_info: n_merges         = 50009
0.00.038.743 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: LF token         = 187 'Ċ'
0.00.038.745 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: max token length = 1024
0.00.454.137 I load_tensors: offloading 24 repeating layers to GPU
0.00.454.146 I load_tensors: offloading output layer to GPU
0.00.454.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.454.163 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.454.164 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.455.057 I llama_init_from_model: n_seq_max     = 1
0.00.455.063 I llama_init_from_model: n_ctx         = 128
0.00.455.063 I llama_init_from_model: n_ctx_per_seq = 128
0.00.455.064 I llama_init_from_model: n_batch       = 128
0.00.455.064 I llama_init_from_model: n_ubatch      = 128
0.00.455.064 I llama_init_from_model: flash_attn    = 0
0.00.455.065 I llama_init_from_model: freq_base     = 10000.0
0.00.455.066 I llama_init_from_model: freq_scale    = 1
0.00.455.069 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.455.070 I ggml_metal_init: allocating
0.00.455.103 I ggml_metal_init: found device: Apple M4
0.00.455.114 I ggml_metal_init: picking default device: Apple M4
0.00.456.113 I ggml_metal_init: using embedded metal library
0.00.460.538 I ggml_metal_init: GPU name:   Apple M4
0.00.460.545 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.545 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.546 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.547 I ggml_metal_init: simdgroup reduction   = true
0.00.460.547 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.547 I ggml_metal_init: has residency sets    = true
0.00.460.548 I ggml_metal_init: has bfloat            = true
0.00.460.548 I ggml_metal_init: use bfloat            = true
0.00.460.549 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.379 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.477.070 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.477.072 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.477.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.478.668 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.478.669 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.478.669 I llama_init_from_model: graph nodes  = 967
0.00.478.669 I llama_init_from_model: graph splits = 2
0.00.478.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.153 I 
0.00.504.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.504.215 I perplexity: tokenizing the input ..
0.00.508.004 I perplexity: tokenization took 3.788 ms
0.00.508.008 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.372 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.654.830 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.654.853 I llama_perf_context_print:        load time =     495.51 ms
0.00.654.854 I llama_perf_context_print: prompt eval time =     145.13 ms /   128 tokens (    1.13 ms per token,   881.96 tokens per second)
0.00.654.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.654.855 I llama_perf_context_print:       total time =     150.70 ms /   129 tokens
0.00.655.242 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.069s
sys	0m0.077s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.020.229 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.779 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.035.423 I llama_model_loader: - type  f32:  194 tensors
0.00.035.423 I llama_model_loader: - type q4_K:   61 tensors
0.00.035.423 I llama_model_loader: - type q5_K:   24 tensors
0.00.035.424 I llama_model_loader: - type q6_K:   13 tensors
0.00.035.425 I print_info: file format = GGUF V3 (latest)
0.00.035.425 I print_info: file type   = Q4_K - Medium
0.00.035.428 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.672 I load: special tokens cache size = 25
0.00.049.552 I load: token to piece cache size = 0.2984 MB
0.00.049.557 I print_info: arch             = gptneox
0.00.049.557 I print_info: vocab_only       = 0
0.00.049.557 I print_info: n_ctx_train      = 2048
0.00.049.558 I print_info: n_embd           = 2048
0.00.049.558 I print_info: n_layer          = 24
0.00.049.562 I print_info: n_head           = 16
0.00.049.563 I print_info: n_head_kv        = 16
0.00.049.563 I print_info: n_rot            = 32
0.00.049.563 I print_info: n_swa            = 0
0.00.049.563 I print_info: n_embd_head_k    = 128
0.00.049.563 I print_info: n_embd_head_v    = 128
0.00.049.564 I print_info: n_gqa            = 1
0.00.049.565 I print_info: n_embd_k_gqa     = 2048
0.00.049.565 I print_info: n_embd_v_gqa     = 2048
0.00.049.566 I print_info: f_norm_eps       = 1.0e-05
0.00.049.566 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.569 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.569 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.569 I print_info: f_logit_scale    = 0.0e+00
0.00.049.569 I print_info: n_ff             = 8192
0.00.049.570 I print_info: n_expert         = 0
0.00.049.570 I print_info: n_expert_used    = 0
0.00.049.570 I print_info: causal attn      = 1
0.00.049.570 I print_info: pooling type     = 0
0.00.049.570 I print_info: rope type        = 2
0.00.049.570 I print_info: rope scaling     = linear
0.00.049.571 I print_info: freq_base_train  = 10000.0
0.00.049.571 I print_info: freq_scale_train = 1
0.00.049.571 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.571 I print_info: rope_finetuned   = unknown
0.00.049.571 I print_info: ssm_d_conv       = 0
0.00.049.572 I print_info: ssm_d_inner      = 0
0.00.049.572 I print_info: ssm_d_state      = 0
0.00.049.572 I print_info: ssm_dt_rank      = 0
0.00.049.572 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.575 I print_info: model type       = 1.4B
0.00.049.577 I print_info: model params     = 1.41 B
0.00.049.577 I print_info: general.name     = 1.4B
0.00.049.577 I print_info: vocab type       = BPE
0.00.049.577 I print_info: n_vocab          = 50304
0.00.049.577 I print_info: n_merges         = 50009
0.00.049.578 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.578 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.578 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.578 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.578 I print_info: LF token         = 187 'Ċ'
0.00.049.579 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.579 I print_info: max token length = 1024
0.00.549.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.549.357 I load_tensors: offloading output layer to GPU
0.00.549.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.375 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.549.376 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.550.280 I llama_init_from_model: n_seq_max     = 1
0.00.550.284 I llama_init_from_model: n_ctx         = 2048
0.00.550.285 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.550.285 I llama_init_from_model: n_batch       = 2048
0.00.550.285 I llama_init_from_model: n_ubatch      = 512
0.00.550.286 I llama_init_from_model: flash_attn    = 0
0.00.550.287 I llama_init_from_model: freq_base     = 10000.0
0.00.550.287 I llama_init_from_model: freq_scale    = 1
0.00.550.289 I ggml_metal_init: allocating
0.00.550.319 I ggml_metal_init: found device: Apple M4
0.00.550.328 I ggml_metal_init: picking default device: Apple M4
0.00.551.403 I ggml_metal_init: using embedded metal library
0.00.555.699 I ggml_metal_init: GPU name:   Apple M4
0.00.555.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.555.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.555.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.555.706 I ggml_metal_init: simdgroup reduction   = true
0.00.555.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.555.706 I ggml_metal_init: has residency sets    = true
0.00.555.707 I ggml_metal_init: has bfloat            = true
0.00.555.707 I ggml_metal_init: use bfloat            = true
0.00.555.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.555.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.569.776 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.601.077 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.601.082 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.601.105 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.605.666 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.605.669 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.605.669 I llama_init_from_model: graph nodes  = 967
0.00.605.669 I llama_init_from_model: graph splits = 2
0.00.605.675 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.605.803 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.605.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.042 I main: llama threadpool init, n_threads = 4
0.00.663.084 I 
0.00.663.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.107 I 
0.00.663.288 I sampler seed: 1234
0.00.663.292 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.329 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.331 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.331 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.416.825 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49339.82 tokens per second)
0.01.416.825 I llama_perf_context_print:        load time =     641.91 ms
0.01.416.826 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.69 tokens per second)
0.01.416.827 I llama_perf_context_print:        eval time =     703.29 ms /    63 runs   (   11.16 ms per token,    89.58 tokens per second)
0.01.416.827 I llama_perf_context_print:       total time =     754.69 ms /    70 tokens
0.01.417.021 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.104s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.692 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.355 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.366 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.368 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.368 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.368 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.157 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.996 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.997 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.998 I llama_model_loader: - type  f32:  194 tensors
0.00.024.998 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.998 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.999 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.999 I print_info: file format = GGUF V3 (latest)
0.00.025.000 I print_info: file type   = Q4_K - Medium
0.00.025.001 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.144 I load: special tokens cache size = 25
0.00.039.195 I load: token to piece cache size = 0.2984 MB
0.00.039.198 I print_info: arch             = gptneox
0.00.039.198 I print_info: vocab_only       = 0
0.00.039.199 I print_info: n_ctx_train      = 2048
0.00.039.199 I print_info: n_embd           = 2048
0.00.039.199 I print_info: n_layer          = 24
0.00.039.202 I print_info: n_head           = 16
0.00.039.203 I print_info: n_head_kv        = 16
0.00.039.203 I print_info: n_rot            = 32
0.00.039.203 I print_info: n_swa            = 0
0.00.039.204 I print_info: n_embd_head_k    = 128
0.00.039.204 I print_info: n_embd_head_v    = 128
0.00.039.205 I print_info: n_gqa            = 1
0.00.039.205 I print_info: n_embd_k_gqa     = 2048
0.00.039.206 I print_info: n_embd_v_gqa     = 2048
0.00.039.207 I print_info: f_norm_eps       = 1.0e-05
0.00.039.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.207 I print_info: f_logit_scale    = 0.0e+00
0.00.039.208 I print_info: n_ff             = 8192
0.00.039.208 I print_info: n_expert         = 0
0.00.039.208 I print_info: n_expert_used    = 0
0.00.039.209 I print_info: causal attn      = 1
0.00.039.209 I print_info: pooling type     = 0
0.00.039.211 I print_info: rope type        = 2
0.00.039.213 I print_info: rope scaling     = linear
0.00.039.213 I print_info: freq_base_train  = 10000.0
0.00.039.214 I print_info: freq_scale_train = 1
0.00.039.214 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.214 I print_info: rope_finetuned   = unknown
0.00.039.214 I print_info: ssm_d_conv       = 0
0.00.039.215 I print_info: ssm_d_inner      = 0
0.00.039.215 I print_info: ssm_d_state      = 0
0.00.039.215 I print_info: ssm_dt_rank      = 0
0.00.039.219 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.219 I print_info: model type       = 1.4B
0.00.039.219 I print_info: model params     = 1.41 B
0.00.039.220 I print_info: general.name     = 1.4B
0.00.039.220 I print_info: vocab type       = BPE
0.00.039.220 I print_info: n_vocab          = 50304
0.00.039.220 I print_info: n_merges         = 50009
0.00.039.221 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.222 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: LF token         = 187 'Ċ'
0.00.039.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: max token length = 1024
0.00.534.196 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.210 I load_tensors: offloading output layer to GPU
0.00.534.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.246 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.248 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.535.531 I llama_init_from_model: n_seq_max     = 1
0.00.535.537 I llama_init_from_model: n_ctx         = 128
0.00.535.537 I llama_init_from_model: n_ctx_per_seq = 128
0.00.535.538 I llama_init_from_model: n_batch       = 128
0.00.535.538 I llama_init_from_model: n_ubatch      = 128
0.00.535.538 I llama_init_from_model: flash_attn    = 0
0.00.535.540 I llama_init_from_model: freq_base     = 10000.0
0.00.535.541 I llama_init_from_model: freq_scale    = 1
0.00.535.541 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.535.548 I ggml_metal_init: allocating
0.00.535.647 I ggml_metal_init: found device: Apple M4
0.00.535.661 I ggml_metal_init: picking default device: Apple M4
0.00.537.394 I ggml_metal_init: using embedded metal library
0.00.543.983 I ggml_metal_init: GPU name:   Apple M4
0.00.543.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.988 I ggml_metal_init: simdgroup reduction   = true
0.00.543.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.989 I ggml_metal_init: has residency sets    = true
0.00.543.989 I ggml_metal_init: has bfloat            = true
0.00.543.990 I ggml_metal_init: use bfloat            = true
0.00.543.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.544.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.225 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.564.663 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.564.666 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.564.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.567.947 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.567.949 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.567.949 I llama_init_from_model: graph nodes  = 967
0.00.567.949 I llama_init_from_model: graph splits = 2
0.00.567.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.567.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.787 I 
0.00.599.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.894 I perplexity: tokenizing the input ..
0.00.606.828 I perplexity: tokenization took 6.93 ms
0.00.606.835 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.593 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.748.930 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.748.952 I llama_perf_context_print:        load time =     590.09 ms
0.00.748.953 I llama_perf_context_print: prompt eval time =     139.87 ms /   128 tokens (    1.09 ms per token,   915.14 tokens per second)
0.00.748.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.954 I llama_perf_context_print:       total time =     149.17 ms /   129 tokens
0.00.749.343 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.078s
sys	0m0.133s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.275 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.275 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.591 I llama_model_loader: - type  f32:  194 tensors
0.00.023.591 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.591 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.592 I print_info: file format = GGUF V3 (latest)
0.00.023.593 I print_info: file type   = Q5_K - Medium
0.00.023.593 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.301 I load: special tokens cache size = 25
0.00.037.384 I load: token to piece cache size = 0.2984 MB
0.00.037.387 I print_info: arch             = gptneox
0.00.037.387 I print_info: vocab_only       = 0
0.00.037.387 I print_info: n_ctx_train      = 2048
0.00.037.388 I print_info: n_embd           = 2048
0.00.037.388 I print_info: n_layer          = 24
0.00.037.390 I print_info: n_head           = 16
0.00.037.391 I print_info: n_head_kv        = 16
0.00.037.391 I print_info: n_rot            = 32
0.00.037.392 I print_info: n_swa            = 0
0.00.037.394 I print_info: n_embd_head_k    = 128
0.00.037.394 I print_info: n_embd_head_v    = 128
0.00.037.394 I print_info: n_gqa            = 1
0.00.037.395 I print_info: n_embd_k_gqa     = 2048
0.00.037.401 I print_info: n_embd_v_gqa     = 2048
0.00.037.401 I print_info: f_norm_eps       = 1.0e-05
0.00.037.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.402 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.402 I print_info: f_logit_scale    = 0.0e+00
0.00.037.403 I print_info: n_ff             = 8192
0.00.037.403 I print_info: n_expert         = 0
0.00.037.403 I print_info: n_expert_used    = 0
0.00.037.404 I print_info: causal attn      = 1
0.00.037.404 I print_info: pooling type     = 0
0.00.037.404 I print_info: rope type        = 2
0.00.037.404 I print_info: rope scaling     = linear
0.00.037.405 I print_info: freq_base_train  = 10000.0
0.00.037.405 I print_info: freq_scale_train = 1
0.00.037.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.405 I print_info: rope_finetuned   = unknown
0.00.037.406 I print_info: ssm_d_conv       = 0
0.00.037.406 I print_info: ssm_d_inner      = 0
0.00.037.408 I print_info: ssm_d_state      = 0
0.00.037.408 I print_info: ssm_dt_rank      = 0
0.00.037.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.409 I print_info: model type       = 1.4B
0.00.037.409 I print_info: model params     = 1.41 B
0.00.037.409 I print_info: general.name     = 1.4B
0.00.037.410 I print_info: vocab type       = BPE
0.00.037.410 I print_info: n_vocab          = 50304
0.00.037.410 I print_info: n_merges         = 50009
0.00.037.411 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.411 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.411 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.412 I print_info: LF token         = 187 'Ċ'
0.00.037.412 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.412 I print_info: max token length = 1024
0.00.585.290 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.293 I load_tensors: offloading output layer to GPU
0.00.585.294 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.316 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.585.317 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.586.541 I llama_init_from_model: n_seq_max     = 1
0.00.586.543 I llama_init_from_model: n_ctx         = 2048
0.00.586.544 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.544 I llama_init_from_model: n_batch       = 2048
0.00.586.545 I llama_init_from_model: n_ubatch      = 512
0.00.586.545 I llama_init_from_model: flash_attn    = 0
0.00.586.546 I llama_init_from_model: freq_base     = 10000.0
0.00.586.547 I llama_init_from_model: freq_scale    = 1
0.00.586.551 I ggml_metal_init: allocating
0.00.586.587 I ggml_metal_init: found device: Apple M4
0.00.586.597 I ggml_metal_init: picking default device: Apple M4
0.00.588.037 I ggml_metal_init: using embedded metal library
0.00.594.176 I ggml_metal_init: GPU name:   Apple M4
0.00.594.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.182 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.182 I ggml_metal_init: simdgroup reduction   = true
0.00.594.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.183 I ggml_metal_init: has residency sets    = true
0.00.594.183 I ggml_metal_init: has bfloat            = true
0.00.594.183 I ggml_metal_init: use bfloat            = true
0.00.594.184 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.185 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.610.804 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.034 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.658.042 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.658.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.408 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.663.411 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.663.411 I llama_init_from_model: graph nodes  = 967
0.00.663.412 I llama_init_from_model: graph splits = 2
0.00.663.418 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.663.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.663.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.302 I main: llama threadpool init, n_threads = 4
0.00.728.344 I 
0.00.728.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.367 I 
0.00.728.542 I sampler seed: 1234
0.00.728.546 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.587 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.590 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.590 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.576.232 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.576.235 I llama_perf_context_print:        load time =     718.66 ms
0.01.576.236 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.66 tokens per second)
0.01.576.237 I llama_perf_context_print:        eval time =     793.18 ms /    63 runs   (   12.59 ms per token,    79.43 tokens per second)
0.01.576.237 I llama_perf_context_print:       total time =     848.84 ms /    70 tokens
0.01.576.495 I ggml_metal_free: deallocating

real	0m1.594s
user	0m0.107s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.430 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.430 I llama_model_loader: - type  f32:  194 tensors
0.00.024.431 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.431 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.431 I print_info: file format = GGUF V3 (latest)
0.00.024.432 I print_info: file type   = Q5_K - Medium
0.00.024.433 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.172 I load: special tokens cache size = 25
0.00.038.099 I load: token to piece cache size = 0.2984 MB
0.00.038.102 I print_info: arch             = gptneox
0.00.038.102 I print_info: vocab_only       = 0
0.00.038.102 I print_info: n_ctx_train      = 2048
0.00.038.102 I print_info: n_embd           = 2048
0.00.038.102 I print_info: n_layer          = 24
0.00.038.105 I print_info: n_head           = 16
0.00.038.106 I print_info: n_head_kv        = 16
0.00.038.106 I print_info: n_rot            = 32
0.00.038.106 I print_info: n_swa            = 0
0.00.038.106 I print_info: n_embd_head_k    = 128
0.00.038.106 I print_info: n_embd_head_v    = 128
0.00.038.108 I print_info: n_gqa            = 1
0.00.038.109 I print_info: n_embd_k_gqa     = 2048
0.00.038.111 I print_info: n_embd_v_gqa     = 2048
0.00.038.112 I print_info: f_norm_eps       = 1.0e-05
0.00.038.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.113 I print_info: f_logit_scale    = 0.0e+00
0.00.038.114 I print_info: n_ff             = 8192
0.00.038.114 I print_info: n_expert         = 0
0.00.038.114 I print_info: n_expert_used    = 0
0.00.038.115 I print_info: causal attn      = 1
0.00.038.115 I print_info: pooling type     = 0
0.00.038.115 I print_info: rope type        = 2
0.00.038.115 I print_info: rope scaling     = linear
0.00.038.116 I print_info: freq_base_train  = 10000.0
0.00.038.116 I print_info: freq_scale_train = 1
0.00.038.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.118 I print_info: rope_finetuned   = unknown
0.00.038.118 I print_info: ssm_d_conv       = 0
0.00.038.118 I print_info: ssm_d_inner      = 0
0.00.038.118 I print_info: ssm_d_state      = 0
0.00.038.118 I print_info: ssm_dt_rank      = 0
0.00.038.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.119 I print_info: model type       = 1.4B
0.00.038.119 I print_info: model params     = 1.41 B
0.00.038.119 I print_info: general.name     = 1.4B
0.00.038.119 I print_info: vocab type       = BPE
0.00.038.120 I print_info: n_vocab          = 50304
0.00.038.120 I print_info: n_merges         = 50009
0.00.038.120 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.122 I print_info: LF token         = 187 'Ċ'
0.00.038.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.124 I print_info: max token length = 1024
0.00.584.876 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.892 I load_tensors: offloading output layer to GPU
0.00.584.892 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.924 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.584.925 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.586.374 I llama_init_from_model: n_seq_max     = 1
0.00.586.379 I llama_init_from_model: n_ctx         = 128
0.00.586.379 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.380 I llama_init_from_model: n_batch       = 128
0.00.586.381 I llama_init_from_model: n_ubatch      = 128
0.00.586.381 I llama_init_from_model: flash_attn    = 0
0.00.586.383 I llama_init_from_model: freq_base     = 10000.0
0.00.586.383 I llama_init_from_model: freq_scale    = 1
0.00.586.384 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.386 I ggml_metal_init: allocating
0.00.586.465 I ggml_metal_init: found device: Apple M4
0.00.586.479 I ggml_metal_init: picking default device: Apple M4
0.00.588.104 I ggml_metal_init: using embedded metal library
0.00.594.560 I ggml_metal_init: GPU name:   Apple M4
0.00.594.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.566 I ggml_metal_init: simdgroup reduction   = true
0.00.594.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.567 I ggml_metal_init: has residency sets    = true
0.00.594.567 I ggml_metal_init: has bfloat            = true
0.00.594.567 I ggml_metal_init: use bfloat            = true
0.00.594.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.477 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.049 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.053 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.079 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.474 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.475 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.476 I llama_init_from_model: graph nodes  = 967
0.00.619.476 I llama_init_from_model: graph splits = 2
0.00.619.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.479 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.909 I 
0.00.655.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.014 I perplexity: tokenizing the input ..
0.00.662.558 I perplexity: tokenization took 6.543 ms
0.00.662.563 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.376 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.698 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.722 I llama_perf_context_print:        load time =     646.93 ms
0.00.809.723 I llama_perf_context_print: prompt eval time =     145.59 ms /   128 tokens (    1.14 ms per token,   879.21 tokens per second)
0.00.809.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.724 I llama_perf_context_print:       total time =     153.82 ms /   129 tokens
0.00.810.103 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.077s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.400 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.402 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.402 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.403 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.403 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.404 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.404 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.405 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.409 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.769 I llama_model_loader: - type  f32:  194 tensors
0.00.023.769 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.770 I print_info: file format = GGUF V3 (latest)
0.00.023.770 I print_info: file type   = Q6_K
0.00.023.771 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.560 I load: special tokens cache size = 25
0.00.037.597 I load: token to piece cache size = 0.2984 MB
0.00.037.600 I print_info: arch             = gptneox
0.00.037.600 I print_info: vocab_only       = 0
0.00.037.601 I print_info: n_ctx_train      = 2048
0.00.037.601 I print_info: n_embd           = 2048
0.00.037.601 I print_info: n_layer          = 24
0.00.037.604 I print_info: n_head           = 16
0.00.037.605 I print_info: n_head_kv        = 16
0.00.037.605 I print_info: n_rot            = 32
0.00.037.605 I print_info: n_swa            = 0
0.00.037.605 I print_info: n_embd_head_k    = 128
0.00.037.605 I print_info: n_embd_head_v    = 128
0.00.037.606 I print_info: n_gqa            = 1
0.00.037.607 I print_info: n_embd_k_gqa     = 2048
0.00.037.607 I print_info: n_embd_v_gqa     = 2048
0.00.037.608 I print_info: f_norm_eps       = 1.0e-05
0.00.037.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.611 I print_info: f_logit_scale    = 0.0e+00
0.00.037.612 I print_info: n_ff             = 8192
0.00.037.613 I print_info: n_expert         = 0
0.00.037.613 I print_info: n_expert_used    = 0
0.00.037.613 I print_info: causal attn      = 1
0.00.037.613 I print_info: pooling type     = 0
0.00.037.613 I print_info: rope type        = 2
0.00.037.614 I print_info: rope scaling     = linear
0.00.037.614 I print_info: freq_base_train  = 10000.0
0.00.037.614 I print_info: freq_scale_train = 1
0.00.037.615 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.615 I print_info: rope_finetuned   = unknown
0.00.037.615 I print_info: ssm_d_conv       = 0
0.00.037.615 I print_info: ssm_d_inner      = 0
0.00.037.615 I print_info: ssm_d_state      = 0
0.00.037.615 I print_info: ssm_dt_rank      = 0
0.00.037.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.616 I print_info: model type       = 1.4B
0.00.037.616 I print_info: model params     = 1.41 B
0.00.037.617 I print_info: general.name     = 1.4B
0.00.037.617 I print_info: vocab type       = BPE
0.00.037.617 I print_info: n_vocab          = 50304
0.00.037.618 I print_info: n_merges         = 50009
0.00.037.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.619 I print_info: LF token         = 187 'Ċ'
0.00.037.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.620 I print_info: max token length = 1024
0.00.636.775 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.779 I load_tensors: offloading output layer to GPU
0.00.636.779 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.803 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.636.806 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.638.062 I llama_init_from_model: n_seq_max     = 1
0.00.638.063 I llama_init_from_model: n_ctx         = 2048
0.00.638.064 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.064 I llama_init_from_model: n_batch       = 2048
0.00.638.065 I llama_init_from_model: n_ubatch      = 512
0.00.638.065 I llama_init_from_model: flash_attn    = 0
0.00.638.066 I llama_init_from_model: freq_base     = 10000.0
0.00.638.066 I llama_init_from_model: freq_scale    = 1
0.00.638.067 I ggml_metal_init: allocating
0.00.638.101 I ggml_metal_init: found device: Apple M4
0.00.638.111 I ggml_metal_init: picking default device: Apple M4
0.00.639.536 I ggml_metal_init: using embedded metal library
0.00.645.364 I ggml_metal_init: GPU name:   Apple M4
0.00.645.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.370 I ggml_metal_init: simdgroup reduction   = true
0.00.645.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.371 I ggml_metal_init: has residency sets    = true
0.00.645.371 I ggml_metal_init: has bfloat            = true
0.00.645.371 I ggml_metal_init: use bfloat            = true
0.00.645.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.328 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.148 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.153 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.176 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.781 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.783 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.783 I llama_init_from_model: graph nodes  = 967
0.00.714.783 I llama_init_from_model: graph splits = 2
0.00.714.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.135 I main: llama threadpool init, n_threads = 4
0.00.785.177 I 
0.00.785.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.199 I 
0.00.785.379 I sampler seed: 1234
0.00.785.383 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.394 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.395 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.395 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.657.532 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.657.532 I llama_perf_context_print:        load time =     775.49 ms
0.01.657.533 I llama_perf_context_print: prompt eval time =      54.10 ms /     7 tokens (    7.73 ms per token,   129.39 tokens per second)
0.01.657.535 I llama_perf_context_print:        eval time =     815.08 ms /    63 runs   (   12.94 ms per token,    77.29 tokens per second)
0.01.657.535 I llama_perf_context_print:       total time =     873.30 ms /    70 tokens
0.01.657.781 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.106s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4602 (1bd3047a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.670 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.671 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.671 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.672 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.672 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.673 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.674 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.678 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.256 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.257 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.258 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.258 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.258 I llama_model_loader: - type  f32:  194 tensors
0.00.024.259 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.259 I print_info: file format = GGUF V3 (latest)
0.00.024.260 I print_info: file type   = Q6_K
0.00.024.260 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.316 I load: special tokens cache size = 25
0.00.038.388 I load: token to piece cache size = 0.2984 MB
0.00.038.390 I print_info: arch             = gptneox
0.00.038.391 I print_info: vocab_only       = 0
0.00.038.391 I print_info: n_ctx_train      = 2048
0.00.038.391 I print_info: n_embd           = 2048
0.00.038.391 I print_info: n_layer          = 24
0.00.038.394 I print_info: n_head           = 16
0.00.038.395 I print_info: n_head_kv        = 16
0.00.038.395 I print_info: n_rot            = 32
0.00.038.396 I print_info: n_swa            = 0
0.00.038.396 I print_info: n_embd_head_k    = 128
0.00.038.396 I print_info: n_embd_head_v    = 128
0.00.038.397 I print_info: n_gqa            = 1
0.00.038.397 I print_info: n_embd_k_gqa     = 2048
0.00.038.400 I print_info: n_embd_v_gqa     = 2048
0.00.038.400 I print_info: f_norm_eps       = 1.0e-05
0.00.038.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.401 I print_info: f_logit_scale    = 0.0e+00
0.00.038.402 I print_info: n_ff             = 8192
0.00.038.403 I print_info: n_expert         = 0
0.00.038.403 I print_info: n_expert_used    = 0
0.00.038.403 I print_info: causal attn      = 1
0.00.038.403 I print_info: pooling type     = 0
0.00.038.403 I print_info: rope type        = 2
0.00.038.404 I print_info: rope scaling     = linear
0.00.038.404 I print_info: freq_base_train  = 10000.0
0.00.038.404 I print_info: freq_scale_train = 1
0.00.038.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.405 I print_info: rope_finetuned   = unknown
0.00.038.405 I print_info: ssm_d_conv       = 0
0.00.038.405 I print_info: ssm_d_inner      = 0
0.00.038.405 I print_info: ssm_d_state      = 0
0.00.038.405 I print_info: ssm_dt_rank      = 0
0.00.038.406 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.407 I print_info: model type       = 1.4B
0.00.038.407 I print_info: model params     = 1.41 B
0.00.038.407 I print_info: general.name     = 1.4B
0.00.038.408 I print_info: vocab type       = BPE
0.00.038.408 I print_info: n_vocab          = 50304
0.00.038.408 I print_info: n_merges         = 50009
0.00.038.409 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.409 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.409 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.409 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.409 I print_info: LF token         = 187 'Ċ'
0.00.038.410 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.410 I print_info: max token length = 1024
0.00.586.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.329 I load_tensors: offloading output layer to GPU
0.00.586.330 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.358 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.586.359 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.587.753 I llama_init_from_model: n_seq_max     = 1
0.00.587.762 I llama_init_from_model: n_ctx         = 128
0.00.587.763 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.763 I llama_init_from_model: n_batch       = 128
0.00.587.763 I llama_init_from_model: n_ubatch      = 128
0.00.587.764 I llama_init_from_model: flash_attn    = 0
0.00.587.764 I llama_init_from_model: freq_base     = 10000.0
0.00.587.765 I llama_init_from_model: freq_scale    = 1
0.00.587.765 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.768 I ggml_metal_init: allocating
0.00.587.814 I ggml_metal_init: found device: Apple M4
0.00.587.827 I ggml_metal_init: picking default device: Apple M4
0.00.589.425 I ggml_metal_init: using embedded metal library
0.00.595.998 I ggml_metal_init: GPU name:   Apple M4
0.00.596.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.003 I ggml_metal_init: simdgroup reduction   = true
0.00.596.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.004 I ggml_metal_init: has residency sets    = true
0.00.596.004 I ggml_metal_init: has bfloat            = true
0.00.596.004 I ggml_metal_init: use bfloat            = true
0.00.596.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.133 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.696 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.700 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.741 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.011 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.013 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.013 I llama_init_from_model: graph nodes  = 967
0.00.620.014 I llama_init_from_model: graph splits = 2
0.00.620.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.381 I 
0.00.654.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.491 I perplexity: tokenizing the input ..
0.00.660.886 I perplexity: tokenization took 6.394 ms
0.00.660.891 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.168 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.802.512 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.802.535 I llama_perf_context_print:        load time =     645.32 ms
0.00.802.537 I llama_perf_context_print: prompt eval time =     139.87 ms /   128 tokens (    1.09 ms per token,   915.17 tokens per second)
0.00.802.538 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.538 I llama_perf_context_print:       total time =     148.16 ms /   129 tokens
0.00.802.908 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.077s
sys	0m0.137s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4602 (1bd3047a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134b05b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134b06270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134b06820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134b06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134b07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134b07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134b07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134b08490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134b08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134b08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134b09440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134b09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134b0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134b0ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134b0b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134b0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134b0c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134b0c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134b0d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134b0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134b0df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134b0e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134b0edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134b0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134b0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134b10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134b10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134b112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134b11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134b11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134b11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134b12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134b12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134b13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134b132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134b13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134b13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134b140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134b14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134b149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134b14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134b15320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134b157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134b15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134b15f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134b16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134b16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134b17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134b17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134b18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134b18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134b18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134b192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134b198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134b1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134b1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134b1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134b1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134b1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134b1bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134b1bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134b1c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134b1c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134b1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134b1cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134b1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134b1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134b1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134b1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134b1e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134b1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134b1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134b1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134b1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134b1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134b204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134b20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134b20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134b214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134b21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134b21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134b224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134b22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134b22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134b234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134b23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134b23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134b244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134b249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134b24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134b25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134b259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134b25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134b26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134b269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134b26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134b27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134b17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134b278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134b28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134b285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134b28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134b29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134b295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134b29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134b2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134b2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134b2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134b2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134b2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134b2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134b2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134b2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134b2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134b2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134b2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134b2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134b2dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134b2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134b2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134b2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134b2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134b2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134b2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134b2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134b301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134b30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134b30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134b30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134b31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134b318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134b31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134b32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134b326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134b32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134b33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134b334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134b33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134b33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134b34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134b34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134b34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134b35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134b35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134b359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134b35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134b362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134b36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134b36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134b370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134b37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134b37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134b37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134b38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134b387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134b38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134b39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134b395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134b39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134b39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134b3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134b3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134b3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134b3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134b3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134b3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134b3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134b3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134b3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134b3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134b3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134b3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134b3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134b3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134b3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134b3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134b3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134b3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134b3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134b3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134b40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134b404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134b40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134b40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134b412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134b41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134b41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134b42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134b42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134b429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134b42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134b43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134b437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134b43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134b44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134b44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134b44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134b44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134b455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134b45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134b461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134b469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134b46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134b47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134b47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134b47d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134b48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134b489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134b48e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134b49310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134b49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134b4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134b4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134b4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134b4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134b4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134b4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134b4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134b4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134b4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134b4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134b4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134b4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134b4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134b4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134b4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134b4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134b4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134b4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134b4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134b50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134b50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134b50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134b514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134b51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134b51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134b524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134b52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134b52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134b534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134b53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134b53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134b544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134b54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134b54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134b554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134b55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134b55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134b564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134b569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134b56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134b57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134b579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134b57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134b58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134b589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134b58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134b59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134b599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134b59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134b5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134b5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134b5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134b5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134b5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134b5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134b5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134b5c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134b5cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134b5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134b5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134b5db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134b5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134b5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134b5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134b5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134b5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134b5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134b5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134b60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134b60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134b609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134b60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134b61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134b61d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134b62450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134b62b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134b62e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134b63620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134b638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134b63ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.711.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134b63ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134b45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134b45260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134b45e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134b18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134b18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134b1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134b479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134b10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134b16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134b17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134b17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134b161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134b18340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134b0f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134b1b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134b27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134b630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134b124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134b127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134b48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134b46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134b10920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134b10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134b10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134b64350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134b64610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134b648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134b64b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134b64e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134b65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134b653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134b65690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134b65950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134b65c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134b65ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134b66190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134b66450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134b66710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134b669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134b66c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134b66f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134b67210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134b674d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134b67790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134b67a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134b67d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134b67fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134b68290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134b68550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134b68810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134b68ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134b68d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134b69050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134b69310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134b695d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134b69890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134b69b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134b69e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134b6a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134b6a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134b6a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134b6a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134b6abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134b6ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134b6b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134b6b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134b6b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134b6b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134b6bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134b6bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134b6c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134b6c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134b6c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134b6ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134b6ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134b6cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134b6d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134b6d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134b6d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134b6da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134b6dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134b6e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134b6e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134b6e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134b6e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134b6eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134b6edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134b6f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134b6f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134b6f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134b6f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134b6fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134b6fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134b70110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134b703d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134b70690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134b70950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134b70c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134b70ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134b71190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134b71450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134b71710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134b719d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134b71c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134b71f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134b72210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134b724d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134b72790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134b72a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134b72d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134b72fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134b73290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134b73550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134b73810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134b73ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134b73d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134b74050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134b74310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134b745d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134b74890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134b74b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134b74e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134b750d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134b75390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134b75650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134b75910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134b75bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134b75e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134b76150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134b76410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134b766d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134b76990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134b76c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134b76f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134b771d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134b77490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134b77750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134b77a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134b77cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134b77f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134b78250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134b78510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134b787d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134b78a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134b78d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134b79010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134b792d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134b79590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134b79850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134b79b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134b79dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134b7a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134b7a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134b7a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134b7a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134b7ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134b7ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134b7b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134b7b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134b7b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134b7b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134b7bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134b7bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134b7c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134b7c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134b7c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134b7c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134b7cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134b7cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134b7d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134b7d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134b7d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134b7da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134b7dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134b7dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134b7e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134b7e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134b7e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134b7ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134b7ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134b7f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134b7f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134b7f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134b7f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134b7fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134b7fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134b800d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134b80390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134b80650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134b80910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134b80bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134b80e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134b81150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134b81410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134b816d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134b81990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134b81c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134b81f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134b821d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134b82490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134b82750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134b82a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134b82cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134b83210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134b83750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134b83a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134b83eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134b84350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134b847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134b84fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134b85260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134b85520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134b85990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134b85e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134b86270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134b866e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134b86b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134b86fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134b87430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134b878a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134b87d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134b88180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134b885f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134b88a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134b88ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134b89340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134b897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134b89c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134b8a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134b8a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134b8a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134b8ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134b8b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134b8b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134b8bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134b8bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134b8c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134b8c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134b8ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134b8d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134b8d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134b8da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134b8deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134b8e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134b8e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134b8ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134b8f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134b8f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134b8f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134b8fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134b90230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134b906a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134b90b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134b90f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134b913f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134b91860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134b91cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134b92140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134b925b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134b92a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134b92e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134b93300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134b93770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134b93be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134b94050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134b944c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134b94930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134b94da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134b95210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134b95680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134b95af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134b95f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134b963d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134b96840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134b96cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134b97120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134b97590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134b97a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134b97e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134b982e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134b98750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134b98bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134b99630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134b99d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134b9a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134b9ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134b9ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134b9b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134b9b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134b9bf10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134b98e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134b9bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134b9b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134b9c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134b9c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134b9c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134b9cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134b9ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134b9d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134b9d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134b9d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134b9d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134b9df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134b9e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134b9eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134b9ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134b9f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134b9f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134b9f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134b9f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134b9fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134b9fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134ba0140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134ba0400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134ba06c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134ba0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134ba0c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134ba0f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134ba11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134ba1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134ba1740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134ba1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134ba1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134ba1f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134ba2240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134ba2500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134ba27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134ba2a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134ba2d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134ba3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134ba32c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134ba3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134ba3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134ba3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134ba3dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134ba4080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134ba4340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134ba4600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134ba48c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134ba4b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134ba4e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134ba5100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134ba53c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134ba5680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134ba5940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134ba5c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134ba5ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134ba6180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134ba6440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134ba6700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134ba69c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134ba6c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134ba6f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134ba7200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134ba74c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134ba7780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134ba7a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134ba7d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134ba7fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134ba8280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134ba8540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134ba8800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134ba8ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134ba8d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134ba9040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134ba9300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134ba95c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134ba9880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134ba9b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134ba9e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134baa0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134baa380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134baa640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134baa900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134baabc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134baae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134bab140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134bab400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134bab6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134bab980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134babc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134babf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134bac1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134bac480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134bac740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134baca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134baccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134bacf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134bad240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134bad500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134bad7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134bada80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134badd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134bae000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134bae2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134bae580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134bae840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134baeb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134baedc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134baf080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134baf340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134baf600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134baf8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134bafb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134bafe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134bb0100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134bb03c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134bb0680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134bb0940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134bb0c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134bb0ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134bb1180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134bb1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134bb1700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134bb19c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134bb1c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134bb1f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134bb2200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134bb24c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134bb2780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134bb2a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134bb2d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134bb2fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134bb3280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134bb3540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134bb3800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134bb3ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134bb3d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134bb4040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134bb4300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134bb45c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134bb4880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134bb4b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134bb4e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134bb50c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134bb5380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134bb5640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134bb5900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134bb5bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134bb5e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134bb6140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134bb6400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134bb66c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134bb6980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134bb6c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134bb6f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134bb71c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134bb7480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134bb7740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134bb7a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134bb7cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134bb7f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134bb8240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134bb8500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134bb87c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134bb8a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134bb8d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134bb9000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134bb92c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134bb9580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134bb9840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134bb9b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134bb9dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134bba080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134bba340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134bba600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134bba8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134bbab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134bbae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134bbb100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134bbb3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134bbb680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134bbb940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134bbbc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134bbbec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134bbc180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134bbc440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134bbc700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134bbc9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134bbcc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134bbcf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134bbd200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134bbd4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134bbd780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134bbda40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134bbdd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134bbdfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134bbe280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134bbe540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134bbe800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134bbeac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134bbed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134bbf040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134bbf300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134bbf5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134bbf880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134bbfb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134bbfe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134bc00c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134bc0380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134bc0950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134bc0c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134bc0ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134bc1190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134bc1450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134bc1710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134bc19d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134bc1c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134bc1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134bc2210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134bc24d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134bc2790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134bc2a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134bc2d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134bc2fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134bc3290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134bc3550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134bc3810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134bc3ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134bc3d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134bc4050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134bc4310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134bc45d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134bc4890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134bc4b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134bc4e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134bc50d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134bc5390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134bc5650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134bc5910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134bc5bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134bc5e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134bc6150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134bc6410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134bc66d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134bc6990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134bc6c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134bc6f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134bc71d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134bc7490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134bc7750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134bc7a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134bc7cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134bc7f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134bc8250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134bc8510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134bc87d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134bc8a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134bc8d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134bc9010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134bc92d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134bc9590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134bc9850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134bc9b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134bc9dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134bca090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134bca350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134bca610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134bca8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134bcab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134bcae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134bcb2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134bcb730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134bcbba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134bcc010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134bcc480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134bcc8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134bccd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134bcd1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134bcd640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134bcdab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134bcdf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134bce390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134bceed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134bcf5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134bcfd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134bd0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134bd06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134bd09b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134bd0e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134bd1290 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.748s
user	0m0.279s
sys	0m0.302s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4602 (1bd3047a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c60e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c60f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c60f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c60fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c610170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c610720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c610cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c611280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c611830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c611d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c612730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c613250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c613a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c614930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c615770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c615e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c616660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c616d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c6174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c617bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c618460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c618b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c619450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c61a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c61a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c61a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c61ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c61b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c61b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c61bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c61c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c61c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c61c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c61ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c61d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c61d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c61dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c61e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c61e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c61ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c61ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c61f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c61f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c620250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c620860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c620e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c621480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c621a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c6220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c6226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c622ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c623340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c6237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c6240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c6248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c624b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c625000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c6254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c625940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c626280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c626720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c626bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c627060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c6279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c627e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c6282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c628830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c628d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c6292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c629820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c629d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c62a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c62a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c62ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c62b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c62b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c62bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c62c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c62c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c62cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c62d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c62d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c62dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c62e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c62e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c62ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c62f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c62f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c62fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c61ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c6306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c630e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c6313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c631920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c631e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c6323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c632910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c632e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c6333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c633900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c633e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c6343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c6348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c634e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c635390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c635830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c635cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c636170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c636610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c636ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c636f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c6373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c637890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c637d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c6381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c638670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c638b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c638fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c639450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c6398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c639d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c63a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c63a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c63ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c63b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c63b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c63b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c63bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c63c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c63c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c63cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c63d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c63d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c63d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c63de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c63e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c63e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c63ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c63f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c63f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c63fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c63feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c640350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c6407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c640c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c641130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c6415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c641a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c641f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c6423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c642850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c642cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c643190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c643ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c643f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c644410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c6448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c644d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c6451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c645690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c645b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c645fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c646470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c646910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c646db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c647250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c6476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c647b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c648030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c6484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c648970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c6492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c649750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c649bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c64a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c64a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c64a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c64ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c64b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c64b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c64bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c64c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c64c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c64cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c64d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c64d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c64dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c64dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c64e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c64e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c64efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c64f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c64fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c64ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c650520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c651320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c6517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c651c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c652100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c6528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c652e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c653350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c6538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c653df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c654340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c654890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c654de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c655330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c655880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c655dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c656870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c656dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c657310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c657860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c657db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c658300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c658850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c658da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c6592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c659840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c659d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c65a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c65a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c65ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c65b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c65b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c65bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c65c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c65c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c65cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c65d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c65d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c65dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c65e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c65e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c65ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c65f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c65f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c65fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c660280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c6607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c660d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c661270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c6617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c661d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c662260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c6627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c662d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c663250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c6637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c663cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c664240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c664790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c664ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c665230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c6656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c665b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c666010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c6664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c666950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c666df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c667290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c667730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c667bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c668070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c668510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c6689b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c668e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c6692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c669790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c669ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c66a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c66ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c66b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c66b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c66bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c66c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c66c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c66cce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.483 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d8377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d83a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d83a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d83ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d83b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d83ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d83bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d83c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d83c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d83cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d83d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d83d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d83d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d83ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d83e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d83e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d83eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d83ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d83f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d83f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d83fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d8402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d8425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d8439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d8450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d8495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d84a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d84a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d84acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d84b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d84be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d84c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d84c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d84e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d84ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d84fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d8508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d8564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d85a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d85a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d85ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d85b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d85b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d85bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d85c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d85cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d85d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d85d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d85dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d85e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d85e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14da046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14da04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14da04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14da05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14da058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14da05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14da06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14da065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14da06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14da06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14da07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14da079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14da08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14da08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14da094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14da09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14da0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14da0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14da0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14da0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14da0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14da0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14da0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14da0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14da0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14da0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14da0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14da0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14da0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14da0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14da0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14da0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14da0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14da10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14da104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14da10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14da10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14da11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14da11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14da11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14da11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14da123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14da12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14da12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14da13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14da13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14da13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14da13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14da142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14da14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14da14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14da15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14da154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14da15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14da15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14da161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14da16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14da16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14da170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14da17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14da179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14da17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14da18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14da18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14da18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14da18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14da19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14da198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14da19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14da1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14da1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14da1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14da1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14da1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14da1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14da1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14da1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14da1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14da1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14da1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14da1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14da1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14da1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14da1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14da1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14da1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14da1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14da1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14da1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14da1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14da1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14da20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14da207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14da20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14da21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14da21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14da21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14da21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14da22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14da226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14da22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14da22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14da23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14da23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14da23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14da243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14da24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14da24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14da25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14da25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14da25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14da25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14da262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14da26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14da26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14da27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14da274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14da27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14da27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14da281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14da28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14da28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14da28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14da293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14da29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14da29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14da2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14da2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14da2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14da2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14da2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14da2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14da2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14da2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14da2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14da2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14da2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14da2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14da2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14da2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14da2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14da2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14da2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14da2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14da2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14da2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14da2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14da2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14da302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14da30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14da30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14da30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14da31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14da318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14da31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14da321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14da32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14da32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14da32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14da33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14da337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14da33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14da340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14da34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14da349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14da34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14da35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14da356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14da35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14da35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14da36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14da368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14da36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14da37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14da37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14da37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14da37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14da38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14da387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14da38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14da390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14da39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14da39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14da39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14da3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14da3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14da3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14da3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14da3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14da3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14da3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14da3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14da3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14da3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14da3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14da3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14da3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14da3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14da3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14da3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14da3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14da3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14da3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14da3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14da3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14da3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14da40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14da40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14da40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14da41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14da41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14da41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14da42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14da426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14da42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14da42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14da43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14da43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14da43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14da44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14da445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14da44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14da44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14da45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14da45790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14da45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14da46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14da464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14da46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14da46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14da47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14da476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14da47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14da47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14da483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14da48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14da48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14da49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14da495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14da49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14da49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14da4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14da4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14da4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14da4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14da4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14da4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14da4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14da4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14da4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14da4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14da4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14da4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14da4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14da4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14da4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14da4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14da4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14da4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14da4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14da4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14da4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14da50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14da504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14da50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14da50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14da511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14da51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14da51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14da51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14da523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14da52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14da52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14da53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14da53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14da539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14da53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14da542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14da54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14da54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14da55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14da55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14da558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14da56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14da56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14da571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14da578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14da57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14da57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14da585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14da58c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.946s
user	0m0.236s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.12 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.57 sec*proc (2 tests)

Total Test time (real) =   1.58 sec
        1.61 real         0.52 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.55 real         0.13 user         0.08 sys
```
