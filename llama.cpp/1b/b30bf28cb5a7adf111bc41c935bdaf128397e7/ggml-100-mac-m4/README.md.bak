### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.49 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.23 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.27 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   24.90 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.34 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    2.16 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.19 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.19 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.24 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.19 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    1.02 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed  173.40 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.31 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.18 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 215.05 sec*proc (27 tests)

Total Test time (real) = 215.05 sec

real	3m35.085s
user	7m29.923s
sys	0m5.133s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.31 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.41 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.19 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   14.08 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.22 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    0.90 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.17 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.22 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.20 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.18 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.29 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed   27.37 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.07 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.15 sec*proc (27 tests)

Total Test time (real) =  49.16 sec

real	0m49.169s
user	1m10.181s
sys	0m4.453s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.122 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.487 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.620 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.629 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.630 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.630 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.631 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.632 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.633 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.634 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.635 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.635 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.636 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.639 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.640 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.641 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.641 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.642 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.642 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.643 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.018 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.020 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.021 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.022 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.022 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.023 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.023 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.024 I llama_model_loader: - type  f32:  124 tensors
0.00.032.024 I llama_model_loader: - type  f16:   73 tensors
0.00.036.623 I llm_load_vocab: special tokens cache size = 5
0.00.039.010 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.039.014 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.039.014 I llm_load_print_meta: arch             = bert
0.00.039.015 I llm_load_print_meta: vocab type       = WPM
0.00.039.015 I llm_load_print_meta: n_vocab          = 30522
0.00.039.016 I llm_load_print_meta: n_merges         = 0
0.00.039.016 I llm_load_print_meta: vocab_only       = 0
0.00.039.016 I llm_load_print_meta: n_ctx_train      = 512
0.00.039.016 I llm_load_print_meta: n_embd           = 384
0.00.039.017 I llm_load_print_meta: n_layer          = 12
0.00.039.020 I llm_load_print_meta: n_head           = 12
0.00.039.021 I llm_load_print_meta: n_head_kv        = 12
0.00.039.021 I llm_load_print_meta: n_rot            = 32
0.00.039.022 I llm_load_print_meta: n_swa            = 0
0.00.039.022 I llm_load_print_meta: n_embd_head_k    = 32
0.00.039.022 I llm_load_print_meta: n_embd_head_v    = 32
0.00.039.023 I llm_load_print_meta: n_gqa            = 1
0.00.039.024 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.039.028 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.039.029 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.039.029 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.039.029 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.039.030 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.039.030 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.039.031 I llm_load_print_meta: n_ff             = 1536
0.00.039.031 I llm_load_print_meta: n_expert         = 0
0.00.039.032 I llm_load_print_meta: n_expert_used    = 0
0.00.039.032 I llm_load_print_meta: causal attn      = 0
0.00.039.032 I llm_load_print_meta: pooling type     = 2
0.00.039.032 I llm_load_print_meta: rope type        = 2
0.00.039.033 I llm_load_print_meta: rope scaling     = linear
0.00.039.033 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.039.047 I llm_load_print_meta: freq_scale_train = 1
0.00.039.050 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.039.051 I llm_load_print_meta: rope_finetuned   = unknown
0.00.039.051 I llm_load_print_meta: ssm_d_conv       = 0
0.00.039.052 I llm_load_print_meta: ssm_d_inner      = 0
0.00.039.052 I llm_load_print_meta: ssm_d_state      = 0
0.00.039.052 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.039.052 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.039.062 I llm_load_print_meta: model type       = 33M
0.00.039.065 I llm_load_print_meta: model ftype      = F16
0.00.039.066 I llm_load_print_meta: model params     = 33.21 M
0.00.039.066 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.039.067 I llm_load_print_meta: general.name     = Bge Small
0.00.039.068 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.039.068 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.039.068 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.039.068 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.039.069 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.039.069 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.039.069 I llm_load_print_meta: max token length = 21
0.00.041.289 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.290 I llm_load_tensors: offloading output layer to GPU
0.00.041.291 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.317 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.319 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.941 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.943 I llama_new_context_with_model: n_ctx         = 512
0.00.041.943 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.943 I llama_new_context_with_model: n_batch       = 2048
0.00.041.944 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.944 I llama_new_context_with_model: flash_attn    = 0
0.00.041.945 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.945 I llama_new_context_with_model: freq_scale    = 1
0.00.041.945 I ggml_metal_init: allocating
0.00.041.950 I ggml_metal_init: found device: Apple M4
0.00.041.953 I ggml_metal_init: picking default device: Apple M4
0.00.042.811 I ggml_metal_init: using embedded metal library
0.00.046.473 I ggml_metal_init: GPU name:   Apple M4
0.00.046.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.477 I ggml_metal_init: simdgroup reduction   = true
0.00.046.477 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.477 I ggml_metal_init: has bfloat            = true
0.00.046.478 I ggml_metal_init: use bfloat            = true
0.00.046.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.916 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.057.919 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.057.920 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.779 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.781 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.781 I llama_new_context_with_model: graph nodes  = 429
0.00.058.782 I llama_new_context_with_model: graph splits = 2
0.00.058.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.698 I 
0.00.065.713 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.066.420 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.617 I llama_perf_context_print:        load time =      44.20 ms
0.00.071.618 I llama_perf_context_print: prompt eval time =       5.03 ms /     9 tokens (    0.56 ms per token,  1788.91 tokens per second)
0.00.071.619 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.620 I llama_perf_context_print:       total time =       5.92 ms /    10 tokens
0.00.071.750 I ggml_metal_free: deallocating

real	0m0.261s
user	0m0.049s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.042 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.736 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.131 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.013.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.136 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.013.136 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.136 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.013.137 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.013.137 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.013.138 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.013.138 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.013.139 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.013.139 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.013.139 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.013.142 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.013.142 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.013.142 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.013.143 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.013.143 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.013.143 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.013.144 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.910 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.649 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.650 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.651 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.651 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.651 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.016.652 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.652 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.016.652 I llama_model_loader: - type  f32:  124 tensors
0.00.016.652 I llama_model_loader: - type q8_0:   73 tensors
0.00.019.359 I llm_load_vocab: special tokens cache size = 5
0.00.020.764 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.020.767 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.020.767 I llm_load_print_meta: arch             = bert
0.00.020.767 I llm_load_print_meta: vocab type       = WPM
0.00.020.767 I llm_load_print_meta: n_vocab          = 30522
0.00.020.768 I llm_load_print_meta: n_merges         = 0
0.00.020.768 I llm_load_print_meta: vocab_only       = 0
0.00.020.768 I llm_load_print_meta: n_ctx_train      = 512
0.00.020.768 I llm_load_print_meta: n_embd           = 384
0.00.020.768 I llm_load_print_meta: n_layer          = 12
0.00.020.770 I llm_load_print_meta: n_head           = 12
0.00.020.771 I llm_load_print_meta: n_head_kv        = 12
0.00.020.771 I llm_load_print_meta: n_rot            = 32
0.00.020.771 I llm_load_print_meta: n_swa            = 0
0.00.020.772 I llm_load_print_meta: n_embd_head_k    = 32
0.00.020.772 I llm_load_print_meta: n_embd_head_v    = 32
0.00.020.772 I llm_load_print_meta: n_gqa            = 1
0.00.020.775 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.020.775 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.020.776 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.020.776 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.020.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.020.776 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.020.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.020.777 I llm_load_print_meta: n_ff             = 1536
0.00.020.777 I llm_load_print_meta: n_expert         = 0
0.00.020.777 I llm_load_print_meta: n_expert_used    = 0
0.00.020.778 I llm_load_print_meta: causal attn      = 0
0.00.020.778 I llm_load_print_meta: pooling type     = 2
0.00.020.778 I llm_load_print_meta: rope type        = 2
0.00.020.778 I llm_load_print_meta: rope scaling     = linear
0.00.020.779 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.020.779 I llm_load_print_meta: freq_scale_train = 1
0.00.020.779 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.020.780 I llm_load_print_meta: rope_finetuned   = unknown
0.00.020.780 I llm_load_print_meta: ssm_d_conv       = 0
0.00.020.780 I llm_load_print_meta: ssm_d_inner      = 0
0.00.020.780 I llm_load_print_meta: ssm_d_state      = 0
0.00.020.781 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.020.781 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.020.786 I llm_load_print_meta: model type       = 33M
0.00.020.787 I llm_load_print_meta: model ftype      = Q8_0
0.00.020.787 I llm_load_print_meta: model params     = 33.21 M
0.00.020.788 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.020.788 I llm_load_print_meta: general.name     = Bge Small
0.00.020.788 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.020.788 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.020.789 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.020.789 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.020.790 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.020.790 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.020.791 I llm_load_print_meta: max token length = 21
0.00.022.015 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.022.016 I llm_load_tensors: offloading output layer to GPU
0.00.022.016 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.022.022 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.023 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.022.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.022.393 I llama_new_context_with_model: n_ctx         = 512
0.00.022.393 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.022.393 I llama_new_context_with_model: n_batch       = 2048
0.00.022.394 I llama_new_context_with_model: n_ubatch      = 2048
0.00.022.394 I llama_new_context_with_model: flash_attn    = 0
0.00.022.394 I llama_new_context_with_model: freq_base     = 10000.0
0.00.022.394 I llama_new_context_with_model: freq_scale    = 1
0.00.022.395 I ggml_metal_init: allocating
0.00.022.398 I ggml_metal_init: found device: Apple M4
0.00.022.400 I ggml_metal_init: picking default device: Apple M4
0.00.022.957 I ggml_metal_init: using embedded metal library
0.00.025.183 I ggml_metal_init: GPU name:   Apple M4
0.00.025.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.186 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.186 I ggml_metal_init: simdgroup reduction   = true
0.00.025.186 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.187 I ggml_metal_init: has bfloat            = true
0.00.025.187 I ggml_metal_init: use bfloat            = true
0.00.025.187 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.246 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.248 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.249 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.829 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.830 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.831 I llama_new_context_with_model: graph nodes  = 429
0.00.033.831 I llama_new_context_with_model: graph splits = 2
0.00.033.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.927 I 
0.00.038.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.039.476 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.481 I llama_perf_context_print:        load time =      28.19 ms
0.00.044.482 I llama_perf_context_print: prompt eval time =       4.88 ms /     9 tokens (    0.54 ms per token,  1842.37 tokens per second)
0.00.044.483 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.484 I llama_perf_context_print:       total time =       5.55 ms /    10 tokens
0.00.044.593 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.028s
sys	0m0.014s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.248 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.861 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.430 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.438 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.439 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.448 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.449 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.449 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.451 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.452 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.452 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.453 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.454 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.457 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.458 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.459 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.459 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.460 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.436 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.436 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.437 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.438 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.438 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.438 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.438 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.439 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.439 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.440 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.440 I llama_model_loader: - type  f32:   41 tensors
0.00.051.440 I llama_model_loader: - type  f16:   29 tensors
0.00.070.277 W llm_load_vocab: empty token at index 5
0.00.074.922 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.076.254 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.076.299 I llm_load_vocab: special tokens cache size = 5
0.00.368.145 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.368.150 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.368.150 I llm_load_print_meta: arch             = jina-bert-v2
0.00.368.151 I llm_load_print_meta: vocab type       = BPE
0.00.368.152 I llm_load_print_meta: n_vocab          = 61056
0.00.368.152 I llm_load_print_meta: n_merges         = 39382
0.00.368.152 I llm_load_print_meta: vocab_only       = 0
0.00.368.152 I llm_load_print_meta: n_ctx_train      = 8192
0.00.368.152 I llm_load_print_meta: n_embd           = 384
0.00.368.153 I llm_load_print_meta: n_layer          = 4
0.00.368.156 I llm_load_print_meta: n_head           = 12
0.00.368.157 I llm_load_print_meta: n_head_kv        = 12
0.00.368.157 I llm_load_print_meta: n_rot            = 32
0.00.368.157 I llm_load_print_meta: n_swa            = 0
0.00.368.157 I llm_load_print_meta: n_embd_head_k    = 32
0.00.368.157 I llm_load_print_meta: n_embd_head_v    = 32
0.00.368.158 I llm_load_print_meta: n_gqa            = 1
0.00.368.158 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.368.159 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.368.160 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.368.161 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.368.162 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.368.163 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.368.163 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.368.164 I llm_load_print_meta: n_ff             = 1536
0.00.368.164 I llm_load_print_meta: n_expert         = 0
0.00.368.164 I llm_load_print_meta: n_expert_used    = 0
0.00.368.164 I llm_load_print_meta: causal attn      = 0
0.00.368.164 I llm_load_print_meta: pooling type     = -1
0.00.368.165 I llm_load_print_meta: rope type        = -1
0.00.368.165 I llm_load_print_meta: rope scaling     = linear
0.00.368.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.368.166 I llm_load_print_meta: freq_scale_train = 1
0.00.368.168 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.368.168 I llm_load_print_meta: rope_finetuned   = unknown
0.00.368.168 I llm_load_print_meta: ssm_d_conv       = 0
0.00.368.168 I llm_load_print_meta: ssm_d_inner      = 0
0.00.368.168 I llm_load_print_meta: ssm_d_state      = 0
0.00.368.169 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.368.169 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.368.193 I llm_load_print_meta: model type       = 33M
0.00.368.193 I llm_load_print_meta: model ftype      = F16
0.00.368.194 I llm_load_print_meta: model params     = 32.90 M
0.00.368.194 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.368.195 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.368.195 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.368.195 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.368.196 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.368.196 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.368.196 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.368.196 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.368.197 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.368.197 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.368.197 I llm_load_print_meta: max token length = 45
0.00.369.456 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.369.457 I llm_load_tensors: offloading output layer to GPU
0.00.369.458 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.369.478 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.369.479 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.370.215 I llama_new_context_with_model: n_seq_max     = 1
0.00.370.216 I llama_new_context_with_model: n_ctx         = 8192
0.00.370.216 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.370.216 I llama_new_context_with_model: n_batch       = 2048
0.00.370.216 I llama_new_context_with_model: n_ubatch      = 2048
0.00.370.217 I llama_new_context_with_model: flash_attn    = 0
0.00.370.217 I llama_new_context_with_model: freq_base     = 10000.0
0.00.370.217 I llama_new_context_with_model: freq_scale    = 1
0.00.370.218 I ggml_metal_init: allocating
0.00.370.221 I ggml_metal_init: found device: Apple M4
0.00.370.223 I ggml_metal_init: picking default device: Apple M4
0.00.371.137 I ggml_metal_init: using embedded metal library
0.00.373.468 I ggml_metal_init: GPU name:   Apple M4
0.00.373.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.373.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.373.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.373.471 I ggml_metal_init: simdgroup reduction   = true
0.00.373.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.373.471 I ggml_metal_init: has bfloat            = true
0.00.373.471 I ggml_metal_init: use bfloat            = true
0.00.373.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.373.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.828 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.383.830 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.383.833 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.384.357 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.384.359 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.384.359 I llama_new_context_with_model: graph nodes  = 154
0.00.384.359 I llama_new_context_with_model: graph splits = 2
0.00.384.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.394.327 I 
0.00.394.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.394.498 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.394.498 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.394.501 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.394.501 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.394.505 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.394.506 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.394.978 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.398.753 I llama_perf_context_print:        load time =     369.46 ms
0.00.398.753 I llama_perf_context_print: prompt eval time =       3.77 ms /    62 tokens (    0.06 ms per token, 16463.09 tokens per second)
0.00.398.754 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.398.755 I llama_perf_context_print:       total time =       4.43 ms /    63 tokens
0.00.398.954 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.374s
sys	0m0.041s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.139 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.299 I main: llama backend init
0.00.000.305 I main: load the model and apply lora adapter, if any
0.00.054.333 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.067.484 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.503 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.505 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.516 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.076.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.085.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.784 I llama_model_loader: - type  f32:  194 tensors
0.00.085.785 I llama_model_loader: - type  f16:   98 tensors
0.00.116.554 I llm_load_vocab: special tokens cache size = 25
0.00.123.487 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.123.490 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.123.490 I llm_load_print_meta: arch             = gptneox
0.00.123.490 I llm_load_print_meta: vocab type       = BPE
0.00.123.491 I llm_load_print_meta: n_vocab          = 50304
0.00.123.491 I llm_load_print_meta: n_merges         = 50009
0.00.123.491 I llm_load_print_meta: vocab_only       = 0
0.00.123.491 I llm_load_print_meta: n_ctx_train      = 2048
0.00.123.491 I llm_load_print_meta: n_embd           = 2048
0.00.123.491 I llm_load_print_meta: n_layer          = 24
0.00.123.494 I llm_load_print_meta: n_head           = 16
0.00.123.495 I llm_load_print_meta: n_head_kv        = 16
0.00.123.495 I llm_load_print_meta: n_rot            = 32
0.00.123.495 I llm_load_print_meta: n_swa            = 0
0.00.123.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.123.496 I llm_load_print_meta: n_embd_head_v    = 128
0.00.123.499 I llm_load_print_meta: n_gqa            = 1
0.00.123.499 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.123.500 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.123.500 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.123.501 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.123.501 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.123.501 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.123.501 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.123.502 I llm_load_print_meta: n_ff             = 8192
0.00.123.502 I llm_load_print_meta: n_expert         = 0
0.00.123.502 I llm_load_print_meta: n_expert_used    = 0
0.00.123.502 I llm_load_print_meta: causal attn      = 1
0.00.123.502 I llm_load_print_meta: pooling type     = 0
0.00.123.503 I llm_load_print_meta: rope type        = 2
0.00.123.503 I llm_load_print_meta: rope scaling     = linear
0.00.123.503 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.123.503 I llm_load_print_meta: freq_scale_train = 1
0.00.123.504 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.123.504 I llm_load_print_meta: rope_finetuned   = unknown
0.00.123.504 I llm_load_print_meta: ssm_d_conv       = 0
0.00.123.504 I llm_load_print_meta: ssm_d_inner      = 0
0.00.123.504 I llm_load_print_meta: ssm_d_state      = 0
0.00.123.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.123.505 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.123.516 I llm_load_print_meta: model type       = 1.4B
0.00.123.517 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.123.517 I llm_load_print_meta: model params     = 1.41 B
0.00.123.518 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.123.518 I llm_load_print_meta: general.name     = 1.4B
0.00.123.518 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.123.519 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.123.520 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.123.520 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.123.520 I llm_load_print_meta: LF token         = 128 ''
0.00.123.520 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.123.520 I llm_load_print_meta: max token length = 1024
0.00.125.562 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.125.562 I llm_load_tensors: offloading output layer to GPU
0.00.125.562 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.125.579 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.125.580 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.126.458 I llama_new_context_with_model: n_seq_max     = 1
0.00.126.459 I llama_new_context_with_model: n_ctx         = 2048
0.00.126.459 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.126.459 I llama_new_context_with_model: n_batch       = 2048
0.00.126.459 I llama_new_context_with_model: n_ubatch      = 512
0.00.126.459 I llama_new_context_with_model: flash_attn    = 0
0.00.126.460 I llama_new_context_with_model: freq_base     = 10000.0
0.00.126.460 I llama_new_context_with_model: freq_scale    = 1
0.00.126.461 I ggml_metal_init: allocating
0.00.126.466 I ggml_metal_init: found device: Apple M4
0.00.126.469 I ggml_metal_init: picking default device: Apple M4
0.00.127.046 I ggml_metal_init: using embedded metal library
0.00.135.000 I ggml_metal_init: GPU name:   Apple M4
0.00.135.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.135.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.135.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.135.003 I ggml_metal_init: simdgroup reduction   = true
0.00.135.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.135.004 I ggml_metal_init: has bfloat            = true
0.00.135.004 I ggml_metal_init: use bfloat            = true
0.00.135.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.135.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.168.483 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.168.488 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.168.504 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.169.480 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.169.482 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.169.482 I llama_new_context_with_model: graph nodes  = 967
0.00.169.482 I llama_new_context_with_model: graph splits = 2
0.00.169.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.278.443 I main: llama threadpool init, n_threads = 4
0.00.278.477 I 
0.00.278.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.278.499 I 
0.00.278.661 I sampler seed: 1234
0.00.278.667 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.278.688 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.278.690 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.278.690 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.088.615 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49271.34 tokens per second)
0.02.088.616 I llama_perf_context_print:        load time =     224.10 ms
0.02.088.617 I llama_perf_context_print: prompt eval time =      37.60 ms /     7 tokens (    5.37 ms per token,   186.19 tokens per second)
0.02.088.617 I llama_perf_context_print:        eval time =    1769.07 ms /    63 runs   (   28.08 ms per token,    35.61 tokens per second)
0.02.088.618 I llama_perf_context_print:       total time =    1810.18 ms /    70 tokens
0.02.088.806 I ggml_metal_free: deallocating

real	0m2.403s
user	0m0.151s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.579 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.574 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.606 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.629 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.634 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.987 I llama_model_loader: - type  f32:  194 tensors
0.00.052.988 I llama_model_loader: - type  f16:   98 tensors
0.00.081.998 I llm_load_vocab: special tokens cache size = 25
0.00.088.597 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.600 I llm_load_print_meta: arch             = gptneox
0.00.088.601 I llm_load_print_meta: vocab type       = BPE
0.00.088.601 I llm_load_print_meta: n_vocab          = 50304
0.00.088.601 I llm_load_print_meta: n_merges         = 50009
0.00.088.601 I llm_load_print_meta: vocab_only       = 0
0.00.088.601 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.601 I llm_load_print_meta: n_embd           = 2048
0.00.088.601 I llm_load_print_meta: n_layer          = 24
0.00.088.604 I llm_load_print_meta: n_head           = 16
0.00.088.605 I llm_load_print_meta: n_head_kv        = 16
0.00.088.605 I llm_load_print_meta: n_rot            = 32
0.00.088.607 I llm_load_print_meta: n_swa            = 0
0.00.088.607 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.607 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.607 I llm_load_print_meta: n_gqa            = 1
0.00.088.608 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.609 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.609 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.610 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.610 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.610 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.610 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.611 I llm_load_print_meta: n_ff             = 8192
0.00.088.611 I llm_load_print_meta: n_expert         = 0
0.00.088.611 I llm_load_print_meta: n_expert_used    = 0
0.00.088.611 I llm_load_print_meta: causal attn      = 1
0.00.088.611 I llm_load_print_meta: pooling type     = 0
0.00.088.611 I llm_load_print_meta: rope type        = 2
0.00.088.611 I llm_load_print_meta: rope scaling     = linear
0.00.088.612 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.612 I llm_load_print_meta: freq_scale_train = 1
0.00.088.612 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.614 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.614 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.614 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.614 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.614 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.631 I llm_load_print_meta: model type       = 1.4B
0.00.088.631 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.632 I llm_load_print_meta: model params     = 1.41 B
0.00.088.632 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.634 I llm_load_print_meta: general.name     = 1.4B
0.00.088.634 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.634 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.635 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.635 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.635 I llm_load_print_meta: LF token         = 128 ''
0.00.088.635 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.635 I llm_load_print_meta: max token length = 1024
0.00.090.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.544 I llm_load_tensors: offloading output layer to GPU
0.00.090.545 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.554 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.555 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.462 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.463 I llama_new_context_with_model: n_ctx         = 128
0.00.091.463 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.463 I llama_new_context_with_model: n_batch       = 128
0.00.091.463 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.464 I llama_new_context_with_model: flash_attn    = 0
0.00.091.464 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.464 I llama_new_context_with_model: freq_scale    = 1
0.00.091.465 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.465 I ggml_metal_init: allocating
0.00.091.471 I ggml_metal_init: found device: Apple M4
0.00.091.478 I ggml_metal_init: picking default device: Apple M4
0.00.092.062 I ggml_metal_init: using embedded metal library
0.00.094.110 I ggml_metal_init: GPU name:   Apple M4
0.00.094.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.113 I ggml_metal_init: simdgroup reduction   = true
0.00.094.113 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.113 I ggml_metal_init: has bfloat            = true
0.00.094.114 I ggml_metal_init: use bfloat            = true
0.00.094.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.717 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.720 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.734 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.566 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.567 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.567 I llama_new_context_with_model: graph nodes  = 967
0.00.103.568 I llama_new_context_with_model: graph splits = 2
0.00.103.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.655.999 I 
0.01.656.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.656.111 I perplexity: tokenizing the input ..
0.01.668.613 I perplexity: tokenization took 12.496 ms
0.01.668.620 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.790.132 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.791.680 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.791.699 I llama_perf_context_print:        load time =    1632.41 ms
0.01.791.701 I llama_perf_context_print: prompt eval time =     120.62 ms /   128 tokens (    0.94 ms per token,  1061.17 tokens per second)
0.01.791.702 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.791.708 I llama_perf_context_print:       total time =     135.71 ms /   129 tokens
0.01.792.261 I ggml_metal_free: deallocating

real	0m1.999s
user	0m0.125s
sys	0m0.381s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.012.145 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.932 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.429 I llama_model_loader: - type  f32:  194 tensors
0.00.030.430 I llama_model_loader: - type q8_0:   98 tensors
0.00.051.281 I llm_load_vocab: special tokens cache size = 25
0.00.057.305 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.308 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.309 I llm_load_print_meta: arch             = gptneox
0.00.057.309 I llm_load_print_meta: vocab type       = BPE
0.00.057.310 I llm_load_print_meta: n_vocab          = 50304
0.00.057.310 I llm_load_print_meta: n_merges         = 50009
0.00.057.310 I llm_load_print_meta: vocab_only       = 0
0.00.057.310 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.310 I llm_load_print_meta: n_embd           = 2048
0.00.057.313 I llm_load_print_meta: n_layer          = 24
0.00.057.316 I llm_load_print_meta: n_head           = 16
0.00.057.317 I llm_load_print_meta: n_head_kv        = 16
0.00.057.318 I llm_load_print_meta: n_rot            = 32
0.00.057.318 I llm_load_print_meta: n_swa            = 0
0.00.057.318 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.318 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.319 I llm_load_print_meta: n_gqa            = 1
0.00.057.320 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.320 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.321 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.321 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.321 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.321 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.322 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.322 I llm_load_print_meta: n_ff             = 8192
0.00.057.323 I llm_load_print_meta: n_expert         = 0
0.00.057.323 I llm_load_print_meta: n_expert_used    = 0
0.00.057.323 I llm_load_print_meta: causal attn      = 1
0.00.057.323 I llm_load_print_meta: pooling type     = 0
0.00.057.324 I llm_load_print_meta: rope type        = 2
0.00.057.324 I llm_load_print_meta: rope scaling     = linear
0.00.057.325 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.325 I llm_load_print_meta: freq_scale_train = 1
0.00.057.325 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.325 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.325 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.327 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.327 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.341 I llm_load_print_meta: model type       = 1.4B
0.00.057.341 I llm_load_print_meta: model ftype      = Q8_0
0.00.057.341 I llm_load_print_meta: model params     = 1.41 B
0.00.057.342 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.057.343 I llm_load_print_meta: general.name     = 1.4B
0.00.057.343 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.344 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.344 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.345 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.345 I llm_load_print_meta: LF token         = 128 ''
0.00.057.345 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.345 I llm_load_print_meta: max token length = 1024
0.00.059.324 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.324 I llm_load_tensors: offloading output layer to GPU
0.00.059.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.334 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.335 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.215 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.216 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.216 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.216 I llama_new_context_with_model: n_batch       = 2048
0.00.060.217 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.217 I llama_new_context_with_model: flash_attn    = 0
0.00.060.217 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.217 I llama_new_context_with_model: freq_scale    = 1
0.00.060.218 I ggml_metal_init: allocating
0.00.060.222 I ggml_metal_init: found device: Apple M4
0.00.060.224 I ggml_metal_init: picking default device: Apple M4
0.00.060.889 I ggml_metal_init: using embedded metal library
0.00.062.992 I ggml_metal_init: GPU name:   Apple M4
0.00.062.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.995 I ggml_metal_init: simdgroup reduction   = true
0.00.062.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.995 I ggml_metal_init: has bfloat            = true
0.00.062.995 I ggml_metal_init: use bfloat            = true
0.00.062.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.180 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.207 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.340 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.341 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.342 I llama_new_context_with_model: graph nodes  = 967
0.00.095.342 I llama_new_context_with_model: graph splits = 2
0.00.095.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.837.373 I main: llama threadpool init, n_threads = 4
0.01.837.401 I 
0.01.837.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.837.422 I 
0.01.837.574 I sampler seed: 1234
0.01.837.579 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.837.609 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.837.625 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.837.628 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.904.630 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48596.85 tokens per second)
0.02.904.630 I llama_perf_context_print:        load time =    1825.22 ms
0.02.904.631 I llama_perf_context_print: prompt eval time =      33.52 ms /     7 tokens (    4.79 ms per token,   208.82 tokens per second)
0.02.904.632 I llama_perf_context_print:        eval time =    1030.10 ms /    63 runs   (   16.35 ms per token,    61.16 tokens per second)
0.02.904.632 I llama_perf_context_print:       total time =    1067.26 ms /    70 tokens
0.02.904.801 I ggml_metal_free: deallocating

real	0m2.921s
user	0m0.109s
sys	0m0.319s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.194 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.020 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.033 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.034 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.037 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.406 I llama_model_loader: - type  f32:  194 tensors
0.00.029.406 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.988 I llm_load_vocab: special tokens cache size = 25
0.00.060.186 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.189 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.189 I llm_load_print_meta: arch             = gptneox
0.00.060.189 I llm_load_print_meta: vocab type       = BPE
0.00.060.189 I llm_load_print_meta: n_vocab          = 50304
0.00.060.189 I llm_load_print_meta: n_merges         = 50009
0.00.060.190 I llm_load_print_meta: vocab_only       = 0
0.00.060.190 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.190 I llm_load_print_meta: n_embd           = 2048
0.00.060.190 I llm_load_print_meta: n_layer          = 24
0.00.060.192 I llm_load_print_meta: n_head           = 16
0.00.060.193 I llm_load_print_meta: n_head_kv        = 16
0.00.060.193 I llm_load_print_meta: n_rot            = 32
0.00.060.193 I llm_load_print_meta: n_swa            = 0
0.00.060.193 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.193 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.194 I llm_load_print_meta: n_gqa            = 1
0.00.060.194 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.195 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.195 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.196 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.198 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.198 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.198 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.198 I llm_load_print_meta: n_ff             = 8192
0.00.060.198 I llm_load_print_meta: n_expert         = 0
0.00.060.199 I llm_load_print_meta: n_expert_used    = 0
0.00.060.199 I llm_load_print_meta: causal attn      = 1
0.00.060.199 I llm_load_print_meta: pooling type     = 0
0.00.060.199 I llm_load_print_meta: rope type        = 2
0.00.060.199 I llm_load_print_meta: rope scaling     = linear
0.00.060.200 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.200 I llm_load_print_meta: freq_scale_train = 1
0.00.060.200 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.201 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.201 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.201 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.201 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.202 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.202 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.213 I llm_load_print_meta: model type       = 1.4B
0.00.060.215 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.215 I llm_load_print_meta: model params     = 1.41 B
0.00.060.215 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.215 I llm_load_print_meta: general.name     = 1.4B
0.00.060.216 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.216 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.216 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.216 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.216 I llm_load_print_meta: LF token         = 128 ''
0.00.060.217 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.217 I llm_load_print_meta: max token length = 1024
0.00.061.886 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.886 I llm_load_tensors: offloading output layer to GPU
0.00.061.887 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.897 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.898 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.806 I llama_new_context_with_model: n_ctx         = 128
0.00.062.806 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.806 I llama_new_context_with_model: n_batch       = 128
0.00.062.806 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.806 I llama_new_context_with_model: flash_attn    = 0
0.00.062.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.807 I llama_new_context_with_model: freq_scale    = 1
0.00.062.807 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.808 I ggml_metal_init: allocating
0.00.062.814 I ggml_metal_init: found device: Apple M4
0.00.062.816 I ggml_metal_init: picking default device: Apple M4
0.00.063.383 I ggml_metal_init: using embedded metal library
0.00.065.468 I ggml_metal_init: GPU name:   Apple M4
0.00.065.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.471 I ggml_metal_init: simdgroup reduction   = true
0.00.065.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.471 I ggml_metal_init: has bfloat            = true
0.00.065.471 I ggml_metal_init: use bfloat            = true
0.00.065.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.357 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.361 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.374 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.231 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.232 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.232 I llama_new_context_with_model: graph nodes  = 967
0.00.076.233 I llama_new_context_with_model: graph splits = 2
0.00.076.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.043.025 I 
0.01.043.045 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.043.072 I perplexity: tokenizing the input ..
0.01.050.382 I perplexity: tokenization took 7.309 ms
0.01.050.385 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.172.008 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.173.190 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.173.202 I llama_perf_context_print:        load time =    1032.83 ms
0.01.173.202 I llama_perf_context_print: prompt eval time =     121.37 ms /   128 tokens (    0.95 ms per token,  1054.63 tokens per second)
0.01.173.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.173.204 I llama_perf_context_print:       total time =     130.18 ms /   129 tokens
0.01.173.580 I ggml_metal_free: deallocating

real	0m1.187s
user	0m0.087s
sys	0m0.212s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.013.180 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.817 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.846 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.953 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.954 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.954 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.954 I llama_model_loader: - type  f32:  194 tensors
0.00.028.955 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.955 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.292 I llm_load_vocab: special tokens cache size = 25
0.00.056.562 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.566 I llm_load_print_meta: arch             = gptneox
0.00.056.567 I llm_load_print_meta: vocab type       = BPE
0.00.056.567 I llm_load_print_meta: n_vocab          = 50304
0.00.056.567 I llm_load_print_meta: n_merges         = 50009
0.00.056.567 I llm_load_print_meta: vocab_only       = 0
0.00.056.568 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.568 I llm_load_print_meta: n_embd           = 2048
0.00.056.568 I llm_load_print_meta: n_layer          = 24
0.00.056.571 I llm_load_print_meta: n_head           = 16
0.00.056.572 I llm_load_print_meta: n_head_kv        = 16
0.00.056.573 I llm_load_print_meta: n_rot            = 32
0.00.056.574 I llm_load_print_meta: n_swa            = 0
0.00.056.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.575 I llm_load_print_meta: n_gqa            = 1
0.00.056.576 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.577 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.577 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.578 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.578 I llm_load_print_meta: n_ff             = 8192
0.00.056.579 I llm_load_print_meta: n_expert         = 0
0.00.056.579 I llm_load_print_meta: n_expert_used    = 0
0.00.056.579 I llm_load_print_meta: causal attn      = 1
0.00.056.579 I llm_load_print_meta: pooling type     = 0
0.00.056.579 I llm_load_print_meta: rope type        = 2
0.00.056.580 I llm_load_print_meta: rope scaling     = linear
0.00.056.580 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.580 I llm_load_print_meta: freq_scale_train = 1
0.00.056.581 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.581 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.582 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.582 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.593 I llm_load_print_meta: model type       = 1.4B
0.00.056.593 I llm_load_print_meta: model ftype      = Q4_0
0.00.056.594 I llm_load_print_meta: model params     = 1.41 B
0.00.056.594 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.056.594 I llm_load_print_meta: general.name     = 1.4B
0.00.056.595 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.595 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.595 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.595 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.597 I llm_load_print_meta: LF token         = 128 ''
0.00.056.597 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.598 I llm_load_print_meta: max token length = 1024
0.00.058.754 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.754 I llm_load_tensors: offloading output layer to GPU
0.00.058.755 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.765 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.058.766 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.059.678 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.679 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.680 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.680 I llama_new_context_with_model: n_batch       = 2048
0.00.059.680 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.680 I llama_new_context_with_model: flash_attn    = 0
0.00.059.681 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.681 I llama_new_context_with_model: freq_scale    = 1
0.00.059.682 I ggml_metal_init: allocating
0.00.059.688 I ggml_metal_init: found device: Apple M4
0.00.059.690 I ggml_metal_init: picking default device: Apple M4
0.00.060.384 I ggml_metal_init: using embedded metal library
0.00.062.695 I ggml_metal_init: GPU name:   Apple M4
0.00.062.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.697 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.698 I ggml_metal_init: simdgroup reduction   = true
0.00.062.698 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.698 I ggml_metal_init: has bfloat            = true
0.00.062.698 I ggml_metal_init: use bfloat            = true
0.00.062.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.047 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.055 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.076 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.157 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.159 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.159 I llama_new_context_with_model: graph nodes  = 967
0.00.095.159 I llama_new_context_with_model: graph splits = 2
0.00.095.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.580 I main: llama threadpool init, n_threads = 4
0.00.780.611 I 
0.00.780.631 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.780.631 I 
0.00.780.778 I sampler seed: 1234
0.00.780.784 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.793 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.793 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.793 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.446.005 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.446.005 I llama_perf_context_print:        load time =     767.40 ms
0.01.446.006 I llama_perf_context_print: prompt eval time =      32.62 ms /     7 tokens (    4.66 ms per token,   214.56 tokens per second)
0.01.446.007 I llama_perf_context_print:        eval time =     629.33 ms /    63 runs   (    9.99 ms per token,   100.11 tokens per second)
0.01.446.009 I llama_perf_context_print:       total time =     665.43 ms /    70 tokens
0.01.446.173 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.889 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.569 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.575 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.576 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.577 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.389 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.175 I llama_model_loader: - type  f32:  194 tensors
0.00.024.176 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.089 I llm_load_vocab: special tokens cache size = 25
0.00.051.173 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.175 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.175 I llm_load_print_meta: arch             = gptneox
0.00.051.176 I llm_load_print_meta: vocab type       = BPE
0.00.051.176 I llm_load_print_meta: n_vocab          = 50304
0.00.051.176 I llm_load_print_meta: n_merges         = 50009
0.00.051.176 I llm_load_print_meta: vocab_only       = 0
0.00.051.176 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.177 I llm_load_print_meta: n_embd           = 2048
0.00.051.177 I llm_load_print_meta: n_layer          = 24
0.00.051.179 I llm_load_print_meta: n_head           = 16
0.00.051.180 I llm_load_print_meta: n_head_kv        = 16
0.00.051.182 I llm_load_print_meta: n_rot            = 32
0.00.051.182 I llm_load_print_meta: n_swa            = 0
0.00.051.182 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.183 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.183 I llm_load_print_meta: n_gqa            = 1
0.00.051.184 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.185 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.185 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.186 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.186 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.186 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.186 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.187 I llm_load_print_meta: n_ff             = 8192
0.00.051.187 I llm_load_print_meta: n_expert         = 0
0.00.051.187 I llm_load_print_meta: n_expert_used    = 0
0.00.051.187 I llm_load_print_meta: causal attn      = 1
0.00.051.188 I llm_load_print_meta: pooling type     = 0
0.00.051.188 I llm_load_print_meta: rope type        = 2
0.00.051.188 I llm_load_print_meta: rope scaling     = linear
0.00.051.188 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.189 I llm_load_print_meta: freq_scale_train = 1
0.00.051.189 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.189 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.189 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.189 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.189 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.190 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.190 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.201 I llm_load_print_meta: model type       = 1.4B
0.00.051.202 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.203 I llm_load_print_meta: model params     = 1.41 B
0.00.051.204 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.204 I llm_load_print_meta: general.name     = 1.4B
0.00.051.204 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.205 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.205 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.205 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.206 I llm_load_print_meta: LF token         = 128 ''
0.00.051.206 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.206 I llm_load_print_meta: max token length = 1024
0.00.052.877 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.877 I llm_load_tensors: offloading output layer to GPU
0.00.052.877 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.887 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.888 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.754 I llama_new_context_with_model: n_ctx         = 128
0.00.053.754 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.754 I llama_new_context_with_model: n_batch       = 128
0.00.053.754 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.754 I llama_new_context_with_model: flash_attn    = 0
0.00.053.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.755 I llama_new_context_with_model: freq_scale    = 1
0.00.053.755 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.756 I ggml_metal_init: allocating
0.00.053.761 I ggml_metal_init: found device: Apple M4
0.00.053.763 I ggml_metal_init: picking default device: Apple M4
0.00.054.288 I ggml_metal_init: using embedded metal library
0.00.056.209 I ggml_metal_init: GPU name:   Apple M4
0.00.056.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.212 I ggml_metal_init: simdgroup reduction   = true
0.00.056.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.212 I ggml_metal_init: has bfloat            = true
0.00.056.213 I ggml_metal_init: use bfloat            = true
0.00.056.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.367 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.371 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.386 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.235 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.237 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.237 I llama_new_context_with_model: graph nodes  = 967
0.00.066.237 I llama_new_context_with_model: graph splits = 2
0.00.066.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.700 I 
0.00.702.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.702.728 I perplexity: tokenizing the input ..
0.00.710.183 I perplexity: tokenization took 7.453 ms
0.00.710.186 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.532 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.833.655 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.833.671 I llama_perf_context_print:        load time =     692.81 ms
0.00.833.673 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.11 tokens per second)
0.00.833.674 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.675 I llama_perf_context_print:       total time =     130.97 ms /   129 tokens
0.00.834.078 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.077s
sys	0m0.143s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.761 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.767 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.771 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.772 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.772 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.772 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.774 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.775 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.650 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.651 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.651 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.651 I llama_model_loader: - type  f32:  194 tensors
0.00.024.652 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.652 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.658 I llm_load_vocab: special tokens cache size = 25
0.00.050.611 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.614 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.614 I llm_load_print_meta: arch             = gptneox
0.00.050.615 I llm_load_print_meta: vocab type       = BPE
0.00.050.615 I llm_load_print_meta: n_vocab          = 50304
0.00.050.615 I llm_load_print_meta: n_merges         = 50009
0.00.050.615 I llm_load_print_meta: vocab_only       = 0
0.00.050.616 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.616 I llm_load_print_meta: n_embd           = 2048
0.00.050.616 I llm_load_print_meta: n_layer          = 24
0.00.050.619 I llm_load_print_meta: n_head           = 16
0.00.050.620 I llm_load_print_meta: n_head_kv        = 16
0.00.050.620 I llm_load_print_meta: n_rot            = 32
0.00.050.620 I llm_load_print_meta: n_swa            = 0
0.00.050.620 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.620 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.621 I llm_load_print_meta: n_gqa            = 1
0.00.050.622 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.623 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.624 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.624 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.624 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.624 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.624 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.625 I llm_load_print_meta: n_ff             = 8192
0.00.050.626 I llm_load_print_meta: n_expert         = 0
0.00.050.626 I llm_load_print_meta: n_expert_used    = 0
0.00.050.626 I llm_load_print_meta: causal attn      = 1
0.00.050.626 I llm_load_print_meta: pooling type     = 0
0.00.050.626 I llm_load_print_meta: rope type        = 2
0.00.050.626 I llm_load_print_meta: rope scaling     = linear
0.00.050.627 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.627 I llm_load_print_meta: freq_scale_train = 1
0.00.050.627 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.628 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.628 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.628 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.628 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.629 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.630 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.636 I llm_load_print_meta: model type       = 1.4B
0.00.050.636 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.638 I llm_load_print_meta: model params     = 1.41 B
0.00.050.639 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.639 I llm_load_print_meta: general.name     = 1.4B
0.00.050.639 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: LF token         = 128 ''
0.00.050.640 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: max token length = 1024
0.00.052.271 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.271 I llm_load_tensors: offloading output layer to GPU
0.00.052.271 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.276 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.278 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.135 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.136 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.136 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.136 I llama_new_context_with_model: n_batch       = 2048
0.00.053.136 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.136 I llama_new_context_with_model: flash_attn    = 0
0.00.053.137 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.137 I llama_new_context_with_model: freq_scale    = 1
0.00.053.138 I ggml_metal_init: allocating
0.00.053.144 I ggml_metal_init: found device: Apple M4
0.00.053.146 I ggml_metal_init: picking default device: Apple M4
0.00.053.672 I ggml_metal_init: using embedded metal library
0.00.055.551 I ggml_metal_init: GPU name:   Apple M4
0.00.055.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.554 I ggml_metal_init: simdgroup reduction   = true
0.00.055.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.554 I ggml_metal_init: has bfloat            = true
0.00.055.554 I ggml_metal_init: use bfloat            = true
0.00.055.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.227 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.233 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.158 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.159 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.159 I llama_new_context_with_model: graph nodes  = 967
0.00.083.159 I llama_new_context_with_model: graph splits = 2
0.00.083.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.600 I main: llama threadpool init, n_threads = 4
0.00.836.630 I 
0.00.836.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.836.673 I 
0.00.836.804 I sampler seed: 1234
0.00.836.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.854 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.854 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.546.688 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.546.688 I llama_perf_context_print:        load time =     827.72 ms
0.01.546.689 I llama_perf_context_print: prompt eval time =      32.87 ms /     7 tokens (    4.70 ms per token,   212.98 tokens per second)
0.01.546.690 I llama_perf_context_print:        eval time =     673.78 ms /    63 runs   (   10.69 ms per token,    93.50 tokens per second)
0.01.546.690 I llama_perf_context_print:       total time =     710.09 ms /    70 tokens
0.01.546.858 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.761 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.525 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.530 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.315 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.075 I llama_model_loader: - type  f32:  194 tensors
0.00.024.075 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.075 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.782 I llm_load_vocab: special tokens cache size = 25
0.00.050.804 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.806 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.807 I llm_load_print_meta: arch             = gptneox
0.00.050.807 I llm_load_print_meta: vocab type       = BPE
0.00.050.807 I llm_load_print_meta: n_vocab          = 50304
0.00.050.807 I llm_load_print_meta: n_merges         = 50009
0.00.050.808 I llm_load_print_meta: vocab_only       = 0
0.00.050.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.808 I llm_load_print_meta: n_embd           = 2048
0.00.050.808 I llm_load_print_meta: n_layer          = 24
0.00.050.810 I llm_load_print_meta: n_head           = 16
0.00.050.811 I llm_load_print_meta: n_head_kv        = 16
0.00.050.811 I llm_load_print_meta: n_rot            = 32
0.00.050.812 I llm_load_print_meta: n_swa            = 0
0.00.050.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.813 I llm_load_print_meta: n_gqa            = 1
0.00.050.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.816 I llm_load_print_meta: n_ff             = 8192
0.00.050.817 I llm_load_print_meta: n_expert         = 0
0.00.050.817 I llm_load_print_meta: n_expert_used    = 0
0.00.050.817 I llm_load_print_meta: causal attn      = 1
0.00.050.817 I llm_load_print_meta: pooling type     = 0
0.00.050.817 I llm_load_print_meta: rope type        = 2
0.00.050.817 I llm_load_print_meta: rope scaling     = linear
0.00.050.818 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.818 I llm_load_print_meta: freq_scale_train = 1
0.00.050.818 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.819 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.819 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.819 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.819 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.821 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.832 I llm_load_print_meta: model type       = 1.4B
0.00.050.833 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.833 I llm_load_print_meta: model params     = 1.41 B
0.00.050.834 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.835 I llm_load_print_meta: general.name     = 1.4B
0.00.050.835 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: LF token         = 128 ''
0.00.050.836 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.837 I llm_load_print_meta: max token length = 1024
0.00.052.573 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.573 I llm_load_tensors: offloading output layer to GPU
0.00.052.574 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.583 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.584 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.438 I llama_new_context_with_model: n_ctx         = 128
0.00.053.438 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.438 I llama_new_context_with_model: n_batch       = 128
0.00.053.439 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.439 I llama_new_context_with_model: flash_attn    = 0
0.00.053.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.439 I llama_new_context_with_model: freq_scale    = 1
0.00.053.440 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.440 I ggml_metal_init: allocating
0.00.053.443 I ggml_metal_init: found device: Apple M4
0.00.053.445 I ggml_metal_init: picking default device: Apple M4
0.00.053.967 I ggml_metal_init: using embedded metal library
0.00.055.906 I ggml_metal_init: GPU name:   Apple M4
0.00.055.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.908 I ggml_metal_init: simdgroup reduction   = true
0.00.055.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.909 I ggml_metal_init: has bfloat            = true
0.00.055.909 I ggml_metal_init: use bfloat            = true
0.00.055.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.787 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.792 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.817 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.646 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.648 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.648 I llama_new_context_with_model: graph nodes  = 967
0.00.065.648 I llama_new_context_with_model: graph splits = 2
0.00.065.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.584 I 
0.00.770.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.770.613 I perplexity: tokenizing the input ..
0.00.778.064 I perplexity: tokenization took 7.449 ms
0.00.778.068 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.412 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.901.512 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.901.527 I llama_perf_context_print:        load time =     760.82 ms
0.00.901.528 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.14 tokens per second)
0.00.901.529 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.529 I llama_perf_context_print:       total time =     130.94 ms /   129 tokens
0.00.901.778 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.076s
sys	0m0.159s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.011.864 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.428 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.439 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.440 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.286 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.082 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.082 I llama_model_loader: - type  f32:  194 tensors
0.00.027.083 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.083 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.051 I llm_load_vocab: special tokens cache size = 25
0.00.053.034 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.038 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.038 I llm_load_print_meta: arch             = gptneox
0.00.053.038 I llm_load_print_meta: vocab type       = BPE
0.00.053.039 I llm_load_print_meta: n_vocab          = 50304
0.00.053.039 I llm_load_print_meta: n_merges         = 50009
0.00.053.039 I llm_load_print_meta: vocab_only       = 0
0.00.053.039 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.039 I llm_load_print_meta: n_embd           = 2048
0.00.053.040 I llm_load_print_meta: n_layer          = 24
0.00.053.042 I llm_load_print_meta: n_head           = 16
0.00.053.042 I llm_load_print_meta: n_head_kv        = 16
0.00.053.043 I llm_load_print_meta: n_rot            = 32
0.00.053.043 I llm_load_print_meta: n_swa            = 0
0.00.053.043 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.043 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.044 I llm_load_print_meta: n_gqa            = 1
0.00.053.045 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.045 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.046 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.046 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.046 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.047 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.047 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.047 I llm_load_print_meta: n_ff             = 8192
0.00.053.048 I llm_load_print_meta: n_expert         = 0
0.00.053.048 I llm_load_print_meta: n_expert_used    = 0
0.00.053.048 I llm_load_print_meta: causal attn      = 1
0.00.053.048 I llm_load_print_meta: pooling type     = 0
0.00.053.048 I llm_load_print_meta: rope type        = 2
0.00.053.048 I llm_load_print_meta: rope scaling     = linear
0.00.053.049 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.049 I llm_load_print_meta: freq_scale_train = 1
0.00.053.049 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.049 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.050 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.050 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.050 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.050 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.061 I llm_load_print_meta: model type       = 1.4B
0.00.053.061 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.062 I llm_load_print_meta: model params     = 1.41 B
0.00.053.062 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.062 I llm_load_print_meta: general.name     = 1.4B
0.00.053.063 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.065 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.065 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.065 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.065 I llm_load_print_meta: LF token         = 128 ''
0.00.053.065 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.066 I llm_load_print_meta: max token length = 1024
0.00.054.824 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.824 I llm_load_tensors: offloading output layer to GPU
0.00.054.825 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.834 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.836 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.681 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.682 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.682 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.682 I llama_new_context_with_model: n_batch       = 2048
0.00.055.682 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.682 I llama_new_context_with_model: flash_attn    = 0
0.00.055.683 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.683 I llama_new_context_with_model: freq_scale    = 1
0.00.055.684 I ggml_metal_init: allocating
0.00.055.689 I ggml_metal_init: found device: Apple M4
0.00.055.692 I ggml_metal_init: picking default device: Apple M4
0.00.056.220 I ggml_metal_init: using embedded metal library
0.00.058.306 I ggml_metal_init: GPU name:   Apple M4
0.00.058.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.310 I ggml_metal_init: simdgroup reduction   = true
0.00.058.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.311 I ggml_metal_init: has bfloat            = true
0.00.058.311 I ggml_metal_init: use bfloat            = true
0.00.058.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.897 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.905 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.923 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.037 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.039 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.039 I llama_new_context_with_model: graph nodes  = 967
0.00.089.039 I llama_new_context_with_model: graph splits = 2
0.00.089.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.378 I main: llama threadpool init, n_threads = 4
0.00.896.414 I 
0.00.896.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.896.429 I 
0.00.896.575 I sampler seed: 1234
0.00.896.579 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.896.588 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.896.590 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.896.590 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.674.110 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.674.110 I llama_perf_context_print:        load time =     884.51 ms
0.01.674.111 I llama_perf_context_print: prompt eval time =      36.71 ms /     7 tokens (    5.24 ms per token,   190.70 tokens per second)
0.01.674.112 I llama_perf_context_print:        eval time =     737.67 ms /    63 runs   (   11.71 ms per token,    85.40 tokens per second)
0.01.674.112 I llama_perf_context_print:       total time =     777.73 ms /    70 tokens
0.01.674.286 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.108s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.421 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.191 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.192 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.197 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.198 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.201 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.205 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.205 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.206 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.605 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.608 I llama_model_loader: - type  f32:  194 tensors
0.00.024.608 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.608 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.443 I llm_load_vocab: special tokens cache size = 25
0.00.050.253 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.256 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.256 I llm_load_print_meta: arch             = gptneox
0.00.050.257 I llm_load_print_meta: vocab type       = BPE
0.00.050.257 I llm_load_print_meta: n_vocab          = 50304
0.00.050.257 I llm_load_print_meta: n_merges         = 50009
0.00.050.257 I llm_load_print_meta: vocab_only       = 0
0.00.050.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.258 I llm_load_print_meta: n_embd           = 2048
0.00.050.258 I llm_load_print_meta: n_layer          = 24
0.00.050.260 I llm_load_print_meta: n_head           = 16
0.00.050.261 I llm_load_print_meta: n_head_kv        = 16
0.00.050.261 I llm_load_print_meta: n_rot            = 32
0.00.050.261 I llm_load_print_meta: n_swa            = 0
0.00.050.261 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.264 I llm_load_print_meta: n_gqa            = 1
0.00.050.265 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.265 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.266 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.266 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.273 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.274 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.274 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.279 I llm_load_print_meta: n_ff             = 8192
0.00.050.279 I llm_load_print_meta: n_expert         = 0
0.00.050.279 I llm_load_print_meta: n_expert_used    = 0
0.00.050.280 I llm_load_print_meta: causal attn      = 1
0.00.050.280 I llm_load_print_meta: pooling type     = 0
0.00.050.280 I llm_load_print_meta: rope type        = 2
0.00.050.280 I llm_load_print_meta: rope scaling     = linear
0.00.050.280 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.281 I llm_load_print_meta: freq_scale_train = 1
0.00.050.281 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.281 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.281 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.282 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.282 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.282 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.282 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.293 I llm_load_print_meta: model type       = 1.4B
0.00.050.294 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.294 I llm_load_print_meta: model params     = 1.41 B
0.00.050.294 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.295 I llm_load_print_meta: general.name     = 1.4B
0.00.050.295 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: LF token         = 128 ''
0.00.050.297 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: max token length = 1024
0.00.051.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.818 I llm_load_tensors: offloading output layer to GPU
0.00.051.818 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.827 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.829 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.637 I llama_new_context_with_model: n_ctx         = 128
0.00.052.637 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.638 I llama_new_context_with_model: n_batch       = 128
0.00.052.638 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.638 I llama_new_context_with_model: flash_attn    = 0
0.00.052.638 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.639 I llama_new_context_with_model: freq_scale    = 1
0.00.052.639 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.639 I ggml_metal_init: allocating
0.00.052.645 I ggml_metal_init: found device: Apple M4
0.00.052.647 I ggml_metal_init: picking default device: Apple M4
0.00.053.202 I ggml_metal_init: using embedded metal library
0.00.055.136 I ggml_metal_init: GPU name:   Apple M4
0.00.055.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.138 I ggml_metal_init: simdgroup reduction   = true
0.00.055.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.139 I ggml_metal_init: has bfloat            = true
0.00.055.139 I ggml_metal_init: use bfloat            = true
0.00.055.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.173 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.186 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.057 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.059 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.059 I llama_new_context_with_model: graph nodes  = 967
0.00.065.059 I llama_new_context_with_model: graph splits = 2
0.00.065.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.225 I 
0.00.820.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.820.253 I perplexity: tokenizing the input ..
0.00.827.659 I perplexity: tokenization took 7.404 ms
0.00.827.663 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.962.278 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.963.363 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.963.380 I llama_perf_context_print:        load time =     809.80 ms
0.00.963.381 I llama_perf_context_print: prompt eval time =     134.39 ms /   128 tokens (    1.05 ms per token,   952.44 tokens per second)
0.00.963.382 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.963.384 I llama_perf_context_print:       total time =     143.16 ms /   129 tokens
0.00.963.688 I ggml_metal_free: deallocating

real	0m0.978s
user	0m0.075s
sys	0m0.182s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.072 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.753 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.478 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.479 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.479 I llama_model_loader: - type  f32:  194 tensors
0.00.024.480 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.480 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.300 I llm_load_vocab: special tokens cache size = 25
0.00.051.290 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.292 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.293 I llm_load_print_meta: arch             = gptneox
0.00.051.293 I llm_load_print_meta: vocab type       = BPE
0.00.051.293 I llm_load_print_meta: n_vocab          = 50304
0.00.051.293 I llm_load_print_meta: n_merges         = 50009
0.00.051.293 I llm_load_print_meta: vocab_only       = 0
0.00.051.294 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.294 I llm_load_print_meta: n_embd           = 2048
0.00.051.294 I llm_load_print_meta: n_layer          = 24
0.00.051.297 I llm_load_print_meta: n_head           = 16
0.00.051.298 I llm_load_print_meta: n_head_kv        = 16
0.00.051.298 I llm_load_print_meta: n_rot            = 32
0.00.051.298 I llm_load_print_meta: n_swa            = 0
0.00.051.298 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.298 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.299 I llm_load_print_meta: n_gqa            = 1
0.00.051.300 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.301 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.302 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.302 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.303 I llm_load_print_meta: n_ff             = 8192
0.00.051.303 I llm_load_print_meta: n_expert         = 0
0.00.051.303 I llm_load_print_meta: n_expert_used    = 0
0.00.051.303 I llm_load_print_meta: causal attn      = 1
0.00.051.304 I llm_load_print_meta: pooling type     = 0
0.00.051.304 I llm_load_print_meta: rope type        = 2
0.00.051.304 I llm_load_print_meta: rope scaling     = linear
0.00.051.304 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.305 I llm_load_print_meta: freq_scale_train = 1
0.00.051.305 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.305 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.305 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.305 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.305 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.306 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.306 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.318 I llm_load_print_meta: model type       = 1.4B
0.00.051.318 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.319 I llm_load_print_meta: model params     = 1.41 B
0.00.051.319 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.319 I llm_load_print_meta: general.name     = 1.4B
0.00.051.320 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.320 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.320 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.320 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.320 I llm_load_print_meta: LF token         = 128 ''
0.00.051.321 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.321 I llm_load_print_meta: max token length = 1024
0.00.053.077 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.078 I llm_load_tensors: offloading output layer to GPU
0.00.053.078 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.088 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.089 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.973 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.973 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.973 I llama_new_context_with_model: n_batch       = 2048
0.00.053.973 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.974 I llama_new_context_with_model: flash_attn    = 0
0.00.053.974 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.974 I llama_new_context_with_model: freq_scale    = 1
0.00.053.975 I ggml_metal_init: allocating
0.00.053.982 I ggml_metal_init: found device: Apple M4
0.00.053.984 I ggml_metal_init: picking default device: Apple M4
0.00.054.518 I ggml_metal_init: using embedded metal library
0.00.056.421 I ggml_metal_init: GPU name:   Apple M4
0.00.056.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.423 I ggml_metal_init: simdgroup reduction   = true
0.00.056.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.423 I ggml_metal_init: has bfloat            = true
0.00.056.424 I ggml_metal_init: use bfloat            = true
0.00.056.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.782 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.801 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.730 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.731 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.731 I llama_new_context_with_model: graph nodes  = 967
0.00.085.732 I llama_new_context_with_model: graph splits = 2
0.00.085.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.549 I main: llama threadpool init, n_threads = 4
0.00.809.583 I 
0.00.809.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.809.604 I 
0.00.809.788 I sampler seed: 1234
0.00.809.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.826 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.826 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.631.963 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.631.964 I llama_perf_context_print:        load time =     799.88 ms
0.01.631.965 I llama_perf_context_print: prompt eval time =      36.55 ms /     7 tokens (    5.22 ms per token,   191.53 tokens per second)
0.01.631.965 I llama_perf_context_print:        eval time =     782.41 ms /    63 runs   (   12.42 ms per token,    80.52 tokens per second)
0.01.631.966 I llama_perf_context_print:       total time =     822.42 ms /    70 tokens
0.01.632.133 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.108s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.991 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.715 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.717 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.718 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.719 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.290 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.292 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.292 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.292 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.293 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.293 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.294 I llama_model_loader: - type  f32:  194 tensors
0.00.023.294 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.294 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.092 I llm_load_vocab: special tokens cache size = 25
0.00.050.213 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.216 I llm_load_print_meta: arch             = gptneox
0.00.050.216 I llm_load_print_meta: vocab type       = BPE
0.00.050.216 I llm_load_print_meta: n_vocab          = 50304
0.00.050.216 I llm_load_print_meta: n_merges         = 50009
0.00.050.217 I llm_load_print_meta: vocab_only       = 0
0.00.050.217 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.217 I llm_load_print_meta: n_embd           = 2048
0.00.050.217 I llm_load_print_meta: n_layer          = 24
0.00.050.219 I llm_load_print_meta: n_head           = 16
0.00.050.220 I llm_load_print_meta: n_head_kv        = 16
0.00.050.221 I llm_load_print_meta: n_rot            = 32
0.00.050.221 I llm_load_print_meta: n_swa            = 0
0.00.050.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.222 I llm_load_print_meta: n_gqa            = 1
0.00.050.223 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.224 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.224 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.224 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.225 I llm_load_print_meta: n_ff             = 8192
0.00.050.225 I llm_load_print_meta: n_expert         = 0
0.00.050.225 I llm_load_print_meta: n_expert_used    = 0
0.00.050.226 I llm_load_print_meta: causal attn      = 1
0.00.050.226 I llm_load_print_meta: pooling type     = 0
0.00.050.226 I llm_load_print_meta: rope type        = 2
0.00.050.226 I llm_load_print_meta: rope scaling     = linear
0.00.050.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.227 I llm_load_print_meta: freq_scale_train = 1
0.00.050.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.227 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.228 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.228 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.229 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.240 I llm_load_print_meta: model type       = 1.4B
0.00.050.240 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.241 I llm_load_print_meta: model params     = 1.41 B
0.00.050.241 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.241 I llm_load_print_meta: general.name     = 1.4B
0.00.050.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: LF token         = 128 ''
0.00.050.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: max token length = 1024
0.00.052.056 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.056 I llm_load_tensors: offloading output layer to GPU
0.00.052.056 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.066 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.067 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.953 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.954 I llama_new_context_with_model: n_ctx         = 128
0.00.052.954 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.954 I llama_new_context_with_model: n_batch       = 128
0.00.052.954 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.955 I llama_new_context_with_model: flash_attn    = 0
0.00.052.955 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.955 I llama_new_context_with_model: freq_scale    = 1
0.00.052.956 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.956 I ggml_metal_init: allocating
0.00.052.959 I ggml_metal_init: found device: Apple M4
0.00.052.961 I ggml_metal_init: picking default device: Apple M4
0.00.053.499 I ggml_metal_init: using embedded metal library
0.00.055.406 I ggml_metal_init: GPU name:   Apple M4
0.00.055.408 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.408 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.409 I ggml_metal_init: simdgroup reduction   = true
0.00.055.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.409 I ggml_metal_init: has bfloat            = true
0.00.055.409 I ggml_metal_init: use bfloat            = true
0.00.055.410 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.411 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.780 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.783 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.797 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.646 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.647 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.647 I llama_new_context_with_model: graph nodes  = 967
0.00.065.647 I llama_new_context_with_model: graph splits = 2
0.00.065.659 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.173 I 
0.00.738.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.738.205 I perplexity: tokenizing the input ..
0.00.745.770 I perplexity: tokenization took 7.564 ms
0.00.745.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.880.417 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.881.551 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.881.561 I llama_perf_context_print:        load time =     729.18 ms
0.00.881.562 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.32 tokens per second)
0.00.881.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.881.563 I llama_perf_context_print:       total time =     143.39 ms /   129 tokens
0.00.881.961 I ggml_metal_free: deallocating

real	0m0.893s
user	0m0.077s
sys	0m0.172s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.691 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.697 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.701 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.703 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.706 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.706 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.064 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.064 I llama_model_loader: - type  f32:  194 tensors
0.00.023.065 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.065 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.031 I llm_load_vocab: special tokens cache size = 25
0.00.049.007 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.010 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.011 I llm_load_print_meta: arch             = gptneox
0.00.049.011 I llm_load_print_meta: vocab type       = BPE
0.00.049.011 I llm_load_print_meta: n_vocab          = 50304
0.00.049.011 I llm_load_print_meta: n_merges         = 50009
0.00.049.012 I llm_load_print_meta: vocab_only       = 0
0.00.049.012 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.012 I llm_load_print_meta: n_embd           = 2048
0.00.049.012 I llm_load_print_meta: n_layer          = 24
0.00.049.015 I llm_load_print_meta: n_head           = 16
0.00.049.015 I llm_load_print_meta: n_head_kv        = 16
0.00.049.016 I llm_load_print_meta: n_rot            = 32
0.00.049.016 I llm_load_print_meta: n_swa            = 0
0.00.049.016 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.017 I llm_load_print_meta: n_gqa            = 1
0.00.049.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.018 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.019 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.019 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.019 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.019 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.020 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.020 I llm_load_print_meta: n_ff             = 8192
0.00.049.020 I llm_load_print_meta: n_expert         = 0
0.00.049.021 I llm_load_print_meta: n_expert_used    = 0
0.00.049.021 I llm_load_print_meta: causal attn      = 1
0.00.049.021 I llm_load_print_meta: pooling type     = 0
0.00.049.021 I llm_load_print_meta: rope type        = 2
0.00.049.021 I llm_load_print_meta: rope scaling     = linear
0.00.049.022 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.022 I llm_load_print_meta: freq_scale_train = 1
0.00.049.022 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.022 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.023 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.023 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.023 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.024 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.024 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.035 I llm_load_print_meta: model type       = 1.4B
0.00.049.035 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.036 I llm_load_print_meta: model params     = 1.41 B
0.00.049.037 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.038 I llm_load_print_meta: general.name     = 1.4B
0.00.049.038 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.038 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.038 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.038 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.039 I llm_load_print_meta: LF token         = 128 ''
0.00.049.039 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.039 I llm_load_print_meta: max token length = 1024
0.00.050.779 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.780 I llm_load_tensors: offloading output layer to GPU
0.00.050.780 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.789 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.791 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.636 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.636 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.636 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.637 I llama_new_context_with_model: n_batch       = 2048
0.00.051.637 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.637 I llama_new_context_with_model: flash_attn    = 0
0.00.051.637 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.638 I llama_new_context_with_model: freq_scale    = 1
0.00.051.638 I ggml_metal_init: allocating
0.00.051.641 I ggml_metal_init: found device: Apple M4
0.00.051.643 I ggml_metal_init: picking default device: Apple M4
0.00.052.184 I ggml_metal_init: using embedded metal library
0.00.054.102 I ggml_metal_init: GPU name:   Apple M4
0.00.054.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.105 I ggml_metal_init: simdgroup reduction   = true
0.00.054.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.105 I ggml_metal_init: has bfloat            = true
0.00.054.105 I ggml_metal_init: use bfloat            = true
0.00.054.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.013 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.019 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.037 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.954 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.081.955 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.081.955 I llama_new_context_with_model: graph nodes  = 967
0.00.081.956 I llama_new_context_with_model: graph splits = 2
0.00.081.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.337 I main: llama threadpool init, n_threads = 4
0.00.504.368 I 
0.00.504.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.504.392 I 
0.00.504.547 I sampler seed: 1234
0.00.504.551 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.562 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.562 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.562 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.180.950 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.180.951 I llama_perf_context_print:        load time =     494.98 ms
0.01.180.952 I llama_perf_context_print: prompt eval time =      35.73 ms /     7 tokens (    5.10 ms per token,   195.90 tokens per second)
0.01.180.952 I llama_perf_context_print:        eval time =     637.44 ms /    63 runs   (   10.12 ms per token,    98.83 tokens per second)
0.01.180.953 I llama_perf_context_print:       total time =     676.62 ms /    70 tokens
0.01.181.112 I ggml_metal_free: deallocating

real	0m1.198s
user	0m0.107s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.339 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.896 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.898 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.899 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.899 I llama_model_loader: - type  f32:  194 tensors
0.00.023.899 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.900 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.900 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.420 I llm_load_vocab: special tokens cache size = 25
0.00.050.182 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.185 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.185 I llm_load_print_meta: arch             = gptneox
0.00.050.185 I llm_load_print_meta: vocab type       = BPE
0.00.050.186 I llm_load_print_meta: n_vocab          = 50304
0.00.050.186 I llm_load_print_meta: n_merges         = 50009
0.00.050.186 I llm_load_print_meta: vocab_only       = 0
0.00.050.186 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.186 I llm_load_print_meta: n_embd           = 2048
0.00.050.186 I llm_load_print_meta: n_layer          = 24
0.00.050.189 I llm_load_print_meta: n_head           = 16
0.00.050.189 I llm_load_print_meta: n_head_kv        = 16
0.00.050.190 I llm_load_print_meta: n_rot            = 32
0.00.050.190 I llm_load_print_meta: n_swa            = 0
0.00.050.190 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.190 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.191 I llm_load_print_meta: n_gqa            = 1
0.00.050.192 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.192 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.193 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.193 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.194 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.194 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.195 I llm_load_print_meta: n_ff             = 8192
0.00.050.195 I llm_load_print_meta: n_expert         = 0
0.00.050.195 I llm_load_print_meta: n_expert_used    = 0
0.00.050.195 I llm_load_print_meta: causal attn      = 1
0.00.050.195 I llm_load_print_meta: pooling type     = 0
0.00.050.195 I llm_load_print_meta: rope type        = 2
0.00.050.196 I llm_load_print_meta: rope scaling     = linear
0.00.050.196 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.196 I llm_load_print_meta: freq_scale_train = 1
0.00.050.196 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.197 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.197 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.197 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.197 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.197 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.198 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.209 I llm_load_print_meta: model type       = 1.4B
0.00.050.209 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.209 I llm_load_print_meta: model params     = 1.41 B
0.00.050.210 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.210 I llm_load_print_meta: general.name     = 1.4B
0.00.050.210 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.210 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.210 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.210 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.211 I llm_load_print_meta: LF token         = 128 ''
0.00.050.211 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.211 I llm_load_print_meta: max token length = 1024
0.00.051.961 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.961 I llm_load_tensors: offloading output layer to GPU
0.00.051.961 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.971 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.972 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.884 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.885 I llama_new_context_with_model: n_ctx         = 128
0.00.052.886 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.886 I llama_new_context_with_model: n_batch       = 128
0.00.052.886 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.886 I llama_new_context_with_model: flash_attn    = 0
0.00.052.886 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.887 I llama_new_context_with_model: freq_scale    = 1
0.00.052.887 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.887 I ggml_metal_init: allocating
0.00.052.891 I ggml_metal_init: found device: Apple M4
0.00.052.893 I ggml_metal_init: picking default device: Apple M4
0.00.053.441 I ggml_metal_init: using embedded metal library
0.00.055.322 I ggml_metal_init: GPU name:   Apple M4
0.00.055.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.325 I ggml_metal_init: simdgroup reduction   = true
0.00.055.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.325 I ggml_metal_init: has bfloat            = true
0.00.055.325 I ggml_metal_init: use bfloat            = true
0.00.055.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.578 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.581 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.471 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.472 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.472 I llama_new_context_with_model: graph nodes  = 967
0.00.065.472 I llama_new_context_with_model: graph splits = 2
0.00.065.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.430.763 I 
0.00.430.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.430.797 I perplexity: tokenizing the input ..
0.00.438.510 I perplexity: tokenization took 7.712 ms
0.00.438.513 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.570.387 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.571.527 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.571.540 I llama_perf_context_print:        load time =     420.93 ms
0.00.571.541 I llama_perf_context_print: prompt eval time =     131.65 ms /   128 tokens (    1.03 ms per token,   972.25 tokens per second)
0.00.571.542 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.571.542 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.571.829 I ggml_metal_free: deallocating

real	0m0.586s
user	0m0.077s
sys	0m0.092s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.252 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.985 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.986 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.740 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.467 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.468 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.469 I llama_model_loader: - type  f32:  194 tensors
0.00.024.469 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.469 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.469 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.470 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.573 I llm_load_vocab: special tokens cache size = 25
0.00.050.576 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.579 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.580 I llm_load_print_meta: arch             = gptneox
0.00.050.580 I llm_load_print_meta: vocab type       = BPE
0.00.050.580 I llm_load_print_meta: n_vocab          = 50304
0.00.050.580 I llm_load_print_meta: n_merges         = 50009
0.00.050.581 I llm_load_print_meta: vocab_only       = 0
0.00.050.581 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.581 I llm_load_print_meta: n_embd           = 2048
0.00.050.581 I llm_load_print_meta: n_layer          = 24
0.00.050.584 I llm_load_print_meta: n_head           = 16
0.00.050.587 I llm_load_print_meta: n_head_kv        = 16
0.00.050.587 I llm_load_print_meta: n_rot            = 32
0.00.050.587 I llm_load_print_meta: n_swa            = 0
0.00.050.588 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.588 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.588 I llm_load_print_meta: n_gqa            = 1
0.00.050.589 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.591 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.591 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.592 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.592 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.592 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.593 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.594 I llm_load_print_meta: n_ff             = 8192
0.00.050.594 I llm_load_print_meta: n_expert         = 0
0.00.050.595 I llm_load_print_meta: n_expert_used    = 0
0.00.050.595 I llm_load_print_meta: causal attn      = 1
0.00.050.595 I llm_load_print_meta: pooling type     = 0
0.00.050.595 I llm_load_print_meta: rope type        = 2
0.00.050.595 I llm_load_print_meta: rope scaling     = linear
0.00.050.596 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.596 I llm_load_print_meta: freq_scale_train = 1
0.00.050.596 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.596 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.596 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.596 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.597 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.597 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.600 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.611 I llm_load_print_meta: model type       = 1.4B
0.00.050.612 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.612 I llm_load_print_meta: model params     = 1.41 B
0.00.050.613 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.613 I llm_load_print_meta: general.name     = 1.4B
0.00.050.613 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: LF token         = 128 ''
0.00.050.614 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: max token length = 1024
0.00.052.447 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.447 I llm_load_tensors: offloading output layer to GPU
0.00.052.447 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.457 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.458 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.309 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.310 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.310 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.310 I llama_new_context_with_model: n_batch       = 2048
0.00.053.311 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.311 I llama_new_context_with_model: flash_attn    = 0
0.00.053.312 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.312 I llama_new_context_with_model: freq_scale    = 1
0.00.053.313 I ggml_metal_init: allocating
0.00.053.316 I ggml_metal_init: found device: Apple M4
0.00.053.318 I ggml_metal_init: picking default device: Apple M4
0.00.053.885 I ggml_metal_init: using embedded metal library
0.00.055.765 I ggml_metal_init: GPU name:   Apple M4
0.00.055.766 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.766 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.767 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.767 I ggml_metal_init: simdgroup reduction   = true
0.00.055.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.767 I ggml_metal_init: has bfloat            = true
0.00.055.767 I ggml_metal_init: use bfloat            = true
0.00.055.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.768 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.402 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.409 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.427 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.336 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.337 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.337 I llama_new_context_with_model: graph nodes  = 967
0.00.083.337 I llama_new_context_with_model: graph splits = 2
0.00.083.350 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.981 I main: llama threadpool init, n_threads = 4
0.00.612.013 I 
0.00.612.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.612.030 I 
0.00.612.199 I sampler seed: 1234
0.00.612.204 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.232 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.232 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.234 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.347.312 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.347.312 I llama_perf_context_print:        load time =     602.73 ms
0.01.347.313 I llama_perf_context_print: prompt eval time =      35.54 ms /     7 tokens (    5.08 ms per token,   196.98 tokens per second)
0.01.347.314 I llama_perf_context_print:        eval time =     696.57 ms /    63 runs   (   11.06 ms per token,    90.44 tokens per second)
0.01.347.314 I llama_perf_context_print:       total time =     735.33 ms /    70 tokens
0.01.347.495 I ggml_metal_free: deallocating

real	0m1.361s
user	0m0.107s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.213 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.910 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.914 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.917 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.920 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.922 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.922 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.373 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.375 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.375 I llama_model_loader: - type  f32:  194 tensors
0.00.023.375 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.376 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.376 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.376 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.216 I llm_load_vocab: special tokens cache size = 25
0.00.049.238 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.241 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.241 I llm_load_print_meta: arch             = gptneox
0.00.049.242 I llm_load_print_meta: vocab type       = BPE
0.00.049.242 I llm_load_print_meta: n_vocab          = 50304
0.00.049.242 I llm_load_print_meta: n_merges         = 50009
0.00.049.242 I llm_load_print_meta: vocab_only       = 0
0.00.049.242 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.243 I llm_load_print_meta: n_embd           = 2048
0.00.049.243 I llm_load_print_meta: n_layer          = 24
0.00.049.245 I llm_load_print_meta: n_head           = 16
0.00.049.246 I llm_load_print_meta: n_head_kv        = 16
0.00.049.246 I llm_load_print_meta: n_rot            = 32
0.00.049.246 I llm_load_print_meta: n_swa            = 0
0.00.049.246 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.248 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.248 I llm_load_print_meta: n_gqa            = 1
0.00.049.249 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.250 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.250 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.251 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.251 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.252 I llm_load_print_meta: n_ff             = 8192
0.00.049.252 I llm_load_print_meta: n_expert         = 0
0.00.049.252 I llm_load_print_meta: n_expert_used    = 0
0.00.049.252 I llm_load_print_meta: causal attn      = 1
0.00.049.253 I llm_load_print_meta: pooling type     = 0
0.00.049.253 I llm_load_print_meta: rope type        = 2
0.00.049.253 I llm_load_print_meta: rope scaling     = linear
0.00.049.253 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.254 I llm_load_print_meta: freq_scale_train = 1
0.00.049.254 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.254 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.254 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.254 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.255 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.255 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.266 I llm_load_print_meta: model type       = 1.4B
0.00.049.268 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.268 I llm_load_print_meta: model params     = 1.41 B
0.00.049.268 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.269 I llm_load_print_meta: general.name     = 1.4B
0.00.049.269 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.269 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.269 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.269 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.270 I llm_load_print_meta: LF token         = 128 ''
0.00.049.270 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.270 I llm_load_print_meta: max token length = 1024
0.00.051.030 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.030 I llm_load_tensors: offloading output layer to GPU
0.00.051.030 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.040 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.041 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.951 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.951 I llama_new_context_with_model: n_ctx         = 128
0.00.051.952 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.952 I llama_new_context_with_model: n_batch       = 128
0.00.051.952 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.952 I llama_new_context_with_model: flash_attn    = 0
0.00.051.952 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.953 I llama_new_context_with_model: freq_scale    = 1
0.00.051.953 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.953 I ggml_metal_init: allocating
0.00.051.957 I ggml_metal_init: found device: Apple M4
0.00.051.958 I ggml_metal_init: picking default device: Apple M4
0.00.052.480 I ggml_metal_init: using embedded metal library
0.00.054.361 I ggml_metal_init: GPU name:   Apple M4
0.00.054.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.364 I ggml_metal_init: simdgroup reduction   = true
0.00.054.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.364 I ggml_metal_init: has bfloat            = true
0.00.054.364 I ggml_metal_init: use bfloat            = true
0.00.054.365 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.331 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.334 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.347 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.223 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.224 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.224 I llama_new_context_with_model: graph nodes  = 967
0.00.064.225 I llama_new_context_with_model: graph splits = 2
0.00.064.237 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.857 I 
0.00.568.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.568.886 I perplexity: tokenizing the input ..
0.00.575.884 I perplexity: tokenization took 6.996 ms
0.00.575.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.408 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.708.606 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.708.616 I llama_perf_context_print:        load time =     559.64 ms
0.00.708.617 I llama_perf_context_print: prompt eval time =     131.30 ms /   128 tokens (    1.03 ms per token,   974.88 tokens per second)
0.00.708.618 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.708.618 I llama_perf_context_print:       total time =     139.76 ms /   129 tokens
0.00.708.884 I ggml_metal_free: deallocating

real	0m0.720s
user	0m0.074s
sys	0m0.119s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.449 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.826 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.639 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.639 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.640 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.640 I llama_model_loader: - type  f32:  194 tensors
0.00.023.641 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.641 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.641 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.471 I llm_load_vocab: special tokens cache size = 25
0.00.050.334 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.336 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.337 I llm_load_print_meta: arch             = gptneox
0.00.050.337 I llm_load_print_meta: vocab type       = BPE
0.00.050.337 I llm_load_print_meta: n_vocab          = 50304
0.00.050.337 I llm_load_print_meta: n_merges         = 50009
0.00.050.338 I llm_load_print_meta: vocab_only       = 0
0.00.050.338 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.338 I llm_load_print_meta: n_embd           = 2048
0.00.050.338 I llm_load_print_meta: n_layer          = 24
0.00.050.340 I llm_load_print_meta: n_head           = 16
0.00.050.341 I llm_load_print_meta: n_head_kv        = 16
0.00.050.343 I llm_load_print_meta: n_rot            = 32
0.00.050.344 I llm_load_print_meta: n_swa            = 0
0.00.050.344 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.344 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.345 I llm_load_print_meta: n_gqa            = 1
0.00.050.346 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.346 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.347 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.347 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.348 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.348 I llm_load_print_meta: n_ff             = 8192
0.00.050.349 I llm_load_print_meta: n_expert         = 0
0.00.050.349 I llm_load_print_meta: n_expert_used    = 0
0.00.050.349 I llm_load_print_meta: causal attn      = 1
0.00.050.349 I llm_load_print_meta: pooling type     = 0
0.00.050.349 I llm_load_print_meta: rope type        = 2
0.00.050.349 I llm_load_print_meta: rope scaling     = linear
0.00.050.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.350 I llm_load_print_meta: freq_scale_train = 1
0.00.050.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.351 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.351 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.351 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.351 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.351 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.358 I llm_load_print_meta: model type       = 1.4B
0.00.050.358 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.358 I llm_load_print_meta: model params     = 1.41 B
0.00.050.360 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.360 I llm_load_print_meta: general.name     = 1.4B
0.00.050.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.361 I llm_load_print_meta: LF token         = 128 ''
0.00.050.361 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.362 I llm_load_print_meta: max token length = 1024
0.00.052.124 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.124 I llm_load_tensors: offloading output layer to GPU
0.00.052.125 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.129 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.129 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.974 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.975 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.975 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.975 I llama_new_context_with_model: n_batch       = 2048
0.00.052.975 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.976 I llama_new_context_with_model: flash_attn    = 0
0.00.052.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.976 I llama_new_context_with_model: freq_scale    = 1
0.00.052.977 I ggml_metal_init: allocating
0.00.052.984 I ggml_metal_init: found device: Apple M4
0.00.052.987 I ggml_metal_init: picking default device: Apple M4
0.00.053.528 I ggml_metal_init: using embedded metal library
0.00.055.425 I ggml_metal_init: GPU name:   Apple M4
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.428 I ggml_metal_init: simdgroup reduction   = true
0.00.055.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.428 I ggml_metal_init: has bfloat            = true
0.00.055.428 I ggml_metal_init: use bfloat            = true
0.00.055.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.802 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.811 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.830 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.713 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.714 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.714 I llama_new_context_with_model: graph nodes  = 967
0.00.083.714 I llama_new_context_with_model: graph splits = 2
0.00.083.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.137 I main: llama threadpool init, n_threads = 4
0.00.713.170 I 
0.00.713.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.713.194 I 
0.00.713.347 I sampler seed: 1234
0.00.713.353 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.366 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.368 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.368 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.454.169 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.454.170 I llama_perf_context_print:        load time =     704.68 ms
0.01.454.171 I llama_perf_context_print: prompt eval time =      36.35 ms /     7 tokens (    5.19 ms per token,   192.59 tokens per second)
0.01.454.172 I llama_perf_context_print:        eval time =     701.29 ms /    63 runs   (   11.13 ms per token,    89.83 tokens per second)
0.01.454.172 I llama_perf_context_print:       total time =     741.03 ms /    70 tokens
0.01.454.355 I ggml_metal_free: deallocating

real	0m1.467s
user	0m0.108s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.749 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.754 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.756 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.758 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.759 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.760 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.305 I llama_model_loader: - type  f32:  194 tensors
0.00.024.305 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.305 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.305 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.367 I llm_load_vocab: special tokens cache size = 25
0.00.051.458 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.461 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.462 I llm_load_print_meta: arch             = gptneox
0.00.051.462 I llm_load_print_meta: vocab type       = BPE
0.00.051.462 I llm_load_print_meta: n_vocab          = 50304
0.00.051.462 I llm_load_print_meta: n_merges         = 50009
0.00.051.463 I llm_load_print_meta: vocab_only       = 0
0.00.051.463 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.463 I llm_load_print_meta: n_embd           = 2048
0.00.051.463 I llm_load_print_meta: n_layer          = 24
0.00.051.466 I llm_load_print_meta: n_head           = 16
0.00.051.466 I llm_load_print_meta: n_head_kv        = 16
0.00.051.467 I llm_load_print_meta: n_rot            = 32
0.00.051.469 I llm_load_print_meta: n_swa            = 0
0.00.051.469 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.470 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.470 I llm_load_print_meta: n_gqa            = 1
0.00.051.471 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.472 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.472 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.473 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.473 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.474 I llm_load_print_meta: n_ff             = 8192
0.00.051.474 I llm_load_print_meta: n_expert         = 0
0.00.051.474 I llm_load_print_meta: n_expert_used    = 0
0.00.051.474 I llm_load_print_meta: causal attn      = 1
0.00.051.474 I llm_load_print_meta: pooling type     = 0
0.00.051.475 I llm_load_print_meta: rope type        = 2
0.00.051.480 I llm_load_print_meta: rope scaling     = linear
0.00.051.481 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.481 I llm_load_print_meta: freq_scale_train = 1
0.00.051.481 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.482 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.482 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.482 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.482 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.482 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.482 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.493 I llm_load_print_meta: model type       = 1.4B
0.00.051.494 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.494 I llm_load_print_meta: model params     = 1.41 B
0.00.051.495 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.495 I llm_load_print_meta: general.name     = 1.4B
0.00.051.495 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: LF token         = 128 ''
0.00.051.496 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.497 I llm_load_print_meta: max token length = 1024
0.00.053.274 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.275 I llm_load_tensors: offloading output layer to GPU
0.00.053.275 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.284 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.286 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.192 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.193 I llama_new_context_with_model: n_ctx         = 128
0.00.054.193 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.193 I llama_new_context_with_model: n_batch       = 128
0.00.054.193 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.193 I llama_new_context_with_model: flash_attn    = 0
0.00.054.194 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.194 I llama_new_context_with_model: freq_scale    = 1
0.00.054.194 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.195 I ggml_metal_init: allocating
0.00.054.198 I ggml_metal_init: found device: Apple M4
0.00.054.200 I ggml_metal_init: picking default device: Apple M4
0.00.054.761 I ggml_metal_init: using embedded metal library
0.00.056.682 I ggml_metal_init: GPU name:   Apple M4
0.00.056.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.684 I ggml_metal_init: simdgroup reduction   = true
0.00.056.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.685 I ggml_metal_init: has bfloat            = true
0.00.056.686 I ggml_metal_init: use bfloat            = true
0.00.056.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.071 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.075 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.088 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.961 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.962 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.962 I llama_new_context_with_model: graph nodes  = 967
0.00.066.963 I llama_new_context_with_model: graph splits = 2
0.00.066.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.939 I 
0.00.668.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.668.968 I perplexity: tokenizing the input ..
0.00.676.328 I perplexity: tokenization took 7.359 ms
0.00.676.331 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.249 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.811.473 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.811.488 I llama_perf_context_print:        load time =     659.18 ms
0.00.811.489 I llama_perf_context_print: prompt eval time =     133.70 ms /   128 tokens (    1.04 ms per token,   957.37 tokens per second)
0.00.811.492 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.492 I llama_perf_context_print:       total time =     142.55 ms /   129 tokens
0.00.811.882 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.077s
sys	0m0.155s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.690 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.038 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.048 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.052 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.054 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.617 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.618 I llama_model_loader: - type  f32:  194 tensors
0.00.024.618 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.618 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.348 I llm_load_vocab: special tokens cache size = 25
0.00.051.230 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.233 I llm_load_print_meta: arch             = gptneox
0.00.051.233 I llm_load_print_meta: vocab type       = BPE
0.00.051.233 I llm_load_print_meta: n_vocab          = 50304
0.00.051.234 I llm_load_print_meta: n_merges         = 50009
0.00.051.234 I llm_load_print_meta: vocab_only       = 0
0.00.051.234 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.234 I llm_load_print_meta: n_embd           = 2048
0.00.051.234 I llm_load_print_meta: n_layer          = 24
0.00.051.237 I llm_load_print_meta: n_head           = 16
0.00.051.237 I llm_load_print_meta: n_head_kv        = 16
0.00.051.238 I llm_load_print_meta: n_rot            = 32
0.00.051.238 I llm_load_print_meta: n_swa            = 0
0.00.051.240 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.240 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.241 I llm_load_print_meta: n_gqa            = 1
0.00.051.242 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.246 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.247 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.247 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.248 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.248 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.249 I llm_load_print_meta: n_ff             = 8192
0.00.051.249 I llm_load_print_meta: n_expert         = 0
0.00.051.249 I llm_load_print_meta: n_expert_used    = 0
0.00.051.249 I llm_load_print_meta: causal attn      = 1
0.00.051.249 I llm_load_print_meta: pooling type     = 0
0.00.051.251 I llm_load_print_meta: rope type        = 2
0.00.051.251 I llm_load_print_meta: rope scaling     = linear
0.00.051.252 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.252 I llm_load_print_meta: freq_scale_train = 1
0.00.051.252 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.252 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.252 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.253 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.253 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.253 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.253 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.259 I llm_load_print_meta: model type       = 1.4B
0.00.051.260 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.260 I llm_load_print_meta: model params     = 1.41 B
0.00.051.260 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.261 I llm_load_print_meta: general.name     = 1.4B
0.00.051.261 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.261 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.261 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.261 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.261 I llm_load_print_meta: LF token         = 128 ''
0.00.051.262 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: max token length = 1024
0.00.052.959 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.959 I llm_load_tensors: offloading output layer to GPU
0.00.052.959 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.964 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.964 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.787 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.787 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.788 I llama_new_context_with_model: n_batch       = 2048
0.00.053.788 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.788 I llama_new_context_with_model: flash_attn    = 0
0.00.053.788 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.789 I llama_new_context_with_model: freq_scale    = 1
0.00.053.789 I ggml_metal_init: allocating
0.00.053.795 I ggml_metal_init: found device: Apple M4
0.00.053.798 I ggml_metal_init: picking default device: Apple M4
0.00.054.328 I ggml_metal_init: using embedded metal library
0.00.056.207 I ggml_metal_init: GPU name:   Apple M4
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.209 I ggml_metal_init: simdgroup reduction   = true
0.00.056.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.210 I ggml_metal_init: has bfloat            = true
0.00.056.210 I ggml_metal_init: use bfloat            = true
0.00.056.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.218 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.225 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.243 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.150 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.151 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.151 I llama_new_context_with_model: graph nodes  = 967
0.00.086.152 I llama_new_context_with_model: graph splits = 2
0.00.086.164 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.009 I main: llama threadpool init, n_threads = 4
0.00.799.039 I 
0.00.799.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.799.058 I 
0.00.799.215 I sampler seed: 1234
0.00.799.220 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.251 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.253 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.253 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.626.826 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.626.827 I llama_perf_context_print:        load time =     789.32 ms
0.01.626.828 I llama_perf_context_print: prompt eval time =      38.45 ms /     7 tokens (    5.49 ms per token,   182.03 tokens per second)
0.01.626.828 I llama_perf_context_print:        eval time =     785.89 ms /    63 runs   (   12.47 ms per token,    80.16 tokens per second)
0.01.626.830 I llama_perf_context_print:       total time =     827.82 ms /    70 tokens
0.01.626.997 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.611 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.468 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.481 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.259 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.069 I llama_model_loader: - type  f32:  194 tensors
0.00.025.069 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.070 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.888 I llm_load_vocab: special tokens cache size = 25
0.00.050.900 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.903 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.903 I llm_load_print_meta: arch             = gptneox
0.00.050.903 I llm_load_print_meta: vocab type       = BPE
0.00.050.904 I llm_load_print_meta: n_vocab          = 50304
0.00.050.904 I llm_load_print_meta: n_merges         = 50009
0.00.050.904 I llm_load_print_meta: vocab_only       = 0
0.00.050.904 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.904 I llm_load_print_meta: n_embd           = 2048
0.00.050.904 I llm_load_print_meta: n_layer          = 24
0.00.050.907 I llm_load_print_meta: n_head           = 16
0.00.050.908 I llm_load_print_meta: n_head_kv        = 16
0.00.050.908 I llm_load_print_meta: n_rot            = 32
0.00.050.908 I llm_load_print_meta: n_swa            = 0
0.00.050.909 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.909 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.910 I llm_load_print_meta: n_gqa            = 1
0.00.050.910 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.911 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.911 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.912 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.912 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.912 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.912 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.913 I llm_load_print_meta: n_ff             = 8192
0.00.050.913 I llm_load_print_meta: n_expert         = 0
0.00.050.913 I llm_load_print_meta: n_expert_used    = 0
0.00.050.913 I llm_load_print_meta: causal attn      = 1
0.00.050.916 I llm_load_print_meta: pooling type     = 0
0.00.050.916 I llm_load_print_meta: rope type        = 2
0.00.050.916 I llm_load_print_meta: rope scaling     = linear
0.00.050.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.917 I llm_load_print_meta: freq_scale_train = 1
0.00.050.917 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.917 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.919 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.931 I llm_load_print_meta: model type       = 1.4B
0.00.050.931 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.932 I llm_load_print_meta: model params     = 1.41 B
0.00.050.932 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.932 I llm_load_print_meta: general.name     = 1.4B
0.00.050.933 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: LF token         = 128 ''
0.00.050.933 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: max token length = 1024
0.00.052.767 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.767 I llm_load_tensors: offloading output layer to GPU
0.00.052.768 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.777 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.778 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.635 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.635 I llama_new_context_with_model: n_ctx         = 128
0.00.053.636 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.636 I llama_new_context_with_model: n_batch       = 128
0.00.053.636 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.636 I llama_new_context_with_model: flash_attn    = 0
0.00.053.636 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.637 I llama_new_context_with_model: freq_scale    = 1
0.00.053.637 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.638 I ggml_metal_init: allocating
0.00.053.643 I ggml_metal_init: found device: Apple M4
0.00.053.645 I ggml_metal_init: picking default device: Apple M4
0.00.054.190 I ggml_metal_init: using embedded metal library
0.00.056.169 I ggml_metal_init: GPU name:   Apple M4
0.00.056.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.171 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.172 I ggml_metal_init: simdgroup reduction   = true
0.00.056.172 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.172 I ggml_metal_init: has bfloat            = true
0.00.056.172 I ggml_metal_init: use bfloat            = true
0.00.056.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.161 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.165 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.190 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.041 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.042 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.043 I llama_new_context_with_model: graph nodes  = 967
0.00.066.043 I llama_new_context_with_model: graph splits = 2
0.00.066.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.310 I 
0.00.736.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.736.340 I perplexity: tokenizing the input ..
0.00.743.974 I perplexity: tokenization took 7.633 ms
0.00.743.978 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.566 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.885.709 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.885.723 I llama_perf_context_print:        load time =     725.70 ms
0.00.885.724 I llama_perf_context_print: prompt eval time =     140.37 ms /   128 tokens (    1.10 ms per token,   911.86 tokens per second)
0.00.885.725 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.885.725 I llama_perf_context_print:       total time =     149.41 ms /   129 tokens
0.00.886.102 I ggml_metal_free: deallocating

real	0m0.900s
user	0m0.076s
sys	0m0.172s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.138 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.143 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.891 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.897 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.616 I llama_model_loader: - type  f32:  194 tensors
0.00.024.616 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.742 I llm_load_vocab: special tokens cache size = 25
0.00.050.713 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.715 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.716 I llm_load_print_meta: arch             = gptneox
0.00.050.716 I llm_load_print_meta: vocab type       = BPE
0.00.050.716 I llm_load_print_meta: n_vocab          = 50304
0.00.050.717 I llm_load_print_meta: n_merges         = 50009
0.00.050.717 I llm_load_print_meta: vocab_only       = 0
0.00.050.717 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.717 I llm_load_print_meta: n_embd           = 2048
0.00.050.717 I llm_load_print_meta: n_layer          = 24
0.00.050.720 I llm_load_print_meta: n_head           = 16
0.00.050.721 I llm_load_print_meta: n_head_kv        = 16
0.00.050.721 I llm_load_print_meta: n_rot            = 32
0.00.050.721 I llm_load_print_meta: n_swa            = 0
0.00.050.723 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.723 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.726 I llm_load_print_meta: n_gqa            = 1
0.00.050.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.727 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.728 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.728 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.730 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.730 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.730 I llm_load_print_meta: n_ff             = 8192
0.00.050.730 I llm_load_print_meta: n_expert         = 0
0.00.050.731 I llm_load_print_meta: n_expert_used    = 0
0.00.050.731 I llm_load_print_meta: causal attn      = 1
0.00.050.731 I llm_load_print_meta: pooling type     = 0
0.00.050.731 I llm_load_print_meta: rope type        = 2
0.00.050.731 I llm_load_print_meta: rope scaling     = linear
0.00.050.732 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.732 I llm_load_print_meta: freq_scale_train = 1
0.00.050.732 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.732 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.733 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.733 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.734 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.734 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.734 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.746 I llm_load_print_meta: model type       = 1.4B
0.00.050.746 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.746 I llm_load_print_meta: model params     = 1.41 B
0.00.050.747 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.747 I llm_load_print_meta: general.name     = 1.4B
0.00.050.747 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.747 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.748 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.748 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.748 I llm_load_print_meta: LF token         = 128 ''
0.00.050.748 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.748 I llm_load_print_meta: max token length = 1024
0.00.052.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.512 I llm_load_tensors: offloading output layer to GPU
0.00.052.513 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.523 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.524 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.743 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.744 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.745 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.745 I llama_new_context_with_model: n_batch       = 2048
0.00.053.745 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.745 I llama_new_context_with_model: flash_attn    = 0
0.00.053.746 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.746 I llama_new_context_with_model: freq_scale    = 1
0.00.053.746 I ggml_metal_init: allocating
0.00.053.752 I ggml_metal_init: found device: Apple M4
0.00.053.755 I ggml_metal_init: picking default device: Apple M4
0.00.054.321 I ggml_metal_init: using embedded metal library
0.00.056.248 I ggml_metal_init: GPU name:   Apple M4
0.00.056.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.250 I ggml_metal_init: simdgroup reduction   = true
0.00.056.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.251 I ggml_metal_init: has bfloat            = true
0.00.056.251 I ggml_metal_init: use bfloat            = true
0.00.056.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.203 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.221 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.114 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.116 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.116 I llama_new_context_with_model: graph nodes  = 967
0.00.084.116 I llama_new_context_with_model: graph splits = 2
0.00.084.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.864.645 I main: llama threadpool init, n_threads = 4
0.00.864.716 I 
0.00.864.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.864.735 I 
0.00.864.880 I sampler seed: 1234
0.00.864.885 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.864.895 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.864.895 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.864.895 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.720.373 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.720.374 I llama_perf_context_print:        load time =     854.91 ms
0.01.720.374 I llama_perf_context_print: prompt eval time =      38.43 ms /     7 tokens (    5.49 ms per token,   182.15 tokens per second)
0.01.720.375 I llama_perf_context_print:        eval time =     813.85 ms /    63 runs   (   12.92 ms per token,    77.41 tokens per second)
0.01.720.375 I llama_perf_context_print:       total time =     855.73 ms /    70 tokens
0.01.720.566 I ggml_metal_free: deallocating

real	0m1.734s
user	0m0.107s
sys	0m0.232s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4149 (1bb30bf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.726 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.511 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.515 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.515 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.517 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.971 I llama_model_loader: - type  f32:  194 tensors
0.00.023.972 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.775 I llm_load_vocab: special tokens cache size = 25
0.00.049.714 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.717 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.717 I llm_load_print_meta: arch             = gptneox
0.00.049.717 I llm_load_print_meta: vocab type       = BPE
0.00.049.717 I llm_load_print_meta: n_vocab          = 50304
0.00.049.718 I llm_load_print_meta: n_merges         = 50009
0.00.049.718 I llm_load_print_meta: vocab_only       = 0
0.00.049.718 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.718 I llm_load_print_meta: n_embd           = 2048
0.00.049.718 I llm_load_print_meta: n_layer          = 24
0.00.049.721 I llm_load_print_meta: n_head           = 16
0.00.049.722 I llm_load_print_meta: n_head_kv        = 16
0.00.049.722 I llm_load_print_meta: n_rot            = 32
0.00.049.722 I llm_load_print_meta: n_swa            = 0
0.00.049.722 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.723 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.723 I llm_load_print_meta: n_gqa            = 1
0.00.049.724 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.725 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.725 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.725 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.726 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.726 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.726 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.727 I llm_load_print_meta: n_ff             = 8192
0.00.049.727 I llm_load_print_meta: n_expert         = 0
0.00.049.727 I llm_load_print_meta: n_expert_used    = 0
0.00.049.727 I llm_load_print_meta: causal attn      = 1
0.00.049.727 I llm_load_print_meta: pooling type     = 0
0.00.049.728 I llm_load_print_meta: rope type        = 2
0.00.049.728 I llm_load_print_meta: rope scaling     = linear
0.00.049.728 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.728 I llm_load_print_meta: freq_scale_train = 1
0.00.049.730 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.730 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.732 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.732 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.732 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.732 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.743 I llm_load_print_meta: model type       = 1.4B
0.00.049.743 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.744 I llm_load_print_meta: model params     = 1.41 B
0.00.049.744 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.744 I llm_load_print_meta: general.name     = 1.4B
0.00.049.744 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.744 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: LF token         = 128 ''
0.00.049.745 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.745 I llm_load_print_meta: max token length = 1024
0.00.051.490 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.491 I llm_load_tensors: offloading output layer to GPU
0.00.051.491 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.500 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.501 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.351 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.351 I llama_new_context_with_model: n_ctx         = 128
0.00.052.352 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.352 I llama_new_context_with_model: n_batch       = 128
0.00.052.352 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.352 I llama_new_context_with_model: flash_attn    = 0
0.00.052.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.353 I llama_new_context_with_model: freq_scale    = 1
0.00.052.353 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.354 I ggml_metal_init: allocating
0.00.052.357 I ggml_metal_init: found device: Apple M4
0.00.052.359 I ggml_metal_init: picking default device: Apple M4
0.00.052.890 I ggml_metal_init: using embedded metal library
0.00.054.782 I ggml_metal_init: GPU name:   Apple M4
0.00.054.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.784 I ggml_metal_init: simdgroup reduction   = true
0.00.054.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.785 I ggml_metal_init: has bfloat            = true
0.00.054.785 I ggml_metal_init: use bfloat            = true
0.00.054.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.746 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.748 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.761 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.599 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.600 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.600 I llama_new_context_with_model: graph nodes  = 967
0.00.064.601 I llama_new_context_with_model: graph splits = 2
0.00.064.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.896 I 
0.00.744.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.744.931 I perplexity: tokenizing the input ..
0.00.752.434 I perplexity: tokenization took 7.503 ms
0.00.752.438 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.891.966 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.893.039 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.893.055 I llama_perf_context_print:        load time =     735.17 ms
0.00.893.056 I llama_perf_context_print: prompt eval time =     139.30 ms /   128 tokens (    1.09 ms per token,   918.87 tokens per second)
0.00.893.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.893.058 I llama_perf_context_print:       total time =     148.16 ms /   129 tokens
0.00.893.385 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.075s
sys	0m0.171s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4149 (1bb30bf2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ab0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ab0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ab0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ab0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ab0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ab0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ab0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ab0da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ab0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ab0e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ab0ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ab0ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ab0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ab101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ab109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ab11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ab11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ab11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ab12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ab12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ab13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ab13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ab143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ab14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ab15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ab15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ab15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ab168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ab16de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ab170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ab17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ab17800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ab18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ab185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ab18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ab18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ab191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ab19670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ab19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ab19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ab1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ab1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ab1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ab1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ab1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ab1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ab1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ab1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ab1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ab1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ab1dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ab1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ab1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ab1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ab1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ab1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ab1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ab20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ab20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ab21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ab21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ab217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ab21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ab22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ab225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ab22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ab22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ab233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ab23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ab23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ab24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ab24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ab24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ab24f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ab25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ab258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ab25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ab261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ab26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ab26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ab26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ab27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ab27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ab27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ab28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ab286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ab28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ab29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ab294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ab29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ab29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ab2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ab2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ab2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ab2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ab2b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ab2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ab1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ab2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ab2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ab2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ab2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ab2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ab2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ab2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ab2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ab2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ab2e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ab2ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ab2f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ab2f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ab2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ab300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ab30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ab30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ab30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ab31350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ab317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ab31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ab32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ab325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ab32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ab32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ab333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ab33850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ab33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ab34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ab34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ab34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ab34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ab35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ab358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ab35d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ab361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ab36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ab36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ab36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ab37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ab37910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ab37db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ab38250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ab386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ab38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ab39030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ab394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ab39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ab39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ab3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ab3a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ab3abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ab3b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ab3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ab3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ab3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ab3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ab3c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ab3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ab3d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ab3d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ab3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ab3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ab3ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ab3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ab3f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ab3fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ab40150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ab405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ab40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ab412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ab41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ab41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ab422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ab42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ab42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ab432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ab43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ab43d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ab442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ab44810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ab44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ab452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ab45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ab45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ab462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ab467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ab46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ab47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ab477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ab47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ab48280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ab487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ab48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ab49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ab497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ab49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ab4a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ab4a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ab4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ab4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ab4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ab4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ab4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ab4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ab4cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ab4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ab4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ab4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ab4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ab4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ab4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ab4f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ab4f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ab4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ab50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ab50750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ab50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ab511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ab51740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ab51c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ab521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ab52730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ab52c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ab531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ab53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ab53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ab54060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ab54500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ab549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ab54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ab552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ab55780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ab55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ab560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ab56560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ab56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ab56ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ab57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ab57890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ab57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ab586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ab58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ab59510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ab597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ab59de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ab5a3f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.152.553 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ab4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ab4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ab4b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ab4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ab4bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ab4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ab4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ab4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ab4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ab4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ab4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ab4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ab4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ab4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ab4f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ab4fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ab502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ab50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ab51080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ab51a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ab520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ab527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ab52ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ab535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ab53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ab54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ab54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ab54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ab54e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ab552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ab55750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ab55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ab56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ab562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ab56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ab56bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ab57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ab574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ab57920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ab57d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ab58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ab58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ab58ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ab58f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ab593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ab59830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ab59ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ab5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ab5a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ab0c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ab0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ab0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ab0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ab0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ab185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ab18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ab18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ab19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ab195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ab19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ab19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ab1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ab1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ab1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ab1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ab1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ab1b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ab1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ab1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ab1c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ab1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ab1cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ab1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ab1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ab1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ab1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ab1e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ab1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ab1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ab1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ab1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ab1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ab20050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ab204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ab20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ab20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ab21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ab21680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ab21af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ab21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ab223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ab22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ab22cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ab23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ab23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ab23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ab23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ab242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ab24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ab24bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ab25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ac05010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ac05480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ac058f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ac05d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ac061d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ac06640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ac06ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ac06f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ac07390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ac07800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ac07c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ac080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ac08550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ac089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ac08e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ac092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ac09710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ac09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ac09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ac0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ac0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ac0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ac0b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ac0b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ac0ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ac0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ac0c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ac0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ac0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ac0d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ac0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ac0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ac0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ac0e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ac0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ac0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ac0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ac0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ac0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ac0fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ac10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ac10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ac10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ac10ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ac11350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ac117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ac11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ac120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ac12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ac12980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ac12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ac13260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ac136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ac13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ac13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ac14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ac14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ac14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ac15170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ac155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ac15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ac15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ac16330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ac167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ac16c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ac17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ac17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ac17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ac181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ac18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ac18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ac18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ac19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ac19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ac19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ac1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ac1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ac1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ac1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ac1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ac1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ac1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ac1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ac1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ac1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ac1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ac1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ac1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ac1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ac1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ac1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ac1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ac1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ac1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ac1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ac1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ac1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ac20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ac206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ac20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ac20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ac21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ac218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ac21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ac22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ac22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ac22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ac22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ac23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ac237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ac23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ac240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ac24510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ac24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ac24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ac25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ac256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ac25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ac25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ac26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ac26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ac26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ac27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ac275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ac27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ac27ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ac28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ac287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ac28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ac29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ac294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ac29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ac29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ac2a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ac2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ac2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ac2af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ac2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ac2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ac2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ac2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ac2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ac2d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ac2da20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ac05470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ac058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ac05d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ac061c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ac06630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ac06aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ac06f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ac07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ac077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ac07c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ac080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ac086b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ac08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ac09720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ac09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ac0a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ac0ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ac0b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ac0bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ac0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ac0cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ac0d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ac0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ac0e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ac0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ac0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ac0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ac0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ac0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ac0fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ac10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ac10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ac10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ac10d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ac111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ac11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ac11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ac11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ac12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ac127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ac12c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ac130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ac13520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ac13990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ac13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ac14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ac146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ac14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ac14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ac15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ac158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ac15d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ac16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ac165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ac16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ac16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ac17340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ac177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ac17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ac18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ac18500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ac18970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ac18de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ac19250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ac196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ac19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ac19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ac1a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ac1a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ac1acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ac1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ac1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ac1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ac1beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ac1c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ac1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ac1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ac1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ac1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ac1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ac1ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ac1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ac1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ac1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ac1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ac1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ac1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ac1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ac20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ac205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ac20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ac20e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ac21300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ac21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ac21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ac22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ac224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ac22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ac22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ac23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ac23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ac23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ac23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ac243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ac24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ac24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ac25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ac25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ac25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ac25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ac262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ac26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ac26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ac27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ac274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ac27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ac27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ac281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ac28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ac28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ac28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ac293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ac29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ac29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ac2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ac2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ac2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ac2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ac2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ac2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ac2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ac2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ac2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ac2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ac2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ac2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ac2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ac2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ac2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ac2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ac2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ac2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ac2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ac2f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ac2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ac30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ac30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ac30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ac31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ac314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ac31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ac31e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ac322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ac32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ac32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ac332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ac33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ac33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ac340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ac346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ac34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ac35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ac35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ac36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ac365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ac36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ac36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ac376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ac37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ac38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ac38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ac38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ac39130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ac39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ac39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ac3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ac3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ac3abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ac3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ac3b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ac3bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ac3c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ac3c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ac3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ac3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ac3d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ac3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ac3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ac3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ac3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ac3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ac3f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ac3fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ac400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ac40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ac40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ac410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ac41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ac41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ac420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ac425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ac42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ac43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ac435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ac43b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ac44080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ac445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ac44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ac45070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ac455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ac45b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ac46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ac465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ac46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ac47050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ac475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ac47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ac48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ac48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ac48ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ac49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ac49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ac49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ac4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ac4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ac4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ac4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ac4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ac4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ac4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ac4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ac4c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ac4c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ac4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ac4d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ac4d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ac4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ac4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ac4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ac4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ac4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ac4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ac500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ac506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ac50cf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.939s
user	0m0.316s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4149 (1bb30bf2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e10bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e11170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e11720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e12280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e12830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e12de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e13390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e14290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e1c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e21490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e22fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e25600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e25c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e28280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e29500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e2a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e2a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e2ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e2b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e2c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e2cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e2f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e2fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e30d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e31390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e31830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e31cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e32f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e33890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e33d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e34b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e34fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e35d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e36b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e37010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e38290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e38730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e39070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e39e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e3a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e3ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e3beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e3cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e3d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e3df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e3e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e3ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e3f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e40410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e40d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e41d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e42550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e43780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e45970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e47660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e48ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e4c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e4d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e50b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e52060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e54040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e55ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e56570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e57010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e58f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e59880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e59d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e5ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e5afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e5b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e5b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e5bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134e5c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134e5c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134e5cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134e5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134e5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134e5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134e5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134e5eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134e5f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134e5f770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.084.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136804b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136805000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136805470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1368058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136805d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1368061c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136806630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136806aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136806f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136807380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1368077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136807ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136808a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1368091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1368099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13680a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13680a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13680af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13680b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13680bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13680c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13680cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13680d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13680d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13680e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13680e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13680e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13680eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13680ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13680f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13680f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13680fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1368101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1368104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136810920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136810d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136811200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136811670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136811ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136811f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1368123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136812830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136812ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136813110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136813580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1368139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136813e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1368142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136814740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136815020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136815490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136815900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136815d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1368161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136816650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136816bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1368170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136817530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1368179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136817e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136818280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1368186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136818b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136818fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136819440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1368198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136819d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13681a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13681a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13681aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13681aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13681b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13681b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13681bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13681c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13681c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13681c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13681cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13681d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13681d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13681db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13681dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13681e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13681e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13681ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13681f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13681f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13681fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13681fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136820330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1368207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136820c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1368214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136821dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136822240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1368226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136822b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136823400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136823870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136823ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136824150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1368245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136824ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136825310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1368264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136826940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136827220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136827f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1368283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136828850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136829130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1368295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136829a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136829e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13682a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13682a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13682abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13682b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13682b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13682b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13682bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13682c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13682c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13682cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13682cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13682d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13682d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13682dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13682e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13682e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13682e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13682ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13682f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13682f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13682fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136830490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136830900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136830d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1368311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136831650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136831ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136831f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1368323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136832810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136832c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1368330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1368339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1368342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136834720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136834b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136835000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136835470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1368362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136836580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1368369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136836e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1368372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136837740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136837bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136838020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136838490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136838900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136838d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1368391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136839650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136839ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136839f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13683a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13683a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13683ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13683b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13683b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13683b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13683be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13683c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13683c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13683cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13683d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13683d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13683d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13683dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13683e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13683e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13683eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13683ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13683f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13683f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13683fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1368400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136840540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1368409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136840e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136841290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136841700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136841b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136841fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136842450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1368428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136842d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1368431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136843610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136843a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136843ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136844360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1368447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136844c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1368450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136845520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136845990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136845e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136846270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1368466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136846b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136846fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136847430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1368478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136847d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136848180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1368485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136848a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136848ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136849340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136849e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13684a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13684acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13684b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13684b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13684b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13684bdd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e11170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e12270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e51730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e52d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e54250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e56790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e56e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e58920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e59200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e59da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e5a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e5b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e5b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e5c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e5ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e5d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e5e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e5ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e5f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e5f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e23670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e3c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e3df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e42a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e4d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134e1c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134e1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134e1cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134e13850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134e13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134e14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134e14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134e15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134e15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134e15a70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.921s
user	0m0.237s
sys	0m0.115s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.53 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.14 real         0.71 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.27 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.16 user         0.04 sys
```
