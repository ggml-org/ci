Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.544s
user	0m0.882s
sys	0m1.227s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Built target llava
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Built target test-arg-parser
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target test-rope
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-bench
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-merge
[ 82%] Generating loading.html.hpp
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Built target llama-cli
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-parallel
[ 84%] Generating index.html.gz.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-run
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tokenize
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.030s
user	0m5.953s
sys	0m9.204s

main: quantize time =  3369.43 ms
main:    total time =  3369.43 ms

main: quantize time =  1858.45 ms
main:    total time =  1858.45 ms

main: quantize time =  1611.17 ms
main:    total time =  1611.17 ms

main: quantize time =  2013.46 ms
main:    total time =  2013.46 ms

main: quantize time =  2608.17 ms
main:    total time =  2608.17 ms

main: quantize time =  5301.61 ms
main:    total time =  5301.61 ms

main: quantize time =  5790.59 ms
main:    total time =  5790.59 ms

main: quantize time =  7057.02 ms
main:    total time =  7057.02 ms

main: quantize time =  5830.56 ms
main:    total time =  5830.56 ms

main: quantize time =  4768.68 ms
main:    total time =  4768.68 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.178 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.291 I main: llama backend init
0.00.000.296 I main: load the model and apply lora adapter, if any
0.00.032.840 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.321 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.396 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.398 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.398 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.399 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.400 I llama_model_loader: - type  f32:  194 tensors
0.00.064.401 I llama_model_loader: - type  f16:   98 tensors
0.00.064.402 I print_info: file format = GGUF V3 (latest)
0.00.064.419 I print_info: file type   = all F32 (guessed)
0.00.064.421 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.949 I load: special tokens cache size = 25
0.00.098.441 I load: token to piece cache size = 0.2984 MB
0.00.098.463 I print_info: arch             = gptneox
0.00.098.464 I print_info: vocab_only       = 0
0.00.098.464 I print_info: n_ctx_train      = 2048
0.00.098.464 I print_info: n_embd           = 2048
0.00.098.465 I print_info: n_layer          = 24
0.00.098.468 I print_info: n_head           = 16
0.00.098.468 I print_info: n_head_kv        = 16
0.00.098.468 I print_info: n_rot            = 32
0.00.098.469 I print_info: n_swa            = 0
0.00.098.469 I print_info: n_embd_head_k    = 128
0.00.098.469 I print_info: n_embd_head_v    = 128
0.00.098.470 I print_info: n_gqa            = 1
0.00.098.471 I print_info: n_embd_k_gqa     = 2048
0.00.098.471 I print_info: n_embd_v_gqa     = 2048
0.00.098.472 I print_info: f_norm_eps       = 1.0e-05
0.00.098.472 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.098.472 I print_info: f_clamp_kqv      = 0.0e+00
0.00.098.473 I print_info: f_max_alibi_bias = 0.0e+00
0.00.098.473 I print_info: f_logit_scale    = 0.0e+00
0.00.098.473 I print_info: n_ff             = 8192
0.00.098.474 I print_info: n_expert         = 0
0.00.098.474 I print_info: n_expert_used    = 0
0.00.098.474 I print_info: causal attn      = 1
0.00.098.474 I print_info: pooling type     = 0
0.00.098.474 I print_info: rope type        = 2
0.00.098.474 I print_info: rope scaling     = linear
0.00.098.475 I print_info: freq_base_train  = 10000.0
0.00.098.477 I print_info: freq_scale_train = 1
0.00.098.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.098.477 I print_info: rope_finetuned   = unknown
0.00.098.477 I print_info: ssm_d_conv       = 0
0.00.098.477 I print_info: ssm_d_inner      = 0
0.00.098.477 I print_info: ssm_d_state      = 0
0.00.098.477 I print_info: ssm_dt_rank      = 0
0.00.098.477 I print_info: ssm_dt_b_c_rms   = 0
0.00.098.479 I print_info: model type       = 1.4B
0.00.098.479 I print_info: model params     = 1.41 B
0.00.098.479 I print_info: general.name     = 1.4B
0.00.098.480 I print_info: vocab type       = BPE
0.00.098.480 I print_info: n_vocab          = 50304
0.00.098.480 I print_info: n_merges         = 50009
0.00.098.480 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.098.480 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.098.481 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.098.481 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.098.481 I print_info: LF token         = 128 'Ä'
0.00.098.481 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.098.481 I print_info: max token length = 1024
0.00.101.056 I load_tensors: offloading 24 repeating layers to GPU
0.00.101.057 I load_tensors: offloading output layer to GPU
0.00.101.057 I load_tensors: offloaded 25/25 layers to GPU
0.00.101.075 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.076 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.101.362 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.363 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.363 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.363 I llama_new_context_with_model: n_batch       = 2048
0.00.101.363 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.363 I llama_new_context_with_model: flash_attn    = 0
0.00.101.364 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.364 I llama_new_context_with_model: freq_scale    = 1
0.00.101.365 I ggml_metal_init: allocating
0.00.101.368 I ggml_metal_init: found device: Apple M4
0.00.101.370 I ggml_metal_init: picking default device: Apple M4
0.00.102.018 I ggml_metal_init: using embedded metal library
0.00.113.689 I ggml_metal_init: GPU name:   Apple M4
0.00.113.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.691 I ggml_metal_init: simdgroup reduction   = true
0.00.113.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.692 I ggml_metal_init: has bfloat            = true
0.00.113.692 I ggml_metal_init: use bfloat            = true
0.00.113.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.137.122 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.157.510 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.517 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.539 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.492 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.494 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.495 I llama_new_context_with_model: graph nodes  = 967
0.00.158.495 I llama_new_context_with_model: graph splits = 2
0.00.158.498 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.158.625 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.779 I main: llama threadpool init, n_threads = 4
0.00.239.823 I 
0.00.239.847 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.239.847 I 
0.00.239.918 I sampler seed: 1234
0.00.239.923 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.239.959 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.239.961 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.239.961 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.084.538 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.02.084.539 I llama_perf_context_print:        load time =     206.93 ms
0.02.084.540 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.57 tokens per second)
0.02.084.540 I llama_perf_context_print:        eval time =    1798.01 ms /    63 runs   (   28.54 ms per token,    35.04 tokens per second)
0.02.084.542 I llama_perf_context_print:       total time =    1844.76 ms /    70 tokens
0.02.084.796 I ggml_metal_free: deallocating

real	0m2.377s
user	0m0.141s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.664 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.274 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.287 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.291 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.293 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.293 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.522 I llama_model_loader: - type  f32:  194 tensors
0.00.038.523 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.524 I print_info: file format = GGUF V3 (latest)
0.00.038.539 I print_info: file type   = Q8_0
0.00.038.542 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.137 I load: special tokens cache size = 25
0.00.066.339 I load: token to piece cache size = 0.2984 MB
0.00.066.355 I print_info: arch             = gptneox
0.00.066.356 I print_info: vocab_only       = 0
0.00.066.357 I print_info: n_ctx_train      = 2048
0.00.066.357 I print_info: n_embd           = 2048
0.00.066.357 I print_info: n_layer          = 24
0.00.066.363 I print_info: n_head           = 16
0.00.066.364 I print_info: n_head_kv        = 16
0.00.066.364 I print_info: n_rot            = 32
0.00.066.364 I print_info: n_swa            = 0
0.00.066.364 I print_info: n_embd_head_k    = 128
0.00.066.366 I print_info: n_embd_head_v    = 128
0.00.066.367 I print_info: n_gqa            = 1
0.00.066.367 I print_info: n_embd_k_gqa     = 2048
0.00.066.368 I print_info: n_embd_v_gqa     = 2048
0.00.066.368 I print_info: f_norm_eps       = 1.0e-05
0.00.066.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.369 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.369 I print_info: f_logit_scale    = 0.0e+00
0.00.066.370 I print_info: n_ff             = 8192
0.00.066.370 I print_info: n_expert         = 0
0.00.066.371 I print_info: n_expert_used    = 0
0.00.066.371 I print_info: causal attn      = 1
0.00.066.371 I print_info: pooling type     = 0
0.00.066.371 I print_info: rope type        = 2
0.00.066.372 I print_info: rope scaling     = linear
0.00.066.372 I print_info: freq_base_train  = 10000.0
0.00.066.372 I print_info: freq_scale_train = 1
0.00.066.372 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.373 I print_info: rope_finetuned   = unknown
0.00.066.373 I print_info: ssm_d_conv       = 0
0.00.066.373 I print_info: ssm_d_inner      = 0
0.00.066.373 I print_info: ssm_d_state      = 0
0.00.066.373 I print_info: ssm_dt_rank      = 0
0.00.066.373 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.374 I print_info: model type       = 1.4B
0.00.066.374 I print_info: model params     = 1.41 B
0.00.066.374 I print_info: general.name     = 1.4B
0.00.066.375 I print_info: vocab type       = BPE
0.00.066.375 I print_info: n_vocab          = 50304
0.00.066.375 I print_info: n_merges         = 50009
0.00.066.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.376 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.376 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.376 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.376 I print_info: LF token         = 128 'Ä'
0.00.066.377 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.377 I print_info: max token length = 1024
0.00.068.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.864 I load_tensors: offloading output layer to GPU
0.00.068.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.876 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.877 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.209 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.210 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.210 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.210 I llama_new_context_with_model: n_batch       = 2048
0.00.069.210 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.210 I llama_new_context_with_model: flash_attn    = 0
0.00.069.211 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.211 I llama_new_context_with_model: freq_scale    = 1
0.00.069.212 I ggml_metal_init: allocating
0.00.069.215 I ggml_metal_init: found device: Apple M4
0.00.069.217 I ggml_metal_init: picking default device: Apple M4
0.00.069.967 I ggml_metal_init: using embedded metal library
0.00.072.750 I ggml_metal_init: GPU name:   Apple M4
0.00.072.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.753 I ggml_metal_init: simdgroup reduction   = true
0.00.072.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.753 I ggml_metal_init: has bfloat            = true
0.00.072.753 I ggml_metal_init: use bfloat            = true
0.00.072.753 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.817 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.016 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.024 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.344 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.346 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.347 I llama_new_context_with_model: graph nodes  = 967
0.00.109.347 I llama_new_context_with_model: graph splits = 2
0.00.109.351 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.119.811 I main: llama threadpool init, n_threads = 4
0.01.119.844 I 
0.01.119.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.119.865 I 
0.01.120.141 I sampler seed: 1234
0.01.120.145 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.120.188 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.120.189 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.120.190 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.211.104 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61418.69 tokens per second)
0.02.211.104 I llama_perf_context_print:        load time =    1110.14 ms
0.02.211.106 I llama_perf_context_print: prompt eval time =      39.95 ms /     7 tokens (    5.71 ms per token,   175.24 tokens per second)
0.02.211.106 I llama_perf_context_print:        eval time =    1048.10 ms /    63 runs   (   16.64 ms per token,    60.11 tokens per second)
0.02.211.107 I llama_perf_context_print:       total time =    1091.30 ms /    70 tokens
0.02.211.327 I ggml_metal_free: deallocating

real	0m2.228s
user	0m0.115s
sys	0m0.214s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.016.448 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.895 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.899 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.750 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.371 I llama_model_loader: - type  f32:  194 tensors
0.00.042.371 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.371 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.372 I print_info: file format = GGUF V3 (latest)
0.00.042.380 I print_info: file type   = Q4_0
0.00.042.381 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.074.504 I load: special tokens cache size = 25
0.00.085.418 I load: token to piece cache size = 0.2984 MB
0.00.085.436 I print_info: arch             = gptneox
0.00.085.437 I print_info: vocab_only       = 0
0.00.085.437 I print_info: n_ctx_train      = 2048
0.00.085.438 I print_info: n_embd           = 2048
0.00.085.438 I print_info: n_layer          = 24
0.00.085.443 I print_info: n_head           = 16
0.00.085.445 I print_info: n_head_kv        = 16
0.00.085.445 I print_info: n_rot            = 32
0.00.085.445 I print_info: n_swa            = 0
0.00.085.445 I print_info: n_embd_head_k    = 128
0.00.085.446 I print_info: n_embd_head_v    = 128
0.00.085.447 I print_info: n_gqa            = 1
0.00.085.448 I print_info: n_embd_k_gqa     = 2048
0.00.085.449 I print_info: n_embd_v_gqa     = 2048
0.00.085.450 I print_info: f_norm_eps       = 1.0e-05
0.00.085.450 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.450 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.451 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.451 I print_info: f_logit_scale    = 0.0e+00
0.00.085.452 I print_info: n_ff             = 8192
0.00.085.452 I print_info: n_expert         = 0
0.00.085.452 I print_info: n_expert_used    = 0
0.00.085.452 I print_info: causal attn      = 1
0.00.085.453 I print_info: pooling type     = 0
0.00.085.453 I print_info: rope type        = 2
0.00.085.453 I print_info: rope scaling     = linear
0.00.085.454 I print_info: freq_base_train  = 10000.0
0.00.085.454 I print_info: freq_scale_train = 1
0.00.085.454 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.454 I print_info: rope_finetuned   = unknown
0.00.085.455 I print_info: ssm_d_conv       = 0
0.00.085.455 I print_info: ssm_d_inner      = 0
0.00.085.455 I print_info: ssm_d_state      = 0
0.00.085.455 I print_info: ssm_dt_rank      = 0
0.00.085.455 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.456 I print_info: model type       = 1.4B
0.00.085.459 I print_info: model params     = 1.41 B
0.00.085.459 I print_info: general.name     = 1.4B
0.00.085.460 I print_info: vocab type       = BPE
0.00.085.460 I print_info: n_vocab          = 50304
0.00.085.460 I print_info: n_merges         = 50009
0.00.085.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.461 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.461 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.462 I print_info: LF token         = 128 'Ä'
0.00.085.462 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.463 I print_info: max token length = 1024
0.00.088.515 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.516 I load_tensors: offloading output layer to GPU
0.00.088.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.530 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.532 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.089.117 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.118 I llama_new_context_with_model: n_ctx         = 2048
0.00.089.119 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.089.119 I llama_new_context_with_model: n_batch       = 2048
0.00.089.119 I llama_new_context_with_model: n_ubatch      = 512
0.00.089.120 I llama_new_context_with_model: flash_attn    = 0
0.00.089.120 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.120 I llama_new_context_with_model: freq_scale    = 1
0.00.089.121 I ggml_metal_init: allocating
0.00.089.125 I ggml_metal_init: found device: Apple M4
0.00.089.128 I ggml_metal_init: picking default device: Apple M4
0.00.090.076 I ggml_metal_init: using embedded metal library
0.00.093.941 I ggml_metal_init: GPU name:   Apple M4
0.00.093.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.945 I ggml_metal_init: simdgroup reduction   = true
0.00.093.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.945 I ggml_metal_init: has bfloat            = true
0.00.093.945 I ggml_metal_init: use bfloat            = true
0.00.093.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.713 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.130.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.031 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.063 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.114 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.115 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.115 I llama_new_context_with_model: graph nodes  = 967
0.00.131.116 I llama_new_context_with_model: graph splits = 2
0.00.131.120 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.131.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.131.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.258 I main: llama threadpool init, n_threads = 4
0.00.690.324 I 
0.00.690.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.355 I 
0.00.690.685 I sampler seed: 1234
0.00.690.694 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.751 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.751 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.751 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.373.379 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.373.380 I llama_perf_context_print:        load time =     673.80 ms
0.01.373.381 I llama_perf_context_print: prompt eval time =      44.78 ms /     7 tokens (    6.40 ms per token,   156.33 tokens per second)
0.01.373.381 I llama_perf_context_print:        eval time =     634.85 ms /    63 runs   (   10.08 ms per token,    99.24 tokens per second)
0.01.373.382 I llama_perf_context_print:       total time =     683.13 ms /    70 tokens
0.01.373.609 I ggml_metal_free: deallocating

real	0m1.410s
user	0m0.141s
sys	0m0.172s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.627 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.654 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.662 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.667 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.244 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.247 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.248 I llama_model_loader: - type  f32:  194 tensors
0.00.027.248 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.248 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.249 I print_info: file format = GGUF V3 (latest)
0.00.027.261 I print_info: file type   = Q4_1
0.00.027.262 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.426 I load: special tokens cache size = 25
0.00.052.157 I load: token to piece cache size = 0.2984 MB
0.00.052.171 I print_info: arch             = gptneox
0.00.052.173 I print_info: vocab_only       = 0
0.00.052.173 I print_info: n_ctx_train      = 2048
0.00.052.173 I print_info: n_embd           = 2048
0.00.052.173 I print_info: n_layer          = 24
0.00.052.176 I print_info: n_head           = 16
0.00.052.177 I print_info: n_head_kv        = 16
0.00.052.177 I print_info: n_rot            = 32
0.00.052.177 I print_info: n_swa            = 0
0.00.052.177 I print_info: n_embd_head_k    = 128
0.00.052.178 I print_info: n_embd_head_v    = 128
0.00.052.178 I print_info: n_gqa            = 1
0.00.052.179 I print_info: n_embd_k_gqa     = 2048
0.00.052.182 I print_info: n_embd_v_gqa     = 2048
0.00.052.182 I print_info: f_norm_eps       = 1.0e-05
0.00.052.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.184 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.184 I print_info: f_logit_scale    = 0.0e+00
0.00.052.184 I print_info: n_ff             = 8192
0.00.052.185 I print_info: n_expert         = 0
0.00.052.185 I print_info: n_expert_used    = 0
0.00.052.186 I print_info: causal attn      = 1
0.00.052.187 I print_info: pooling type     = 0
0.00.052.187 I print_info: rope type        = 2
0.00.052.188 I print_info: rope scaling     = linear
0.00.052.188 I print_info: freq_base_train  = 10000.0
0.00.052.189 I print_info: freq_scale_train = 1
0.00.052.189 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.189 I print_info: rope_finetuned   = unknown
0.00.052.189 I print_info: ssm_d_conv       = 0
0.00.052.189 I print_info: ssm_d_inner      = 0
0.00.052.189 I print_info: ssm_d_state      = 0
0.00.052.190 I print_info: ssm_dt_rank      = 0
0.00.052.190 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.191 I print_info: model type       = 1.4B
0.00.052.191 I print_info: model params     = 1.41 B
0.00.052.191 I print_info: general.name     = 1.4B
0.00.052.191 I print_info: vocab type       = BPE
0.00.052.192 I print_info: n_vocab          = 50304
0.00.052.192 I print_info: n_merges         = 50009
0.00.052.192 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.192 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.192 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.192 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.194 I print_info: LF token         = 128 'Ä'
0.00.052.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.194 I print_info: max token length = 1024
0.00.054.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.134 I load_tensors: offloading output layer to GPU
0.00.054.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.145 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.147 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.431 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.432 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.432 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.433 I llama_new_context_with_model: n_batch       = 2048
0.00.054.433 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.433 I llama_new_context_with_model: flash_attn    = 0
0.00.054.433 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.434 I llama_new_context_with_model: freq_scale    = 1
0.00.054.434 I ggml_metal_init: allocating
0.00.054.437 I ggml_metal_init: found device: Apple M4
0.00.054.439 I ggml_metal_init: picking default device: Apple M4
0.00.055.043 I ggml_metal_init: using embedded metal library
0.00.057.428 I ggml_metal_init: GPU name:   Apple M4
0.00.057.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.430 I ggml_metal_init: simdgroup reduction   = true
0.00.057.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.430 I ggml_metal_init: has bfloat            = true
0.00.057.430 I ggml_metal_init: use bfloat            = true
0.00.057.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.259 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.262 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.273 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.292 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.321 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.322 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.323 I llama_new_context_with_model: graph nodes  = 967
0.00.089.323 I llama_new_context_with_model: graph splits = 2
0.00.089.326 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.924.853 I main: llama threadpool init, n_threads = 4
0.00.924.913 I 
0.00.924.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.924.979 I 
0.00.925.271 I sampler seed: 1234
0.00.925.277 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.925.308 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.925.310 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.925.310 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.674.531 I llama_perf_sampler_print:    sampling time =       1.63 ms /    71 runs   (    0.02 ms per token, 43531.58 tokens per second)
0.01.674.532 I llama_perf_context_print:        load time =     916.22 ms
0.01.674.532 I llama_perf_context_print: prompt eval time =      49.31 ms /     7 tokens (    7.04 ms per token,   141.96 tokens per second)
0.01.674.533 I llama_perf_context_print:        eval time =     696.34 ms /    63 runs   (   11.05 ms per token,    90.47 tokens per second)
0.01.674.533 I llama_perf_context_print:       total time =     749.68 ms /    70 tokens
0.01.674.754 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.128s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.607 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.030.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.478 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.479 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.479 I llama_model_loader: - type  f32:  194 tensors
0.00.039.479 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.480 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.480 I print_info: file format = GGUF V3 (latest)
0.00.039.492 I print_info: file type   = Q5_0
0.00.039.493 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.063.092 I load: special tokens cache size = 25
0.00.070.560 I load: token to piece cache size = 0.2984 MB
0.00.070.575 I print_info: arch             = gptneox
0.00.070.576 I print_info: vocab_only       = 0
0.00.070.576 I print_info: n_ctx_train      = 2048
0.00.070.576 I print_info: n_embd           = 2048
0.00.070.576 I print_info: n_layer          = 24
0.00.070.579 I print_info: n_head           = 16
0.00.070.580 I print_info: n_head_kv        = 16
0.00.070.580 I print_info: n_rot            = 32
0.00.070.580 I print_info: n_swa            = 0
0.00.070.580 I print_info: n_embd_head_k    = 128
0.00.070.581 I print_info: n_embd_head_v    = 128
0.00.070.581 I print_info: n_gqa            = 1
0.00.070.585 I print_info: n_embd_k_gqa     = 2048
0.00.070.585 I print_info: n_embd_v_gqa     = 2048
0.00.070.586 I print_info: f_norm_eps       = 1.0e-05
0.00.070.586 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.587 I print_info: f_logit_scale    = 0.0e+00
0.00.070.587 I print_info: n_ff             = 8192
0.00.070.588 I print_info: n_expert         = 0
0.00.070.589 I print_info: n_expert_used    = 0
0.00.070.589 I print_info: causal attn      = 1
0.00.070.589 I print_info: pooling type     = 0
0.00.070.589 I print_info: rope type        = 2
0.00.070.589 I print_info: rope scaling     = linear
0.00.070.590 I print_info: freq_base_train  = 10000.0
0.00.070.591 I print_info: freq_scale_train = 1
0.00.070.591 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.592 I print_info: rope_finetuned   = unknown
0.00.070.592 I print_info: ssm_d_conv       = 0
0.00.070.592 I print_info: ssm_d_inner      = 0
0.00.070.592 I print_info: ssm_d_state      = 0
0.00.070.592 I print_info: ssm_dt_rank      = 0
0.00.070.592 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.592 I print_info: model type       = 1.4B
0.00.070.593 I print_info: model params     = 1.41 B
0.00.070.593 I print_info: general.name     = 1.4B
0.00.070.593 I print_info: vocab type       = BPE
0.00.070.599 I print_info: n_vocab          = 50304
0.00.070.600 I print_info: n_merges         = 50009
0.00.070.601 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.601 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.601 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.602 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.602 I print_info: LF token         = 128 'Ä'
0.00.070.602 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.602 I print_info: max token length = 1024
0.00.072.879 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.879 I load_tensors: offloading output layer to GPU
0.00.072.879 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.890 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.072.891 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.073.231 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.232 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.232 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.232 I llama_new_context_with_model: n_batch       = 2048
0.00.073.233 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.233 I llama_new_context_with_model: flash_attn    = 0
0.00.073.233 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.234 I llama_new_context_with_model: freq_scale    = 1
0.00.073.234 I ggml_metal_init: allocating
0.00.073.237 I ggml_metal_init: found device: Apple M4
0.00.073.239 I ggml_metal_init: picking default device: Apple M4
0.00.073.917 I ggml_metal_init: using embedded metal library
0.00.076.756 I ggml_metal_init: GPU name:   Apple M4
0.00.076.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.759 I ggml_metal_init: simdgroup reduction   = true
0.00.076.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.759 I ggml_metal_init: has bfloat            = true
0.00.076.759 I ggml_metal_init: use bfloat            = true
0.00.076.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.313 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.449 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.455 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.474 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.540 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.542 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.542 I llama_new_context_with_model: graph nodes  = 967
0.00.111.543 I llama_new_context_with_model: graph splits = 2
0.00.111.545 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.880.220 I main: llama threadpool init, n_threads = 4
0.00.880.267 I 
0.00.880.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.880.296 I 
0.00.880.556 I sampler seed: 1234
0.00.880.563 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.880.594 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.880.595 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.880.595 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.673.242 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.673.242 I llama_perf_context_print:        load time =     869.60 ms
0.01.673.243 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.56 tokens per second)
0.01.673.244 I llama_perf_context_print:        eval time =     742.55 ms /    63 runs   (   11.79 ms per token,    84.84 tokens per second)
0.01.673.244 I llama_perf_context_print:       total time =     793.03 ms /    70 tokens
0.01.673.482 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.119s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.872 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.617 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.622 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.627 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.628 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.628 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.628 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.629 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.629 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.632 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.632 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.647 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.578 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.578 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.578 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.579 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.579 I llama_model_loader: - type  f32:  194 tensors
0.00.027.580 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.580 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.580 I print_info: file format = GGUF V3 (latest)
0.00.027.593 I print_info: file type   = Q5_1
0.00.027.593 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.047.390 I load: special tokens cache size = 25
0.00.053.351 I load: token to piece cache size = 0.2984 MB
0.00.053.366 I print_info: arch             = gptneox
0.00.053.367 I print_info: vocab_only       = 0
0.00.053.367 I print_info: n_ctx_train      = 2048
0.00.053.368 I print_info: n_embd           = 2048
0.00.053.368 I print_info: n_layer          = 24
0.00.053.370 I print_info: n_head           = 16
0.00.053.371 I print_info: n_head_kv        = 16
0.00.053.372 I print_info: n_rot            = 32
0.00.053.372 I print_info: n_swa            = 0
0.00.053.372 I print_info: n_embd_head_k    = 128
0.00.053.372 I print_info: n_embd_head_v    = 128
0.00.053.373 I print_info: n_gqa            = 1
0.00.053.374 I print_info: n_embd_k_gqa     = 2048
0.00.053.374 I print_info: n_embd_v_gqa     = 2048
0.00.053.375 I print_info: f_norm_eps       = 1.0e-05
0.00.053.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.376 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.376 I print_info: f_logit_scale    = 0.0e+00
0.00.053.377 I print_info: n_ff             = 8192
0.00.053.377 I print_info: n_expert         = 0
0.00.053.377 I print_info: n_expert_used    = 0
0.00.053.378 I print_info: causal attn      = 1
0.00.053.380 I print_info: pooling type     = 0
0.00.053.380 I print_info: rope type        = 2
0.00.053.381 I print_info: rope scaling     = linear
0.00.053.381 I print_info: freq_base_train  = 10000.0
0.00.053.381 I print_info: freq_scale_train = 1
0.00.053.381 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.382 I print_info: rope_finetuned   = unknown
0.00.053.382 I print_info: ssm_d_conv       = 0
0.00.053.382 I print_info: ssm_d_inner      = 0
0.00.053.383 I print_info: ssm_d_state      = 0
0.00.053.383 I print_info: ssm_dt_rank      = 0
0.00.053.383 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.383 I print_info: model type       = 1.4B
0.00.053.383 I print_info: model params     = 1.41 B
0.00.053.383 I print_info: general.name     = 1.4B
0.00.053.384 I print_info: vocab type       = BPE
0.00.053.384 I print_info: n_vocab          = 50304
0.00.053.384 I print_info: n_merges         = 50009
0.00.053.384 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.385 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.386 I print_info: LF token         = 128 'Ä'
0.00.053.386 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.386 I print_info: max token length = 1024
0.00.055.428 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.428 I load_tensors: offloading output layer to GPU
0.00.055.428 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.439 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.440 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.055.725 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.726 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.726 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.726 I llama_new_context_with_model: n_batch       = 2048
0.00.055.726 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.726 I llama_new_context_with_model: flash_attn    = 0
0.00.055.727 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.727 I llama_new_context_with_model: freq_scale    = 1
0.00.055.728 I ggml_metal_init: allocating
0.00.055.731 I ggml_metal_init: found device: Apple M4
0.00.055.733 I ggml_metal_init: picking default device: Apple M4
0.00.056.346 I ggml_metal_init: using embedded metal library
0.00.058.736 I ggml_metal_init: GPU name:   Apple M4
0.00.058.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.739 I ggml_metal_init: simdgroup reduction   = true
0.00.058.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.739 I ggml_metal_init: has bfloat            = true
0.00.058.739 I ggml_metal_init: use bfloat            = true
0.00.058.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.740 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.768 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.482 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.487 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.506 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.640 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.641 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.642 I llama_new_context_with_model: graph nodes  = 967
0.00.090.642 I llama_new_context_with_model: graph splits = 2
0.00.090.645 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.791 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.164 I main: llama threadpool init, n_threads = 4
0.00.833.202 I 
0.00.833.225 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.226 I 
0.00.833.454 I sampler seed: 1234
0.00.833.460 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.505 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.509 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.509 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.674.959 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.674.959 I llama_perf_context_print:        load time =     824.29 ms
0.01.674.960 I llama_perf_context_print: prompt eval time =      46.16 ms /     7 tokens (    6.59 ms per token,   151.66 tokens per second)
0.01.674.960 I llama_perf_context_print:        eval time =     792.36 ms /    63 runs   (   12.58 ms per token,    79.51 tokens per second)
0.01.674.962 I llama_perf_context_print:       total time =     841.80 ms /    70 tokens
0.01.675.186 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.016.586 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.520 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.023.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.531 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.945 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.946 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.947 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.032.948 I llama_model_loader: - type  f32:  194 tensors
0.00.032.948 I llama_model_loader: - type q2_K:   49 tensors
0.00.032.948 I llama_model_loader: - type q3_K:   48 tensors
0.00.032.948 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.949 I print_info: file format = GGUF V3 (latest)
0.00.032.960 I print_info: file type   = Q2_K - Medium
0.00.032.961 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.052.165 I load: special tokens cache size = 25
0.00.058.074 I load: token to piece cache size = 0.2984 MB
0.00.058.089 I print_info: arch             = gptneox
0.00.058.090 I print_info: vocab_only       = 0
0.00.058.090 I print_info: n_ctx_train      = 2048
0.00.058.090 I print_info: n_embd           = 2048
0.00.058.091 I print_info: n_layer          = 24
0.00.058.093 I print_info: n_head           = 16
0.00.058.094 I print_info: n_head_kv        = 16
0.00.058.094 I print_info: n_rot            = 32
0.00.058.095 I print_info: n_swa            = 0
0.00.058.095 I print_info: n_embd_head_k    = 128
0.00.058.095 I print_info: n_embd_head_v    = 128
0.00.058.096 I print_info: n_gqa            = 1
0.00.058.097 I print_info: n_embd_k_gqa     = 2048
0.00.058.097 I print_info: n_embd_v_gqa     = 2048
0.00.058.098 I print_info: f_norm_eps       = 1.0e-05
0.00.058.098 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.099 I print_info: f_logit_scale    = 0.0e+00
0.00.058.099 I print_info: n_ff             = 8192
0.00.058.100 I print_info: n_expert         = 0
0.00.058.100 I print_info: n_expert_used    = 0
0.00.058.100 I print_info: causal attn      = 1
0.00.058.100 I print_info: pooling type     = 0
0.00.058.100 I print_info: rope type        = 2
0.00.058.100 I print_info: rope scaling     = linear
0.00.058.101 I print_info: freq_base_train  = 10000.0
0.00.058.101 I print_info: freq_scale_train = 1
0.00.058.101 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.101 I print_info: rope_finetuned   = unknown
0.00.058.102 I print_info: ssm_d_conv       = 0
0.00.058.102 I print_info: ssm_d_inner      = 0
0.00.058.103 I print_info: ssm_d_state      = 0
0.00.058.103 I print_info: ssm_dt_rank      = 0
0.00.058.103 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.105 I print_info: model type       = 1.4B
0.00.058.105 I print_info: model params     = 1.41 B
0.00.058.105 I print_info: general.name     = 1.4B
0.00.058.106 I print_info: vocab type       = BPE
0.00.058.106 I print_info: n_vocab          = 50304
0.00.058.106 I print_info: n_merges         = 50009
0.00.058.106 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.107 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.107 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.108 I print_info: LF token         = 128 'Ä'
0.00.058.108 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.108 I print_info: max token length = 1024
0.00.060.028 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.028 I load_tensors: offloading output layer to GPU
0.00.060.029 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.040 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.060.041 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.060.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.328 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.329 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.329 I llama_new_context_with_model: n_batch       = 2048
0.00.060.329 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.329 I llama_new_context_with_model: flash_attn    = 0
0.00.060.330 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.330 I llama_new_context_with_model: freq_scale    = 1
0.00.060.330 I ggml_metal_init: allocating
0.00.060.333 I ggml_metal_init: found device: Apple M4
0.00.060.335 I ggml_metal_init: picking default device: Apple M4
0.00.060.927 I ggml_metal_init: using embedded metal library
0.00.063.285 I ggml_metal_init: GPU name:   Apple M4
0.00.063.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.287 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.287 I ggml_metal_init: simdgroup reduction   = true
0.00.063.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.288 I ggml_metal_init: has bfloat            = true
0.00.063.288 I ggml_metal_init: use bfloat            = true
0.00.063.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.289 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.093 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.102 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.129 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.266 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.267 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.268 I llama_new_context_with_model: graph nodes  = 967
0.00.095.268 I llama_new_context_with_model: graph splits = 2
0.00.095.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.868 I main: llama threadpool init, n_threads = 4
0.00.520.900 I 
0.00.520.921 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.921 I 
0.00.521.150 I sampler seed: 1234
0.00.521.154 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.521.193 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.521.193 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.521.193 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.203.682 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.203.683 I llama_perf_context_print:        load time =     504.28 ms
0.01.203.684 I llama_perf_context_print: prompt eval time =      39.66 ms /     7 tokens (    5.67 ms per token,   176.49 tokens per second)
0.01.203.685 I llama_perf_context_print:        eval time =     640.12 ms /    63 runs   (   10.16 ms per token,    98.42 tokens per second)
0.01.203.685 I llama_perf_context_print:       total time =     682.82 ms /    70 tokens
0.01.203.927 I ggml_metal_free: deallocating

real	0m1.222s
user	0m0.110s
sys	0m0.117s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.953 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.031.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.644 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.649 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.657 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.657 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.039 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.042 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.042 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.043 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.044.043 I llama_model_loader: - type  f32:  194 tensors
0.00.044.043 I llama_model_loader: - type q3_K:   25 tensors
0.00.044.044 I llama_model_loader: - type q4_K:   71 tensors
0.00.044.044 I llama_model_loader: - type q5_K:    1 tensors
0.00.044.044 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.045 I print_info: file format = GGUF V3 (latest)
0.00.044.057 I print_info: file type   = Q3_K - Medium
0.00.044.058 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.075.424 I load: special tokens cache size = 25
0.00.085.274 I load: token to piece cache size = 0.2984 MB
0.00.085.285 I print_info: arch             = gptneox
0.00.085.286 I print_info: vocab_only       = 0
0.00.085.287 I print_info: n_ctx_train      = 2048
0.00.085.287 I print_info: n_embd           = 2048
0.00.085.287 I print_info: n_layer          = 24
0.00.085.290 I print_info: n_head           = 16
0.00.085.291 I print_info: n_head_kv        = 16
0.00.085.292 I print_info: n_rot            = 32
0.00.085.292 I print_info: n_swa            = 0
0.00.085.292 I print_info: n_embd_head_k    = 128
0.00.085.292 I print_info: n_embd_head_v    = 128
0.00.085.293 I print_info: n_gqa            = 1
0.00.085.294 I print_info: n_embd_k_gqa     = 2048
0.00.085.295 I print_info: n_embd_v_gqa     = 2048
0.00.085.296 I print_info: f_norm_eps       = 1.0e-05
0.00.085.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.296 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.297 I print_info: f_logit_scale    = 0.0e+00
0.00.085.298 I print_info: n_ff             = 8192
0.00.085.299 I print_info: n_expert         = 0
0.00.085.301 I print_info: n_expert_used    = 0
0.00.085.302 I print_info: causal attn      = 1
0.00.085.302 I print_info: pooling type     = 0
0.00.085.302 I print_info: rope type        = 2
0.00.085.302 I print_info: rope scaling     = linear
0.00.085.303 I print_info: freq_base_train  = 10000.0
0.00.085.303 I print_info: freq_scale_train = 1
0.00.085.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.305 I print_info: rope_finetuned   = unknown
0.00.085.305 I print_info: ssm_d_conv       = 0
0.00.085.305 I print_info: ssm_d_inner      = 0
0.00.085.306 I print_info: ssm_d_state      = 0
0.00.085.306 I print_info: ssm_dt_rank      = 0
0.00.085.306 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.306 I print_info: model type       = 1.4B
0.00.085.307 I print_info: model params     = 1.41 B
0.00.085.307 I print_info: general.name     = 1.4B
0.00.085.307 I print_info: vocab type       = BPE
0.00.085.307 I print_info: n_vocab          = 50304
0.00.085.308 I print_info: n_merges         = 50009
0.00.085.308 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.308 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.309 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.309 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.309 I print_info: LF token         = 128 'Ä'
0.00.085.309 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.310 I print_info: max token length = 1024
0.00.087.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.749 I load_tensors: offloading output layer to GPU
0.00.087.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.755 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.087.756 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.088.163 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.164 I llama_new_context_with_model: n_ctx         = 2048
0.00.088.164 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.088.165 I llama_new_context_with_model: n_batch       = 2048
0.00.088.165 I llama_new_context_with_model: n_ubatch      = 512
0.00.088.165 I llama_new_context_with_model: flash_attn    = 0
0.00.088.166 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.166 I llama_new_context_with_model: freq_scale    = 1
0.00.088.167 I ggml_metal_init: allocating
0.00.088.170 I ggml_metal_init: found device: Apple M4
0.00.088.173 I ggml_metal_init: picking default device: Apple M4
0.00.088.950 I ggml_metal_init: using embedded metal library
0.00.092.330 I ggml_metal_init: GPU name:   Apple M4
0.00.092.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.333 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.333 I ggml_metal_init: simdgroup reduction   = true
0.00.092.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.334 I ggml_metal_init: has bfloat            = true
0.00.092.334 I ggml_metal_init: use bfloat            = true
0.00.092.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.242 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.124.225 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.235 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.254 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.250 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.251 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.251 I llama_new_context_with_model: graph nodes  = 967
0.00.125.252 I llama_new_context_with_model: graph splits = 2
0.00.125.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.125.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.125.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.876 I main: llama threadpool init, n_threads = 4
0.00.630.958 I 
0.00.631.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.012 I 
0.00.631.508 I sampler seed: 1234
0.00.631.515 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.631.598 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.631.601 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.631.601 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.388.650 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.388.651 I llama_perf_context_print:        load time =     621.91 ms
0.01.388.651 I llama_perf_context_print: prompt eval time =      48.83 ms /     7 tokens (    6.98 ms per token,   143.35 tokens per second)
0.01.388.652 I llama_perf_context_print:        eval time =     705.21 ms /    63 runs   (   11.19 ms per token,    89.33 tokens per second)
0.01.388.652 I llama_perf_context_print:       total time =     757.78 ms /    70 tokens
0.01.388.876 I ggml_metal_free: deallocating

real	0m1.420s
user	0m0.144s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.818 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.210 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.211 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.216 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.314 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.314 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.315 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.315 I llama_model_loader: - type  f32:  194 tensors
0.00.026.315 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.316 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.316 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.316 I print_info: file format = GGUF V3 (latest)
0.00.026.328 I print_info: file type   = Q4_K - Medium
0.00.026.329 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.497 I load: special tokens cache size = 25
0.00.051.441 I load: token to piece cache size = 0.2984 MB
0.00.051.456 I print_info: arch             = gptneox
0.00.051.457 I print_info: vocab_only       = 0
0.00.051.457 I print_info: n_ctx_train      = 2048
0.00.051.457 I print_info: n_embd           = 2048
0.00.051.458 I print_info: n_layer          = 24
0.00.051.460 I print_info: n_head           = 16
0.00.051.461 I print_info: n_head_kv        = 16
0.00.051.461 I print_info: n_rot            = 32
0.00.051.461 I print_info: n_swa            = 0
0.00.051.461 I print_info: n_embd_head_k    = 128
0.00.051.462 I print_info: n_embd_head_v    = 128
0.00.051.462 I print_info: n_gqa            = 1
0.00.051.463 I print_info: n_embd_k_gqa     = 2048
0.00.051.464 I print_info: n_embd_v_gqa     = 2048
0.00.051.465 I print_info: f_norm_eps       = 1.0e-05
0.00.051.465 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.465 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.465 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.466 I print_info: f_logit_scale    = 0.0e+00
0.00.051.467 I print_info: n_ff             = 8192
0.00.051.467 I print_info: n_expert         = 0
0.00.051.467 I print_info: n_expert_used    = 0
0.00.051.467 I print_info: causal attn      = 1
0.00.051.467 I print_info: pooling type     = 0
0.00.051.467 I print_info: rope type        = 2
0.00.051.467 I print_info: rope scaling     = linear
0.00.051.468 I print_info: freq_base_train  = 10000.0
0.00.051.468 I print_info: freq_scale_train = 1
0.00.051.468 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.468 I print_info: rope_finetuned   = unknown
0.00.051.468 I print_info: ssm_d_conv       = 0
0.00.051.469 I print_info: ssm_d_inner      = 0
0.00.051.469 I print_info: ssm_d_state      = 0
0.00.051.470 I print_info: ssm_dt_rank      = 0
0.00.051.470 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.470 I print_info: model type       = 1.4B
0.00.051.471 I print_info: model params     = 1.41 B
0.00.051.471 I print_info: general.name     = 1.4B
0.00.051.471 I print_info: vocab type       = BPE
0.00.051.471 I print_info: n_vocab          = 50304
0.00.051.471 I print_info: n_merges         = 50009
0.00.051.472 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.472 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.472 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.472 I print_info: LF token         = 128 'Ä'
0.00.051.476 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.476 I print_info: max token length = 1024
0.00.053.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.493 I load_tensors: offloading output layer to GPU
0.00.053.493 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.504 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.505 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.783 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.783 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.784 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.784 I llama_new_context_with_model: n_batch       = 2048
0.00.053.784 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.784 I llama_new_context_with_model: flash_attn    = 0
0.00.053.784 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.785 I llama_new_context_with_model: freq_scale    = 1
0.00.053.785 I ggml_metal_init: allocating
0.00.053.788 I ggml_metal_init: found device: Apple M4
0.00.053.790 I ggml_metal_init: picking default device: Apple M4
0.00.054.378 I ggml_metal_init: using embedded metal library
0.00.056.724 I ggml_metal_init: GPU name:   Apple M4
0.00.056.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.726 I ggml_metal_init: simdgroup reduction   = true
0.00.056.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.727 I ggml_metal_init: has bfloat            = true
0.00.056.727 I ggml_metal_init: use bfloat            = true
0.00.056.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.589 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.818 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.826 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.851 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.917 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.919 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.919 I llama_new_context_with_model: graph nodes  = 967
0.00.086.919 I llama_new_context_with_model: graph splits = 2
0.00.086.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.151 I main: llama threadpool init, n_threads = 4
0.00.613.197 I 
0.00.613.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.227 I 
0.00.613.463 I sampler seed: 1234
0.00.613.468 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.503 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.503 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.377.651 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.377.651 I llama_perf_context_print:        load time =     603.33 ms
0.01.377.652 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.40 tokens per second)
0.01.377.654 I llama_perf_context_print:        eval time =     714.03 ms /    63 runs   (   11.33 ms per token,    88.23 tokens per second)
0.01.377.655 I llama_perf_context_print:       total time =     764.50 ms /    70 tokens
0.01.377.885 I ggml_metal_free: deallocating

real	0m1.397s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.758 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.520 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.521 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.603 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.916 I llama_model_loader: - type  f32:  194 tensors
0.00.025.917 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.917 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.918 I print_info: file format = GGUF V3 (latest)
0.00.025.930 I print_info: file type   = Q5_K - Medium
0.00.025.932 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.155 I load: special tokens cache size = 25
0.00.052.274 I load: token to piece cache size = 0.2984 MB
0.00.052.295 I print_info: arch             = gptneox
0.00.052.296 I print_info: vocab_only       = 0
0.00.052.296 I print_info: n_ctx_train      = 2048
0.00.052.296 I print_info: n_embd           = 2048
0.00.052.296 I print_info: n_layer          = 24
0.00.052.300 I print_info: n_head           = 16
0.00.052.301 I print_info: n_head_kv        = 16
0.00.052.301 I print_info: n_rot            = 32
0.00.052.301 I print_info: n_swa            = 0
0.00.052.301 I print_info: n_embd_head_k    = 128
0.00.052.302 I print_info: n_embd_head_v    = 128
0.00.052.302 I print_info: n_gqa            = 1
0.00.052.303 I print_info: n_embd_k_gqa     = 2048
0.00.052.303 I print_info: n_embd_v_gqa     = 2048
0.00.052.304 I print_info: f_norm_eps       = 1.0e-05
0.00.052.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.304 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.304 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.305 I print_info: f_logit_scale    = 0.0e+00
0.00.052.305 I print_info: n_ff             = 8192
0.00.052.305 I print_info: n_expert         = 0
0.00.052.305 I print_info: n_expert_used    = 0
0.00.052.308 I print_info: causal attn      = 1
0.00.052.310 I print_info: pooling type     = 0
0.00.052.310 I print_info: rope type        = 2
0.00.052.310 I print_info: rope scaling     = linear
0.00.052.310 I print_info: freq_base_train  = 10000.0
0.00.052.311 I print_info: freq_scale_train = 1
0.00.052.311 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.311 I print_info: rope_finetuned   = unknown
0.00.052.311 I print_info: ssm_d_conv       = 0
0.00.052.311 I print_info: ssm_d_inner      = 0
0.00.052.311 I print_info: ssm_d_state      = 0
0.00.052.312 I print_info: ssm_dt_rank      = 0
0.00.052.312 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.312 I print_info: model type       = 1.4B
0.00.052.312 I print_info: model params     = 1.41 B
0.00.052.312 I print_info: general.name     = 1.4B
0.00.052.313 I print_info: vocab type       = BPE
0.00.052.313 I print_info: n_vocab          = 50304
0.00.052.313 I print_info: n_merges         = 50009
0.00.052.313 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.314 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.314 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.314 I print_info: LF token         = 128 'Ä'
0.00.052.314 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.314 I print_info: max token length = 1024
0.00.054.306 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.306 I load_tensors: offloading output layer to GPU
0.00.054.306 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.318 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.319 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.608 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.609 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.609 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.609 I llama_new_context_with_model: n_batch       = 2048
0.00.054.609 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.610 I llama_new_context_with_model: flash_attn    = 0
0.00.054.610 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.610 I llama_new_context_with_model: freq_scale    = 1
0.00.054.612 I ggml_metal_init: allocating
0.00.054.616 I ggml_metal_init: found device: Apple M4
0.00.054.618 I ggml_metal_init: picking default device: Apple M4
0.00.055.289 I ggml_metal_init: using embedded metal library
0.00.057.884 I ggml_metal_init: GPU name:   Apple M4
0.00.057.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.887 I ggml_metal_init: simdgroup reduction   = true
0.00.057.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.887 I ggml_metal_init: has bfloat            = true
0.00.057.887 I ggml_metal_init: use bfloat            = true
0.00.057.888 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.169 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.789 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.799 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.830 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.868 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.869 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.870 I llama_new_context_with_model: graph nodes  = 967
0.00.088.870 I llama_new_context_with_model: graph splits = 2
0.00.088.873 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.034 I main: llama threadpool init, n_threads = 4
0.00.682.081 I 
0.00.682.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.106 I 
0.00.682.333 I sampler seed: 1234
0.00.682.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.368 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.369 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.369 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.525.771 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.525.771 I llama_perf_context_print:        load time =     673.27 ms
0.01.525.773 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.75 tokens per second)
0.01.525.773 I llama_perf_context_print:        eval time =     789.10 ms /    63 runs   (   12.53 ms per token,    79.84 tokens per second)
0.01.525.774 I llama_perf_context_print:       total time =     843.74 ms /    70 tokens
0.01.525.993 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.111s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.907 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.214 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.217 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.101 I llama_model_loader: - type  f32:  194 tensors
0.00.027.102 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.102 I print_info: file format = GGUF V3 (latest)
0.00.027.109 I print_info: file type   = Q6_K
0.00.027.110 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.103 I load: special tokens cache size = 25
0.00.052.065 I load: token to piece cache size = 0.2984 MB
0.00.052.075 I print_info: arch             = gptneox
0.00.052.076 I print_info: vocab_only       = 0
0.00.052.076 I print_info: n_ctx_train      = 2048
0.00.052.076 I print_info: n_embd           = 2048
0.00.052.076 I print_info: n_layer          = 24
0.00.052.080 I print_info: n_head           = 16
0.00.052.080 I print_info: n_head_kv        = 16
0.00.052.080 I print_info: n_rot            = 32
0.00.052.081 I print_info: n_swa            = 0
0.00.052.083 I print_info: n_embd_head_k    = 128
0.00.052.083 I print_info: n_embd_head_v    = 128
0.00.052.084 I print_info: n_gqa            = 1
0.00.052.085 I print_info: n_embd_k_gqa     = 2048
0.00.052.086 I print_info: n_embd_v_gqa     = 2048
0.00.052.086 I print_info: f_norm_eps       = 1.0e-05
0.00.052.086 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.088 I print_info: f_logit_scale    = 0.0e+00
0.00.052.089 I print_info: n_ff             = 8192
0.00.052.089 I print_info: n_expert         = 0
0.00.052.090 I print_info: n_expert_used    = 0
0.00.052.090 I print_info: causal attn      = 1
0.00.052.092 I print_info: pooling type     = 0
0.00.052.092 I print_info: rope type        = 2
0.00.052.092 I print_info: rope scaling     = linear
0.00.052.093 I print_info: freq_base_train  = 10000.0
0.00.052.093 I print_info: freq_scale_train = 1
0.00.052.093 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.093 I print_info: rope_finetuned   = unknown
0.00.052.093 I print_info: ssm_d_conv       = 0
0.00.052.094 I print_info: ssm_d_inner      = 0
0.00.052.094 I print_info: ssm_d_state      = 0
0.00.052.094 I print_info: ssm_dt_rank      = 0
0.00.052.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.097 I print_info: model type       = 1.4B
0.00.052.098 I print_info: model params     = 1.41 B
0.00.052.098 I print_info: general.name     = 1.4B
0.00.052.098 I print_info: vocab type       = BPE
0.00.052.098 I print_info: n_vocab          = 50304
0.00.052.098 I print_info: n_merges         = 50009
0.00.052.099 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.099 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.099 I print_info: LF token         = 128 'Ä'
0.00.052.099 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.102 I print_info: max token length = 1024
0.00.053.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.868 I load_tensors: offloading output layer to GPU
0.00.053.869 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.874 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.875 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.285 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.286 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.286 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.286 I llama_new_context_with_model: n_batch       = 2048
0.00.054.286 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.286 I llama_new_context_with_model: flash_attn    = 0
0.00.054.287 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.287 I llama_new_context_with_model: freq_scale    = 1
0.00.054.288 I ggml_metal_init: allocating
0.00.054.291 I ggml_metal_init: found device: Apple M4
0.00.054.292 I ggml_metal_init: picking default device: Apple M4
0.00.054.876 I ggml_metal_init: using embedded metal library
0.00.057.207 I ggml_metal_init: GPU name:   Apple M4
0.00.057.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.210 I ggml_metal_init: simdgroup reduction   = true
0.00.057.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.210 I ggml_metal_init: has bfloat            = true
0.00.057.210 I ggml_metal_init: use bfloat            = true
0.00.057.211 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.949 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.603 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.620 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.654 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.716 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.718 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.718 I llama_new_context_with_model: graph nodes  = 967
0.00.087.718 I llama_new_context_with_model: graph splits = 2
0.00.087.724 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.859 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.450 I main: llama threadpool init, n_threads = 4
0.00.741.487 I 
0.00.741.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.505 I 
0.00.741.738 I sampler seed: 1234
0.00.741.744 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.783 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.784 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.624.218 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.624.219 I llama_perf_context_print:        load time =     731.54 ms
0.01.624.219 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.01.624.220 I llama_perf_context_print:        eval time =     824.96 ms /    63 runs   (   13.09 ms per token,    76.37 tokens per second)
0.01.624.221 I llama_perf_context_print:       total time =     882.77 ms /    70 tokens
0.01.624.436 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.445 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.206 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.381 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.395 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.396 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.611 I llama_model_loader: - type  f32:  194 tensors
0.00.054.611 I llama_model_loader: - type  f16:   98 tensors
0.00.054.612 I print_info: file format = GGUF V3 (latest)
0.00.054.627 I print_info: file type   = all F32 (guessed)
0.00.054.629 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.572 I load: special tokens cache size = 25
0.00.090.290 I load: token to piece cache size = 0.2984 MB
0.00.090.304 I print_info: arch             = gptneox
0.00.090.305 I print_info: vocab_only       = 0
0.00.090.306 I print_info: n_ctx_train      = 2048
0.00.090.306 I print_info: n_embd           = 2048
0.00.090.306 I print_info: n_layer          = 24
0.00.090.309 I print_info: n_head           = 16
0.00.090.310 I print_info: n_head_kv        = 16
0.00.090.310 I print_info: n_rot            = 32
0.00.090.310 I print_info: n_swa            = 0
0.00.090.312 I print_info: n_embd_head_k    = 128
0.00.090.312 I print_info: n_embd_head_v    = 128
0.00.090.313 I print_info: n_gqa            = 1
0.00.090.313 I print_info: n_embd_k_gqa     = 2048
0.00.090.314 I print_info: n_embd_v_gqa     = 2048
0.00.090.314 I print_info: f_norm_eps       = 1.0e-05
0.00.090.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.315 I print_info: f_logit_scale    = 0.0e+00
0.00.090.316 I print_info: n_ff             = 8192
0.00.090.318 I print_info: n_expert         = 0
0.00.090.318 I print_info: n_expert_used    = 0
0.00.090.318 I print_info: causal attn      = 1
0.00.090.318 I print_info: pooling type     = 0
0.00.090.318 I print_info: rope type        = 2
0.00.090.318 I print_info: rope scaling     = linear
0.00.090.325 I print_info: freq_base_train  = 10000.0
0.00.090.326 I print_info: freq_scale_train = 1
0.00.090.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.326 I print_info: rope_finetuned   = unknown
0.00.090.327 I print_info: ssm_d_conv       = 0
0.00.090.327 I print_info: ssm_d_inner      = 0
0.00.090.327 I print_info: ssm_d_state      = 0
0.00.090.327 I print_info: ssm_dt_rank      = 0
0.00.090.327 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.327 I print_info: model type       = 1.4B
0.00.090.328 I print_info: model params     = 1.41 B
0.00.090.328 I print_info: general.name     = 1.4B
0.00.090.329 I print_info: vocab type       = BPE
0.00.090.329 I print_info: n_vocab          = 50304
0.00.090.329 I print_info: n_merges         = 50009
0.00.090.329 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.329 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.329 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.330 I print_info: LF token         = 128 'Ä'
0.00.090.332 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.332 I print_info: max token length = 1024
0.00.092.923 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.923 I load_tensors: offloading output layer to GPU
0.00.092.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.934 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.935 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.236 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.237 I llama_new_context_with_model: n_ctx         = 128
0.00.093.237 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.237 I llama_new_context_with_model: n_batch       = 128
0.00.093.237 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.238 I llama_new_context_with_model: flash_attn    = 0
0.00.093.238 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.238 I llama_new_context_with_model: freq_scale    = 1
0.00.093.239 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.239 I ggml_metal_init: allocating
0.00.093.242 I ggml_metal_init: found device: Apple M4
0.00.093.244 I ggml_metal_init: picking default device: Apple M4
0.00.093.880 I ggml_metal_init: using embedded metal library
0.00.096.481 I ggml_metal_init: GPU name:   Apple M4
0.00.096.482 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.483 I ggml_metal_init: simdgroup reduction   = true
0.00.096.483 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.484 I ggml_metal_init: has bfloat            = true
0.00.096.484 I ggml_metal_init: use bfloat            = true
0.00.096.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.575 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.827 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.832 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.847 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.718 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.719 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.719 I llama_new_context_with_model: graph nodes  = 967
0.00.107.720 I llama_new_context_with_model: graph splits = 2
0.00.107.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.041.310 I 
0.01.041.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.041.369 I perplexity: tokenizing the input ..
0.01.054.266 I perplexity: tokenization took 12.894 ms
0.01.054.281 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.177.627 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.179.718 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.179.786 I llama_perf_context_print:        load time =    1019.09 ms
0.01.179.792 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.38 tokens per second)
0.01.179.794 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.179.794 I llama_perf_context_print:       total time =     138.47 ms /   129 tokens
0.01.180.406 I ggml_metal_free: deallocating

real	0m1.368s
user	0m0.124s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.341 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.865 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.865 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.865 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.866 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.866 I llama_model_loader: - type  f32:  194 tensors
0.00.031.867 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.867 I print_info: file format = GGUF V3 (latest)
0.00.031.880 I print_info: file type   = Q8_0
0.00.031.882 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.054.732 I load: special tokens cache size = 25
0.00.061.061 I load: token to piece cache size = 0.2984 MB
0.00.061.075 I print_info: arch             = gptneox
0.00.061.076 I print_info: vocab_only       = 0
0.00.061.077 I print_info: n_ctx_train      = 2048
0.00.061.077 I print_info: n_embd           = 2048
0.00.061.077 I print_info: n_layer          = 24
0.00.061.081 I print_info: n_head           = 16
0.00.061.084 I print_info: n_head_kv        = 16
0.00.061.084 I print_info: n_rot            = 32
0.00.061.084 I print_info: n_swa            = 0
0.00.061.084 I print_info: n_embd_head_k    = 128
0.00.061.085 I print_info: n_embd_head_v    = 128
0.00.061.085 I print_info: n_gqa            = 1
0.00.061.086 I print_info: n_embd_k_gqa     = 2048
0.00.061.086 I print_info: n_embd_v_gqa     = 2048
0.00.061.087 I print_info: f_norm_eps       = 1.0e-05
0.00.061.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.088 I print_info: f_logit_scale    = 0.0e+00
0.00.061.088 I print_info: n_ff             = 8192
0.00.061.088 I print_info: n_expert         = 0
0.00.061.088 I print_info: n_expert_used    = 0
0.00.061.088 I print_info: causal attn      = 1
0.00.061.089 I print_info: pooling type     = 0
0.00.061.089 I print_info: rope type        = 2
0.00.061.089 I print_info: rope scaling     = linear
0.00.061.089 I print_info: freq_base_train  = 10000.0
0.00.061.089 I print_info: freq_scale_train = 1
0.00.061.090 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.090 I print_info: rope_finetuned   = unknown
0.00.061.090 I print_info: ssm_d_conv       = 0
0.00.061.090 I print_info: ssm_d_inner      = 0
0.00.061.090 I print_info: ssm_d_state      = 0
0.00.061.090 I print_info: ssm_dt_rank      = 0
0.00.061.090 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.090 I print_info: model type       = 1.4B
0.00.061.091 I print_info: model params     = 1.41 B
0.00.061.091 I print_info: general.name     = 1.4B
0.00.061.091 I print_info: vocab type       = BPE
0.00.061.091 I print_info: n_vocab          = 50304
0.00.061.092 I print_info: n_merges         = 50009
0.00.061.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.104 I print_info: LF token         = 128 'Ä'
0.00.061.105 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.105 I print_info: max token length = 1024
0.00.063.261 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.261 I load_tensors: offloading output layer to GPU
0.00.063.261 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.272 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.273 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.618 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.619 I llama_new_context_with_model: n_ctx         = 128
0.00.063.619 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.619 I llama_new_context_with_model: n_batch       = 128
0.00.063.620 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.620 I llama_new_context_with_model: flash_attn    = 0
0.00.063.620 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.620 I llama_new_context_with_model: freq_scale    = 1
0.00.063.621 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.621 I ggml_metal_init: allocating
0.00.063.624 I ggml_metal_init: found device: Apple M4
0.00.063.626 I ggml_metal_init: picking default device: Apple M4
0.00.064.230 I ggml_metal_init: using embedded metal library
0.00.066.718 I ggml_metal_init: GPU name:   Apple M4
0.00.066.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.720 I ggml_metal_init: simdgroup reduction   = true
0.00.066.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.720 I ggml_metal_init: has bfloat            = true
0.00.066.721 I ggml_metal_init: use bfloat            = true
0.00.066.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.878 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.078.168 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.171 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.187 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.105 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.106 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.107 I llama_new_context_with_model: graph nodes  = 967
0.00.079.107 I llama_new_context_with_model: graph splits = 2
0.00.079.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.108 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.071 I 
0.00.805.097 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.106 I perplexity: tokenizing the input ..
0.00.812.666 I perplexity: tokenization took 7.558 ms
0.00.812.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.936.729 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.937.882 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.937.906 I llama_perf_context_print:        load time =     794.72 ms
0.00.937.907 I llama_perf_context_print: prompt eval time =     123.84 ms /   128 tokens (    0.97 ms per token,  1033.63 tokens per second)
0.00.937.907 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.937.908 I llama_perf_context_print:       total time =     132.84 ms /   129 tokens
0.00.938.359 I ggml_metal_free: deallocating

real	0m0.956s
user	0m0.090s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.258 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.521 I llama_model_loader: - type  f32:  194 tensors
0.00.026.521 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.521 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.522 I print_info: file format = GGUF V3 (latest)
0.00.026.534 I print_info: file type   = Q4_0
0.00.026.537 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.188 I load: special tokens cache size = 25
0.00.052.155 I load: token to piece cache size = 0.2984 MB
0.00.052.169 I print_info: arch             = gptneox
0.00.052.170 I print_info: vocab_only       = 0
0.00.052.171 I print_info: n_ctx_train      = 2048
0.00.052.171 I print_info: n_embd           = 2048
0.00.052.171 I print_info: n_layer          = 24
0.00.052.174 I print_info: n_head           = 16
0.00.052.175 I print_info: n_head_kv        = 16
0.00.052.175 I print_info: n_rot            = 32
0.00.052.175 I print_info: n_swa            = 0
0.00.052.175 I print_info: n_embd_head_k    = 128
0.00.052.175 I print_info: n_embd_head_v    = 128
0.00.052.176 I print_info: n_gqa            = 1
0.00.052.177 I print_info: n_embd_k_gqa     = 2048
0.00.052.178 I print_info: n_embd_v_gqa     = 2048
0.00.052.178 I print_info: f_norm_eps       = 1.0e-05
0.00.052.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.183 I print_info: f_logit_scale    = 0.0e+00
0.00.052.184 I print_info: n_ff             = 8192
0.00.052.184 I print_info: n_expert         = 0
0.00.052.184 I print_info: n_expert_used    = 0
0.00.052.184 I print_info: causal attn      = 1
0.00.052.184 I print_info: pooling type     = 0
0.00.052.184 I print_info: rope type        = 2
0.00.052.185 I print_info: rope scaling     = linear
0.00.052.185 I print_info: freq_base_train  = 10000.0
0.00.052.185 I print_info: freq_scale_train = 1
0.00.052.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.186 I print_info: rope_finetuned   = unknown
0.00.052.186 I print_info: ssm_d_conv       = 0
0.00.052.186 I print_info: ssm_d_inner      = 0
0.00.052.187 I print_info: ssm_d_state      = 0
0.00.052.187 I print_info: ssm_dt_rank      = 0
0.00.052.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.188 I print_info: model type       = 1.4B
0.00.052.188 I print_info: model params     = 1.41 B
0.00.052.189 I print_info: general.name     = 1.4B
0.00.052.190 I print_info: vocab type       = BPE
0.00.052.190 I print_info: n_vocab          = 50304
0.00.052.190 I print_info: n_merges         = 50009
0.00.052.190 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.191 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.192 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.192 I print_info: LF token         = 128 'Ä'
0.00.052.192 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.192 I print_info: max token length = 1024
0.00.054.183 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.183 I load_tensors: offloading output layer to GPU
0.00.054.184 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.194 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.196 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.465 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.466 I llama_new_context_with_model: n_ctx         = 128
0.00.054.466 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.466 I llama_new_context_with_model: n_batch       = 128
0.00.054.466 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.467 I llama_new_context_with_model: flash_attn    = 0
0.00.054.467 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.467 I llama_new_context_with_model: freq_scale    = 1
0.00.054.468 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.468 I ggml_metal_init: allocating
0.00.054.471 I ggml_metal_init: found device: Apple M4
0.00.054.473 I ggml_metal_init: picking default device: Apple M4
0.00.055.066 I ggml_metal_init: using embedded metal library
0.00.057.500 I ggml_metal_init: GPU name:   Apple M4
0.00.057.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.503 I ggml_metal_init: simdgroup reduction   = true
0.00.057.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.503 I ggml_metal_init: has bfloat            = true
0.00.057.503 I ggml_metal_init: use bfloat            = true
0.00.057.503 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.504 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.550 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.824 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.826 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.840 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.800 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.801 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.802 I llama_new_context_with_model: graph nodes  = 967
0.00.069.802 I llama_new_context_with_model: graph splits = 2
0.00.069.803 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.803 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.769 I 
0.00.613.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.849 I perplexity: tokenizing the input ..
0.00.621.664 I perplexity: tokenization took 7.812 ms
0.00.621.668 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.538 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.745.728 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.745.755 I llama_perf_context_print:        load time =     603.82 ms
0.00.745.756 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.78 tokens per second)
0.00.745.757 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.758 I llama_perf_context_print:       total time =     131.99 ms /   129 tokens
0.00.746.323 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.080s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.677 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.789 I llama_model_loader: - type  f32:  194 tensors
0.00.024.790 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.790 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.790 I print_info: file format = GGUF V3 (latest)
0.00.024.797 I print_info: file type   = Q4_1
0.00.024.798 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.761 I load: special tokens cache size = 25
0.00.049.729 I load: token to piece cache size = 0.2984 MB
0.00.049.744 I print_info: arch             = gptneox
0.00.049.745 I print_info: vocab_only       = 0
0.00.049.745 I print_info: n_ctx_train      = 2048
0.00.049.745 I print_info: n_embd           = 2048
0.00.049.745 I print_info: n_layer          = 24
0.00.049.748 I print_info: n_head           = 16
0.00.049.749 I print_info: n_head_kv        = 16
0.00.049.750 I print_info: n_rot            = 32
0.00.049.754 I print_info: n_swa            = 0
0.00.049.755 I print_info: n_embd_head_k    = 128
0.00.049.755 I print_info: n_embd_head_v    = 128
0.00.049.756 I print_info: n_gqa            = 1
0.00.049.756 I print_info: n_embd_k_gqa     = 2048
0.00.049.757 I print_info: n_embd_v_gqa     = 2048
0.00.049.758 I print_info: f_norm_eps       = 1.0e-05
0.00.049.758 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.758 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.758 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.759 I print_info: f_logit_scale    = 0.0e+00
0.00.049.759 I print_info: n_ff             = 8192
0.00.049.759 I print_info: n_expert         = 0
0.00.049.760 I print_info: n_expert_used    = 0
0.00.049.760 I print_info: causal attn      = 1
0.00.049.760 I print_info: pooling type     = 0
0.00.049.760 I print_info: rope type        = 2
0.00.049.760 I print_info: rope scaling     = linear
0.00.049.762 I print_info: freq_base_train  = 10000.0
0.00.049.763 I print_info: freq_scale_train = 1
0.00.049.763 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.763 I print_info: rope_finetuned   = unknown
0.00.049.764 I print_info: ssm_d_conv       = 0
0.00.049.764 I print_info: ssm_d_inner      = 0
0.00.049.764 I print_info: ssm_d_state      = 0
0.00.049.764 I print_info: ssm_dt_rank      = 0
0.00.049.764 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.764 I print_info: model type       = 1.4B
0.00.049.765 I print_info: model params     = 1.41 B
0.00.049.765 I print_info: general.name     = 1.4B
0.00.049.765 I print_info: vocab type       = BPE
0.00.049.766 I print_info: n_vocab          = 50304
0.00.049.766 I print_info: n_merges         = 50009
0.00.049.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.768 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.768 I print_info: LF token         = 128 'Ä'
0.00.049.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.768 I print_info: max token length = 1024
0.00.051.752 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.753 I load_tensors: offloading output layer to GPU
0.00.051.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.764 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.765 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.050 I llama_new_context_with_model: n_ctx         = 128
0.00.052.050 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.050 I llama_new_context_with_model: n_batch       = 128
0.00.052.050 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.050 I llama_new_context_with_model: flash_attn    = 0
0.00.052.051 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.051 I llama_new_context_with_model: freq_scale    = 1
0.00.052.051 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.052 I ggml_metal_init: allocating
0.00.052.054 I ggml_metal_init: found device: Apple M4
0.00.052.056 I ggml_metal_init: picking default device: Apple M4
0.00.052.626 I ggml_metal_init: using embedded metal library
0.00.054.979 I ggml_metal_init: GPU name:   Apple M4
0.00.054.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.981 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.982 I ggml_metal_init: simdgroup reduction   = true
0.00.054.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.982 I ggml_metal_init: has bfloat            = true
0.00.054.982 I ggml_metal_init: use bfloat            = true
0.00.054.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.962 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.964 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.977 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.852 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.854 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.854 I llama_new_context_with_model: graph nodes  = 967
0.00.066.854 I llama_new_context_with_model: graph splits = 2
0.00.066.855 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.136 I 
0.00.597.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.185 I perplexity: tokenizing the input ..
0.00.604.933 I perplexity: tokenization took 7.746 ms
0.00.604.937 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.295 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.728.421 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.728.449 I llama_perf_context_print:        load time =     588.46 ms
0.00.728.450 I llama_perf_context_print: prompt eval time =     122.13 ms /   128 tokens (    0.95 ms per token,  1048.05 tokens per second)
0.00.728.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.451 I llama_perf_context_print:       total time =     131.31 ms /   129 tokens
0.00.728.866 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.078s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.913 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.131 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.132 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.132 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.132 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.133 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.134 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.134 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.135 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.140 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.350 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.727 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.728 I llama_model_loader: - type  f32:  194 tensors
0.00.026.728 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.729 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.729 I print_info: file format = GGUF V3 (latest)
0.00.026.741 I print_info: file type   = Q5_0
0.00.026.742 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.591 I load: special tokens cache size = 25
0.00.051.524 I load: token to piece cache size = 0.2984 MB
0.00.051.539 I print_info: arch             = gptneox
0.00.051.540 I print_info: vocab_only       = 0
0.00.051.540 I print_info: n_ctx_train      = 2048
0.00.051.540 I print_info: n_embd           = 2048
0.00.051.541 I print_info: n_layer          = 24
0.00.051.543 I print_info: n_head           = 16
0.00.051.544 I print_info: n_head_kv        = 16
0.00.051.544 I print_info: n_rot            = 32
0.00.051.544 I print_info: n_swa            = 0
0.00.051.544 I print_info: n_embd_head_k    = 128
0.00.051.545 I print_info: n_embd_head_v    = 128
0.00.051.545 I print_info: n_gqa            = 1
0.00.051.546 I print_info: n_embd_k_gqa     = 2048
0.00.051.547 I print_info: n_embd_v_gqa     = 2048
0.00.051.547 I print_info: f_norm_eps       = 1.0e-05
0.00.051.548 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.548 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.548 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.548 I print_info: f_logit_scale    = 0.0e+00
0.00.051.549 I print_info: n_ff             = 8192
0.00.051.549 I print_info: n_expert         = 0
0.00.051.549 I print_info: n_expert_used    = 0
0.00.051.549 I print_info: causal attn      = 1
0.00.051.549 I print_info: pooling type     = 0
0.00.051.550 I print_info: rope type        = 2
0.00.051.550 I print_info: rope scaling     = linear
0.00.051.550 I print_info: freq_base_train  = 10000.0
0.00.051.550 I print_info: freq_scale_train = 1
0.00.051.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.551 I print_info: rope_finetuned   = unknown
0.00.051.551 I print_info: ssm_d_conv       = 0
0.00.051.551 I print_info: ssm_d_inner      = 0
0.00.051.551 I print_info: ssm_d_state      = 0
0.00.051.551 I print_info: ssm_dt_rank      = 0
0.00.051.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.552 I print_info: model type       = 1.4B
0.00.051.554 I print_info: model params     = 1.41 B
0.00.051.554 I print_info: general.name     = 1.4B
0.00.051.555 I print_info: vocab type       = BPE
0.00.051.555 I print_info: n_vocab          = 50304
0.00.051.555 I print_info: n_merges         = 50009
0.00.051.555 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.557 I print_info: LF token         = 128 'Ä'
0.00.051.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.559 I print_info: max token length = 1024
0.00.053.492 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.492 I load_tensors: offloading output layer to GPU
0.00.053.492 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.503 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.504 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.796 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.797 I llama_new_context_with_model: n_ctx         = 128
0.00.053.797 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.797 I llama_new_context_with_model: n_batch       = 128
0.00.053.798 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.798 I llama_new_context_with_model: flash_attn    = 0
0.00.053.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.798 I llama_new_context_with_model: freq_scale    = 1
0.00.053.799 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.799 I ggml_metal_init: allocating
0.00.053.802 I ggml_metal_init: found device: Apple M4
0.00.053.804 I ggml_metal_init: picking default device: Apple M4
0.00.054.375 I ggml_metal_init: using embedded metal library
0.00.056.724 I ggml_metal_init: GPU name:   Apple M4
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.727 I ggml_metal_init: simdgroup reduction   = true
0.00.056.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.727 I ggml_metal_init: has bfloat            = true
0.00.056.727 I ggml_metal_init: use bfloat            = true
0.00.056.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.831 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.834 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.852 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.702 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.703 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.704 I llama_new_context_with_model: graph nodes  = 967
0.00.068.704 I llama_new_context_with_model: graph splits = 2
0.00.068.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.889 I 
0.00.714.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.933 I perplexity: tokenizing the input ..
0.00.722.639 I perplexity: tokenization took 7.705 ms
0.00.722.643 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.840 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.859.011 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.859.035 I llama_perf_context_print:        load time =     704.97 ms
0.00.859.036 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.34 tokens per second)
0.00.859.037 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.037 I llama_perf_context_print:       total time =     144.15 ms /   129 tokens
0.00.859.540 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.078s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.851 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.186 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.191 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.199 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.199 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.200 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.200 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.200 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.463 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.464 I llama_model_loader: - type  f32:  194 tensors
0.00.025.464 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.464 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.465 I print_info: file format = GGUF V3 (latest)
0.00.025.472 I print_info: file type   = Q5_1
0.00.025.473 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.274 I load: special tokens cache size = 25
0.00.051.243 I load: token to piece cache size = 0.2984 MB
0.00.051.258 I print_info: arch             = gptneox
0.00.051.259 I print_info: vocab_only       = 0
0.00.051.259 I print_info: n_ctx_train      = 2048
0.00.051.259 I print_info: n_embd           = 2048
0.00.051.259 I print_info: n_layer          = 24
0.00.051.262 I print_info: n_head           = 16
0.00.051.263 I print_info: n_head_kv        = 16
0.00.051.263 I print_info: n_rot            = 32
0.00.051.263 I print_info: n_swa            = 0
0.00.051.263 I print_info: n_embd_head_k    = 128
0.00.051.263 I print_info: n_embd_head_v    = 128
0.00.051.266 I print_info: n_gqa            = 1
0.00.051.266 I print_info: n_embd_k_gqa     = 2048
0.00.051.267 I print_info: n_embd_v_gqa     = 2048
0.00.051.268 I print_info: f_norm_eps       = 1.0e-05
0.00.051.268 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.268 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.269 I print_info: f_logit_scale    = 0.0e+00
0.00.051.269 I print_info: n_ff             = 8192
0.00.051.269 I print_info: n_expert         = 0
0.00.051.270 I print_info: n_expert_used    = 0
0.00.051.270 I print_info: causal attn      = 1
0.00.051.270 I print_info: pooling type     = 0
0.00.051.270 I print_info: rope type        = 2
0.00.051.270 I print_info: rope scaling     = linear
0.00.051.272 I print_info: freq_base_train  = 10000.0
0.00.051.273 I print_info: freq_scale_train = 1
0.00.051.273 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.273 I print_info: rope_finetuned   = unknown
0.00.051.274 I print_info: ssm_d_conv       = 0
0.00.051.274 I print_info: ssm_d_inner      = 0
0.00.051.274 I print_info: ssm_d_state      = 0
0.00.051.274 I print_info: ssm_dt_rank      = 0
0.00.051.274 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.274 I print_info: model type       = 1.4B
0.00.051.275 I print_info: model params     = 1.41 B
0.00.051.275 I print_info: general.name     = 1.4B
0.00.051.275 I print_info: vocab type       = BPE
0.00.051.275 I print_info: n_vocab          = 50304
0.00.051.277 I print_info: n_merges         = 50009
0.00.051.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.277 I print_info: LF token         = 128 'Ä'
0.00.051.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.278 I print_info: max token length = 1024
0.00.053.296 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.297 I load_tensors: offloading output layer to GPU
0.00.053.297 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.307 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.309 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.588 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.589 I llama_new_context_with_model: n_ctx         = 128
0.00.053.589 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.589 I llama_new_context_with_model: n_batch       = 128
0.00.053.589 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.589 I llama_new_context_with_model: flash_attn    = 0
0.00.053.590 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.590 I llama_new_context_with_model: freq_scale    = 1
0.00.053.590 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.591 I ggml_metal_init: allocating
0.00.053.594 I ggml_metal_init: found device: Apple M4
0.00.053.596 I ggml_metal_init: picking default device: Apple M4
0.00.054.175 I ggml_metal_init: using embedded metal library
0.00.056.553 I ggml_metal_init: GPU name:   Apple M4
0.00.056.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.556 I ggml_metal_init: simdgroup reduction   = true
0.00.056.556 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.556 I ggml_metal_init: has bfloat            = true
0.00.056.556 I ggml_metal_init: use bfloat            = true
0.00.056.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.557 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.537 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.830 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.832 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.845 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.699 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.700 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.701 I llama_new_context_with_model: graph nodes  = 967
0.00.068.701 I llama_new_context_with_model: graph splits = 2
0.00.068.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.517 I 
0.00.712.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.554 I perplexity: tokenizing the input ..
0.00.720.086 I perplexity: tokenization took 7.53 ms
0.00.720.090 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.584 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.855.837 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.855.864 I llama_perf_context_print:        load time =     703.66 ms
0.00.855.865 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.31 tokens per second)
0.00.855.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.866 I llama_perf_context_print:       total time =     143.35 ms /   129 tokens
0.00.856.306 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.079s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.873 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.887 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.893 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.894 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.894 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.895 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.896 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.896 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.897 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.897 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.900 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.888 I llama_model_loader: - type  f32:  194 tensors
0.00.025.889 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.889 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.889 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.890 I print_info: file format = GGUF V3 (latest)
0.00.025.896 I print_info: file type   = Q2_K - Medium
0.00.025.897 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.949 I load: special tokens cache size = 25
0.00.050.883 I load: token to piece cache size = 0.2984 MB
0.00.050.892 I print_info: arch             = gptneox
0.00.050.893 I print_info: vocab_only       = 0
0.00.050.894 I print_info: n_ctx_train      = 2048
0.00.050.894 I print_info: n_embd           = 2048
0.00.050.894 I print_info: n_layer          = 24
0.00.050.896 I print_info: n_head           = 16
0.00.050.897 I print_info: n_head_kv        = 16
0.00.050.897 I print_info: n_rot            = 32
0.00.050.899 I print_info: n_swa            = 0
0.00.050.899 I print_info: n_embd_head_k    = 128
0.00.050.899 I print_info: n_embd_head_v    = 128
0.00.050.900 I print_info: n_gqa            = 1
0.00.050.901 I print_info: n_embd_k_gqa     = 2048
0.00.050.901 I print_info: n_embd_v_gqa     = 2048
0.00.050.902 I print_info: f_norm_eps       = 1.0e-05
0.00.050.902 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.902 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.902 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.903 I print_info: f_logit_scale    = 0.0e+00
0.00.050.903 I print_info: n_ff             = 8192
0.00.050.903 I print_info: n_expert         = 0
0.00.050.904 I print_info: n_expert_used    = 0
0.00.050.904 I print_info: causal attn      = 1
0.00.050.904 I print_info: pooling type     = 0
0.00.050.904 I print_info: rope type        = 2
0.00.050.904 I print_info: rope scaling     = linear
0.00.050.905 I print_info: freq_base_train  = 10000.0
0.00.050.905 I print_info: freq_scale_train = 1
0.00.050.906 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.906 I print_info: rope_finetuned   = unknown
0.00.050.906 I print_info: ssm_d_conv       = 0
0.00.050.908 I print_info: ssm_d_inner      = 0
0.00.050.908 I print_info: ssm_d_state      = 0
0.00.050.908 I print_info: ssm_dt_rank      = 0
0.00.050.908 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.908 I print_info: model type       = 1.4B
0.00.050.909 I print_info: model params     = 1.41 B
0.00.050.909 I print_info: general.name     = 1.4B
0.00.050.909 I print_info: vocab type       = BPE
0.00.050.909 I print_info: n_vocab          = 50304
0.00.050.909 I print_info: n_merges         = 50009
0.00.050.910 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.914 I print_info: LF token         = 128 'Ä'
0.00.050.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.914 I print_info: max token length = 1024
0.00.052.657 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.657 I load_tensors: offloading output layer to GPU
0.00.052.657 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.663 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.663 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.944 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.945 I llama_new_context_with_model: n_ctx         = 128
0.00.052.945 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.945 I llama_new_context_with_model: n_batch       = 128
0.00.052.946 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.946 I llama_new_context_with_model: flash_attn    = 0
0.00.052.946 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.946 I llama_new_context_with_model: freq_scale    = 1
0.00.052.947 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.947 I ggml_metal_init: allocating
0.00.052.950 I ggml_metal_init: found device: Apple M4
0.00.052.952 I ggml_metal_init: picking default device: Apple M4
0.00.053.501 I ggml_metal_init: using embedded metal library
0.00.055.782 I ggml_metal_init: GPU name:   Apple M4
0.00.055.784 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.785 I ggml_metal_init: simdgroup reduction   = true
0.00.055.785 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.785 I ggml_metal_init: has bfloat            = true
0.00.055.785 I ggml_metal_init: use bfloat            = true
0.00.055.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.590 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.863 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.866 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.880 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.798 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.799 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.799 I llama_new_context_with_model: graph nodes  = 967
0.00.067.800 I llama_new_context_with_model: graph splits = 2
0.00.067.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.960 I 
0.00.454.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.454.997 I perplexity: tokenizing the input ..
0.00.462.677 I perplexity: tokenization took 7.679 ms
0.00.462.681 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.594.977 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.596.163 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.596.199 I llama_perf_context_print:        load time =     445.08 ms
0.00.596.200 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.18 tokens per second)
0.00.596.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.596.201 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.596.658 I ggml_metal_free: deallocating

real	0m0.612s
user	0m0.078s
sys	0m0.075s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.696 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.853 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.854 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.855 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.866 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.920 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.040 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.121 I llama_model_loader: - type  f32:  194 tensors
0.00.025.121 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.121 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.122 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.122 I print_info: file format = GGUF V3 (latest)
0.00.025.129 I print_info: file type   = Q3_K - Medium
0.00.025.130 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.266 I load: special tokens cache size = 25
0.00.050.208 I load: token to piece cache size = 0.2984 MB
0.00.050.222 I print_info: arch             = gptneox
0.00.050.223 I print_info: vocab_only       = 0
0.00.050.224 I print_info: n_ctx_train      = 2048
0.00.050.224 I print_info: n_embd           = 2048
0.00.050.224 I print_info: n_layer          = 24
0.00.050.227 I print_info: n_head           = 16
0.00.050.228 I print_info: n_head_kv        = 16
0.00.050.228 I print_info: n_rot            = 32
0.00.050.228 I print_info: n_swa            = 0
0.00.050.228 I print_info: n_embd_head_k    = 128
0.00.050.231 I print_info: n_embd_head_v    = 128
0.00.050.231 I print_info: n_gqa            = 1
0.00.050.232 I print_info: n_embd_k_gqa     = 2048
0.00.050.233 I print_info: n_embd_v_gqa     = 2048
0.00.050.234 I print_info: f_norm_eps       = 1.0e-05
0.00.050.235 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.235 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.235 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.235 I print_info: f_logit_scale    = 0.0e+00
0.00.050.236 I print_info: n_ff             = 8192
0.00.050.236 I print_info: n_expert         = 0
0.00.050.236 I print_info: n_expert_used    = 0
0.00.050.236 I print_info: causal attn      = 1
0.00.050.236 I print_info: pooling type     = 0
0.00.050.238 I print_info: rope type        = 2
0.00.050.241 I print_info: rope scaling     = linear
0.00.050.242 I print_info: freq_base_train  = 10000.0
0.00.050.242 I print_info: freq_scale_train = 1
0.00.050.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.247 I print_info: rope_finetuned   = unknown
0.00.050.248 I print_info: ssm_d_conv       = 0
0.00.050.248 I print_info: ssm_d_inner      = 0
0.00.050.249 I print_info: ssm_d_state      = 0
0.00.050.249 I print_info: ssm_dt_rank      = 0
0.00.050.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.250 I print_info: model type       = 1.4B
0.00.050.250 I print_info: model params     = 1.41 B
0.00.050.250 I print_info: general.name     = 1.4B
0.00.050.251 I print_info: vocab type       = BPE
0.00.050.251 I print_info: n_vocab          = 50304
0.00.050.251 I print_info: n_merges         = 50009
0.00.050.253 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.253 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.253 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.253 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.254 I print_info: LF token         = 128 'Ä'
0.00.050.255 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.256 I print_info: max token length = 1024
0.00.052.156 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.156 I load_tensors: offloading output layer to GPU
0.00.052.157 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.167 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.168 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.521 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.522 I llama_new_context_with_model: n_ctx         = 128
0.00.052.522 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.523 I llama_new_context_with_model: n_batch       = 128
0.00.052.523 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.523 I llama_new_context_with_model: flash_attn    = 0
0.00.052.523 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.524 I llama_new_context_with_model: freq_scale    = 1
0.00.052.524 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.524 I ggml_metal_init: allocating
0.00.052.527 I ggml_metal_init: found device: Apple M4
0.00.052.529 I ggml_metal_init: picking default device: Apple M4
0.00.053.105 I ggml_metal_init: using embedded metal library
0.00.055.443 I ggml_metal_init: GPU name:   Apple M4
0.00.055.444 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.445 I ggml_metal_init: simdgroup reduction   = true
0.00.055.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.445 I ggml_metal_init: has bfloat            = true
0.00.055.446 I ggml_metal_init: use bfloat            = true
0.00.055.446 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.135 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.415 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.417 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.434 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.282 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.284 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.284 I llama_new_context_with_model: graph nodes  = 967
0.00.067.284 I llama_new_context_with_model: graph splits = 2
0.00.067.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.134 I 
0.00.476.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.173 I perplexity: tokenizing the input ..
0.00.483.833 I perplexity: tokenization took 7.658 ms
0.00.483.837 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.275 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.578 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.594 I llama_perf_context_print:        load time =     467.43 ms
0.00.616.596 I llama_perf_context_print: prompt eval time =     131.19 ms /   128 tokens (    1.02 ms per token,   975.70 tokens per second)
0.00.616.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.598 I llama_perf_context_print:       total time =     140.46 ms /   129 tokens
0.00.616.998 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.079s
sys	0m0.073s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.850 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.137 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.142 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.143 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.143 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.146 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.146 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.330 I llama_model_loader: - type  f32:  194 tensors
0.00.025.331 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.331 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.331 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.332 I print_info: file format = GGUF V3 (latest)
0.00.025.340 I print_info: file type   = Q4_K - Medium
0.00.025.341 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.035 I load: special tokens cache size = 25
0.00.051.116 I load: token to piece cache size = 0.2984 MB
0.00.051.135 I print_info: arch             = gptneox
0.00.051.136 I print_info: vocab_only       = 0
0.00.051.137 I print_info: n_ctx_train      = 2048
0.00.051.137 I print_info: n_embd           = 2048
0.00.051.137 I print_info: n_layer          = 24
0.00.051.141 I print_info: n_head           = 16
0.00.051.142 I print_info: n_head_kv        = 16
0.00.051.142 I print_info: n_rot            = 32
0.00.051.142 I print_info: n_swa            = 0
0.00.051.142 I print_info: n_embd_head_k    = 128
0.00.051.142 I print_info: n_embd_head_v    = 128
0.00.051.144 I print_info: n_gqa            = 1
0.00.051.145 I print_info: n_embd_k_gqa     = 2048
0.00.051.146 I print_info: n_embd_v_gqa     = 2048
0.00.051.147 I print_info: f_norm_eps       = 1.0e-05
0.00.051.147 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.154 I print_info: f_logit_scale    = 0.0e+00
0.00.051.155 I print_info: n_ff             = 8192
0.00.051.155 I print_info: n_expert         = 0
0.00.051.155 I print_info: n_expert_used    = 0
0.00.051.155 I print_info: causal attn      = 1
0.00.051.156 I print_info: pooling type     = 0
0.00.051.156 I print_info: rope type        = 2
0.00.051.156 I print_info: rope scaling     = linear
0.00.051.156 I print_info: freq_base_train  = 10000.0
0.00.051.157 I print_info: freq_scale_train = 1
0.00.051.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.158 I print_info: rope_finetuned   = unknown
0.00.051.159 I print_info: ssm_d_conv       = 0
0.00.051.159 I print_info: ssm_d_inner      = 0
0.00.051.159 I print_info: ssm_d_state      = 0
0.00.051.159 I print_info: ssm_dt_rank      = 0
0.00.051.159 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.159 I print_info: model type       = 1.4B
0.00.051.160 I print_info: model params     = 1.41 B
0.00.051.160 I print_info: general.name     = 1.4B
0.00.051.160 I print_info: vocab type       = BPE
0.00.051.160 I print_info: n_vocab          = 50304
0.00.051.160 I print_info: n_merges         = 50009
0.00.051.161 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.161 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.161 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.161 I print_info: LF token         = 128 'Ä'
0.00.051.162 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.162 I print_info: max token length = 1024
0.00.053.166 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.166 I load_tensors: offloading output layer to GPU
0.00.053.166 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.177 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.179 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.461 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.462 I llama_new_context_with_model: n_ctx         = 128
0.00.053.462 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.462 I llama_new_context_with_model: n_batch       = 128
0.00.053.463 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.463 I llama_new_context_with_model: flash_attn    = 0
0.00.053.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.463 I llama_new_context_with_model: freq_scale    = 1
0.00.053.464 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.464 I ggml_metal_init: allocating
0.00.053.467 I ggml_metal_init: found device: Apple M4
0.00.053.469 I ggml_metal_init: picking default device: Apple M4
0.00.054.079 I ggml_metal_init: using embedded metal library
0.00.056.491 I ggml_metal_init: GPU name:   Apple M4
0.00.056.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.494 I ggml_metal_init: simdgroup reduction   = true
0.00.056.495 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.495 I ggml_metal_init: has bfloat            = true
0.00.056.495 I ggml_metal_init: use bfloat            = true
0.00.056.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.674 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.980 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.982 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.998 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.849 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.851 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.851 I llama_new_context_with_model: graph nodes  = 967
0.00.068.851 I llama_new_context_with_model: graph splits = 2
0.00.068.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.287 I 
0.00.572.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.348 I perplexity: tokenizing the input ..
0.00.580.072 I perplexity: tokenization took 7.723 ms
0.00.580.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.104 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.715.300 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.715.325 I llama_perf_context_print:        load time =     563.43 ms
0.00.715.326 I llama_perf_context_print: prompt eval time =     133.80 ms /   128 tokens (    1.05 ms per token,   956.64 tokens per second)
0.00.715.327 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.327 I llama_perf_context_print:       total time =     143.04 ms /   129 tokens
0.00.715.855 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.080s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.703 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.922 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.923 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.923 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.924 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.925 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.925 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.926 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.926 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.927 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.931 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.616 I llama_model_loader: - type  f32:  194 tensors
0.00.025.617 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.617 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.617 I print_info: file format = GGUF V3 (latest)
0.00.025.624 I print_info: file type   = Q5_K - Medium
0.00.025.625 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.676 I load: special tokens cache size = 25
0.00.050.607 I load: token to piece cache size = 0.2984 MB
0.00.050.622 I print_info: arch             = gptneox
0.00.050.623 I print_info: vocab_only       = 0
0.00.050.623 I print_info: n_ctx_train      = 2048
0.00.050.623 I print_info: n_embd           = 2048
0.00.050.624 I print_info: n_layer          = 24
0.00.050.627 I print_info: n_head           = 16
0.00.050.627 I print_info: n_head_kv        = 16
0.00.050.630 I print_info: n_rot            = 32
0.00.050.631 I print_info: n_swa            = 0
0.00.050.631 I print_info: n_embd_head_k    = 128
0.00.050.631 I print_info: n_embd_head_v    = 128
0.00.050.632 I print_info: n_gqa            = 1
0.00.050.632 I print_info: n_embd_k_gqa     = 2048
0.00.050.633 I print_info: n_embd_v_gqa     = 2048
0.00.050.633 I print_info: f_norm_eps       = 1.0e-05
0.00.050.634 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.634 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.634 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.634 I print_info: f_logit_scale    = 0.0e+00
0.00.050.638 I print_info: n_ff             = 8192
0.00.050.640 I print_info: n_expert         = 0
0.00.050.640 I print_info: n_expert_used    = 0
0.00.050.640 I print_info: causal attn      = 1
0.00.050.640 I print_info: pooling type     = 0
0.00.050.640 I print_info: rope type        = 2
0.00.050.641 I print_info: rope scaling     = linear
0.00.050.641 I print_info: freq_base_train  = 10000.0
0.00.050.641 I print_info: freq_scale_train = 1
0.00.050.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.642 I print_info: rope_finetuned   = unknown
0.00.050.642 I print_info: ssm_d_conv       = 0
0.00.050.642 I print_info: ssm_d_inner      = 0
0.00.050.642 I print_info: ssm_d_state      = 0
0.00.050.642 I print_info: ssm_dt_rank      = 0
0.00.050.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.642 I print_info: model type       = 1.4B
0.00.050.645 I print_info: model params     = 1.41 B
0.00.050.645 I print_info: general.name     = 1.4B
0.00.050.645 I print_info: vocab type       = BPE
0.00.050.646 I print_info: n_vocab          = 50304
0.00.050.646 I print_info: n_merges         = 50009
0.00.050.646 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.647 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.647 I print_info: LF token         = 128 'Ä'
0.00.050.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.648 I print_info: max token length = 1024
0.00.052.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.602 I load_tensors: offloading output layer to GPU
0.00.052.602 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.612 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.614 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.907 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.908 I llama_new_context_with_model: n_ctx         = 128
0.00.052.908 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.908 I llama_new_context_with_model: n_batch       = 128
0.00.052.908 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.908 I llama_new_context_with_model: flash_attn    = 0
0.00.052.909 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.909 I llama_new_context_with_model: freq_scale    = 1
0.00.052.909 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.910 I ggml_metal_init: allocating
0.00.052.912 I ggml_metal_init: found device: Apple M4
0.00.052.914 I ggml_metal_init: picking default device: Apple M4
0.00.053.492 I ggml_metal_init: using embedded metal library
0.00.055.839 I ggml_metal_init: GPU name:   Apple M4
0.00.055.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.841 I ggml_metal_init: simdgroup reduction   = true
0.00.055.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.841 I ggml_metal_init: has bfloat            = true
0.00.055.841 I ggml_metal_init: use bfloat            = true
0.00.055.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.545 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.815 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.817 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.831 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.713 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.714 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.714 I llama_new_context_with_model: graph nodes  = 967
0.00.067.714 I llama_new_context_with_model: graph splits = 2
0.00.067.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.101 I 
0.00.616.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.137 I perplexity: tokenizing the input ..
0.00.623.740 I perplexity: tokenization took 7.601 ms
0.00.623.743 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.681 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.765.942 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.765.980 I llama_perf_context_print:        load time =     606.39 ms
0.00.765.982 I llama_perf_context_print: prompt eval time =     140.71 ms /   128 tokens (    1.10 ms per token,   909.70 tokens per second)
0.00.765.983 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.984 I llama_perf_context_print:       total time =     149.88 ms /   129 tokens
0.00.766.397 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.968 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.020 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.034 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.125 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.106 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.107 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.109 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.109 I llama_model_loader: - type  f32:  194 tensors
0.00.025.109 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.110 I print_info: file format = GGUF V3 (latest)
0.00.025.117 I print_info: file type   = Q6_K
0.00.025.118 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.722 I load: special tokens cache size = 25
0.00.050.640 I load: token to piece cache size = 0.2984 MB
0.00.050.654 I print_info: arch             = gptneox
0.00.050.656 I print_info: vocab_only       = 0
0.00.050.656 I print_info: n_ctx_train      = 2048
0.00.050.656 I print_info: n_embd           = 2048
0.00.050.656 I print_info: n_layer          = 24
0.00.050.659 I print_info: n_head           = 16
0.00.050.659 I print_info: n_head_kv        = 16
0.00.050.659 I print_info: n_rot            = 32
0.00.050.660 I print_info: n_swa            = 0
0.00.050.660 I print_info: n_embd_head_k    = 128
0.00.050.660 I print_info: n_embd_head_v    = 128
0.00.050.661 I print_info: n_gqa            = 1
0.00.050.661 I print_info: n_embd_k_gqa     = 2048
0.00.050.662 I print_info: n_embd_v_gqa     = 2048
0.00.050.662 I print_info: f_norm_eps       = 1.0e-05
0.00.050.663 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.663 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.663 I print_info: f_logit_scale    = 0.0e+00
0.00.050.664 I print_info: n_ff             = 8192
0.00.050.664 I print_info: n_expert         = 0
0.00.050.665 I print_info: n_expert_used    = 0
0.00.050.665 I print_info: causal attn      = 1
0.00.050.665 I print_info: pooling type     = 0
0.00.050.665 I print_info: rope type        = 2
0.00.050.665 I print_info: rope scaling     = linear
0.00.050.665 I print_info: freq_base_train  = 10000.0
0.00.050.666 I print_info: freq_scale_train = 1
0.00.050.666 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.666 I print_info: rope_finetuned   = unknown
0.00.050.666 I print_info: ssm_d_conv       = 0
0.00.050.666 I print_info: ssm_d_inner      = 0
0.00.050.666 I print_info: ssm_d_state      = 0
0.00.050.666 I print_info: ssm_dt_rank      = 0
0.00.050.669 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.669 I print_info: model type       = 1.4B
0.00.050.669 I print_info: model params     = 1.41 B
0.00.050.669 I print_info: general.name     = 1.4B
0.00.050.670 I print_info: vocab type       = BPE
0.00.050.670 I print_info: n_vocab          = 50304
0.00.050.670 I print_info: n_merges         = 50009
0.00.050.670 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.670 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.671 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.671 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.671 I print_info: LF token         = 128 'Ä'
0.00.050.671 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.671 I print_info: max token length = 1024
0.00.052.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.701 I load_tensors: offloading output layer to GPU
0.00.052.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.712 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.714 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.028 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.029 I llama_new_context_with_model: n_ctx         = 128
0.00.053.029 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.029 I llama_new_context_with_model: n_batch       = 128
0.00.053.029 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.029 I llama_new_context_with_model: flash_attn    = 0
0.00.053.029 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.030 I llama_new_context_with_model: freq_scale    = 1
0.00.053.030 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.031 I ggml_metal_init: allocating
0.00.053.034 I ggml_metal_init: found device: Apple M4
0.00.053.036 I ggml_metal_init: picking default device: Apple M4
0.00.053.619 I ggml_metal_init: using embedded metal library
0.00.056.009 I ggml_metal_init: GPU name:   Apple M4
0.00.056.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.012 I ggml_metal_init: simdgroup reduction   = true
0.00.056.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.012 I ggml_metal_init: has bfloat            = true
0.00.056.013 I ggml_metal_init: use bfloat            = true
0.00.056.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.940 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.195 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.200 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.214 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.173 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.175 I llama_new_context_with_model: graph nodes  = 967
0.00.068.175 I llama_new_context_with_model: graph splits = 2
0.00.068.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.321.359 I 
0.00.321.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.321.393 I perplexity: tokenizing the input ..
0.00.328.657 I perplexity: tokenization took 7.262 ms
0.00.328.660 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.468.762 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.469.959 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.469.985 I llama_perf_context_print:        load time =     312.39 ms
0.00.469.986 I llama_perf_context_print: prompt eval time =     139.88 ms /   128 tokens (    1.09 ms per token,   915.10 tokens per second)
0.00.469.987 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.469.988 I llama_perf_context_print:       total time =     148.63 ms /   129 tokens
0.00.470.526 I ggml_metal_free: deallocating

real	0m0.484s
user	0m0.079s
sys	0m0.072s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.221 I build: 4479 (1d1f2649) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.824 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.541 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.551 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.552 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.553 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.120 I llama_model_loader: - type  f32:  194 tensors
0.00.056.121 I llama_model_loader: - type  f16:   98 tensors
0.00.056.122 I print_info: file format = GGUF V3 (latest)
0.00.056.136 I print_info: file type   = all F32 (guessed)
0.00.056.137 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.813 I load: special tokens cache size = 25
0.00.088.269 I load: token to piece cache size = 0.2984 MB
0.00.088.283 I print_info: arch             = gptneox
0.00.088.284 I print_info: vocab_only       = 0
0.00.088.285 I print_info: n_ctx_train      = 2048
0.00.088.285 I print_info: n_embd           = 2048
0.00.088.285 I print_info: n_layer          = 24
0.00.088.288 I print_info: n_head           = 16
0.00.088.289 I print_info: n_head_kv        = 16
0.00.088.289 I print_info: n_rot            = 32
0.00.088.289 I print_info: n_swa            = 0
0.00.088.289 I print_info: n_embd_head_k    = 128
0.00.088.291 I print_info: n_embd_head_v    = 128
0.00.088.291 I print_info: n_gqa            = 1
0.00.088.292 I print_info: n_embd_k_gqa     = 2048
0.00.088.293 I print_info: n_embd_v_gqa     = 2048
0.00.088.293 I print_info: f_norm_eps       = 1.0e-05
0.00.088.293 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.294 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.294 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.294 I print_info: f_logit_scale    = 0.0e+00
0.00.088.295 I print_info: n_ff             = 8192
0.00.088.295 I print_info: n_expert         = 0
0.00.088.295 I print_info: n_expert_used    = 0
0.00.088.295 I print_info: causal attn      = 1
0.00.088.295 I print_info: pooling type     = 0
0.00.088.295 I print_info: rope type        = 2
0.00.088.296 I print_info: rope scaling     = linear
0.00.088.296 I print_info: freq_base_train  = 10000.0
0.00.088.296 I print_info: freq_scale_train = 1
0.00.088.297 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.297 I print_info: rope_finetuned   = unknown
0.00.088.297 I print_info: ssm_d_conv       = 0
0.00.088.297 I print_info: ssm_d_inner      = 0
0.00.088.297 I print_info: ssm_d_state      = 0
0.00.088.297 I print_info: ssm_dt_rank      = 0
0.00.088.297 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.298 I print_info: model type       = 1.4B
0.00.088.299 I print_info: model params     = 1.41 B
0.00.088.299 I print_info: general.name     = 1.4B
0.00.088.300 I print_info: vocab type       = BPE
0.00.088.300 I print_info: n_vocab          = 50304
0.00.088.301 I print_info: n_merges         = 50009
0.00.088.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.302 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.302 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.302 I print_info: LF token         = 128 'Ä'
0.00.088.302 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.303 I print_info: max token length = 1024
0.00.090.818 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.819 I load_tensors: offloading output layer to GPU
0.00.090.819 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.829 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.831 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.158 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.159 I llama_new_context_with_model: n_ctx         = 128
0.00.091.159 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.160 I llama_new_context_with_model: n_batch       = 128
0.00.091.160 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.160 I llama_new_context_with_model: flash_attn    = 0
0.00.091.160 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.160 I llama_new_context_with_model: freq_scale    = 1
0.00.091.161 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.161 I ggml_metal_init: allocating
0.00.091.164 I ggml_metal_init: found device: Apple M4
0.00.091.166 I ggml_metal_init: picking default device: Apple M4
0.00.091.770 I ggml_metal_init: using embedded metal library
0.00.094.283 I ggml_metal_init: GPU name:   Apple M4
0.00.094.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.286 I ggml_metal_init: simdgroup reduction   = true
0.00.094.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.286 I ggml_metal_init: has bfloat            = true
0.00.094.286 I ggml_metal_init: use bfloat            = true
0.00.094.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.569 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.767 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.769 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.783 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.655 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.656 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.656 I llama_new_context_with_model: graph nodes  = 967
0.00.105.657 I llama_new_context_with_model: graph splits = 2
0.00.105.658 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.658 I 
0.00.105.681 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.682 I compute_imatrix: tokenizing the input ..
0.00.112.718 I compute_imatrix: tokenization took 7.035 ms
0.00.112.720 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.643.374 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.645.846 I llama_perf_context_print:        load time =    1619.55 ms
0.01.645.846 I llama_perf_context_print: prompt eval time =    1530.05 ms /   128 tokens (   11.95 ms per token,    83.66 tokens per second)
0.01.645.847 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.645.848 I llama_perf_context_print:       total time =    1622.01 ms /   129 tokens
0.01.646.432 I ggml_metal_free: deallocating

real	0m1.843s
user	0m0.165s
sys	0m0.250s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4479 (1d1f2649)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13960a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13960aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13960b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13960b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13960bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13960c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13960c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13960cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13960d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13960d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13960dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13960e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13960ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13960f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13960fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139610390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1396111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1396118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1396120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1396127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139612f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139613620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139613ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1396145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1396148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139614eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139615b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139616060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1396167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139616a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139617850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139617b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139617fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1396188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139619230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1396196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139619b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13961a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13961a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13961a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13961ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13961b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13961bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13961c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13961c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13961cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13961d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13961db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13961e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13961e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13961eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13961f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13961f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13961fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139620300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1396205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139620a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139620f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1396213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139621840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139621ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139622180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139622620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139622ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139622f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139623400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1396238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1396247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1396257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139625d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139626270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1396267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139627260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1396277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139627d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1396287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139629790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139629ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13962a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13962a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13962acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13962b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13962b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13962bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13961b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13962c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13962c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13962ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13962d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13962d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13962de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13962e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13962e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13962ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13962f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13962f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13962fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139630350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1396308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139630df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139631290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139631730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139632070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139632510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1396329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1396332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1396340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139634570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139634a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139634eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139635350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1396357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139635c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1396365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139636a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139636f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1396373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139637850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139637cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139638190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139638ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139638f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1396398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139639d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13963a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13963a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13963ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13963afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13963b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13963b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13963bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13963c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13963c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13963cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13963d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13963d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13963d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13963de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13963e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13963e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13963ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13963f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13963f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13963f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13963fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139640310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1396407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139640c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1396410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139641590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139641a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139641ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139642370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139642810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139642cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139643150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1396435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139643a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1396443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139644870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139644d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1396451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139645650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139645af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1396468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139646d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1396476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139647b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139648a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139648fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139649530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1396497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139649e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13964a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13964aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13964b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13964b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13964b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13964bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13964c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13964cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13964d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13964d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13964db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13964e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13964e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13964edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13964f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13964f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13964fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1396502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139650d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1396512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139651d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1396522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139652d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1396532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1396542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1396552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1396557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139656290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1396567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139656d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139657280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1396577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139658270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1396587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1396597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139659d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13965a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13965a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13965acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13965b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13965b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13965bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13965c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13965c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13965ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13965d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13965d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13965dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13965e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13965e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13965ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13965f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13965f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13965fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1396601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139660740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139660c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139661130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1396615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139661a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139661f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1396623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139662850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139662cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139663190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139663630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139663ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139663f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139664410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1396648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139664d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1396651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139665740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139665e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139666580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139666ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1396673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139667e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139668130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139668740 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1380058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1380065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138006ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138007340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1380077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138007ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1380089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138009170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138009980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13800a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13800a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13800aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13800b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13800bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13800c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13800cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13800d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13800d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13800e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13800e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13800e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13800eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13800ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13800f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13800f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13800fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1380101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138010470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1380108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138010d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1380111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138011630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138011aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138011f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138012380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1380127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138012c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1380130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138013540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1380139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138013e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138014290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138014700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138014b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138014fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138015450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1380158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138015d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1380161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138016610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138016b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138017080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1380174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138017960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138017dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138018240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1380186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138018b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138018f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138019400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138019870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138019ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13801a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13801a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13801aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13801aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13801b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13801b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13801bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13801c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13801c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13801c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13801cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13801d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13801d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13801db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13801df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13801e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13801e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13801ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13801f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13801f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13801fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13801fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1380202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138020760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138020bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138021040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1380214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138021920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138021d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138022200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138022670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138022ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138022f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1380233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138023830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138023ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138024110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138024580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1380249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138024e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1380252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138025740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138025bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138026020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138026490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138026900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138026d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1380271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138027650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138027ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138027f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1380283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138028810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138028c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1380290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138029560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1380299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138029e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13802a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13802a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13802ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13802b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13802b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13802b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13802bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13802c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13802c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13802caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13802cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13802d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13802d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13802dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13802e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13802e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13802e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13802ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13802f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13802f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13802fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13802ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138030450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1380308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138030d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1380311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138031610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138031a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138031ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138032360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1380327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138032c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1380330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138033520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138033990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138033e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138034270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1380346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138034b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138034fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138035bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138035eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138036170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1380365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138036a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138036ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138037330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1380377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138037c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138038080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1380384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138038960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138038dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138039240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1380396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138039b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138039f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13803a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13803a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13803ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13803b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13803b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13803ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13803bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13803c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13803c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13803cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13803d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13803d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13803d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13964bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13964a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1396683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13964a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13961d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13961d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13961f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13964c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139614b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13961b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13961bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13961c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13961b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13961ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13961cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139613b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13960e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13961fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13962c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139667940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139616d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139617000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13964c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13964ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139615170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139615430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1396156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139668ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139668e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139669120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1396693e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1396696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139669960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139669c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139669ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13966a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13966a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13966a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13966a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13966aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13966af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13966b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13966b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13966b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13966ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13966bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13966bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13966c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13966c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13966c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13966cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13966cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13966d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13966d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13966d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13966d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13966db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13966de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13966e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13966e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13966e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13966e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13966ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13966eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13966f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13966f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13966f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13966f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13966fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13966ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1396701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1396704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139670760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139670a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139670ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139670fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139671260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139671520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1396717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139671aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139671d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139672020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1396722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1396725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139672860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139672b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139672de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1396730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139673360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139673620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1396738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139673ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139673e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139674120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13803dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138008160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138035280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138004680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13800b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13803e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13803e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13803e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13803e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13803ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13803ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13803f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13803f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13803fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1380403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138040660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138040ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1380410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138041620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138041df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138042330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138042870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138042db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1380432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138043830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138043d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138044030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1380442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1380445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138044b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138044df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1380450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138045370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138045630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1380458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138045bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138045e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138046130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1380463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1380466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138046970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138046c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138046ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1380471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138047470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138047730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1380479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138047cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138047f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138048230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1380484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1380487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138048a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138048d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138048ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1380492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138049570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138049830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138049af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138049db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13804a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13804a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13804a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13804a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13804ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13804ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13804b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13804b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13804b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13804b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13804bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13804beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13804c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13804c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13804c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13804c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13804cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13804cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13804d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13804d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13804d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13804da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13804dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13804dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13804e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13804e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13804e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13804eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13804ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13804f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13804f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13804f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13804f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13804fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13804fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1380500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138050370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138050630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1380508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138050e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138051130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1380513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1380516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138051970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138051c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138051ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1380521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138052470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138052730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1380529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138052cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138052f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138053230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1380534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1380537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138053a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138053d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138053ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1380542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138054570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138054830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138054af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138054db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138055070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138055330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1380555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1380558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138055b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138055e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1380560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1380563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138056670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138056930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138056bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138056eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138057170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138057430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1380576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1380579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138057c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138057f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1380581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1380584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138058770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138058a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138058cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138058fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138059270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138059530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1380597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138059ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138059d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13805a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13805a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13805a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13805a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13805ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13805adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13805b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13805b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13805b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13805b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13805bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13805be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13805c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13805c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13805c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13805c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13805cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13805cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13805d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13805d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13805d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13805d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13805dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13805df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13805e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13805e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13805e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13805ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13805ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13805eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13805f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13805f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13805f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13805faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13805fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1380601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138060470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1380608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138060d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1380611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138061630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138061aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138061f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138062380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1380627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138062c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1380630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138063540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1380639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138063ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138064330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1380647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138064c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138065080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1380655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138065ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138066620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1380668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138066ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138067460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138067a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138067fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1380685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138068b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138069120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1380696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138069ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13806a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13806a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13806ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13806b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13806b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13806bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13806c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13806caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13806d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13806d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13806dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13806e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13806e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13806ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13806f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13806f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13806fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138070420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1380709e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138070fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138071560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138071b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1380720e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1380726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138072c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138073220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1380737e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138073da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138074360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138074920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138074ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1380754a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138075a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138076020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1380765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138076ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138077160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138077720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138077ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1380782a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138078860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138078e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1380793e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1380799a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138079f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13807a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13807aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13807afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13807b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13807b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13807bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13807c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13807c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13807cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13807d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13807d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13807dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13807e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13807e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13807ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13807f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13807f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13807fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138080710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138080e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138081550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138081810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138082000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1380822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1380828d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.823s
user	0m0.297s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4479 (1d1f2649)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15600b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15600be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15600c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15600c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15600cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15600d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15600dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15600e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15600e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15600eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15600f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15600f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156010050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156010800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156011010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156011730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156011e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156012570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156012c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156013460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156013b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1560142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1560149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156015260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156015980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156015c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156016250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156016ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156017400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1560176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156017b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156017e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1560186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156018bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156018eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156019350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1560197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156019c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15601a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15601a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15601aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15601af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15601b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15601b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15601bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15601c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15601c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15601d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15601d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15601dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15601e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15601e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15601eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15601f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15601fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156020140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1560205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1560208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156020eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1560216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156021960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156021e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1560222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156022740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156022be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156023080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156023520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1560239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156023e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156024300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1560247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156024c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1560250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156025630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156025b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1560260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156026620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156026b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1560270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156027610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156027b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1560280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156028600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156028b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1560290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1560295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156029b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15602a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15602a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15602ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15602b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15602b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15602bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15602c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15602c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15602cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15602d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15601cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15602d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15602dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15602e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15602e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15602ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15602f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15602f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15602fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1560301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156030700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156030c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1560311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1560316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156031c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156032190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156032630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156032ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156032f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156033410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1560338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156033d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1560341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156034690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156034b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156034fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156035470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156035910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156035db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156036250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1560366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156036b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156037030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1560374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156037970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156037e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1560382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156038750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156038bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156039090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156039530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1560399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156039e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15603a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15603a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15603ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15603b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15603b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15603ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15603bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15603c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15603c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15603ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15603d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15603d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15603da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15603df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15603e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15603e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15603ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15603f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15603f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15603faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15603ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1560408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156040d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156041210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1560416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156041b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156041ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156042490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156042930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156043270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156043710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156043bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156044050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1560444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156044990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156044e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1560452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156045770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156045c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1560460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156046550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1560469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156046e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156047330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1560477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156047c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156048110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1560485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156048ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156049390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1560498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156049e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15604a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15604a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15604ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15604b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15604b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15604bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15604c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15604ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15604cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15604d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15604d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15604e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15604e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15604ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15604ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15604f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15604fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156050150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1560506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156050bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156051140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156051690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156051be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156052130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156052680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156052bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156053120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156053670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156053bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156054110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156054660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156054bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156055100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156055650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156055ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1560560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156056640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156056b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1560570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156057630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156057b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1560580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156058620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156058b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1560590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156059610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156059b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15605a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15605a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15605ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15605b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15605b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15605bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15605c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15605c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15605cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15605d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15605d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15605db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15605e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15605e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15605eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15605f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15605f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15605fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156060050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1560605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156060af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156061040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156061590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156061ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156062030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1560624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156062970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156062e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1560632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156063750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156063bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156064090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156064530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1560649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156064e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156065310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1560657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156065c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1560660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156066590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156066ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156067200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156067920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156068040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156068760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156068a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156069210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1560694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156069ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154708f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1547093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154709860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154709cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15470a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15470a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15470aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15470ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15470b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15470b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15470bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15470c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15470c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15470cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15470cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15470d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15470d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15470de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15470e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15470ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15470f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15470f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15470fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154710360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154710920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154710ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1547114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154711a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154712020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1547125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154712ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154713160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154713720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1547142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154714860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154714e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1547153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1547159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154715f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154716520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154716ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1547170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154717660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154717c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1547181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1547187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154718d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154719320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1547198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154719ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15471a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15471aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15471afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15471b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15471bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15471c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15471c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15471cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15471d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15471d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15471dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15471e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15471e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15471ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15471f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15471faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154720be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1547211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154721760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154721d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1547222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1547228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154723360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154723d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154724260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154724760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154724c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154725160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154725660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154725b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154726060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154726560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154726a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154726f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154727460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154728a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1547291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1547298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154729b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15472a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15472a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15472ac50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147f08130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147f35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147f04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147f0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147f3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147f3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147f3cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147f3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147f3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147f3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147f3ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147f3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147f3ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147f3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147f3f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147f3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147f400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147f405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147f40db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147f412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147f41830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147f41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147f422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147f427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147f42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147f42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147f432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147f43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147f43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147f43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147f43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147f44070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147f44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147f445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147f448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147f44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147f44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147f450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147f453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147f45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147f45930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147f45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147f45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147f46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147f46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147f466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147f469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147f46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147f46f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147f471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147f474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147f47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147f47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147f47cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147f47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147f48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147f48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147f487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147f48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147f48d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147f49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147f492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147f495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147f49870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147f49b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147f49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147f4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147f4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147f4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147f4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147f4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147f4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147f4b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147f4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147f4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147f4bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147f4bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147f4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147f4c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147f4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147f4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147f4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147f4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147f4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147f4d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147f4d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147f4da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147f4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147f4dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147f4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147f4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147f4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147f4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147f4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147f4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147f4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147f4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147f4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147f4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147f4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147f500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147f503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147f50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147f50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147f50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147f50eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147f51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147f51430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147f516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147f519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147f51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147f51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147f521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147f524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147f52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147f52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147f52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147f52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147f53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147f53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147f537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147f53ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147f53d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147f54030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147f542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147f545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147f54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147f54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147f54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147f550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147f55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147f55630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147f558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147f55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147f55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147f56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147f563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147f56970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147f56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147f56ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147f571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147f57470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147f57730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147f579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147f57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147f57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147f58230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147f584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147f587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147f58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147f58d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147f592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147f59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147f59830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147f59af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147f59db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147f5a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147f5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147f5a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147f5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147f5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147f5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147f5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147f5b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147f5b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147f5b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147f5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147f5beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147f5c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147f5c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147f5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147f5c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147f5cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147f5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147f5d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147f5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147f5dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147f5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147f5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147f5e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147f5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147f5ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147f5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147f5f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147f5f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147f5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147f60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147f60690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147f60b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147f60f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147f613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147f61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147f61cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147f62130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147f625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147f62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147f62fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147f634e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147f63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147f63dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147f64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147f646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147f64bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147f650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147f65c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147f65f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147f664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147f66a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147f67040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147f67bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147f68180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147f68740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147f68d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147f692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147f69880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147f69e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147f6a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147f6a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147f6af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147f6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147f6bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147f6c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147f6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147f6cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147f6d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147f6d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147f6dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147f6e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147f6e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147f6eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147f6f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147f6fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147f70000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147f705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147f70b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147f71140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147f71700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147f71cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147f72280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147f72840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147f72e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147f733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147f73980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147f73f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147f74500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147f74ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147f75080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147f75640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147f75c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147f761c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147f76780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147f76d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147f77300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147f778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147f77e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147f78440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147f78a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147f78fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147f79580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147f79b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147f7a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147f7a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147f7ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147f7b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147f7b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147f7ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147f7bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147f7c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147f7c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147f7ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147f7d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147f7d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147f7dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147f7e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147f7e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147f7ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147f7f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147f7fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147f80450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147f80b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147f80e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147f81620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147f818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147f81ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.243s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
