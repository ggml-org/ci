Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:49 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:305 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.583s
user	0m0.712s
sys	0m0.935s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Built target llama-gguf-hash
[ 22%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-run
[ 30%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Built target llava
[ 34%] Linking CXX static library libcommon.a
[ 34%] Built target llama-simple
[ 34%] Built target test-c
[ 34%] Built target llama-simple-chat
[ 34%] Built target llama-run
[ 34%] Built target llama-quantize-stats
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 35%] Built target common
[ 35%] Built target llava_static
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-log
[ 44%] Linking CXX executable ../bin/test-chat-template
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-log
[ 47%] Built target test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-chat-template
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Built target test-grammar-integration
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Built target test-llama-grammar
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-rope
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Built target test-json-schema-to-grammar
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-imatrix
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target llama-infill
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Built target llama-bench
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Built target llama-lookahead
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 84%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 85%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Built target llama-lookup-create
[ 85%] Built target llama-lookup-stats
[ 85%] Built target llama-lookup-merge
[ 85%] Built target llama-lookup
[ 85%] Built target llama-cli
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Built target llama-parallel
[ 86%] Generating loading.html.hpp
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Generating index.html.hpp
[ 87%] Built target llama-passkey
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 89%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 90%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-save-load-state
[ 91%] Built target llama-perplexity
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-speculative
[ 93%] Built target llama-quantize
[ 93%] Linking CXX executable ../../bin/llama-tokenize
[ 93%] Built target llama-retrieval
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-save-load-state
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-speculative
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 99%] Built target llama-q8dot
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.391s
user	0m5.275s
sys	0m9.199s

main: quantize time =  5091.51 ms
main:    total time =  5091.51 ms

main: quantize time =  1866.06 ms
main:    total time =  1866.06 ms

main: quantize time =  1640.42 ms
main:    total time =  1640.42 ms

main: quantize time =  2065.20 ms
main:    total time =  2065.20 ms

main: quantize time =  2725.83 ms
main:    total time =  2725.83 ms

main: quantize time =  4959.59 ms
main:    total time =  4959.59 ms

main: quantize time =  5629.00 ms
main:    total time =  5629.00 ms

main: quantize time =  6801.72 ms
main:    total time =  6801.72 ms

main: quantize time =  5794.74 ms
main:    total time =  5794.74 ms

main: quantize time =  4462.90 ms
main:    total time =  4462.90 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.112 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.244 I main: llama backend init
0.00.000.251 I main: load the model and apply lora adapter, if any
0.00.065.314 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.076.931 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.076.962 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.076.968 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.076.969 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.076.969 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.076.970 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.076.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.076.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.076.973 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.076.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.076.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.076.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.076.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.076.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.076.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.076.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.076.980 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.084.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.086.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.093.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.093.706 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.093.707 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.093.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.093.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.093.710 I llama_model_loader: - type  f32:  194 tensors
0.00.093.710 I llama_model_loader: - type  f16:   98 tensors
0.00.132.713 I llm_load_vocab: special tokens cache size = 25
0.00.140.674 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.140.677 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.140.677 I llm_load_print_meta: arch             = gptneox
0.00.140.677 I llm_load_print_meta: vocab type       = BPE
0.00.140.678 I llm_load_print_meta: n_vocab          = 50304
0.00.140.678 I llm_load_print_meta: n_merges         = 50009
0.00.140.678 I llm_load_print_meta: vocab_only       = 0
0.00.140.678 I llm_load_print_meta: n_ctx_train      = 2048
0.00.140.678 I llm_load_print_meta: n_embd           = 2048
0.00.140.679 I llm_load_print_meta: n_layer          = 24
0.00.140.682 I llm_load_print_meta: n_head           = 16
0.00.140.683 I llm_load_print_meta: n_head_kv        = 16
0.00.140.702 I llm_load_print_meta: n_rot            = 32
0.00.140.703 I llm_load_print_meta: n_swa            = 0
0.00.140.703 I llm_load_print_meta: n_embd_head_k    = 128
0.00.140.703 I llm_load_print_meta: n_embd_head_v    = 128
0.00.140.704 I llm_load_print_meta: n_gqa            = 1
0.00.140.705 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.140.705 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.140.706 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.140.706 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.140.708 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.140.708 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.140.708 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.140.709 I llm_load_print_meta: n_ff             = 8192
0.00.140.709 I llm_load_print_meta: n_expert         = 0
0.00.140.709 I llm_load_print_meta: n_expert_used    = 0
0.00.140.709 I llm_load_print_meta: causal attn      = 1
0.00.140.709 I llm_load_print_meta: pooling type     = 0
0.00.140.709 I llm_load_print_meta: rope type        = 2
0.00.140.710 I llm_load_print_meta: rope scaling     = linear
0.00.140.710 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.140.710 I llm_load_print_meta: freq_scale_train = 1
0.00.140.710 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.140.711 I llm_load_print_meta: rope_finetuned   = unknown
0.00.140.711 I llm_load_print_meta: ssm_d_conv       = 0
0.00.140.711 I llm_load_print_meta: ssm_d_inner      = 0
0.00.140.711 I llm_load_print_meta: ssm_d_state      = 0
0.00.140.711 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.140.711 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.140.721 I llm_load_print_meta: model type       = 1.4B
0.00.140.721 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.140.722 I llm_load_print_meta: model params     = 1.41 B
0.00.140.722 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.140.723 I llm_load_print_meta: general.name     = 1.4B
0.00.140.723 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.140.723 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.140.723 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.140.723 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.140.724 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.140.724 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.140.724 I llm_load_print_meta: max token length = 1024
0.00.143.453 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.143.453 I llm_load_tensors: offloading output layer to GPU
0.00.143.454 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.143.473 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.143.474 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.144.563 I llama_new_context_with_model: n_seq_max     = 1
0.00.144.564 I llama_new_context_with_model: n_ctx         = 2048
0.00.144.564 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.144.564 I llama_new_context_with_model: n_batch       = 2048
0.00.144.565 I llama_new_context_with_model: n_ubatch      = 512
0.00.144.565 I llama_new_context_with_model: flash_attn    = 0
0.00.144.565 I llama_new_context_with_model: freq_base     = 10000.0
0.00.144.566 I llama_new_context_with_model: freq_scale    = 1
0.00.144.566 I ggml_metal_init: allocating
0.00.144.577 I ggml_metal_init: found device: Apple M4
0.00.144.580 I ggml_metal_init: picking default device: Apple M4
0.00.145.283 I ggml_metal_init: using embedded metal library
0.00.155.374 I ggml_metal_init: GPU name:   Apple M4
0.00.155.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.155.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.155.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.155.378 I ggml_metal_init: simdgroup reduction   = true
0.00.155.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.155.378 I ggml_metal_init: has bfloat            = true
0.00.155.378 I ggml_metal_init: use bfloat            = true
0.00.155.378 I ggml_metal_init: hasUnifiedMemory      = true
0.00.155.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.200.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.200.883 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.200.901 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.201.795 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.201.796 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.201.797 I llama_new_context_with_model: graph nodes  = 967
0.00.201.797 I llama_new_context_with_model: graph splits = 2
0.00.201.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.277.810 I main: llama threadpool init, n_threads = 4
0.00.277.850 I 
0.00.277.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.277.882 I 
0.00.277.963 I sampler seed: 1234
0.00.277.968 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.277.991 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.277.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.277.993 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.123.079 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.02.123.080 I llama_perf_context_print:        load time =     212.48 ms
0.02.123.081 I llama_perf_context_print: prompt eval time =      43.82 ms /     7 tokens (    6.26 ms per token,   159.73 tokens per second)
0.02.123.081 I llama_perf_context_print:        eval time =    1798.30 ms /    63 runs   (   28.54 ms per token,    35.03 tokens per second)
0.02.123.082 I llama_perf_context_print:       total time =    1845.27 ms /    70 tokens
0.02.123.272 I ggml_metal_free: deallocating

real	0m2.423s
user	0m0.153s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.561 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.687 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.979 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.139 I llama_model_loader: - type  f32:  194 tensors
0.00.033.139 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.406 I llm_load_vocab: special tokens cache size = 25
0.00.060.360 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.364 I llm_load_print_meta: arch             = gptneox
0.00.060.365 I llm_load_print_meta: vocab type       = BPE
0.00.060.365 I llm_load_print_meta: n_vocab          = 50304
0.00.060.365 I llm_load_print_meta: n_merges         = 50009
0.00.060.365 I llm_load_print_meta: vocab_only       = 0
0.00.060.366 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.366 I llm_load_print_meta: n_embd           = 2048
0.00.060.366 I llm_load_print_meta: n_layer          = 24
0.00.060.370 I llm_load_print_meta: n_head           = 16
0.00.060.371 I llm_load_print_meta: n_head_kv        = 16
0.00.060.384 I llm_load_print_meta: n_rot            = 32
0.00.060.385 I llm_load_print_meta: n_swa            = 0
0.00.060.385 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.385 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.386 I llm_load_print_meta: n_gqa            = 1
0.00.060.387 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.387 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.388 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.388 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.388 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.388 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.389 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.389 I llm_load_print_meta: n_ff             = 8192
0.00.060.389 I llm_load_print_meta: n_expert         = 0
0.00.060.389 I llm_load_print_meta: n_expert_used    = 0
0.00.060.390 I llm_load_print_meta: causal attn      = 1
0.00.060.390 I llm_load_print_meta: pooling type     = 0
0.00.060.390 I llm_load_print_meta: rope type        = 2
0.00.060.390 I llm_load_print_meta: rope scaling     = linear
0.00.060.390 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.391 I llm_load_print_meta: freq_scale_train = 1
0.00.060.391 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.391 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.391 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.391 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.391 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.391 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.392 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.401 I llm_load_print_meta: model type       = 1.4B
0.00.060.402 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.402 I llm_load_print_meta: model params     = 1.41 B
0.00.060.402 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.402 I llm_load_print_meta: general.name     = 1.4B
0.00.060.403 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.403 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.403 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.403 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.403 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.404 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.404 I llm_load_print_meta: max token length = 1024
0.00.062.785 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.786 I llm_load_tensors: offloading output layer to GPU
0.00.062.786 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.797 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.798 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.745 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.745 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.745 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.746 I llama_new_context_with_model: n_batch       = 2048
0.00.063.746 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.746 I llama_new_context_with_model: flash_attn    = 0
0.00.063.746 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.747 I llama_new_context_with_model: freq_scale    = 1
0.00.063.747 I ggml_metal_init: allocating
0.00.063.752 I ggml_metal_init: found device: Apple M4
0.00.063.754 I ggml_metal_init: picking default device: Apple M4
0.00.064.425 I ggml_metal_init: using embedded metal library
0.00.066.883 I ggml_metal_init: GPU name:   Apple M4
0.00.066.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.885 I ggml_metal_init: simdgroup reduction   = true
0.00.066.886 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.886 I ggml_metal_init: has bfloat            = true
0.00.066.886 I ggml_metal_init: use bfloat            = true
0.00.066.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.761 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.779 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.806 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.930 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.932 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.932 I llama_new_context_with_model: graph nodes  = 967
0.00.102.933 I llama_new_context_with_model: graph splits = 2
0.00.102.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.238.389 I main: llama threadpool init, n_threads = 4
0.01.238.420 I 
0.01.238.450 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.238.450 I 
0.01.238.691 I sampler seed: 1234
0.01.238.695 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.238.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.238.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.238.707 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.334.018 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.02.334.019 I llama_perf_context_print:        load time =    1228.83 ms
0.02.334.020 I llama_perf_context_print: prompt eval time =      39.93 ms /     7 tokens (    5.70 ms per token,   175.32 tokens per second)
0.02.334.021 I llama_perf_context_print:        eval time =    1052.38 ms /    63 runs   (   16.70 ms per token,    59.86 tokens per second)
0.02.334.021 I llama_perf_context_print:       total time =    1095.63 ms /    70 tokens
0.02.334.182 I ggml_metal_free: deallocating

real	0m2.351s
user	0m0.113s
sys	0m0.208s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.206 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.576 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.577 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.547 I llama_model_loader: - type  f32:  194 tensors
0.00.040.547 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.332 I llm_load_vocab: special tokens cache size = 25
0.00.073.425 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.429 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.430 I llm_load_print_meta: arch             = gptneox
0.00.073.430 I llm_load_print_meta: vocab type       = BPE
0.00.073.431 I llm_load_print_meta: n_vocab          = 50304
0.00.073.431 I llm_load_print_meta: n_merges         = 50009
0.00.073.431 I llm_load_print_meta: vocab_only       = 0
0.00.073.431 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.432 I llm_load_print_meta: n_embd           = 2048
0.00.073.434 I llm_load_print_meta: n_layer          = 24
0.00.073.440 I llm_load_print_meta: n_head           = 16
0.00.073.441 I llm_load_print_meta: n_head_kv        = 16
0.00.073.454 I llm_load_print_meta: n_rot            = 32
0.00.073.454 I llm_load_print_meta: n_swa            = 0
0.00.073.454 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.454 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.455 I llm_load_print_meta: n_gqa            = 1
0.00.073.457 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.457 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.458 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.459 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.459 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.459 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.459 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.460 I llm_load_print_meta: n_ff             = 8192
0.00.073.460 I llm_load_print_meta: n_expert         = 0
0.00.073.460 I llm_load_print_meta: n_expert_used    = 0
0.00.073.461 I llm_load_print_meta: causal attn      = 1
0.00.073.461 I llm_load_print_meta: pooling type     = 0
0.00.073.461 I llm_load_print_meta: rope type        = 2
0.00.073.461 I llm_load_print_meta: rope scaling     = linear
0.00.073.462 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.462 I llm_load_print_meta: freq_scale_train = 1
0.00.073.465 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.465 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.465 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.465 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.466 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.466 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.466 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.476 I llm_load_print_meta: model type       = 1.4B
0.00.073.476 I llm_load_print_meta: model ftype      = Q4_0
0.00.073.477 I llm_load_print_meta: model params     = 1.41 B
0.00.073.477 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.073.477 I llm_load_print_meta: general.name     = 1.4B
0.00.073.478 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.479 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.479 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.073.479 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.479 I llm_load_print_meta: max token length = 1024
0.00.075.978 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.978 I llm_load_tensors: offloading output layer to GPU
0.00.075.979 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.990 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.075.991 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.077.335 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.336 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.336 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.337 I llama_new_context_with_model: n_batch       = 2048
0.00.077.337 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.337 I llama_new_context_with_model: flash_attn    = 0
0.00.077.338 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.338 I llama_new_context_with_model: freq_scale    = 1
0.00.077.338 I ggml_metal_init: allocating
0.00.077.343 I ggml_metal_init: found device: Apple M4
0.00.077.345 I ggml_metal_init: picking default device: Apple M4
0.00.078.214 I ggml_metal_init: using embedded metal library
0.00.081.995 I ggml_metal_init: GPU name:   Apple M4
0.00.081.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.998 I ggml_metal_init: simdgroup reduction   = true
0.00.081.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.999 I ggml_metal_init: has bfloat            = true
0.00.081.999 I ggml_metal_init: use bfloat            = true
0.00.082.000 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.125.528 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.536 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.560 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.818 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.821 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.821 I llama_new_context_with_model: graph nodes  = 967
0.00.126.821 I llama_new_context_with_model: graph splits = 2
0.00.126.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.047 I main: llama threadpool init, n_threads = 4
0.00.836.091 I 
0.00.836.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.121 I 
0.00.836.351 I sampler seed: 1234
0.00.836.357 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.399 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.418 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.418 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.520.501 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.520.501 I llama_perf_context_print:        load time =     825.83 ms
0.01.520.502 I llama_perf_context_print: prompt eval time =      45.71 ms /     7 tokens (    6.53 ms per token,   153.15 tokens per second)
0.01.520.503 I llama_perf_context_print:        eval time =     635.33 ms /    63 runs   (   10.08 ms per token,    99.16 tokens per second)
0.01.520.504 I llama_perf_context_print:       total time =     684.46 ms /    70 tokens
0.01.520.693 I ggml_metal_free: deallocating

real	0m1.537s
user	0m0.126s
sys	0m0.180s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.625 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.684 I llama_model_loader: - type  f32:  194 tensors
0.00.024.684 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.684 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.866 I llm_load_vocab: special tokens cache size = 25
0.00.050.858 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.860 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.860 I llm_load_print_meta: arch             = gptneox
0.00.050.861 I llm_load_print_meta: vocab type       = BPE
0.00.050.861 I llm_load_print_meta: n_vocab          = 50304
0.00.050.861 I llm_load_print_meta: n_merges         = 50009
0.00.050.861 I llm_load_print_meta: vocab_only       = 0
0.00.050.861 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.862 I llm_load_print_meta: n_embd           = 2048
0.00.050.862 I llm_load_print_meta: n_layer          = 24
0.00.050.864 I llm_load_print_meta: n_head           = 16
0.00.050.865 I llm_load_print_meta: n_head_kv        = 16
0.00.050.877 I llm_load_print_meta: n_rot            = 32
0.00.050.877 I llm_load_print_meta: n_swa            = 0
0.00.050.877 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.877 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.878 I llm_load_print_meta: n_gqa            = 1
0.00.050.879 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.880 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.880 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.881 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.881 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.881 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.881 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.884 I llm_load_print_meta: n_ff             = 8192
0.00.050.884 I llm_load_print_meta: n_expert         = 0
0.00.050.884 I llm_load_print_meta: n_expert_used    = 0
0.00.050.886 I llm_load_print_meta: causal attn      = 1
0.00.050.887 I llm_load_print_meta: pooling type     = 0
0.00.050.887 I llm_load_print_meta: rope type        = 2
0.00.050.888 I llm_load_print_meta: rope scaling     = linear
0.00.050.888 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.888 I llm_load_print_meta: freq_scale_train = 1
0.00.050.889 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.889 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.889 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.889 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.889 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.889 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.889 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.899 I llm_load_print_meta: model type       = 1.4B
0.00.050.899 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.900 I llm_load_print_meta: model params     = 1.41 B
0.00.050.900 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.900 I llm_load_print_meta: general.name     = 1.4B
0.00.050.901 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.901 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: max token length = 1024
0.00.052.847 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.847 I llm_load_tensors: offloading output layer to GPU
0.00.052.848 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.858 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.859 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.764 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.765 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.765 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.766 I llama_new_context_with_model: n_batch       = 2048
0.00.053.766 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.766 I llama_new_context_with_model: flash_attn    = 0
0.00.053.766 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.767 I llama_new_context_with_model: freq_scale    = 1
0.00.053.767 I ggml_metal_init: allocating
0.00.053.770 I ggml_metal_init: found device: Apple M4
0.00.053.772 I ggml_metal_init: picking default device: Apple M4
0.00.054.310 I ggml_metal_init: using embedded metal library
0.00.056.605 I ggml_metal_init: GPU name:   Apple M4
0.00.056.606 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.606 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.607 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.607 I ggml_metal_init: simdgroup reduction   = true
0.00.056.607 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.607 I ggml_metal_init: has bfloat            = true
0.00.056.608 I ggml_metal_init: use bfloat            = true
0.00.056.609 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.192 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.211 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.217 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.218 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.219 I llama_new_context_with_model: graph nodes  = 967
0.00.086.219 I llama_new_context_with_model: graph splits = 2
0.00.086.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.492 I main: llama threadpool init, n_threads = 4
0.00.680.529 I 
0.00.680.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.568 I 
0.00.680.810 I sampler seed: 1234
0.00.680.814 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.859 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.861 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.861 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.408.212 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66666.67 tokens per second)
0.01.408.213 I llama_perf_context_print:        load time =     671.86 ms
0.01.408.214 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.57 tokens per second)
0.01.408.216 I llama_perf_context_print:        eval time =     680.98 ms /    63 runs   (   10.81 ms per token,    92.51 tokens per second)
0.01.408.217 I llama_perf_context_print:       total time =     727.72 ms /    70 tokens
0.01.408.388 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.161 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.138 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.149 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.149 I llama_model_loader: - type  f32:  194 tensors
0.00.026.149 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.150 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.915 I llm_load_vocab: special tokens cache size = 25
0.00.052.822 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.825 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.825 I llm_load_print_meta: arch             = gptneox
0.00.052.826 I llm_load_print_meta: vocab type       = BPE
0.00.052.826 I llm_load_print_meta: n_vocab          = 50304
0.00.052.826 I llm_load_print_meta: n_merges         = 50009
0.00.052.826 I llm_load_print_meta: vocab_only       = 0
0.00.052.826 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.826 I llm_load_print_meta: n_embd           = 2048
0.00.052.828 I llm_load_print_meta: n_layer          = 24
0.00.052.832 I llm_load_print_meta: n_head           = 16
0.00.052.832 I llm_load_print_meta: n_head_kv        = 16
0.00.052.844 I llm_load_print_meta: n_rot            = 32
0.00.052.844 I llm_load_print_meta: n_swa            = 0
0.00.052.844 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.845 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.845 I llm_load_print_meta: n_gqa            = 1
0.00.052.846 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.847 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.847 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.847 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.848 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.848 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.848 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.849 I llm_load_print_meta: n_ff             = 8192
0.00.052.849 I llm_load_print_meta: n_expert         = 0
0.00.052.849 I llm_load_print_meta: n_expert_used    = 0
0.00.052.851 I llm_load_print_meta: causal attn      = 1
0.00.052.852 I llm_load_print_meta: pooling type     = 0
0.00.052.852 I llm_load_print_meta: rope type        = 2
0.00.052.852 I llm_load_print_meta: rope scaling     = linear
0.00.052.853 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.853 I llm_load_print_meta: freq_scale_train = 1
0.00.052.853 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.853 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.853 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.854 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.855 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.855 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.855 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.864 I llm_load_print_meta: model type       = 1.4B
0.00.052.865 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.865 I llm_load_print_meta: model params     = 1.41 B
0.00.052.865 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.866 I llm_load_print_meta: general.name     = 1.4B
0.00.052.866 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.866 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.866 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.866 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.867 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.867 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.867 I llm_load_print_meta: max token length = 1024
0.00.054.849 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.850 I llm_load_tensors: offloading output layer to GPU
0.00.054.850 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.861 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.862 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.759 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.760 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.760 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.760 I llama_new_context_with_model: n_batch       = 2048
0.00.055.760 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.760 I llama_new_context_with_model: flash_attn    = 0
0.00.055.761 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.761 I llama_new_context_with_model: freq_scale    = 1
0.00.055.762 I ggml_metal_init: allocating
0.00.055.768 I ggml_metal_init: found device: Apple M4
0.00.055.770 I ggml_metal_init: picking default device: Apple M4
0.00.056.334 I ggml_metal_init: using embedded metal library
0.00.058.831 I ggml_metal_init: GPU name:   Apple M4
0.00.058.833 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.833 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.834 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.834 I ggml_metal_init: simdgroup reduction   = true
0.00.058.834 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.834 I ggml_metal_init: has bfloat            = true
0.00.058.834 I ggml_metal_init: use bfloat            = true
0.00.058.835 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.702 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.712 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.730 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.808 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.809 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.810 I llama_new_context_with_model: graph nodes  = 967
0.00.090.810 I llama_new_context_with_model: graph splits = 2
0.00.090.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.550 I main: llama threadpool init, n_threads = 4
0.00.717.587 I 
0.00.717.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.618 I 
0.00.717.854 I sampler seed: 1234
0.00.717.858 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.896 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.901 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.901 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.504.428 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56483.69 tokens per second)
0.01.504.428 I llama_perf_context_print:        load time =     707.39 ms
0.01.504.429 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.37 tokens per second)
0.01.504.430 I llama_perf_context_print:        eval time =     740.40 ms /    63 runs   (   11.75 ms per token,    85.09 tokens per second)
0.01.504.430 I llama_perf_context_print:       total time =     786.88 ms /    70 tokens
0.01.504.613 I ggml_metal_free: deallocating

real	0m1.523s
user	0m0.110s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.723 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.729 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.730 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.731 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.732 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.732 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.732 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.733 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.733 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.733 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.734 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.868 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.989 I llama_model_loader: - type  f32:  194 tensors
0.00.024.989 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.989 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.885 I llm_load_vocab: special tokens cache size = 25
0.00.051.906 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.909 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.909 I llm_load_print_meta: arch             = gptneox
0.00.051.910 I llm_load_print_meta: vocab type       = BPE
0.00.051.910 I llm_load_print_meta: n_vocab          = 50304
0.00.051.910 I llm_load_print_meta: n_merges         = 50009
0.00.051.910 I llm_load_print_meta: vocab_only       = 0
0.00.051.910 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.911 I llm_load_print_meta: n_embd           = 2048
0.00.051.911 I llm_load_print_meta: n_layer          = 24
0.00.051.913 I llm_load_print_meta: n_head           = 16
0.00.051.914 I llm_load_print_meta: n_head_kv        = 16
0.00.051.926 I llm_load_print_meta: n_rot            = 32
0.00.051.926 I llm_load_print_meta: n_swa            = 0
0.00.051.926 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.927 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.927 I llm_load_print_meta: n_gqa            = 1
0.00.051.928 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.931 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.932 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.932 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.932 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.932 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.932 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.933 I llm_load_print_meta: n_ff             = 8192
0.00.051.933 I llm_load_print_meta: n_expert         = 0
0.00.051.933 I llm_load_print_meta: n_expert_used    = 0
0.00.051.935 I llm_load_print_meta: causal attn      = 1
0.00.051.936 I llm_load_print_meta: pooling type     = 0
0.00.051.936 I llm_load_print_meta: rope type        = 2
0.00.051.936 I llm_load_print_meta: rope scaling     = linear
0.00.051.936 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.937 I llm_load_print_meta: freq_scale_train = 1
0.00.051.937 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.938 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.938 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.938 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.938 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.939 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.948 I llm_load_print_meta: model type       = 1.4B
0.00.051.948 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.948 I llm_load_print_meta: model params     = 1.41 B
0.00.051.949 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.949 I llm_load_print_meta: general.name     = 1.4B
0.00.051.949 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.950 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.950 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.950 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.950 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.950 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.950 I llm_load_print_meta: max token length = 1024
0.00.054.001 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.001 I llm_load_tensors: offloading output layer to GPU
0.00.054.002 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.012 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.013 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.900 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.901 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.901 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.901 I llama_new_context_with_model: n_batch       = 2048
0.00.054.901 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.901 I llama_new_context_with_model: flash_attn    = 0
0.00.054.902 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.902 I llama_new_context_with_model: freq_scale    = 1
0.00.054.903 I ggml_metal_init: allocating
0.00.054.909 I ggml_metal_init: found device: Apple M4
0.00.054.911 I ggml_metal_init: picking default device: Apple M4
0.00.055.475 I ggml_metal_init: using embedded metal library
0.00.058.217 I ggml_metal_init: GPU name:   Apple M4
0.00.058.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.220 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.220 I ggml_metal_init: simdgroup reduction   = true
0.00.058.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.222 I ggml_metal_init: has bfloat            = true
0.00.058.222 I ggml_metal_init: use bfloat            = true
0.00.058.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.744 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.753 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.781 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.836 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.837 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.838 I llama_new_context_with_model: graph nodes  = 967
0.00.087.838 I llama_new_context_with_model: graph splits = 2
0.00.087.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.253 I main: llama threadpool init, n_threads = 4
0.00.797.296 I 
0.00.797.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.346 I 
0.00.797.586 I sampler seed: 1234
0.00.797.590 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.636 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.638 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.638 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.635.362 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.635.363 I llama_perf_context_print:        load time =     788.50 ms
0.01.635.363 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.56 tokens per second)
0.01.635.365 I llama_perf_context_print:        eval time =     792.41 ms /    63 runs   (   12.58 ms per token,    79.50 tokens per second)
0.01.635.365 I llama_perf_context_print:       total time =     838.11 ms /    70 tokens
0.01.635.564 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.721 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.437 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.440 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.677 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.678 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.679 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.680 I llama_model_loader: - type  f32:  194 tensors
0.00.024.680 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.680 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.680 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.733 I llm_load_vocab: special tokens cache size = 25
0.00.050.634 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.637 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.637 I llm_load_print_meta: arch             = gptneox
0.00.050.638 I llm_load_print_meta: vocab type       = BPE
0.00.050.638 I llm_load_print_meta: n_vocab          = 50304
0.00.050.638 I llm_load_print_meta: n_merges         = 50009
0.00.050.638 I llm_load_print_meta: vocab_only       = 0
0.00.050.638 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.638 I llm_load_print_meta: n_embd           = 2048
0.00.050.639 I llm_load_print_meta: n_layer          = 24
0.00.050.642 I llm_load_print_meta: n_head           = 16
0.00.050.643 I llm_load_print_meta: n_head_kv        = 16
0.00.050.655 I llm_load_print_meta: n_rot            = 32
0.00.050.655 I llm_load_print_meta: n_swa            = 0
0.00.050.655 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.657 I llm_load_print_meta: n_gqa            = 1
0.00.050.657 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.658 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.659 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.659 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.659 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.660 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.660 I llm_load_print_meta: n_ff             = 8192
0.00.050.661 I llm_load_print_meta: n_expert         = 0
0.00.050.661 I llm_load_print_meta: n_expert_used    = 0
0.00.050.661 I llm_load_print_meta: causal attn      = 1
0.00.050.661 I llm_load_print_meta: pooling type     = 0
0.00.050.661 I llm_load_print_meta: rope type        = 2
0.00.050.661 I llm_load_print_meta: rope scaling     = linear
0.00.050.662 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.662 I llm_load_print_meta: freq_scale_train = 1
0.00.050.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.662 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.662 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.662 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.663 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.663 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.663 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.672 I llm_load_print_meta: model type       = 1.4B
0.00.050.672 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.673 I llm_load_print_meta: model params     = 1.41 B
0.00.050.673 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.673 I llm_load_print_meta: general.name     = 1.4B
0.00.050.674 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.674 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.674 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.674 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.676 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.676 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.676 I llm_load_print_meta: max token length = 1024
0.00.052.515 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.515 I llm_load_tensors: offloading output layer to GPU
0.00.052.515 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.525 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.527 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.430 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.431 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.432 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.432 I llama_new_context_with_model: n_batch       = 2048
0.00.053.432 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.432 I llama_new_context_with_model: flash_attn    = 0
0.00.053.432 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.433 I llama_new_context_with_model: freq_scale    = 1
0.00.053.433 I ggml_metal_init: allocating
0.00.053.436 I ggml_metal_init: found device: Apple M4
0.00.053.438 I ggml_metal_init: picking default device: Apple M4
0.00.053.976 I ggml_metal_init: using embedded metal library
0.00.056.580 I ggml_metal_init: GPU name:   Apple M4
0.00.056.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.582 I ggml_metal_init: simdgroup reduction   = true
0.00.056.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.583 I ggml_metal_init: has bfloat            = true
0.00.056.583 I ggml_metal_init: use bfloat            = true
0.00.056.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.338 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.353 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.372 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.439 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.440 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.440 I llama_new_context_with_model: graph nodes  = 967
0.00.086.440 I llama_new_context_with_model: graph splits = 2
0.00.086.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.928 I main: llama threadpool init, n_threads = 4
0.00.494.971 I 
0.00.494.997 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.999 I 
0.00.495.237 I sampler seed: 1234
0.00.495.242 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.495.284 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.495.288 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.495.288 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.179.707 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.179.708 I llama_perf_context_print:        load time =     485.20 ms
0.01.179.709 I llama_perf_context_print: prompt eval time =      41.71 ms /     7 tokens (    5.96 ms per token,   167.83 tokens per second)
0.01.179.711 I llama_perf_context_print:        eval time =     639.75 ms /    63 runs   (   10.15 ms per token,    98.48 tokens per second)
0.01.179.712 I llama_perf_context_print:       total time =     684.78 ms /    70 tokens
0.01.179.901 I ggml_metal_free: deallocating

real	0m1.198s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.590 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.022 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.457 I llama_model_loader: - type  f32:  194 tensors
0.00.024.457 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.457 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.458 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.458 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.376 I llm_load_vocab: special tokens cache size = 25
0.00.051.389 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.392 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.392 I llm_load_print_meta: arch             = gptneox
0.00.051.393 I llm_load_print_meta: vocab type       = BPE
0.00.051.393 I llm_load_print_meta: n_vocab          = 50304
0.00.051.393 I llm_load_print_meta: n_merges         = 50009
0.00.051.393 I llm_load_print_meta: vocab_only       = 0
0.00.051.393 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.393 I llm_load_print_meta: n_embd           = 2048
0.00.051.394 I llm_load_print_meta: n_layer          = 24
0.00.051.396 I llm_load_print_meta: n_head           = 16
0.00.051.397 I llm_load_print_meta: n_head_kv        = 16
0.00.051.409 I llm_load_print_meta: n_rot            = 32
0.00.051.409 I llm_load_print_meta: n_swa            = 0
0.00.051.409 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.409 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.410 I llm_load_print_meta: n_gqa            = 1
0.00.051.411 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.412 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.412 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.413 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.413 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.413 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.413 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.414 I llm_load_print_meta: n_ff             = 8192
0.00.051.414 I llm_load_print_meta: n_expert         = 0
0.00.051.414 I llm_load_print_meta: n_expert_used    = 0
0.00.051.414 I llm_load_print_meta: causal attn      = 1
0.00.051.414 I llm_load_print_meta: pooling type     = 0
0.00.051.414 I llm_load_print_meta: rope type        = 2
0.00.051.415 I llm_load_print_meta: rope scaling     = linear
0.00.051.416 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.416 I llm_load_print_meta: freq_scale_train = 1
0.00.051.416 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.416 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.416 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.416 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.417 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.417 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.417 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.426 I llm_load_print_meta: model type       = 1.4B
0.00.051.426 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.427 I llm_load_print_meta: model params     = 1.41 B
0.00.051.427 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.427 I llm_load_print_meta: general.name     = 1.4B
0.00.051.427 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.428 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.428 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.428 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.428 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.428 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.429 I llm_load_print_meta: max token length = 1024
0.00.053.363 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.363 I llm_load_tensors: offloading output layer to GPU
0.00.053.363 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.374 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.375 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.259 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.260 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.260 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.260 I llama_new_context_with_model: n_batch       = 2048
0.00.054.261 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.261 I llama_new_context_with_model: flash_attn    = 0
0.00.054.261 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.261 I llama_new_context_with_model: freq_scale    = 1
0.00.054.262 I ggml_metal_init: allocating
0.00.054.265 I ggml_metal_init: found device: Apple M4
0.00.054.267 I ggml_metal_init: picking default device: Apple M4
0.00.054.840 I ggml_metal_init: using embedded metal library
0.00.057.135 I ggml_metal_init: GPU name:   Apple M4
0.00.057.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.138 I ggml_metal_init: simdgroup reduction   = true
0.00.057.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.138 I ggml_metal_init: has bfloat            = true
0.00.057.138 I ggml_metal_init: use bfloat            = true
0.00.057.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.153 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.158 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.176 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.116 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.117 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.117 I llama_new_context_with_model: graph nodes  = 967
0.00.087.118 I llama_new_context_with_model: graph splits = 2
0.00.087.127 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.542.628 I main: llama threadpool init, n_threads = 4
0.00.542.674 I 
0.00.542.724 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.542.725 I 
0.00.542.966 I sampler seed: 1234
0.00.542.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.015 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.017 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.017 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.294.609 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.294.610 I llama_perf_context_print:        load time =     533.03 ms
0.01.294.610 I llama_perf_context_print: prompt eval time =      43.80 ms /     7 tokens (    6.26 ms per token,   159.82 tokens per second)
0.01.294.612 I llama_perf_context_print:        eval time =     704.77 ms /    63 runs   (   11.19 ms per token,    89.39 tokens per second)
0.01.294.612 I llama_perf_context_print:       total time =     751.99 ms /    70 tokens
0.01.294.810 I ggml_metal_free: deallocating

real	0m1.312s
user	0m0.110s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.011.960 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.546 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.554 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.555 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.713 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.713 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.714 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.714 I llama_model_loader: - type  f32:  194 tensors
0.00.027.714 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.714 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.715 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.793 I llm_load_vocab: special tokens cache size = 25
0.00.053.696 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.699 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.699 I llm_load_print_meta: arch             = gptneox
0.00.053.699 I llm_load_print_meta: vocab type       = BPE
0.00.053.700 I llm_load_print_meta: n_vocab          = 50304
0.00.053.700 I llm_load_print_meta: n_merges         = 50009
0.00.053.700 I llm_load_print_meta: vocab_only       = 0
0.00.053.700 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.700 I llm_load_print_meta: n_embd           = 2048
0.00.053.701 I llm_load_print_meta: n_layer          = 24
0.00.053.704 I llm_load_print_meta: n_head           = 16
0.00.053.704 I llm_load_print_meta: n_head_kv        = 16
0.00.053.716 I llm_load_print_meta: n_rot            = 32
0.00.053.716 I llm_load_print_meta: n_swa            = 0
0.00.053.717 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.717 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.718 I llm_load_print_meta: n_gqa            = 1
0.00.053.720 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.721 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.722 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.722 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.722 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.722 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.723 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.725 I llm_load_print_meta: n_ff             = 8192
0.00.053.725 I llm_load_print_meta: n_expert         = 0
0.00.053.727 I llm_load_print_meta: n_expert_used    = 0
0.00.053.728 I llm_load_print_meta: causal attn      = 1
0.00.053.728 I llm_load_print_meta: pooling type     = 0
0.00.053.729 I llm_load_print_meta: rope type        = 2
0.00.053.729 I llm_load_print_meta: rope scaling     = linear
0.00.053.729 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.729 I llm_load_print_meta: freq_scale_train = 1
0.00.053.729 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.730 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.730 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.730 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.730 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.733 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.734 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.743 I llm_load_print_meta: model type       = 1.4B
0.00.053.744 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.744 I llm_load_print_meta: model params     = 1.41 B
0.00.053.744 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.745 I llm_load_print_meta: general.name     = 1.4B
0.00.053.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.745 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.745 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.745 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.746 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.746 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.746 I llm_load_print_meta: max token length = 1024
0.00.055.723 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.723 I llm_load_tensors: offloading output layer to GPU
0.00.055.723 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.734 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.735 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.677 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.678 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.678 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.678 I llama_new_context_with_model: n_batch       = 2048
0.00.056.678 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.678 I llama_new_context_with_model: flash_attn    = 0
0.00.056.679 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.679 I llama_new_context_with_model: freq_scale    = 1
0.00.056.679 I ggml_metal_init: allocating
0.00.056.685 I ggml_metal_init: found device: Apple M4
0.00.056.688 I ggml_metal_init: picking default device: Apple M4
0.00.057.240 I ggml_metal_init: using embedded metal library
0.00.059.775 I ggml_metal_init: GPU name:   Apple M4
0.00.059.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.779 I ggml_metal_init: simdgroup reduction   = true
0.00.059.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.780 I ggml_metal_init: has bfloat            = true
0.00.059.780 I ggml_metal_init: use bfloat            = true
0.00.059.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.059 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.064 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.196 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.197 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.198 I llama_new_context_with_model: graph nodes  = 967
0.00.092.198 I llama_new_context_with_model: graph splits = 2
0.00.092.210 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.229 I main: llama threadpool init, n_threads = 4
0.00.618.266 I 
0.00.618.293 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.294 I 
0.00.618.434 I sampler seed: 1234
0.00.618.438 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.618.448 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.618.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.618.448 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.600 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.378.601 I llama_perf_context_print:        load time =     606.26 ms
0.01.378.602 I llama_perf_context_print: prompt eval time =      46.98 ms /     7 tokens (    6.71 ms per token,   148.99 tokens per second)
0.01.378.603 I llama_perf_context_print:        eval time =     710.10 ms /    63 runs   (   11.27 ms per token,    88.72 tokens per second)
0.01.378.603 I llama_perf_context_print:       total time =     760.37 ms /    70 tokens
0.01.378.786 I ggml_metal_free: deallocating

real	0m1.397s
user	0m0.109s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.012 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.577 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.577 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.577 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.579 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.580 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.583 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.583 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.713 I llama_model_loader: - type  f32:  194 tensors
0.00.024.713 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.713 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.785 I llm_load_vocab: special tokens cache size = 25
0.00.050.715 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.718 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.718 I llm_load_print_meta: arch             = gptneox
0.00.050.718 I llm_load_print_meta: vocab type       = BPE
0.00.050.719 I llm_load_print_meta: n_vocab          = 50304
0.00.050.719 I llm_load_print_meta: n_merges         = 50009
0.00.050.719 I llm_load_print_meta: vocab_only       = 0
0.00.050.719 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.719 I llm_load_print_meta: n_embd           = 2048
0.00.050.719 I llm_load_print_meta: n_layer          = 24
0.00.050.722 I llm_load_print_meta: n_head           = 16
0.00.050.723 I llm_load_print_meta: n_head_kv        = 16
0.00.050.735 I llm_load_print_meta: n_rot            = 32
0.00.050.735 I llm_load_print_meta: n_swa            = 0
0.00.050.735 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.735 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.736 I llm_load_print_meta: n_gqa            = 1
0.00.050.738 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.739 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.739 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.740 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.740 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.740 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.740 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.741 I llm_load_print_meta: n_ff             = 8192
0.00.050.741 I llm_load_print_meta: n_expert         = 0
0.00.050.741 I llm_load_print_meta: n_expert_used    = 0
0.00.050.742 I llm_load_print_meta: causal attn      = 1
0.00.050.744 I llm_load_print_meta: pooling type     = 0
0.00.050.744 I llm_load_print_meta: rope type        = 2
0.00.050.744 I llm_load_print_meta: rope scaling     = linear
0.00.050.744 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.745 I llm_load_print_meta: freq_scale_train = 1
0.00.050.745 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.746 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.746 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.746 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.746 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.746 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.746 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.756 I llm_load_print_meta: model type       = 1.4B
0.00.050.756 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.757 I llm_load_print_meta: model params     = 1.41 B
0.00.050.758 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.758 I llm_load_print_meta: general.name     = 1.4B
0.00.050.758 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.758 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.758 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.758 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.759 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.759 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.759 I llm_load_print_meta: max token length = 1024
0.00.052.751 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.751 I llm_load_tensors: offloading output layer to GPU
0.00.052.752 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.762 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.763 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.783 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.784 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.785 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.785 I llama_new_context_with_model: n_batch       = 2048
0.00.053.785 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.785 I llama_new_context_with_model: flash_attn    = 0
0.00.053.786 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.786 I llama_new_context_with_model: freq_scale    = 1
0.00.053.786 I ggml_metal_init: allocating
0.00.053.792 I ggml_metal_init: found device: Apple M4
0.00.053.795 I ggml_metal_init: picking default device: Apple M4
0.00.054.346 I ggml_metal_init: using embedded metal library
0.00.056.678 I ggml_metal_init: GPU name:   Apple M4
0.00.056.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.682 I ggml_metal_init: simdgroup reduction   = true
0.00.056.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.682 I ggml_metal_init: has bfloat            = true
0.00.056.682 I ggml_metal_init: use bfloat            = true
0.00.056.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.800 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.821 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.787 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.788 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.789 I llama_new_context_with_model: graph nodes  = 967
0.00.086.789 I llama_new_context_with_model: graph splits = 2
0.00.086.803 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.290 I main: llama threadpool init, n_threads = 4
0.00.694.330 I 
0.00.694.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.370 I 
0.00.694.613 I sampler seed: 1234
0.00.694.619 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.654 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.656 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.656 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.551.478 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.551.479 I llama_perf_context_print:        load time =     685.27 ms
0.01.551.480 I llama_perf_context_print: prompt eval time =      51.64 ms /     7 tokens (    7.38 ms per token,   135.55 tokens per second)
0.01.551.480 I llama_perf_context_print:        eval time =     802.19 ms /    63 runs   (   12.73 ms per token,    78.54 tokens per second)
0.01.551.481 I llama_perf_context_print:       total time =     857.19 ms /    70 tokens
0.01.551.683 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.083 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.219 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.301 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.225 I llama_model_loader: - type  f32:  194 tensors
0.00.026.226 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.244 I llm_load_vocab: special tokens cache size = 25
0.00.052.266 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.269 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.270 I llm_load_print_meta: arch             = gptneox
0.00.052.270 I llm_load_print_meta: vocab type       = BPE
0.00.052.270 I llm_load_print_meta: n_vocab          = 50304
0.00.052.270 I llm_load_print_meta: n_merges         = 50009
0.00.052.271 I llm_load_print_meta: vocab_only       = 0
0.00.052.271 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.271 I llm_load_print_meta: n_embd           = 2048
0.00.052.271 I llm_load_print_meta: n_layer          = 24
0.00.052.274 I llm_load_print_meta: n_head           = 16
0.00.052.274 I llm_load_print_meta: n_head_kv        = 16
0.00.052.286 I llm_load_print_meta: n_rot            = 32
0.00.052.286 I llm_load_print_meta: n_swa            = 0
0.00.052.286 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.287 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.290 I llm_load_print_meta: n_gqa            = 1
0.00.052.291 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.291 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.292 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.292 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.292 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.293 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.293 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.293 I llm_load_print_meta: n_ff             = 8192
0.00.052.294 I llm_load_print_meta: n_expert         = 0
0.00.052.294 I llm_load_print_meta: n_expert_used    = 0
0.00.052.294 I llm_load_print_meta: causal attn      = 1
0.00.052.295 I llm_load_print_meta: pooling type     = 0
0.00.052.295 I llm_load_print_meta: rope type        = 2
0.00.052.295 I llm_load_print_meta: rope scaling     = linear
0.00.052.295 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.296 I llm_load_print_meta: freq_scale_train = 1
0.00.052.296 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.296 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.296 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.296 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.297 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.297 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.297 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.306 I llm_load_print_meta: model type       = 1.4B
0.00.052.306 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.307 I llm_load_print_meta: model params     = 1.41 B
0.00.052.307 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.307 I llm_load_print_meta: general.name     = 1.4B
0.00.052.308 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.308 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.308 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.308 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.308 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.309 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.309 I llm_load_print_meta: max token length = 1024
0.00.053.904 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.904 I llm_load_tensors: offloading output layer to GPU
0.00.053.904 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.914 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.915 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.827 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.827 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.827 I llama_new_context_with_model: n_batch       = 2048
0.00.054.827 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.828 I llama_new_context_with_model: flash_attn    = 0
0.00.054.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.828 I llama_new_context_with_model: freq_scale    = 1
0.00.054.829 I ggml_metal_init: allocating
0.00.054.835 I ggml_metal_init: found device: Apple M4
0.00.054.838 I ggml_metal_init: picking default device: Apple M4
0.00.055.402 I ggml_metal_init: using embedded metal library
0.00.057.708 I ggml_metal_init: GPU name:   Apple M4
0.00.057.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.710 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.710 I ggml_metal_init: simdgroup reduction   = true
0.00.057.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.711 I ggml_metal_init: has bfloat            = true
0.00.057.711 I ggml_metal_init: use bfloat            = true
0.00.057.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.652 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.660 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.681 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.760 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.761 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.762 I llama_new_context_with_model: graph nodes  = 967
0.00.087.762 I llama_new_context_with_model: graph splits = 2
0.00.087.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.390 I main: llama threadpool init, n_threads = 4
0.00.770.430 I 
0.00.770.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.460 I 
0.00.770.640 I sampler seed: 1234
0.00.770.644 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.664 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.664 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.664 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.694.264 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.694.265 I llama_perf_context_print:        load time =     760.30 ms
0.01.694.266 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.50 tokens per second)
0.01.694.267 I llama_perf_context_print:        eval time =     866.10 ms /    63 runs   (   13.75 ms per token,    72.74 tokens per second)
0.01.694.267 I llama_perf_context_print:       total time =     923.88 ms /    70 tokens
0.01.694.466 I ggml_metal_free: deallocating

real	0m1.714s
user	0m0.109s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.739 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.874 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.720 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.728 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.730 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.735 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.736 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.852 I llama_model_loader: - type  f32:  194 tensors
0.00.050.853 I llama_model_loader: - type  f16:   98 tensors
0.00.078.992 I llm_load_vocab: special tokens cache size = 25
0.00.085.311 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.314 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.314 I llm_load_print_meta: arch             = gptneox
0.00.085.314 I llm_load_print_meta: vocab type       = BPE
0.00.085.315 I llm_load_print_meta: n_vocab          = 50304
0.00.085.315 I llm_load_print_meta: n_merges         = 50009
0.00.085.315 I llm_load_print_meta: vocab_only       = 0
0.00.085.315 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.315 I llm_load_print_meta: n_embd           = 2048
0.00.085.315 I llm_load_print_meta: n_layer          = 24
0.00.085.318 I llm_load_print_meta: n_head           = 16
0.00.085.319 I llm_load_print_meta: n_head_kv        = 16
0.00.085.330 I llm_load_print_meta: n_rot            = 32
0.00.085.331 I llm_load_print_meta: n_swa            = 0
0.00.085.331 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.331 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.332 I llm_load_print_meta: n_gqa            = 1
0.00.085.333 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.333 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.334 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.334 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.334 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.334 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.334 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.335 I llm_load_print_meta: n_ff             = 8192
0.00.085.335 I llm_load_print_meta: n_expert         = 0
0.00.085.335 I llm_load_print_meta: n_expert_used    = 0
0.00.085.335 I llm_load_print_meta: causal attn      = 1
0.00.085.335 I llm_load_print_meta: pooling type     = 0
0.00.085.335 I llm_load_print_meta: rope type        = 2
0.00.085.336 I llm_load_print_meta: rope scaling     = linear
0.00.085.336 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.336 I llm_load_print_meta: freq_scale_train = 1
0.00.085.336 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.337 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.337 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.337 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.337 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.337 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.347 I llm_load_print_meta: model type       = 1.4B
0.00.085.347 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.348 I llm_load_print_meta: model params     = 1.41 B
0.00.085.348 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.348 I llm_load_print_meta: general.name     = 1.4B
0.00.085.349 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.349 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.349 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.349 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.349 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.350 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.350 I llm_load_print_meta: max token length = 1024
0.00.088.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.763 I llm_load_tensors: offloading output layer to GPU
0.00.088.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.774 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.775 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.696 I llama_new_context_with_model: n_ctx         = 128
0.00.089.696 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.696 I llama_new_context_with_model: n_batch       = 128
0.00.089.697 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.697 I llama_new_context_with_model: flash_attn    = 0
0.00.089.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.697 I llama_new_context_with_model: freq_scale    = 1
0.00.089.698 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.698 I ggml_metal_init: allocating
0.00.089.701 I ggml_metal_init: found device: Apple M4
0.00.089.703 I ggml_metal_init: picking default device: Apple M4
0.00.090.274 I ggml_metal_init: using embedded metal library
0.00.092.836 I ggml_metal_init: GPU name:   Apple M4
0.00.092.838 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.839 I ggml_metal_init: simdgroup reduction   = true
0.00.092.839 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.839 I ggml_metal_init: has bfloat            = true
0.00.092.840 I ggml_metal_init: use bfloat            = true
0.00.092.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.411 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.415 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.428 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.331 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.332 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.332 I llama_new_context_with_model: graph nodes  = 967
0.00.111.333 I llama_new_context_with_model: graph splits = 2
0.00.111.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.006.659 I 
0.01.006.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.006.731 I perplexity: tokenizing the input ..
0.01.018.918 I perplexity: tokenization took 12.184 ms
0.01.018.946 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.141.543 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.143.755 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.143.787 I llama_perf_context_print:        load time =     985.78 ms
0.01.143.797 I llama_perf_context_print: prompt eval time =     121.64 ms /   128 tokens (    0.95 ms per token,  1052.33 tokens per second)
0.01.143.799 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.143.801 I llama_perf_context_print:       total time =     137.13 ms /   129 tokens
0.01.144.638 I ggml_metal_free: deallocating

real	0m1.334s
user	0m0.125s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.613 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.279 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.280 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.280 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.282 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.282 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.284 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.289 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.290 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.384 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.388 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.389 I llama_model_loader: - type  f32:  194 tensors
0.00.035.389 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.320 I llm_load_vocab: special tokens cache size = 25
0.00.068.528 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.531 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.531 I llm_load_print_meta: arch             = gptneox
0.00.068.532 I llm_load_print_meta: vocab type       = BPE
0.00.068.532 I llm_load_print_meta: n_vocab          = 50304
0.00.068.532 I llm_load_print_meta: n_merges         = 50009
0.00.068.532 I llm_load_print_meta: vocab_only       = 0
0.00.068.532 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.533 I llm_load_print_meta: n_embd           = 2048
0.00.068.533 I llm_load_print_meta: n_layer          = 24
0.00.068.536 I llm_load_print_meta: n_head           = 16
0.00.068.537 I llm_load_print_meta: n_head_kv        = 16
0.00.068.549 I llm_load_print_meta: n_rot            = 32
0.00.068.549 I llm_load_print_meta: n_swa            = 0
0.00.068.549 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.549 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.550 I llm_load_print_meta: n_gqa            = 1
0.00.068.551 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.551 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.552 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.552 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.552 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.553 I llm_load_print_meta: n_ff             = 8192
0.00.068.553 I llm_load_print_meta: n_expert         = 0
0.00.068.553 I llm_load_print_meta: n_expert_used    = 0
0.00.068.553 I llm_load_print_meta: causal attn      = 1
0.00.068.554 I llm_load_print_meta: pooling type     = 0
0.00.068.554 I llm_load_print_meta: rope type        = 2
0.00.068.554 I llm_load_print_meta: rope scaling     = linear
0.00.068.554 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.556 I llm_load_print_meta: freq_scale_train = 1
0.00.068.556 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.557 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.557 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.558 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.558 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.558 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.558 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.568 I llm_load_print_meta: model type       = 1.4B
0.00.068.568 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.568 I llm_load_print_meta: model params     = 1.41 B
0.00.068.569 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.569 I llm_load_print_meta: general.name     = 1.4B
0.00.068.569 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.569 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.569 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.569 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.570 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.570 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.570 I llm_load_print_meta: max token length = 1024
0.00.070.810 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.811 I llm_load_tensors: offloading output layer to GPU
0.00.070.811 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.821 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.822 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.790 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.791 I llama_new_context_with_model: n_ctx         = 128
0.00.071.791 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.791 I llama_new_context_with_model: n_batch       = 128
0.00.071.791 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.791 I llama_new_context_with_model: flash_attn    = 0
0.00.071.792 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.792 I llama_new_context_with_model: freq_scale    = 1
0.00.071.793 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.793 I ggml_metal_init: allocating
0.00.071.799 I ggml_metal_init: found device: Apple M4
0.00.071.801 I ggml_metal_init: picking default device: Apple M4
0.00.072.365 I ggml_metal_init: using embedded metal library
0.00.074.843 I ggml_metal_init: GPU name:   Apple M4
0.00.074.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.846 I ggml_metal_init: simdgroup reduction   = true
0.00.074.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.846 I ggml_metal_init: has bfloat            = true
0.00.074.846 I ggml_metal_init: use bfloat            = true
0.00.074.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.759 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.762 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.776 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.664 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.665 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.665 I llama_new_context_with_model: graph nodes  = 967
0.00.085.666 I llama_new_context_with_model: graph splits = 2
0.00.085.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.568 I 
0.00.860.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.638 I perplexity: tokenizing the input ..
0.00.868.526 I perplexity: tokenization took 7.887 ms
0.00.868.541 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.992.118 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.993.562 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.993.573 I llama_perf_context_print:        load time =     848.95 ms
0.00.993.573 I llama_perf_context_print: prompt eval time =     123.34 ms /   128 tokens (    0.96 ms per token,  1037.75 tokens per second)
0.00.993.574 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.993.575 I llama_perf_context_print:       total time =     133.01 ms /   129 tokens
0.00.993.940 I ggml_metal_free: deallocating

real	0m1.012s
user	0m0.097s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.445 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.446 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.450 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.452 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.452 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.452 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.453 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.454 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.454 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.455 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.457 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.457 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.457 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.458 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.459 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.459 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.642 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.643 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.643 I llama_model_loader: - type  f32:  194 tensors
0.00.024.643 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.644 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.502 I llm_load_vocab: special tokens cache size = 25
0.00.050.473 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.476 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.476 I llm_load_print_meta: arch             = gptneox
0.00.050.477 I llm_load_print_meta: vocab type       = BPE
0.00.050.477 I llm_load_print_meta: n_vocab          = 50304
0.00.050.477 I llm_load_print_meta: n_merges         = 50009
0.00.050.477 I llm_load_print_meta: vocab_only       = 0
0.00.050.478 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.478 I llm_load_print_meta: n_embd           = 2048
0.00.050.478 I llm_load_print_meta: n_layer          = 24
0.00.050.480 I llm_load_print_meta: n_head           = 16
0.00.050.481 I llm_load_print_meta: n_head_kv        = 16
0.00.050.488 I llm_load_print_meta: n_rot            = 32
0.00.050.488 I llm_load_print_meta: n_swa            = 0
0.00.050.488 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.488 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.490 I llm_load_print_meta: n_gqa            = 1
0.00.050.490 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.491 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.492 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.492 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.492 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.494 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.494 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.494 I llm_load_print_meta: n_ff             = 8192
0.00.050.494 I llm_load_print_meta: n_expert         = 0
0.00.050.495 I llm_load_print_meta: n_expert_used    = 0
0.00.050.495 I llm_load_print_meta: causal attn      = 1
0.00.050.495 I llm_load_print_meta: pooling type     = 0
0.00.050.495 I llm_load_print_meta: rope type        = 2
0.00.050.495 I llm_load_print_meta: rope scaling     = linear
0.00.050.496 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.496 I llm_load_print_meta: freq_scale_train = 1
0.00.050.496 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.496 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.498 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.498 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.498 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.498 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.498 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.502 I llm_load_print_meta: model type       = 1.4B
0.00.050.503 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.503 I llm_load_print_meta: model params     = 1.41 B
0.00.050.504 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.505 I llm_load_print_meta: general.name     = 1.4B
0.00.050.505 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.506 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: max token length = 1024
0.00.052.226 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.226 I llm_load_tensors: offloading output layer to GPU
0.00.052.226 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.232 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.232 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.087 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.089 I llama_new_context_with_model: n_ctx         = 128
0.00.053.089 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.089 I llama_new_context_with_model: n_batch       = 128
0.00.053.089 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.089 I llama_new_context_with_model: flash_attn    = 0
0.00.053.090 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.090 I llama_new_context_with_model: freq_scale    = 1
0.00.053.090 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.091 I ggml_metal_init: allocating
0.00.053.094 I ggml_metal_init: found device: Apple M4
0.00.053.096 I ggml_metal_init: picking default device: Apple M4
0.00.053.640 I ggml_metal_init: using embedded metal library
0.00.056.167 I ggml_metal_init: GPU name:   Apple M4
0.00.056.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.169 I ggml_metal_init: simdgroup reduction   = true
0.00.056.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.169 I ggml_metal_init: has bfloat            = true
0.00.056.169 I ggml_metal_init: use bfloat            = true
0.00.056.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.657 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.670 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.536 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.537 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.537 I llama_new_context_with_model: graph nodes  = 967
0.00.067.538 I llama_new_context_with_model: graph splits = 2
0.00.067.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.225 I 
0.00.598.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.288 I perplexity: tokenizing the input ..
0.00.606.127 I perplexity: tokenization took 7.838 ms
0.00.606.138 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.651 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.730.031 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.730.044 I llama_perf_context_print:        load time =     588.77 ms
0.00.730.045 I llama_perf_context_print: prompt eval time =     122.29 ms /   128 tokens (    0.96 ms per token,  1046.71 tokens per second)
0.00.730.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.046 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.730.526 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.692 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.339 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.345 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.351 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.547 I llama_model_loader: - type  f32:  194 tensors
0.00.023.547 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.444 I llm_load_vocab: special tokens cache size = 25
0.00.049.259 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.261 I llm_load_print_meta: arch             = gptneox
0.00.049.262 I llm_load_print_meta: vocab type       = BPE
0.00.049.262 I llm_load_print_meta: n_vocab          = 50304
0.00.049.262 I llm_load_print_meta: n_merges         = 50009
0.00.049.262 I llm_load_print_meta: vocab_only       = 0
0.00.049.263 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.263 I llm_load_print_meta: n_embd           = 2048
0.00.049.263 I llm_load_print_meta: n_layer          = 24
0.00.049.266 I llm_load_print_meta: n_head           = 16
0.00.049.266 I llm_load_print_meta: n_head_kv        = 16
0.00.049.277 I llm_load_print_meta: n_rot            = 32
0.00.049.278 I llm_load_print_meta: n_swa            = 0
0.00.049.278 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.278 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.279 I llm_load_print_meta: n_gqa            = 1
0.00.049.279 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.280 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.282 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.283 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.283 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.283 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.283 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.284 I llm_load_print_meta: n_ff             = 8192
0.00.049.284 I llm_load_print_meta: n_expert         = 0
0.00.049.287 I llm_load_print_meta: n_expert_used    = 0
0.00.049.288 I llm_load_print_meta: causal attn      = 1
0.00.049.288 I llm_load_print_meta: pooling type     = 0
0.00.049.288 I llm_load_print_meta: rope type        = 2
0.00.049.288 I llm_load_print_meta: rope scaling     = linear
0.00.049.288 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.289 I llm_load_print_meta: freq_scale_train = 1
0.00.049.289 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.289 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.289 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.290 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.291 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.291 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.291 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.300 I llm_load_print_meta: model type       = 1.4B
0.00.049.301 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.301 I llm_load_print_meta: model params     = 1.41 B
0.00.049.302 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.302 I llm_load_print_meta: general.name     = 1.4B
0.00.049.302 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.302 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.302 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.302 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.303 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.303 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.303 I llm_load_print_meta: max token length = 1024
0.00.050.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.826 I llm_load_tensors: offloading output layer to GPU
0.00.050.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.836 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.837 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.674 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.675 I llama_new_context_with_model: n_ctx         = 128
0.00.051.675 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.675 I llama_new_context_with_model: n_batch       = 128
0.00.051.675 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.676 I llama_new_context_with_model: flash_attn    = 0
0.00.051.676 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.676 I llama_new_context_with_model: freq_scale    = 1
0.00.051.677 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.677 I ggml_metal_init: allocating
0.00.051.680 I ggml_metal_init: found device: Apple M4
0.00.051.682 I ggml_metal_init: picking default device: Apple M4
0.00.052.226 I ggml_metal_init: using embedded metal library
0.00.054.478 I ggml_metal_init: GPU name:   Apple M4
0.00.054.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.481 I ggml_metal_init: simdgroup reduction   = true
0.00.054.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.481 I ggml_metal_init: has bfloat            = true
0.00.054.481 I ggml_metal_init: use bfloat            = true
0.00.054.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.482 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.152 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.155 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.168 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.023 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.024 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.024 I llama_new_context_with_model: graph nodes  = 967
0.00.066.025 I llama_new_context_with_model: graph splits = 2
0.00.066.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.425 I 
0.00.628.461 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.474 I perplexity: tokenizing the input ..
0.00.636.676 I perplexity: tokenization took 8.2 ms
0.00.636.686 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.529 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.760.860 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.760.882 I llama_perf_context_print:        load time =     619.73 ms
0.00.760.883 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.93 tokens per second)
0.00.760.884 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.760.884 I llama_perf_context_print:       total time =     132.46 ms /   129 tokens
0.00.761.289 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.895 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.861 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.892 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.893 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.893 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.894 I llama_model_loader: - type  f32:  194 tensors
0.00.024.894 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.895 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.644 I llm_load_vocab: special tokens cache size = 25
0.00.050.541 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.543 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.543 I llm_load_print_meta: arch             = gptneox
0.00.050.544 I llm_load_print_meta: vocab type       = BPE
0.00.050.544 I llm_load_print_meta: n_vocab          = 50304
0.00.050.544 I llm_load_print_meta: n_merges         = 50009
0.00.050.544 I llm_load_print_meta: vocab_only       = 0
0.00.050.545 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.545 I llm_load_print_meta: n_embd           = 2048
0.00.050.545 I llm_load_print_meta: n_layer          = 24
0.00.050.548 I llm_load_print_meta: n_head           = 16
0.00.050.549 I llm_load_print_meta: n_head_kv        = 16
0.00.050.560 I llm_load_print_meta: n_rot            = 32
0.00.050.561 I llm_load_print_meta: n_swa            = 0
0.00.050.561 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.561 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.562 I llm_load_print_meta: n_gqa            = 1
0.00.050.562 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.563 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.564 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.564 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.564 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.565 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.565 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.565 I llm_load_print_meta: n_ff             = 8192
0.00.050.565 I llm_load_print_meta: n_expert         = 0
0.00.050.566 I llm_load_print_meta: n_expert_used    = 0
0.00.050.566 I llm_load_print_meta: causal attn      = 1
0.00.050.566 I llm_load_print_meta: pooling type     = 0
0.00.050.566 I llm_load_print_meta: rope type        = 2
0.00.050.566 I llm_load_print_meta: rope scaling     = linear
0.00.050.567 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.567 I llm_load_print_meta: freq_scale_train = 1
0.00.050.567 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.567 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.567 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.567 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.568 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.568 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.568 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.577 I llm_load_print_meta: model type       = 1.4B
0.00.050.577 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.578 I llm_load_print_meta: model params     = 1.41 B
0.00.050.578 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.578 I llm_load_print_meta: general.name     = 1.4B
0.00.050.580 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.581 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.581 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.581 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.581 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.583 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: max token length = 1024
0.00.052.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I llm_load_tensors: offloading output layer to GPU
0.00.052.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.505 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.506 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.458 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.458 I llama_new_context_with_model: n_ctx         = 128
0.00.053.459 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.459 I llama_new_context_with_model: n_batch       = 128
0.00.053.459 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.459 I llama_new_context_with_model: flash_attn    = 0
0.00.053.460 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.460 I llama_new_context_with_model: freq_scale    = 1
0.00.053.460 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.461 I ggml_metal_init: allocating
0.00.053.467 I ggml_metal_init: found device: Apple M4
0.00.053.469 I ggml_metal_init: picking default device: Apple M4
0.00.053.995 I ggml_metal_init: using embedded metal library
0.00.056.351 I ggml_metal_init: GPU name:   Apple M4
0.00.056.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.353 I ggml_metal_init: simdgroup reduction   = true
0.00.056.353 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.353 I ggml_metal_init: has bfloat            = true
0.00.056.354 I ggml_metal_init: use bfloat            = true
0.00.056.354 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.942 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.945 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.961 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.825 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.826 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.826 I llama_new_context_with_model: graph nodes  = 967
0.00.067.826 I llama_new_context_with_model: graph splits = 2
0.00.067.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.684 I 
0.00.655.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.728 I perplexity: tokenizing the input ..
0.00.664.027 I perplexity: tokenization took 8.297 ms
0.00.664.039 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.186 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.800.536 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.800.556 I llama_perf_context_print:        load time =     645.78 ms
0.00.800.558 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.72 tokens per second)
0.00.800.559 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.559 I llama_perf_context_print:       total time =     144.87 ms /   129 tokens
0.00.800.972 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.768 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.709 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.761 I llama_model_loader: - type  f32:  194 tensors
0.00.023.762 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.762 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.732 I llm_load_vocab: special tokens cache size = 25
0.00.049.576 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.579 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.579 I llm_load_print_meta: arch             = gptneox
0.00.049.579 I llm_load_print_meta: vocab type       = BPE
0.00.049.580 I llm_load_print_meta: n_vocab          = 50304
0.00.049.580 I llm_load_print_meta: n_merges         = 50009
0.00.049.580 I llm_load_print_meta: vocab_only       = 0
0.00.049.580 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.580 I llm_load_print_meta: n_embd           = 2048
0.00.049.580 I llm_load_print_meta: n_layer          = 24
0.00.049.584 I llm_load_print_meta: n_head           = 16
0.00.049.584 I llm_load_print_meta: n_head_kv        = 16
0.00.049.596 I llm_load_print_meta: n_rot            = 32
0.00.049.598 I llm_load_print_meta: n_swa            = 0
0.00.049.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.599 I llm_load_print_meta: n_gqa            = 1
0.00.049.600 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.601 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.601 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.602 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.602 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.602 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.602 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.603 I llm_load_print_meta: n_ff             = 8192
0.00.049.603 I llm_load_print_meta: n_expert         = 0
0.00.049.603 I llm_load_print_meta: n_expert_used    = 0
0.00.049.603 I llm_load_print_meta: causal attn      = 1
0.00.049.603 I llm_load_print_meta: pooling type     = 0
0.00.049.603 I llm_load_print_meta: rope type        = 2
0.00.049.603 I llm_load_print_meta: rope scaling     = linear
0.00.049.604 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.604 I llm_load_print_meta: freq_scale_train = 1
0.00.049.604 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.605 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.605 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.605 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.605 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.605 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.605 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.615 I llm_load_print_meta: model type       = 1.4B
0.00.049.615 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.615 I llm_load_print_meta: model params     = 1.41 B
0.00.049.616 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.616 I llm_load_print_meta: general.name     = 1.4B
0.00.049.616 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.616 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.616 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.617 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.617 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.617 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.618 I llm_load_print_meta: max token length = 1024
0.00.051.561 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.562 I llm_load_tensors: offloading output layer to GPU
0.00.051.562 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.572 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.573 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.859 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.860 I llama_new_context_with_model: n_ctx         = 128
0.00.052.860 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.860 I llama_new_context_with_model: n_batch       = 128
0.00.052.861 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.861 I llama_new_context_with_model: flash_attn    = 0
0.00.052.861 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.861 I llama_new_context_with_model: freq_scale    = 1
0.00.052.862 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.862 I ggml_metal_init: allocating
0.00.052.869 I ggml_metal_init: found device: Apple M4
0.00.052.871 I ggml_metal_init: picking default device: Apple M4
0.00.053.429 I ggml_metal_init: using embedded metal library
0.00.055.742 I ggml_metal_init: GPU name:   Apple M4
0.00.055.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.745 I ggml_metal_init: simdgroup reduction   = true
0.00.055.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.745 I ggml_metal_init: has bfloat            = true
0.00.055.745 I ggml_metal_init: use bfloat            = true
0.00.055.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.549 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.553 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.566 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.433 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.435 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.435 I llama_new_context_with_model: graph nodes  = 967
0.00.067.435 I llama_new_context_with_model: graph splits = 2
0.00.067.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.096 I 
0.00.749.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.144 I perplexity: tokenizing the input ..
0.00.757.479 I perplexity: tokenization took 8.332 ms
0.00.757.494 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.892.321 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.893.676 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.893.692 I llama_perf_context_print:        load time =     740.32 ms
0.00.893.693 I llama_perf_context_print: prompt eval time =     134.60 ms /   128 tokens (    1.05 ms per token,   950.97 tokens per second)
0.00.893.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.893.694 I llama_perf_context_print:       total time =     144.60 ms /   129 tokens
0.00.894.073 I ggml_metal_free: deallocating

real	0m0.907s
user	0m0.079s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.597 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.231 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.234 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.476 I llama_model_loader: - type  f32:  194 tensors
0.00.024.476 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.476 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.476 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.337 I llm_load_vocab: special tokens cache size = 25
0.00.050.200 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.203 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.204 I llm_load_print_meta: arch             = gptneox
0.00.050.204 I llm_load_print_meta: vocab type       = BPE
0.00.050.204 I llm_load_print_meta: n_vocab          = 50304
0.00.050.205 I llm_load_print_meta: n_merges         = 50009
0.00.050.205 I llm_load_print_meta: vocab_only       = 0
0.00.050.205 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.205 I llm_load_print_meta: n_embd           = 2048
0.00.050.205 I llm_load_print_meta: n_layer          = 24
0.00.050.208 I llm_load_print_meta: n_head           = 16
0.00.050.209 I llm_load_print_meta: n_head_kv        = 16
0.00.050.221 I llm_load_print_meta: n_rot            = 32
0.00.050.221 I llm_load_print_meta: n_swa            = 0
0.00.050.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.223 I llm_load_print_meta: n_gqa            = 1
0.00.050.224 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.225 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.225 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.226 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.226 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.227 I llm_load_print_meta: n_ff             = 8192
0.00.050.227 I llm_load_print_meta: n_expert         = 0
0.00.050.227 I llm_load_print_meta: n_expert_used    = 0
0.00.050.227 I llm_load_print_meta: causal attn      = 1
0.00.050.227 I llm_load_print_meta: pooling type     = 0
0.00.050.227 I llm_load_print_meta: rope type        = 2
0.00.050.228 I llm_load_print_meta: rope scaling     = linear
0.00.050.228 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.228 I llm_load_print_meta: freq_scale_train = 1
0.00.050.228 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.229 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.229 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.229 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.229 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.229 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.229 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.240 I llm_load_print_meta: model type       = 1.4B
0.00.050.241 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.241 I llm_load_print_meta: model params     = 1.41 B
0.00.050.242 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.242 I llm_load_print_meta: general.name     = 1.4B
0.00.050.242 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.243 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: max token length = 1024
0.00.052.062 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.062 I llm_load_tensors: offloading output layer to GPU
0.00.052.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.073 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.074 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.975 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.975 I llama_new_context_with_model: n_ctx         = 128
0.00.052.976 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.976 I llama_new_context_with_model: n_batch       = 128
0.00.052.976 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.976 I llama_new_context_with_model: flash_attn    = 0
0.00.052.977 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.977 I llama_new_context_with_model: freq_scale    = 1
0.00.052.977 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.978 I ggml_metal_init: allocating
0.00.052.981 I ggml_metal_init: found device: Apple M4
0.00.052.983 I ggml_metal_init: picking default device: Apple M4
0.00.053.534 I ggml_metal_init: using embedded metal library
0.00.055.855 I ggml_metal_init: GPU name:   Apple M4
0.00.055.857 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.857 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.858 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.858 I ggml_metal_init: simdgroup reduction   = true
0.00.055.858 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.858 I ggml_metal_init: has bfloat            = true
0.00.055.858 I ggml_metal_init: use bfloat            = true
0.00.055.859 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.859 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.330 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.332 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.347 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.268 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.269 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.270 I llama_new_context_with_model: graph nodes  = 967
0.00.067.270 I llama_new_context_with_model: graph splits = 2
0.00.067.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.494 I 
0.00.431.526 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.431.540 I perplexity: tokenizing the input ..
0.00.439.538 I perplexity: tokenization took 7.996 ms
0.00.439.549 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.572.353 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.573.881 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.573.907 I llama_perf_context_print:        load time =     421.89 ms
0.00.573.910 I llama_perf_context_print: prompt eval time =     132.56 ms /   128 tokens (    1.04 ms per token,   965.60 tokens per second)
0.00.573.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.573.911 I llama_perf_context_print:       total time =     142.41 ms /   129 tokens
0.00.574.444 I ggml_metal_free: deallocating

real	0m0.590s
user	0m0.078s
sys	0m0.076s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.441 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.442 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.447 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.778 I llama_model_loader: - type  f32:  194 tensors
0.00.023.778 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.779 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.779 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.779 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.516 I llm_load_vocab: special tokens cache size = 25
0.00.050.359 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.362 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.362 I llm_load_print_meta: arch             = gptneox
0.00.050.363 I llm_load_print_meta: vocab type       = BPE
0.00.050.363 I llm_load_print_meta: n_vocab          = 50304
0.00.050.363 I llm_load_print_meta: n_merges         = 50009
0.00.050.363 I llm_load_print_meta: vocab_only       = 0
0.00.050.363 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.364 I llm_load_print_meta: n_embd           = 2048
0.00.050.364 I llm_load_print_meta: n_layer          = 24
0.00.050.367 I llm_load_print_meta: n_head           = 16
0.00.050.368 I llm_load_print_meta: n_head_kv        = 16
0.00.050.379 I llm_load_print_meta: n_rot            = 32
0.00.050.379 I llm_load_print_meta: n_swa            = 0
0.00.050.380 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.380 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.380 I llm_load_print_meta: n_gqa            = 1
0.00.050.381 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.382 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.382 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.383 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.383 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.383 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.384 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.384 I llm_load_print_meta: n_ff             = 8192
0.00.050.384 I llm_load_print_meta: n_expert         = 0
0.00.050.384 I llm_load_print_meta: n_expert_used    = 0
0.00.050.384 I llm_load_print_meta: causal attn      = 1
0.00.050.385 I llm_load_print_meta: pooling type     = 0
0.00.050.386 I llm_load_print_meta: rope type        = 2
0.00.050.388 I llm_load_print_meta: rope scaling     = linear
0.00.050.388 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.388 I llm_load_print_meta: freq_scale_train = 1
0.00.050.389 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.389 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.390 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.390 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.390 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.390 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.390 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.400 I llm_load_print_meta: model type       = 1.4B
0.00.050.400 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.400 I llm_load_print_meta: model params     = 1.41 B
0.00.050.401 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.401 I llm_load_print_meta: general.name     = 1.4B
0.00.050.401 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.401 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.401 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.401 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.403 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.403 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.403 I llm_load_print_meta: max token length = 1024
0.00.052.358 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.358 I llm_load_tensors: offloading output layer to GPU
0.00.052.358 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.369 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.370 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.319 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.320 I llama_new_context_with_model: n_ctx         = 128
0.00.053.320 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.321 I llama_new_context_with_model: n_batch       = 128
0.00.053.321 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.321 I llama_new_context_with_model: flash_attn    = 0
0.00.053.321 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.322 I llama_new_context_with_model: freq_scale    = 1
0.00.053.322 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.322 I ggml_metal_init: allocating
0.00.053.326 I ggml_metal_init: found device: Apple M4
0.00.053.328 I ggml_metal_init: picking default device: Apple M4
0.00.053.887 I ggml_metal_init: using embedded metal library
0.00.056.199 I ggml_metal_init: GPU name:   Apple M4
0.00.056.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.203 I ggml_metal_init: simdgroup reduction   = true
0.00.056.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.203 I ggml_metal_init: has bfloat            = true
0.00.056.203 I ggml_metal_init: use bfloat            = true
0.00.056.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.918 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.920 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.934 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.826 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.827 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.828 I llama_new_context_with_model: graph nodes  = 967
0.00.067.828 I llama_new_context_with_model: graph splits = 2
0.00.067.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.527 I 
0.00.493.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.565 I perplexity: tokenizing the input ..
0.00.500.706 I perplexity: tokenization took 7.14 ms
0.00.500.718 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.632.889 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.297 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.310 I llama_perf_context_print:        load time =     484.55 ms
0.00.634.311 I llama_perf_context_print: prompt eval time =     131.94 ms /   128 tokens (    1.03 ms per token,   970.12 tokens per second)
0.00.634.312 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.312 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.634.612 I ggml_metal_free: deallocating

real	0m0.648s
user	0m0.079s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.398 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.292 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.180 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.129 I llama_model_loader: - type  f32:  194 tensors
0.00.026.129 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.129 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.129 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.929 I llm_load_vocab: special tokens cache size = 25
0.00.051.853 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.856 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.857 I llm_load_print_meta: arch             = gptneox
0.00.051.857 I llm_load_print_meta: vocab type       = BPE
0.00.051.857 I llm_load_print_meta: n_vocab          = 50304
0.00.051.857 I llm_load_print_meta: n_merges         = 50009
0.00.051.858 I llm_load_print_meta: vocab_only       = 0
0.00.051.858 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.858 I llm_load_print_meta: n_embd           = 2048
0.00.051.858 I llm_load_print_meta: n_layer          = 24
0.00.051.861 I llm_load_print_meta: n_head           = 16
0.00.051.861 I llm_load_print_meta: n_head_kv        = 16
0.00.051.873 I llm_load_print_meta: n_rot            = 32
0.00.051.874 I llm_load_print_meta: n_swa            = 0
0.00.051.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.875 I llm_load_print_meta: n_gqa            = 1
0.00.051.875 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.878 I llm_load_print_meta: n_ff             = 8192
0.00.051.878 I llm_load_print_meta: n_expert         = 0
0.00.051.879 I llm_load_print_meta: n_expert_used    = 0
0.00.051.879 I llm_load_print_meta: causal attn      = 1
0.00.051.879 I llm_load_print_meta: pooling type     = 0
0.00.051.879 I llm_load_print_meta: rope type        = 2
0.00.051.879 I llm_load_print_meta: rope scaling     = linear
0.00.051.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.880 I llm_load_print_meta: freq_scale_train = 1
0.00.051.880 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.880 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.883 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.883 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.883 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.883 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.883 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.892 I llm_load_print_meta: model type       = 1.4B
0.00.051.893 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.893 I llm_load_print_meta: model params     = 1.41 B
0.00.051.894 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.894 I llm_load_print_meta: general.name     = 1.4B
0.00.051.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.895 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.895 I llm_load_print_meta: max token length = 1024
0.00.053.897 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.897 I llm_load_tensors: offloading output layer to GPU
0.00.053.897 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.907 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.909 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.806 I llama_new_context_with_model: n_ctx         = 128
0.00.054.806 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.806 I llama_new_context_with_model: n_batch       = 128
0.00.054.806 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.807 I llama_new_context_with_model: flash_attn    = 0
0.00.054.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.807 I llama_new_context_with_model: freq_scale    = 1
0.00.054.808 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.808 I ggml_metal_init: allocating
0.00.054.815 I ggml_metal_init: found device: Apple M4
0.00.054.818 I ggml_metal_init: picking default device: Apple M4
0.00.055.400 I ggml_metal_init: using embedded metal library
0.00.057.756 I ggml_metal_init: GPU name:   Apple M4
0.00.057.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.758 I ggml_metal_init: simdgroup reduction   = true
0.00.057.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.759 I ggml_metal_init: has bfloat            = true
0.00.057.759 I ggml_metal_init: use bfloat            = true
0.00.057.759 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.313 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.317 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.263 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.264 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.264 I llama_new_context_with_model: graph nodes  = 967
0.00.069.265 I llama_new_context_with_model: graph splits = 2
0.00.069.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.156 I 
0.00.578.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.260 I perplexity: tokenizing the input ..
0.00.586.491 I perplexity: tokenization took 8.229 ms
0.00.586.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.350 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.721.692 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.721.706 I llama_perf_context_print:        load time =     568.75 ms
0.00.721.707 I llama_perf_context_print: prompt eval time =     133.62 ms /   128 tokens (    1.04 ms per token,   957.95 tokens per second)
0.00.721.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.721.708 I llama_perf_context_print:       total time =     143.55 ms /   129 tokens
0.00.722.140 I ggml_metal_free: deallocating

real	0m0.736s
user	0m0.079s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.734 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.523 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.525 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.531 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.531 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.354 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.354 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.355 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.355 I llama_model_loader: - type  f32:  194 tensors
0.00.023.356 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.356 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.153 I llm_load_vocab: special tokens cache size = 25
0.00.048.971 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.973 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.973 I llm_load_print_meta: arch             = gptneox
0.00.048.974 I llm_load_print_meta: vocab type       = BPE
0.00.048.974 I llm_load_print_meta: n_vocab          = 50304
0.00.048.974 I llm_load_print_meta: n_merges         = 50009
0.00.048.974 I llm_load_print_meta: vocab_only       = 0
0.00.048.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.975 I llm_load_print_meta: n_embd           = 2048
0.00.048.975 I llm_load_print_meta: n_layer          = 24
0.00.048.978 I llm_load_print_meta: n_head           = 16
0.00.048.978 I llm_load_print_meta: n_head_kv        = 16
0.00.048.990 I llm_load_print_meta: n_rot            = 32
0.00.048.990 I llm_load_print_meta: n_swa            = 0
0.00.048.990 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.990 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.991 I llm_load_print_meta: n_gqa            = 1
0.00.048.992 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.992 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.993 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.993 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.993 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.996 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.996 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.997 I llm_load_print_meta: n_ff             = 8192
0.00.048.997 I llm_load_print_meta: n_expert         = 0
0.00.048.997 I llm_load_print_meta: n_expert_used    = 0
0.00.048.997 I llm_load_print_meta: causal attn      = 1
0.00.048.997 I llm_load_print_meta: pooling type     = 0
0.00.048.999 I llm_load_print_meta: rope type        = 2
0.00.048.999 I llm_load_print_meta: rope scaling     = linear
0.00.048.999 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.000 I llm_load_print_meta: freq_scale_train = 1
0.00.049.000 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.000 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.000 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.000 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.000 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.000 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.001 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.009 I llm_load_print_meta: model type       = 1.4B
0.00.049.010 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.010 I llm_load_print_meta: model params     = 1.41 B
0.00.049.011 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.011 I llm_load_print_meta: general.name     = 1.4B
0.00.049.011 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.012 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.013 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.013 I llm_load_print_meta: max token length = 1024
0.00.050.794 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.795 I llm_load_tensors: offloading output layer to GPU
0.00.050.795 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.805 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.806 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.703 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.704 I llama_new_context_with_model: n_ctx         = 128
0.00.051.704 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.704 I llama_new_context_with_model: n_batch       = 128
0.00.051.705 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.705 I llama_new_context_with_model: flash_attn    = 0
0.00.051.705 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.705 I llama_new_context_with_model: freq_scale    = 1
0.00.051.706 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.706 I ggml_metal_init: allocating
0.00.051.713 I ggml_metal_init: found device: Apple M4
0.00.051.715 I ggml_metal_init: picking default device: Apple M4
0.00.052.266 I ggml_metal_init: using embedded metal library
0.00.054.583 I ggml_metal_init: GPU name:   Apple M4
0.00.054.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.585 I ggml_metal_init: simdgroup reduction   = true
0.00.054.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.585 I ggml_metal_init: has bfloat            = true
0.00.054.585 I ggml_metal_init: use bfloat            = true
0.00.054.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.313 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.317 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.331 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.193 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.195 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.195 I llama_new_context_with_model: graph nodes  = 967
0.00.066.195 I llama_new_context_with_model: graph splits = 2
0.00.066.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.470 I 
0.00.651.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.548 I perplexity: tokenizing the input ..
0.00.659.886 I perplexity: tokenization took 8.336 ms
0.00.659.900 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.424 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.755 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.773 I llama_perf_context_print:        load time =     642.73 ms
0.00.801.776 I llama_perf_context_print: prompt eval time =     140.30 ms /   128 tokens (    1.10 ms per token,   912.34 tokens per second)
0.00.801.779 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.779 I llama_perf_context_print:       total time =     150.31 ms /   129 tokens
0.00.802.181 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.745 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.080 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.089 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.090 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.092 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.092 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.093 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.084 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.097 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.098 I llama_model_loader: - type  f32:  194 tensors
0.00.027.098 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.723 I llm_load_vocab: special tokens cache size = 25
0.00.052.581 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.583 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.584 I llm_load_print_meta: arch             = gptneox
0.00.052.584 I llm_load_print_meta: vocab type       = BPE
0.00.052.584 I llm_load_print_meta: n_vocab          = 50304
0.00.052.584 I llm_load_print_meta: n_merges         = 50009
0.00.052.585 I llm_load_print_meta: vocab_only       = 0
0.00.052.585 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.585 I llm_load_print_meta: n_embd           = 2048
0.00.052.585 I llm_load_print_meta: n_layer          = 24
0.00.052.588 I llm_load_print_meta: n_head           = 16
0.00.052.589 I llm_load_print_meta: n_head_kv        = 16
0.00.052.600 I llm_load_print_meta: n_rot            = 32
0.00.052.600 I llm_load_print_meta: n_swa            = 0
0.00.052.600 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.602 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.603 I llm_load_print_meta: n_gqa            = 1
0.00.052.604 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.604 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.605 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.605 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.605 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.605 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.605 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.606 I llm_load_print_meta: n_ff             = 8192
0.00.052.606 I llm_load_print_meta: n_expert         = 0
0.00.052.606 I llm_load_print_meta: n_expert_used    = 0
0.00.052.606 I llm_load_print_meta: causal attn      = 1
0.00.052.607 I llm_load_print_meta: pooling type     = 0
0.00.052.607 I llm_load_print_meta: rope type        = 2
0.00.052.607 I llm_load_print_meta: rope scaling     = linear
0.00.052.608 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.608 I llm_load_print_meta: freq_scale_train = 1
0.00.052.609 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.609 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.609 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.609 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.609 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.609 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.609 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.618 I llm_load_print_meta: model type       = 1.4B
0.00.052.619 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.619 I llm_load_print_meta: model params     = 1.41 B
0.00.052.619 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.619 I llm_load_print_meta: general.name     = 1.4B
0.00.052.621 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.622 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.622 I llm_load_print_meta: max token length = 1024
0.00.054.143 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.144 I llm_load_tensors: offloading output layer to GPU
0.00.054.144 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.154 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.155 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.016 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.017 I llama_new_context_with_model: n_ctx         = 128
0.00.055.017 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.017 I llama_new_context_with_model: n_batch       = 128
0.00.055.017 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.017 I llama_new_context_with_model: flash_attn    = 0
0.00.055.018 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.018 I llama_new_context_with_model: freq_scale    = 1
0.00.055.018 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.019 I ggml_metal_init: allocating
0.00.055.025 I ggml_metal_init: found device: Apple M4
0.00.055.027 I ggml_metal_init: picking default device: Apple M4
0.00.055.567 I ggml_metal_init: using embedded metal library
0.00.057.864 I ggml_metal_init: GPU name:   Apple M4
0.00.057.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.866 I ggml_metal_init: simdgroup reduction   = true
0.00.057.866 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.866 I ggml_metal_init: has bfloat            = true
0.00.057.867 I ggml_metal_init: use bfloat            = true
0.00.057.867 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.610 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.624 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.481 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.483 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.483 I llama_new_context_with_model: graph nodes  = 967
0.00.069.483 I llama_new_context_with_model: graph splits = 2
0.00.069.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.224.159 I 
0.00.224.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.224.216 I perplexity: tokenizing the input ..
0.00.231.820 I perplexity: tokenization took 7.603 ms
0.00.231.831 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.372.351 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.373.761 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.373.786 I llama_perf_context_print:        load time =     213.41 ms
0.00.373.787 I llama_perf_context_print: prompt eval time =     140.26 ms /   128 tokens (    1.10 ms per token,   912.60 tokens per second)
0.00.373.788 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.373.788 I llama_perf_context_print:       total time =     149.63 ms /   129 tokens
0.00.374.293 I ggml_metal_free: deallocating

real	0m0.389s
user	0m0.078s
sys	0m0.052s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.240 I build: 4266 (1da7b765) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.054 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.573 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.578 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.707 I llama_model_loader: - type  f32:  194 tensors
0.00.052.707 I llama_model_loader: - type  f16:   98 tensors
0.00.081.857 I llm_load_vocab: special tokens cache size = 25
0.00.088.477 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.479 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.479 I llm_load_print_meta: arch             = gptneox
0.00.088.480 I llm_load_print_meta: vocab type       = BPE
0.00.088.480 I llm_load_print_meta: n_vocab          = 50304
0.00.088.480 I llm_load_print_meta: n_merges         = 50009
0.00.088.480 I llm_load_print_meta: vocab_only       = 0
0.00.088.480 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.480 I llm_load_print_meta: n_embd           = 2048
0.00.088.481 I llm_load_print_meta: n_layer          = 24
0.00.088.483 I llm_load_print_meta: n_head           = 16
0.00.088.484 I llm_load_print_meta: n_head_kv        = 16
0.00.088.496 I llm_load_print_meta: n_rot            = 32
0.00.088.496 I llm_load_print_meta: n_swa            = 0
0.00.088.497 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.497 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.497 I llm_load_print_meta: n_gqa            = 1
0.00.088.498 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.499 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.499 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.499 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.500 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.500 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.500 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.501 I llm_load_print_meta: n_ff             = 8192
0.00.088.501 I llm_load_print_meta: n_expert         = 0
0.00.088.501 I llm_load_print_meta: n_expert_used    = 0
0.00.088.501 I llm_load_print_meta: causal attn      = 1
0.00.088.501 I llm_load_print_meta: pooling type     = 0
0.00.088.503 I llm_load_print_meta: rope type        = 2
0.00.088.503 I llm_load_print_meta: rope scaling     = linear
0.00.088.503 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.504 I llm_load_print_meta: freq_scale_train = 1
0.00.088.504 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.504 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.504 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.504 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.504 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.505 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.514 I llm_load_print_meta: model type       = 1.4B
0.00.088.514 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.515 I llm_load_print_meta: model params     = 1.41 B
0.00.088.515 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.515 I llm_load_print_meta: general.name     = 1.4B
0.00.088.516 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.516 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.516 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.516 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.517 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.517 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.517 I llm_load_print_meta: max token length = 1024
0.00.091.156 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.157 I llm_load_tensors: offloading output layer to GPU
0.00.091.157 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.168 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.169 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.095 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.095 I llama_new_context_with_model: n_ctx         = 128
0.00.092.096 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.096 I llama_new_context_with_model: n_batch       = 128
0.00.092.096 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.096 I llama_new_context_with_model: flash_attn    = 0
0.00.092.096 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.097 I llama_new_context_with_model: freq_scale    = 1
0.00.092.097 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.097 I ggml_metal_init: allocating
0.00.092.100 I ggml_metal_init: found device: Apple M4
0.00.092.102 I ggml_metal_init: picking default device: Apple M4
0.00.092.676 I ggml_metal_init: using embedded metal library
0.00.095.218 I ggml_metal_init: GPU name:   Apple M4
0.00.095.220 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.221 I ggml_metal_init: simdgroup reduction   = true
0.00.095.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.221 I ggml_metal_init: has bfloat            = true
0.00.095.221 I ggml_metal_init: use bfloat            = true
0.00.095.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.711 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.713 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.726 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.592 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.593 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.593 I llama_new_context_with_model: graph nodes  = 967
0.00.106.594 I llama_new_context_with_model: graph splits = 2
0.00.106.605 I 
0.00.106.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.637 I compute_imatrix: tokenizing the input ..
0.00.113.176 I compute_imatrix: tokenization took 6.538 ms
0.00.113.177 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.587.869 I compute_imatrix: 1.47 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.590.690 I llama_perf_context_print:        load time =    1565.81 ms
0.01.590.691 I llama_perf_context_print: prompt eval time =    1474.07 ms /   128 tokens (   11.52 ms per token,    86.83 tokens per second)
0.01.590.692 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.590.694 I llama_perf_context_print:       total time =    1568.62 ms /   129 tokens
0.01.591.226 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.163s
sys	0m0.245s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4266 (1da7b765)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106f0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106f0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106f0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106f0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106f0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106f0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106f0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106f0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106f0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106f0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106f0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106f0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106f0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106f0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106f0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106f102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106f109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106f11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106f11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106f12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106f13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106f13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106f14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106f147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106f14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106f15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106f15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106f16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106f166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106f169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106f17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106f17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106f17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106f18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106f18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106f18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106f19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106f195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106f19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106f19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106f1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106f1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106f1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106f1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106f1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106f1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106f1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106f1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106f1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106f1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106f1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106f1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106f20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106f204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106f20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106f20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106f21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106f21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106f220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106f22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106f229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106f22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106f23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106f237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106f23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106f241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106f24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106f24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106f251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106f256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106f25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106f26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106f266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106f26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106f27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106f276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106f27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106f28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106f286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106f28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106f29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106f296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106f29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106f2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106f2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106f2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106f2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106f2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106f2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106f1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106f2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106f2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106f2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106f2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106f2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106f2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106f2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106f2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106f2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106f2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106f2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106f2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106f30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106f307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106f30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106f311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106f31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106f31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106f31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106f32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106f328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106f32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106f33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106f336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106f33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106f33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106f34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106f34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106f35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106f35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106f35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106f36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106f364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106f36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106f36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106f372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106f37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106f380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106f38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106f389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106f38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106f39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106f397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106f39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106f3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106f3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106f3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106f3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106f3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106f3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106f3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106f3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106f3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106f3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106f3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106f3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106f3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106f3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106f3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106f3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106f3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106f3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106f3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106f3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106f3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106f40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106f406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106f41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106f414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106f41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106f41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106f42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106f42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106f42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106f43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106f43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106f439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106f43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106f442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106f44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106f44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106f450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106f45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106f45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106f45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106f46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106f467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106f46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106f47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106f475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106f47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106f47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106f48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106f489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106f48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106f49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106f49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106f49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106f4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106f4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106f4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106f4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106f4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106f4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106f4c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106f4cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106f4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106f4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106f4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106f4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106f4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106f4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106f4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106f4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106f50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106f506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106f50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106f51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106f51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106f52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106f52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106f52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106f53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106f53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106f53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106f54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106f54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106f54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106f55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106f55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106f55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106f560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106f56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106f56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106f570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106f57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106f57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106f580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106f58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106f58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106f590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106f59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106f59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106f5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106f5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106f5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106f5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106f5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106f5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106f5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106f5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106f5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106f5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106f5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106f5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106f5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106f5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106f5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106f5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106f5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106f5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106f60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106f605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106f60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106f60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106f61380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106f61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106f61cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106f62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106f62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106f62aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106f62f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106f633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106f63880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106f63d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106f641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106f64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106f64e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106f65550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106f65c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106f66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106f66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106f66e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106f67100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106f67710 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.147.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106f0e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106f0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106f0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106f0f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106f0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106f0fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106f108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106f10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106f11180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106f115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106f12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106f12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106f13530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106f13c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106f14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106f14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106f15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106f15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106f16160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106f16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106f16f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106f17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106f17aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106f17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106f18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106f187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106f18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106f190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106f19540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106f199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106f19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106f1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106f1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106f1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106f1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106f1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106f1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106f1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106f1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106f1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106f1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106f1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106f1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106f1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106f1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106f1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106f1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106f1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106f1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106f1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106f1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106f1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106f1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106f20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106f206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106f20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106f20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106f21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106f218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106f21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106f22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106f22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106f22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106f22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106f23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106f237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106f23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106f240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106f24510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106f24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106f24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106f25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106f256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106f25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106f25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106f26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106f26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106f26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106f27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106f275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106f27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106f27ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106f28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106f287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106f28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106f29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106f294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106f29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106f29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106f2a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106f2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106f2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106f2af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106f2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106f2b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106f2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106f2c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106f2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106f2ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106f2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106f2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106f2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106f2dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106f2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106f2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106f2e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106f2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106f2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106f2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106f2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106f2ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106f303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106f30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106f30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106f31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106f315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106f31a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106f31e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106f322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106f32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106f32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106f33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106f334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106f33920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106f33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106f34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106f34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106f34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106f34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106f353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106f35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106f35ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106f36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106f36580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106f369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106f36e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106f372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106f37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106f37bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106f38020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106f38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106f38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106f38d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106f391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106f39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106f39ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106f39f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106f3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106f3a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106f3ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106f3b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106f3b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106f3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106f3be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106f3c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106f3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106f3cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106f3d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106f3d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106f3d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106f3dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106f3e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106f3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106f3eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106f3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106f3f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106f3f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106f3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106f400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106f40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106f409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106f40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106f41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106f41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106f41b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106f41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106f42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106f428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106f42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106f431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106f43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106f43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106f43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106f44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106f447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106f44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106f450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106f45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125f04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125f046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125f04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125f04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125f053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125f05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125f05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125f06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125f065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125f06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125f06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125f07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125f07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125f07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125f08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125f084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125f08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125f08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125f09970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125f09c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125f09ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125f0a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125f0a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125f0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125f0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125f0b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125f0b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125f0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125f0c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125f0c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125f0cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125f0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125f0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125f0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125f0dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125f0e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125f0e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125f0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125f0eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125f0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125f0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125f0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125f10090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125f10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125f10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125f10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125f11250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125f116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125f11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125f11fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125f12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125f12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125f12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125f13160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125f135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125f13a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125f13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125f14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125f14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125f14c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125f15070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125f154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125f15950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125f15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125f16230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125f166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125f16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125f16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125f173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125f17860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125f17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125f18140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125f185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125f18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125f18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125f19300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125f19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125f19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125f1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125f1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125f1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125f1ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125f1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125f1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125f1baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125f1bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125f1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125f1c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125f1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125f1d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125f1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125f1e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125f1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125f1f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125f1f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125f1f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125f1fbb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106e044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106e04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106e04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106e05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106e056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106e05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106e05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106e063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106e06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106e06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106e07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106e07870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106e08390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106e08b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106e09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106e09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106e0a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106e0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106e0afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106e0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106e0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106e0c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106e0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106e0d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106e0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106e0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106e0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106e0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106e0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106e0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106e0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106e0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106e0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106e0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106e102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106e10720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106e10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106e11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106e11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106e118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106e11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106e121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106e12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106e12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106e12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106e13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106e137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106e13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106e140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106e14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106e149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106e14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106e15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106e15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106e15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106e15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106e16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106e16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106e16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106e17330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106e177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106e17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106e18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106e184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106e18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106e18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106e19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106e196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106e19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106e19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106e1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106e1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106e1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106e1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106e1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106e1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106e1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106e1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106e1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106e1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106e1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106e1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106e1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106e1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106e1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106e1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106e1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106e1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106e1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106e1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106e1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106e20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106e205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106e20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106e20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106e212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106e21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106e22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106e224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106e22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106e22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106e23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106e23670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106e23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106e23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106e24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106e24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106e25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106e25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106e259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106e25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106e27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106e27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106e27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106e27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106e281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106e28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106e28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106e28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106e293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106e29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106e29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106e2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106e2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106e2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106e2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106e2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106e2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106e2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106e2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106e2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106e2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106e2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106e2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106e2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106e2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106e2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106e2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106e2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106e30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106e30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106e30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106e31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106e318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106e31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106e321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106e32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106e32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106e32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106e33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106e337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106e33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106e340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106e34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106e34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106e35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106e37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106e375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106e39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106e3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106e3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106e3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106e3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106e3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106e3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106e3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106e3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106e3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106e3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106e3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106e3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106e3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106e403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106e40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106e413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106e41690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106e42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106e426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106e42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106e42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106e433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106e43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106e43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106e44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106e445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106e44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106e44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106e45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106e45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106e45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106e46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106e464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106e46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106e46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106e47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106e47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106e47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106e47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106e483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106e48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106e48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106e49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106e49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106e49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106e49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106e4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106e4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106e4afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106e4b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106e4ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106e4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106e4c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106e4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106e4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106e4dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106e4e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106e4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106e4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106e4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106e4f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106e4fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106e50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106e50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106e51580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106e51b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106e520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106e52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106e52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106e53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106e53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106e53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106e54040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106e54540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106e54a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106e54f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106e55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106e55940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106e55e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106e56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106e56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106e56d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106e57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106e57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106e58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106e58cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106e58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106e59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106e59a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106e5a030 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.766s
user	0m0.307s
sys	0m0.271s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4266 (1da7b765)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e70d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e70da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e70dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e70e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e70eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e70f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e70f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e70fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e710210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e710c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e711110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e711c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e7123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e713310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e713a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e714150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e714870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e715040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e715760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e715e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e7165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e716e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e717560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e717820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e717e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e718aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e718fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e7192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e719740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e719a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e71a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e71a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e71aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e71af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e71b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e71b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e71bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e71c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e71c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e71caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e71cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e71d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e71d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e71dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e71e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e71ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e71f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e71f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e71fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e720470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e720a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e721090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e721880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e721d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e7221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e722a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e723280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e723540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e7239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e723e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e724320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e7247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e724c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e725100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e7255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e725a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e725ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e726380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e726820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e726cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e727210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e727760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e727cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e728200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e728750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e728ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e7291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e729740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e729c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e72a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e72a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e72ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e72b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e72b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e72bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e72c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e72c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e72cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e72d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e72d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e72dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e72e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e72e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e72ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e71e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e72f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e72f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e72fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e730300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e730850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e730da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e7312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e731840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e731d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e7322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e732830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e732d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e7332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e733820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e733d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e734210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e7346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e734b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e734ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e735490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e735930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e735dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e736270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e736710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e736bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e7374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e737990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e737e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e7382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e738770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e738c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e7390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e739550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e7399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e739e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e73a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e73a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e73ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e73b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e73b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e73ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e73bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e73c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e73c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e73ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e73d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e73d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e73dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e73df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e73e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e73e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e73ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e73f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e73f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e73fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e73ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e740450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e7408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e740d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e741230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e7416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e741b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e742010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e7424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e742950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e742df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e743290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e743730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e743bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e744070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e744510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e7449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e744e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e7452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e745790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e745c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e7460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e746570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e746a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e746eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e747350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e7477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e747c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e7485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e748a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e748f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e7493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e749850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e749cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e74a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e74a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e74aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e74af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e74b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e74ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e74bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e74c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e74c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e74cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e74d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e74d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e74e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e74e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e74e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e74ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e74f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e74fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e750030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e7504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e750c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e7511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e751720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e751c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e7521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e752710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e752c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e7531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e753700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e753c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e7541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e7546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e754c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e755190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e7556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e755c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e756180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e7566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e756c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e7576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e758160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e7586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e758c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e759150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e7596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e759bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e75a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e75a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e75abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e75b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e75b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e75bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e75c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e75c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e75cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e75d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e75d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e75dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e75e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e75e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e75eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e75f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e75f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e75fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e7600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e760630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e760b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e7610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e761620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e7620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e762b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e7630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e763600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e763aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e763f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e7643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e764880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e764d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e7651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e765b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e765fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e766440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e7668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e766d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e767770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e767e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e7685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e768cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e7693f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e7696b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e769ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e76a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e76a770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e608d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e6091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e609650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e609ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e609f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e60a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e60a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e60ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e60b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e60b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e60bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e60c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e60cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e60d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e60dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e60e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e60ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e60f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e60f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e610090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e6107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e610ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e6115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e611d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e612430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e6126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e6129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e612e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e613290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e613700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e613c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e614110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e614580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e614840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e614cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e615680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e615b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e616580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e616a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e617480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e6182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e618760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e618bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e619040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e6194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e619920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e619d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e61a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e61a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e61aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e61b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e61b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e61ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e61c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e61c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e61ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e61d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e61d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e61da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e61df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e61e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e61e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e61ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e61f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e61f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e61faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e61ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e620430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e620980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e620ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e621420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e621970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e622410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e622960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e622eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e623400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e623950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e623ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e6243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e624940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e624e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e6253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e625930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e625e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e6263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e626920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e626e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e6273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e627910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e627e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e6283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e628e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e6293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e6298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e629e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e62a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e62a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e62ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e62b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e62b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e62be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e62c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e62c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e62ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e62d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e62d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e62dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e62e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e62e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e62eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e62efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e62f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e62f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e62fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e6306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e631030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e6314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e631970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e631e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e6322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e632750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e632bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e633090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e633530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e6339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e633e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e634310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e6347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e634c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e6350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e635590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e635a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e635ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e636370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e636810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e636cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e637150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e6375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e637a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e637f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e6383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e638870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e638d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e6391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e639650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e639af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e639f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e63a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e63a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e63ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e63b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e63b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e63bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e63bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e63c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e63c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e63cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e63d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e63d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e63dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e63e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e63e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e63e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e63f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e63f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e63fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e6400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e640550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e6409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e640e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e641330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e6417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e641c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e642110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e6425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e642ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e643390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e643830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e643cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e644170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e644610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e645000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e645550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e645aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e645ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e6462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e6468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e646ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e6474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e647cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e648170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e648430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e648a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e6496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e649b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e64a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e64a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e64ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e64b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e64b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e64bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e64c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e64c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e64ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e64d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e64d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e64dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e64e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e64e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e64ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e64f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e64f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e64fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e650210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e650760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e650cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e651200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e651750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e6521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e652740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e652c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e6531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e653730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e653c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e6541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e654720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e654c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e6551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e655710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e655c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e6561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e656700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e656c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e6571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e6576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e657c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e6586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e658c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e659180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e6596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e659c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e65a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e65a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e65ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e65b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e65b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e65bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e65c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e65c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e65cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e65d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e65d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e65da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e65df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e65e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e65e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e65ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e65f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e65f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e65fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e65ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e660420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e6608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e660d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e6612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e6619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e6620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e662810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e662f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e6631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e6639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e663ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e6642b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1068046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1068058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1068065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106807a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106808580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106808d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106809540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10680a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10680aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10680b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10680b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10680c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10680c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10680ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10680d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10680dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10680df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10680e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10680e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10680eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10680ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10680f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10680f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10680fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106810030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1068104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106810910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106810d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1068111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106811660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106811ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106811f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1068123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106812820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106812c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106813100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106813570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1068139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106813e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1068142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106814730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106814ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106815010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106815480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1068158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106815d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1068161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106816740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106816c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1068170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106817520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106817990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106817e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1068186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106818b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106818fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106819430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1068198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106819d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10681a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10681a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10681aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10681aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10681b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10681b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10681bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10681c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10681c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10681c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10681cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10681d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10681d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10681db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10681dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10681e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10681e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10681ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10681f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10681f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10681fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10681feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106820320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106820790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106820c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106821070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1068214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106821950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106821dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106822230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1068226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106822b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106822f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1068233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106823860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106823cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106824140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1068245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106824a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106824e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106825300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106825770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106825be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106826050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1068264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106826930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106826da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106827210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106827680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106827af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106827f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1068283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106828840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106828cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106829120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106829590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106829a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106829e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10682a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10682a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10682abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10682b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10682b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10682b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10682bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10682c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10682c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10682cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10682cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10682d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10682d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10682dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10682e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10682e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10682e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10682ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10682f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10682f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10682fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106830010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106830480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1068308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106830d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1068311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106831640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106831ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106831f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106832390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106832800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1068330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106833550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1068339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106833e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1068342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106834710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106834b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106834ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106835460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1068358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106835d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1068361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106836620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106836a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106836f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106837370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1068377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106837c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1068380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106838530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1068389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106838e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106839280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1068396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106839b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106839fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10683a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10683a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10683ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10683b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10683b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10683ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10683bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10683c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10683c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10683cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10683d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10683d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10683d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10683ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10683e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10683e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10683eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10683efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10683f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10683f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10683fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106840170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1068405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106840a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1068415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106841880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106841b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106841fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106842420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106842d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106843170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1068435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106843a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106843ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106844330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1068447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106844c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106845080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1068454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106845960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106845dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106846240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1068466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106846b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106846f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106847400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106847870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106847ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106848150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1068485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106848a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106848ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106849310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106849780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106849bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10684a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10684a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10684a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10684b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10684b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10684bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10684c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10684c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10684cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10684d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10684d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10684de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10684e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10684e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10684efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10684f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10684fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1068500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106850660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106850c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1068511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106851770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106851d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1068522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106852880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106852e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106853330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106853830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106853d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106854230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106854730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106854c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106855130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106855630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106855b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106856030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106856530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106856a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106856f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106857940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106858060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106858780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106858ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106859160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106859950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106859c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10685a220 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.933s
user	0m0.242s
sys	0m0.143s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
