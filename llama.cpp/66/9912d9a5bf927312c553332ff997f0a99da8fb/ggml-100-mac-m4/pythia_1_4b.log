Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.077s
user	0m1.027s
sys	0m1.464s
++ nproc
+ make -j10
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target llama-simple
[ 34%] Built target test-c
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-log
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-arg-parser
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-gguf
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-arg-parser
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-gguf
[ 63%] Built target test-chat-template
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Built target llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-infill
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-create
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-cli
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Built target llama-parallel
[ 82%] Built target llama-passkey
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Built target llama-perplexity
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-quantize
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tts
[ 93%] Built target llama-run
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.217s
user	0m6.536s
sys	0m10.182s

main: quantize time =  5334.41 ms
main:    total time =  5334.41 ms

main: quantize time =  2165.68 ms
main:    total time =  2165.68 ms

main: quantize time =  2032.57 ms
main:    total time =  2032.57 ms

main: quantize time =  2260.52 ms
main:    total time =  2260.52 ms

main: quantize time =  2584.27 ms
main:    total time =  2584.27 ms

main: quantize time =  5165.60 ms
main:    total time =  5165.60 ms

main: quantize time =  5844.78 ms
main:    total time =  5844.78 ms

main: quantize time =  7132.56 ms
main:    total time =  7132.56 ms

main: quantize time =  5983.42 ms
main:    total time =  5983.42 ms

main: quantize time =  4654.57 ms
main:    total time =  4654.57 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.149 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.312 I main: llama backend init
0.00.000.317 I main: load the model and apply lora adapter, if any
0.00.031.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.423 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.441 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.442 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.451 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.453 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.453 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.459 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.520 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.936 I llama_model_loader: - type  f32:  194 tensors
0.00.062.937 I llama_model_loader: - type  f16:   98 tensors
0.00.062.941 I print_info: file format = GGUF V3 (latest)
0.00.062.942 I print_info: file type   = all F32 (guessed)
0.00.062.944 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.941 I load: special tokens cache size = 25
0.00.088.974 I load: token to piece cache size = 0.2984 MB
0.00.089.004 I print_info: arch             = gptneox
0.00.089.005 I print_info: vocab_only       = 0
0.00.089.005 I print_info: n_ctx_train      = 2048
0.00.089.006 I print_info: n_embd           = 2048
0.00.089.006 I print_info: n_layer          = 24
0.00.089.013 I print_info: n_head           = 16
0.00.089.014 I print_info: n_head_kv        = 16
0.00.089.014 I print_info: n_rot            = 32
0.00.089.014 I print_info: n_swa            = 0
0.00.089.015 I print_info: n_embd_head_k    = 128
0.00.089.015 I print_info: n_embd_head_v    = 128
0.00.089.017 I print_info: n_gqa            = 1
0.00.089.023 I print_info: n_embd_k_gqa     = 2048
0.00.089.026 I print_info: n_embd_v_gqa     = 2048
0.00.089.027 I print_info: f_norm_eps       = 1.0e-05
0.00.089.028 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.028 I print_info: f_logit_scale    = 0.0e+00
0.00.089.033 I print_info: n_ff             = 8192
0.00.089.033 I print_info: n_expert         = 0
0.00.089.034 I print_info: n_expert_used    = 0
0.00.089.034 I print_info: causal attn      = 1
0.00.089.034 I print_info: pooling type     = 0
0.00.089.034 I print_info: rope type        = 2
0.00.089.035 I print_info: rope scaling     = linear
0.00.089.035 I print_info: freq_base_train  = 10000.0
0.00.089.036 I print_info: freq_scale_train = 1
0.00.089.036 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.038 I print_info: rope_finetuned   = unknown
0.00.089.038 I print_info: ssm_d_conv       = 0
0.00.089.038 I print_info: ssm_d_inner      = 0
0.00.089.038 I print_info: ssm_d_state      = 0
0.00.089.039 I print_info: ssm_dt_rank      = 0
0.00.089.039 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.039 I print_info: model type       = 1.4B
0.00.089.040 I print_info: model params     = 1.41 B
0.00.089.040 I print_info: general.name     = 1.4B
0.00.089.041 I print_info: vocab type       = BPE
0.00.089.041 I print_info: n_vocab          = 50304
0.00.089.042 I print_info: n_merges         = 50009
0.00.089.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.043 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.043 I print_info: LF token         = 187 'Ċ'
0.00.089.044 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.044 I print_info: max token length = 1024
0.00.089.045 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.193.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.193.179 I load_tensors: offloading output layer to GPU
0.00.193.179 I load_tensors: offloaded 25/25 layers to GPU
0.00.193.208 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.193.210 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.193.985 I llama_init_from_model: n_seq_max     = 1
0.00.193.986 I llama_init_from_model: n_ctx         = 2048
0.00.193.986 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.193.986 I llama_init_from_model: n_batch       = 2048
0.00.193.987 I llama_init_from_model: n_ubatch      = 512
0.00.193.987 I llama_init_from_model: flash_attn    = 0
0.00.193.988 I llama_init_from_model: freq_base     = 10000.0
0.00.193.988 I llama_init_from_model: freq_scale    = 1
0.00.193.989 I ggml_metal_init: allocating
0.00.194.212 I ggml_metal_init: found device: Apple M4
0.00.194.219 I ggml_metal_init: picking default device: Apple M4
0.00.195.243 I ggml_metal_init: using embedded metal library
0.00.204.945 I ggml_metal_init: GPU name:   Apple M4
0.00.204.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.204.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.204.948 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.204.948 I ggml_metal_init: simdgroup reduction   = true
0.00.204.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.204.949 I ggml_metal_init: has residency sets    = true
0.00.204.949 I ggml_metal_init: has bfloat            = true
0.00.204.949 I ggml_metal_init: use bfloat            = true
0.00.204.949 I ggml_metal_init: hasUnifiedMemory      = true
0.00.204.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.230.223 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.259.865 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.259.872 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.259.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.263.693 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.263.695 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.263.696 I llama_init_from_model: graph nodes  = 967
0.00.263.696 I llama_init_from_model: graph splits = 2
0.00.263.701 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.263.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.263.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.329.620 I main: llama threadpool init, n_threads = 4
0.00.329.670 I 
0.00.329.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.329.705 I 
0.00.329.896 I sampler seed: 1234
0.00.329.900 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.329.934 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.329.936 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.329.936 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.167.303 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.02.167.304 I llama_perf_context_print:        load time =     296.67 ms
0.02.167.305 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.30 tokens per second)
0.02.167.306 I llama_perf_context_print:        eval time =    1790.71 ms /    63 runs   (   28.42 ms per token,    35.18 tokens per second)
0.02.167.306 I llama_perf_context_print:       total time =    1838.64 ms /    70 tokens
0.02.167.583 I ggml_metal_free: deallocating

real	0m2.464s
user	0m0.133s
sys	0m0.188s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.100 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.106 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.106 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.107 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.114 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.114 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.114 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.668 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.670 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.671 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.672 I llama_model_loader: - type  f32:  194 tensors
0.00.034.672 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.673 I print_info: file format = GGUF V3 (latest)
0.00.034.674 I print_info: file type   = Q8_0
0.00.034.675 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.368 I load: special tokens cache size = 25
0.00.049.683 I load: token to piece cache size = 0.2984 MB
0.00.049.701 I print_info: arch             = gptneox
0.00.049.702 I print_info: vocab_only       = 0
0.00.049.703 I print_info: n_ctx_train      = 2048
0.00.049.703 I print_info: n_embd           = 2048
0.00.049.703 I print_info: n_layer          = 24
0.00.049.707 I print_info: n_head           = 16
0.00.049.708 I print_info: n_head_kv        = 16
0.00.049.708 I print_info: n_rot            = 32
0.00.049.708 I print_info: n_swa            = 0
0.00.049.709 I print_info: n_embd_head_k    = 128
0.00.049.709 I print_info: n_embd_head_v    = 128
0.00.049.709 I print_info: n_gqa            = 1
0.00.049.710 I print_info: n_embd_k_gqa     = 2048
0.00.049.711 I print_info: n_embd_v_gqa     = 2048
0.00.049.712 I print_info: f_norm_eps       = 1.0e-05
0.00.049.712 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.714 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.714 I print_info: f_logit_scale    = 0.0e+00
0.00.049.715 I print_info: n_ff             = 8192
0.00.049.715 I print_info: n_expert         = 0
0.00.049.715 I print_info: n_expert_used    = 0
0.00.049.715 I print_info: causal attn      = 1
0.00.049.716 I print_info: pooling type     = 0
0.00.049.716 I print_info: rope type        = 2
0.00.049.717 I print_info: rope scaling     = linear
0.00.049.718 I print_info: freq_base_train  = 10000.0
0.00.049.718 I print_info: freq_scale_train = 1
0.00.049.718 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.720 I print_info: rope_finetuned   = unknown
0.00.049.720 I print_info: ssm_d_conv       = 0
0.00.049.720 I print_info: ssm_d_inner      = 0
0.00.049.721 I print_info: ssm_d_state      = 0
0.00.049.721 I print_info: ssm_dt_rank      = 0
0.00.049.721 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.722 I print_info: model type       = 1.4B
0.00.049.722 I print_info: model params     = 1.41 B
0.00.049.722 I print_info: general.name     = 1.4B
0.00.049.723 I print_info: vocab type       = BPE
0.00.049.724 I print_info: n_vocab          = 50304
0.00.049.724 I print_info: n_merges         = 50009
0.00.049.724 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.725 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.725 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.725 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.725 I print_info: LF token         = 187 'Ċ'
0.00.049.726 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.726 I print_info: max token length = 1024
0.00.049.726 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.198.692 I load_tensors: offloading 24 repeating layers to GPU
0.01.198.698 I load_tensors: offloading output layer to GPU
0.01.198.700 I load_tensors: offloaded 25/25 layers to GPU
0.01.198.722 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.198.725 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.199.890 I llama_init_from_model: n_seq_max     = 1
0.01.199.892 I llama_init_from_model: n_ctx         = 2048
0.01.199.892 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.199.892 I llama_init_from_model: n_batch       = 2048
0.01.199.893 I llama_init_from_model: n_ubatch      = 512
0.01.199.893 I llama_init_from_model: flash_attn    = 0
0.01.199.894 I llama_init_from_model: freq_base     = 10000.0
0.01.199.894 I llama_init_from_model: freq_scale    = 1
0.01.199.895 I ggml_metal_init: allocating
0.01.199.911 I ggml_metal_init: found device: Apple M4
0.01.199.919 I ggml_metal_init: picking default device: Apple M4
0.01.201.218 I ggml_metal_init: using embedded metal library
0.01.206.616 I ggml_metal_init: GPU name:   Apple M4
0.01.206.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.206.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.206.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.206.621 I ggml_metal_init: simdgroup reduction   = true
0.01.206.621 I ggml_metal_init: simdgroup matrix mul. = true
0.01.206.621 I ggml_metal_init: has residency sets    = true
0.01.206.621 I ggml_metal_init: has bfloat            = true
0.01.206.621 I ggml_metal_init: use bfloat            = true
0.01.206.622 I ggml_metal_init: hasUnifiedMemory      = true
0.01.206.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.223.579 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.274.381 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.274.392 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.274.416 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.278.864 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.278.866 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.278.867 I llama_init_from_model: graph nodes  = 967
0.01.278.867 I llama_init_from_model: graph splits = 2
0.01.278.874 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.279.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.279.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.334.687 I main: llama threadpool init, n_threads = 4
0.01.334.734 I 
0.01.334.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.334.758 I 
0.01.334.921 I sampler seed: 1234
0.01.334.926 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.334.941 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.334.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.334.943 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.427.292 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.02.427.293 I llama_perf_context_print:        load time =    1323.44 ms
0.02.427.294 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.40 tokens per second)
0.02.427.295 I llama_perf_context_print:        eval time =    1040.53 ms /    63 runs   (   16.52 ms per token,    60.55 tokens per second)
0.02.427.295 I llama_perf_context_print:       total time =    1093.36 ms /    70 tokens
0.02.427.544 I ggml_metal_free: deallocating

real	0m2.445s
user	0m0.109s
sys	0m0.286s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.306 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.309 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.316 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.322 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.322 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.323 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.326 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.074 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.077 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.078 I llama_model_loader: - type  f32:  194 tensors
0.00.028.079 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.079 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.080 I print_info: file format = GGUF V3 (latest)
0.00.028.080 I print_info: file type   = Q4_0
0.00.028.081 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.310 I load: special tokens cache size = 25
0.00.042.577 I load: token to piece cache size = 0.2984 MB
0.00.042.592 I print_info: arch             = gptneox
0.00.042.593 I print_info: vocab_only       = 0
0.00.042.593 I print_info: n_ctx_train      = 2048
0.00.042.593 I print_info: n_embd           = 2048
0.00.042.593 I print_info: n_layer          = 24
0.00.042.598 I print_info: n_head           = 16
0.00.042.599 I print_info: n_head_kv        = 16
0.00.042.599 I print_info: n_rot            = 32
0.00.042.599 I print_info: n_swa            = 0
0.00.042.599 I print_info: n_embd_head_k    = 128
0.00.042.599 I print_info: n_embd_head_v    = 128
0.00.042.600 I print_info: n_gqa            = 1
0.00.042.604 I print_info: n_embd_k_gqa     = 2048
0.00.042.604 I print_info: n_embd_v_gqa     = 2048
0.00.042.605 I print_info: f_norm_eps       = 1.0e-05
0.00.042.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.606 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.606 I print_info: f_logit_scale    = 0.0e+00
0.00.042.606 I print_info: n_ff             = 8192
0.00.042.610 I print_info: n_expert         = 0
0.00.042.610 I print_info: n_expert_used    = 0
0.00.042.610 I print_info: causal attn      = 1
0.00.042.611 I print_info: pooling type     = 0
0.00.042.611 I print_info: rope type        = 2
0.00.042.612 I print_info: rope scaling     = linear
0.00.042.613 I print_info: freq_base_train  = 10000.0
0.00.042.613 I print_info: freq_scale_train = 1
0.00.042.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.613 I print_info: rope_finetuned   = unknown
0.00.042.613 I print_info: ssm_d_conv       = 0
0.00.042.613 I print_info: ssm_d_inner      = 0
0.00.042.614 I print_info: ssm_d_state      = 0
0.00.042.614 I print_info: ssm_dt_rank      = 0
0.00.042.614 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.614 I print_info: model type       = 1.4B
0.00.042.614 I print_info: model params     = 1.41 B
0.00.042.614 I print_info: general.name     = 1.4B
0.00.042.615 I print_info: vocab type       = BPE
0.00.042.615 I print_info: n_vocab          = 50304
0.00.042.616 I print_info: n_merges         = 50009
0.00.042.616 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.616 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.616 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.616 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.617 I print_info: LF token         = 187 'Ċ'
0.00.042.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.617 I print_info: max token length = 1024
0.00.042.617 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.918 I load_tensors: offloading output layer to GPU
0.00.584.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.955 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.584.957 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.268 I llama_init_from_model: n_seq_max     = 1
0.00.586.272 I llama_init_from_model: n_ctx         = 2048
0.00.586.273 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.273 I llama_init_from_model: n_batch       = 2048
0.00.586.274 I llama_init_from_model: n_ubatch      = 512
0.00.586.274 I llama_init_from_model: flash_attn    = 0
0.00.586.276 I llama_init_from_model: freq_base     = 10000.0
0.00.586.277 I llama_init_from_model: freq_scale    = 1
0.00.586.280 I ggml_metal_init: allocating
0.00.586.355 I ggml_metal_init: found device: Apple M4
0.00.586.368 I ggml_metal_init: picking default device: Apple M4
0.00.588.259 I ggml_metal_init: using embedded metal library
0.00.594.020 I ggml_metal_init: GPU name:   Apple M4
0.00.594.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.046 I ggml_metal_init: simdgroup reduction   = true
0.00.594.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.046 I ggml_metal_init: has residency sets    = true
0.00.594.047 I ggml_metal_init: has bfloat            = true
0.00.594.047 I ggml_metal_init: use bfloat            = true
0.00.594.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.557 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.224 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.666.233 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.666.257 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.500 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.503 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.503 I llama_init_from_model: graph nodes  = 967
0.00.684.503 I llama_init_from_model: graph splits = 2
0.00.684.515 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.644 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.952 I main: llama threadpool init, n_threads = 4
0.00.731.997 I 
0.00.732.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.020 I 
0.00.732.140 I sampler seed: 1234
0.00.732.144 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.158 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.159 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.453.233 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51749.27 tokens per second)
0.01.453.234 I llama_perf_context_print:        load time =     719.86 ms
0.01.453.235 I llama_perf_context_print: prompt eval time =      49.18 ms /     7 tokens (    7.03 ms per token,   142.34 tokens per second)
0.01.453.235 I llama_perf_context_print:        eval time =     669.11 ms /    63 runs   (   10.62 ms per token,    94.15 tokens per second)
0.01.453.237 I llama_perf_context_print:       total time =     722.06 ms /    70 tokens
0.01.453.470 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.112s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.013.129 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.089 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.090 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.423 I llama_model_loader: - type  f32:  194 tensors
0.00.029.424 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.425 I print_info: file format = GGUF V3 (latest)
0.00.029.425 I print_info: file type   = Q4_1
0.00.029.426 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.037.633 I load: special tokens cache size = 25
0.00.043.938 I load: token to piece cache size = 0.2984 MB
0.00.043.954 I print_info: arch             = gptneox
0.00.043.955 I print_info: vocab_only       = 0
0.00.043.956 I print_info: n_ctx_train      = 2048
0.00.043.956 I print_info: n_embd           = 2048
0.00.043.956 I print_info: n_layer          = 24
0.00.043.959 I print_info: n_head           = 16
0.00.043.960 I print_info: n_head_kv        = 16
0.00.043.960 I print_info: n_rot            = 32
0.00.043.960 I print_info: n_swa            = 0
0.00.043.960 I print_info: n_embd_head_k    = 128
0.00.043.960 I print_info: n_embd_head_v    = 128
0.00.043.961 I print_info: n_gqa            = 1
0.00.043.962 I print_info: n_embd_k_gqa     = 2048
0.00.043.962 I print_info: n_embd_v_gqa     = 2048
0.00.043.963 I print_info: f_norm_eps       = 1.0e-05
0.00.043.963 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.964 I print_info: f_logit_scale    = 0.0e+00
0.00.043.964 I print_info: n_ff             = 8192
0.00.043.965 I print_info: n_expert         = 0
0.00.043.965 I print_info: n_expert_used    = 0
0.00.043.965 I print_info: causal attn      = 1
0.00.043.965 I print_info: pooling type     = 0
0.00.043.965 I print_info: rope type        = 2
0.00.043.966 I print_info: rope scaling     = linear
0.00.043.966 I print_info: freq_base_train  = 10000.0
0.00.043.966 I print_info: freq_scale_train = 1
0.00.043.966 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.966 I print_info: rope_finetuned   = unknown
0.00.043.967 I print_info: ssm_d_conv       = 0
0.00.043.967 I print_info: ssm_d_inner      = 0
0.00.043.967 I print_info: ssm_d_state      = 0
0.00.043.970 I print_info: ssm_dt_rank      = 0
0.00.043.971 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.971 I print_info: model type       = 1.4B
0.00.043.971 I print_info: model params     = 1.41 B
0.00.043.971 I print_info: general.name     = 1.4B
0.00.043.972 I print_info: vocab type       = BPE
0.00.043.972 I print_info: n_vocab          = 50304
0.00.043.972 I print_info: n_merges         = 50009
0.00.043.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.974 I print_info: LF token         = 187 'Ċ'
0.00.043.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.975 I print_info: max token length = 1024
0.00.043.975 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.470 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.486 I load_tensors: offloading output layer to GPU
0.00.585.487 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.517 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.585.519 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.586.884 I llama_init_from_model: n_seq_max     = 1
0.00.586.888 I llama_init_from_model: n_ctx         = 2048
0.00.586.889 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.890 I llama_init_from_model: n_batch       = 2048
0.00.586.890 I llama_init_from_model: n_ubatch      = 512
0.00.586.890 I llama_init_from_model: flash_attn    = 0
0.00.586.892 I llama_init_from_model: freq_base     = 10000.0
0.00.586.893 I llama_init_from_model: freq_scale    = 1
0.00.586.895 I ggml_metal_init: allocating
0.00.586.971 I ggml_metal_init: found device: Apple M4
0.00.586.985 I ggml_metal_init: picking default device: Apple M4
0.00.588.863 I ggml_metal_init: using embedded metal library
0.00.595.405 I ggml_metal_init: GPU name:   Apple M4
0.00.595.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.412 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.413 I ggml_metal_init: simdgroup reduction   = true
0.00.595.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.414 I ggml_metal_init: has residency sets    = true
0.00.595.414 I ggml_metal_init: has bfloat            = true
0.00.595.415 I ggml_metal_init: use bfloat            = true
0.00.595.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.818 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.346 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.669.354 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.669.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.649 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.673.651 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.673.652 I llama_init_from_model: graph nodes  = 967
0.00.673.652 I llama_init_from_model: graph splits = 2
0.00.673.657 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.673.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.673.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.090 I main: llama threadpool init, n_threads = 4
0.00.723.134 I 
0.00.723.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.159 I 
0.00.723.279 I sampler seed: 1234
0.00.723.283 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.298 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.299 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.299 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.494.275 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.494.276 I llama_perf_context_print:        load time =     709.20 ms
0.01.494.277 I llama_perf_context_print: prompt eval time =      49.63 ms /     7 tokens (    7.09 ms per token,   141.06 tokens per second)
0.01.494.278 I llama_perf_context_print:        eval time =     718.66 ms /    63 runs   (   11.41 ms per token,    87.66 tokens per second)
0.01.494.278 I llama_perf_context_print:       total time =     771.94 ms /    70 tokens
0.01.494.541 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.110s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.885 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.693 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.448 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.449 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.449 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.450 I llama_model_loader: - type  f32:  194 tensors
0.00.026.450 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.450 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.451 I print_info: file format = GGUF V3 (latest)
0.00.026.451 I print_info: file type   = Q5_0
0.00.026.452 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.263 I load: special tokens cache size = 25
0.00.040.607 I load: token to piece cache size = 0.2984 MB
0.00.040.621 I print_info: arch             = gptneox
0.00.040.622 I print_info: vocab_only       = 0
0.00.040.622 I print_info: n_ctx_train      = 2048
0.00.040.622 I print_info: n_embd           = 2048
0.00.040.622 I print_info: n_layer          = 24
0.00.040.625 I print_info: n_head           = 16
0.00.040.626 I print_info: n_head_kv        = 16
0.00.040.626 I print_info: n_rot            = 32
0.00.040.626 I print_info: n_swa            = 0
0.00.040.626 I print_info: n_embd_head_k    = 128
0.00.040.626 I print_info: n_embd_head_v    = 128
0.00.040.627 I print_info: n_gqa            = 1
0.00.040.628 I print_info: n_embd_k_gqa     = 2048
0.00.040.628 I print_info: n_embd_v_gqa     = 2048
0.00.040.629 I print_info: f_norm_eps       = 1.0e-05
0.00.040.630 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.630 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.630 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.630 I print_info: f_logit_scale    = 0.0e+00
0.00.040.632 I print_info: n_ff             = 8192
0.00.040.633 I print_info: n_expert         = 0
0.00.040.633 I print_info: n_expert_used    = 0
0.00.040.633 I print_info: causal attn      = 1
0.00.040.633 I print_info: pooling type     = 0
0.00.040.633 I print_info: rope type        = 2
0.00.040.635 I print_info: rope scaling     = linear
0.00.040.635 I print_info: freq_base_train  = 10000.0
0.00.040.635 I print_info: freq_scale_train = 1
0.00.040.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.636 I print_info: rope_finetuned   = unknown
0.00.040.636 I print_info: ssm_d_conv       = 0
0.00.040.636 I print_info: ssm_d_inner      = 0
0.00.040.636 I print_info: ssm_d_state      = 0
0.00.040.637 I print_info: ssm_dt_rank      = 0
0.00.040.637 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.637 I print_info: model type       = 1.4B
0.00.040.637 I print_info: model params     = 1.41 B
0.00.040.638 I print_info: general.name     = 1.4B
0.00.040.638 I print_info: vocab type       = BPE
0.00.040.638 I print_info: n_vocab          = 50304
0.00.040.638 I print_info: n_merges         = 50009
0.00.040.639 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.639 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.639 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.639 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.639 I print_info: LF token         = 187 'Ċ'
0.00.040.640 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.640 I print_info: max token length = 1024
0.00.040.640 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.340 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.361 I load_tensors: offloading output layer to GPU
0.00.650.362 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.398 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.650.406 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.651.890 I llama_init_from_model: n_seq_max     = 1
0.00.651.893 I llama_init_from_model: n_ctx         = 2048
0.00.651.894 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.894 I llama_init_from_model: n_batch       = 2048
0.00.651.894 I llama_init_from_model: n_ubatch      = 512
0.00.651.895 I llama_init_from_model: flash_attn    = 0
0.00.651.896 I llama_init_from_model: freq_base     = 10000.0
0.00.651.897 I llama_init_from_model: freq_scale    = 1
0.00.651.899 I ggml_metal_init: allocating
0.00.651.993 I ggml_metal_init: found device: Apple M4
0.00.652.008 I ggml_metal_init: picking default device: Apple M4
0.00.653.974 I ggml_metal_init: using embedded metal library
0.00.660.897 I ggml_metal_init: GPU name:   Apple M4
0.00.660.903 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.904 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.905 I ggml_metal_init: simdgroup reduction   = true
0.00.660.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.906 I ggml_metal_init: has residency sets    = true
0.00.660.906 I ggml_metal_init: has bfloat            = true
0.00.660.906 I ggml_metal_init: use bfloat            = true
0.00.660.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.907 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.170 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.178 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.206 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.223 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.737.225 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.737.225 I llama_init_from_model: graph nodes  = 967
0.00.737.226 I llama_init_from_model: graph splits = 2
0.00.737.231 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.737.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.094 I main: llama threadpool init, n_threads = 4
0.00.786.140 I 
0.00.786.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.165 I 
0.00.786.304 I sampler seed: 1234
0.00.786.308 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.325 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.325 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.599.017 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.599.018 I llama_perf_context_print:        load time =     775.57 ms
0.01.599.019 I llama_perf_context_print: prompt eval time =      42.91 ms /     7 tokens (    6.13 ms per token,   163.12 tokens per second)
0.01.599.020 I llama_perf_context_print:        eval time =     766.91 ms /    63 runs   (   12.17 ms per token,    82.15 tokens per second)
0.01.599.021 I llama_perf_context_print:       total time =     813.68 ms /    70 tokens
0.01.599.308 I ggml_metal_free: deallocating

real	0m1.618s
user	0m0.110s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.372 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.411 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.416 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.424 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.426 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.427 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.428 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.982 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.983 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.984 I llama_model_loader: - type  f32:  194 tensors
0.00.026.985 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.985 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.985 I print_info: file format = GGUF V3 (latest)
0.00.026.986 I print_info: file type   = Q5_1
0.00.026.987 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.206 I load: special tokens cache size = 25
0.00.041.642 I load: token to piece cache size = 0.2984 MB
0.00.041.652 I print_info: arch             = gptneox
0.00.041.653 I print_info: vocab_only       = 0
0.00.041.653 I print_info: n_ctx_train      = 2048
0.00.041.653 I print_info: n_embd           = 2048
0.00.041.653 I print_info: n_layer          = 24
0.00.041.656 I print_info: n_head           = 16
0.00.041.657 I print_info: n_head_kv        = 16
0.00.041.657 I print_info: n_rot            = 32
0.00.041.657 I print_info: n_swa            = 0
0.00.041.658 I print_info: n_embd_head_k    = 128
0.00.041.658 I print_info: n_embd_head_v    = 128
0.00.041.659 I print_info: n_gqa            = 1
0.00.041.659 I print_info: n_embd_k_gqa     = 2048
0.00.041.660 I print_info: n_embd_v_gqa     = 2048
0.00.041.660 I print_info: f_norm_eps       = 1.0e-05
0.00.041.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.661 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.661 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.661 I print_info: f_logit_scale    = 0.0e+00
0.00.041.662 I print_info: n_ff             = 8192
0.00.041.662 I print_info: n_expert         = 0
0.00.041.662 I print_info: n_expert_used    = 0
0.00.041.662 I print_info: causal attn      = 1
0.00.041.662 I print_info: pooling type     = 0
0.00.041.663 I print_info: rope type        = 2
0.00.041.663 I print_info: rope scaling     = linear
0.00.041.663 I print_info: freq_base_train  = 10000.0
0.00.041.664 I print_info: freq_scale_train = 1
0.00.041.664 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.664 I print_info: rope_finetuned   = unknown
0.00.041.668 I print_info: ssm_d_conv       = 0
0.00.041.669 I print_info: ssm_d_inner      = 0
0.00.041.669 I print_info: ssm_d_state      = 0
0.00.041.669 I print_info: ssm_dt_rank      = 0
0.00.041.669 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.669 I print_info: model type       = 1.4B
0.00.041.670 I print_info: model params     = 1.41 B
0.00.041.670 I print_info: general.name     = 1.4B
0.00.041.670 I print_info: vocab type       = BPE
0.00.041.670 I print_info: n_vocab          = 50304
0.00.041.673 I print_info: n_merges         = 50009
0.00.041.675 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.676 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.677 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.677 I print_info: LF token         = 187 'Ċ'
0.00.041.678 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.678 I print_info: max token length = 1024
0.00.041.678 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.657.357 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.371 I load_tensors: offloading output layer to GPU
0.00.657.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.405 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.657.406 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.658.920 I llama_init_from_model: n_seq_max     = 1
0.00.658.923 I llama_init_from_model: n_ctx         = 2048
0.00.658.924 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.924 I llama_init_from_model: n_batch       = 2048
0.00.658.924 I llama_init_from_model: n_ubatch      = 512
0.00.658.925 I llama_init_from_model: flash_attn    = 0
0.00.658.928 I llama_init_from_model: freq_base     = 10000.0
0.00.658.928 I llama_init_from_model: freq_scale    = 1
0.00.658.931 I ggml_metal_init: allocating
0.00.659.006 I ggml_metal_init: found device: Apple M4
0.00.659.020 I ggml_metal_init: picking default device: Apple M4
0.00.660.956 I ggml_metal_init: using embedded metal library
0.00.667.643 I ggml_metal_init: GPU name:   Apple M4
0.00.667.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.649 I ggml_metal_init: simdgroup reduction   = true
0.00.667.650 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.650 I ggml_metal_init: has residency sets    = true
0.00.667.650 I ggml_metal_init: has bfloat            = true
0.00.667.650 I ggml_metal_init: use bfloat            = true
0.00.667.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.653 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.322 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.329 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.360 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.736.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.736.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.736.445 I llama_init_from_model: graph nodes  = 967
0.00.736.445 I llama_init_from_model: graph splits = 2
0.00.736.452 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.736.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.618 I main: llama threadpool init, n_threads = 4
0.00.792.662 I 
0.00.792.687 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.687 I 
0.00.792.868 I sampler seed: 1234
0.00.792.872 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.889 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.622.998 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.01.622.999 I llama_perf_context_print:        load time =     781.51 ms
0.01.622.999 I llama_perf_context_print: prompt eval time =      41.94 ms /     7 tokens (    5.99 ms per token,   166.91 tokens per second)
0.01.623.001 I llama_perf_context_print:        eval time =     785.38 ms /    63 runs   (   12.47 ms per token,    80.22 tokens per second)
0.01.623.002 I llama_perf_context_print:       total time =     831.11 ms /    70 tokens
0.01.623.226 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.617 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.618 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.624 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.124 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.125 I llama_model_loader: - type  f32:  194 tensors
0.00.024.125 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.125 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.126 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.126 I print_info: file format = GGUF V3 (latest)
0.00.024.127 I print_info: file type   = Q2_K - Medium
0.00.024.128 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.289 I load: special tokens cache size = 25
0.00.038.559 I load: token to piece cache size = 0.2984 MB
0.00.038.568 I print_info: arch             = gptneox
0.00.038.570 I print_info: vocab_only       = 0
0.00.038.570 I print_info: n_ctx_train      = 2048
0.00.038.570 I print_info: n_embd           = 2048
0.00.038.570 I print_info: n_layer          = 24
0.00.038.573 I print_info: n_head           = 16
0.00.038.574 I print_info: n_head_kv        = 16
0.00.038.574 I print_info: n_rot            = 32
0.00.038.574 I print_info: n_swa            = 0
0.00.038.575 I print_info: n_embd_head_k    = 128
0.00.038.575 I print_info: n_embd_head_v    = 128
0.00.038.576 I print_info: n_gqa            = 1
0.00.038.576 I print_info: n_embd_k_gqa     = 2048
0.00.038.577 I print_info: n_embd_v_gqa     = 2048
0.00.038.578 I print_info: f_norm_eps       = 1.0e-05
0.00.038.578 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.578 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.578 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.578 I print_info: f_logit_scale    = 0.0e+00
0.00.038.579 I print_info: n_ff             = 8192
0.00.038.579 I print_info: n_expert         = 0
0.00.038.579 I print_info: n_expert_used    = 0
0.00.038.580 I print_info: causal attn      = 1
0.00.038.580 I print_info: pooling type     = 0
0.00.038.580 I print_info: rope type        = 2
0.00.038.585 I print_info: rope scaling     = linear
0.00.038.585 I print_info: freq_base_train  = 10000.0
0.00.038.585 I print_info: freq_scale_train = 1
0.00.038.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.586 I print_info: rope_finetuned   = unknown
0.00.038.587 I print_info: ssm_d_conv       = 0
0.00.038.587 I print_info: ssm_d_inner      = 0
0.00.038.587 I print_info: ssm_d_state      = 0
0.00.038.587 I print_info: ssm_dt_rank      = 0
0.00.038.588 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.588 I print_info: model type       = 1.4B
0.00.038.588 I print_info: model params     = 1.41 B
0.00.038.588 I print_info: general.name     = 1.4B
0.00.038.589 I print_info: vocab type       = BPE
0.00.038.589 I print_info: n_vocab          = 50304
0.00.038.589 I print_info: n_merges         = 50009
0.00.038.589 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.592 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.592 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.592 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.593 I print_info: LF token         = 187 'Ċ'
0.00.038.595 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.595 I print_info: max token length = 1024
0.00.038.596 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.395.286 I load_tensors: offloading 24 repeating layers to GPU
0.00.395.301 I load_tensors: offloading output layer to GPU
0.00.395.302 I load_tensors: offloaded 25/25 layers to GPU
0.00.395.337 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.395.338 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.396.920 I llama_init_from_model: n_seq_max     = 1
0.00.396.922 I llama_init_from_model: n_ctx         = 2048
0.00.396.923 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.396.923 I llama_init_from_model: n_batch       = 2048
0.00.396.923 I llama_init_from_model: n_ubatch      = 512
0.00.396.924 I llama_init_from_model: flash_attn    = 0
0.00.396.926 I llama_init_from_model: freq_base     = 10000.0
0.00.396.926 I llama_init_from_model: freq_scale    = 1
0.00.396.929 I ggml_metal_init: allocating
0.00.397.032 I ggml_metal_init: found device: Apple M4
0.00.397.045 I ggml_metal_init: picking default device: Apple M4
0.00.398.970 I ggml_metal_init: using embedded metal library
0.00.404.669 I ggml_metal_init: GPU name:   Apple M4
0.00.404.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.404.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.404.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.404.689 I ggml_metal_init: simdgroup reduction   = true
0.00.404.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.404.689 I ggml_metal_init: has residency sets    = true
0.00.404.690 I ggml_metal_init: has bfloat            = true
0.00.404.690 I ggml_metal_init: use bfloat            = true
0.00.404.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.404.699 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.426.949 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.481.883 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.481.890 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.481.915 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.486.528 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.486.530 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.486.531 I llama_init_from_model: graph nodes  = 967
0.00.486.531 I llama_init_from_model: graph splits = 2
0.00.486.537 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.486.664 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.486.664 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.932 I main: llama threadpool init, n_threads = 4
0.00.547.977 I 
0.00.548.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.001 I 
0.00.548.183 I sampler seed: 1234
0.00.548.188 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.548.203 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.548.205 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.548.205 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.235.261 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.235.262 I llama_perf_context_print:        load time =     538.41 ms
0.01.235.262 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.51 tokens per second)
0.01.235.263 I llama_perf_context_print:        eval time =     640.07 ms /    63 runs   (   10.16 ms per token,    98.43 tokens per second)
0.01.235.264 I llama_perf_context_print:       total time =     688.08 ms /    70 tokens
0.01.235.512 I ggml_metal_free: deallocating

real	0m1.252s
user	0m0.113s
sys	0m0.176s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.436 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.443 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.453 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.455 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.456 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.264 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.984 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.985 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.985 I llama_model_loader: - type  f32:  194 tensors
0.00.025.986 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.986 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.986 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.986 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.987 I print_info: file format = GGUF V3 (latest)
0.00.025.987 I print_info: file type   = Q3_K - Medium
0.00.025.988 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.914 I load: special tokens cache size = 25
0.00.040.134 I load: token to piece cache size = 0.2984 MB
0.00.040.148 I print_info: arch             = gptneox
0.00.040.149 I print_info: vocab_only       = 0
0.00.040.149 I print_info: n_ctx_train      = 2048
0.00.040.149 I print_info: n_embd           = 2048
0.00.040.150 I print_info: n_layer          = 24
0.00.040.152 I print_info: n_head           = 16
0.00.040.153 I print_info: n_head_kv        = 16
0.00.040.153 I print_info: n_rot            = 32
0.00.040.154 I print_info: n_swa            = 0
0.00.040.154 I print_info: n_embd_head_k    = 128
0.00.040.154 I print_info: n_embd_head_v    = 128
0.00.040.155 I print_info: n_gqa            = 1
0.00.040.155 I print_info: n_embd_k_gqa     = 2048
0.00.040.156 I print_info: n_embd_v_gqa     = 2048
0.00.040.157 I print_info: f_norm_eps       = 1.0e-05
0.00.040.157 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.157 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.158 I print_info: f_logit_scale    = 0.0e+00
0.00.040.158 I print_info: n_ff             = 8192
0.00.040.159 I print_info: n_expert         = 0
0.00.040.159 I print_info: n_expert_used    = 0
0.00.040.160 I print_info: causal attn      = 1
0.00.040.161 I print_info: pooling type     = 0
0.00.040.161 I print_info: rope type        = 2
0.00.040.162 I print_info: rope scaling     = linear
0.00.040.162 I print_info: freq_base_train  = 10000.0
0.00.040.162 I print_info: freq_scale_train = 1
0.00.040.162 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.162 I print_info: rope_finetuned   = unknown
0.00.040.163 I print_info: ssm_d_conv       = 0
0.00.040.163 I print_info: ssm_d_inner      = 0
0.00.040.163 I print_info: ssm_d_state      = 0
0.00.040.163 I print_info: ssm_dt_rank      = 0
0.00.040.163 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.163 I print_info: model type       = 1.4B
0.00.040.163 I print_info: model params     = 1.41 B
0.00.040.164 I print_info: general.name     = 1.4B
0.00.040.164 I print_info: vocab type       = BPE
0.00.040.164 I print_info: n_vocab          = 50304
0.00.040.164 I print_info: n_merges         = 50009
0.00.040.165 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: LF token         = 187 'Ċ'
0.00.040.165 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.166 I print_info: max token length = 1024
0.00.040.166 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.054 I load_tensors: offloading output layer to GPU
0.00.443.055 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.088 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.089 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.830 I llama_init_from_model: n_seq_max     = 1
0.00.444.834 I llama_init_from_model: n_ctx         = 2048
0.00.444.834 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.835 I llama_init_from_model: n_batch       = 2048
0.00.444.835 I llama_init_from_model: n_ubatch      = 512
0.00.444.836 I llama_init_from_model: flash_attn    = 0
0.00.444.838 I llama_init_from_model: freq_base     = 10000.0
0.00.444.839 I llama_init_from_model: freq_scale    = 1
0.00.444.841 I ggml_metal_init: allocating
0.00.444.917 I ggml_metal_init: found device: Apple M4
0.00.444.935 I ggml_metal_init: picking default device: Apple M4
0.00.446.834 I ggml_metal_init: using embedded metal library
0.00.452.333 I ggml_metal_init: GPU name:   Apple M4
0.00.452.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.341 I ggml_metal_init: simdgroup reduction   = true
0.00.452.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.341 I ggml_metal_init: has residency sets    = true
0.00.452.342 I ggml_metal_init: has bfloat            = true
0.00.452.342 I ggml_metal_init: use bfloat            = true
0.00.452.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.314 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.765 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.527.773 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.527.798 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.533.851 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.533.854 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.533.854 I llama_init_from_model: graph nodes  = 967
0.00.533.854 I llama_init_from_model: graph splits = 2
0.00.533.859 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.533.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.533.985 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.603 I main: llama threadpool init, n_threads = 4
0.00.591.650 I 
0.00.591.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.674 I 
0.00.591.841 I sampler seed: 1234
0.00.591.845 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.885 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.889 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.894 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50859.60 tokens per second)
0.01.342.894 I llama_perf_context_print:        load time =     581.05 ms
0.01.342.895 I llama_perf_context_print: prompt eval time =      45.51 ms /     7 tokens (    6.50 ms per token,   153.82 tokens per second)
0.01.342.896 I llama_perf_context_print:        eval time =     702.57 ms /    63 runs   (   11.15 ms per token,    89.67 tokens per second)
0.01.342.897 I llama_perf_context_print:       total time =     752.05 ms /    70 tokens
0.01.343.134 I ggml_metal_free: deallocating

real	0m1.362s
user	0m0.111s
sys	0m0.188s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.294 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.871 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.876 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.521 I llama_model_loader: - type  f32:  194 tensors
0.00.026.521 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.521 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.522 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.522 I print_info: file format = GGUF V3 (latest)
0.00.026.523 I print_info: file type   = Q4_K - Medium
0.00.026.524 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.419 I load: special tokens cache size = 25
0.00.040.662 I load: token to piece cache size = 0.2984 MB
0.00.040.676 I print_info: arch             = gptneox
0.00.040.677 I print_info: vocab_only       = 0
0.00.040.677 I print_info: n_ctx_train      = 2048
0.00.040.678 I print_info: n_embd           = 2048
0.00.040.678 I print_info: n_layer          = 24
0.00.040.680 I print_info: n_head           = 16
0.00.040.681 I print_info: n_head_kv        = 16
0.00.040.681 I print_info: n_rot            = 32
0.00.040.681 I print_info: n_swa            = 0
0.00.040.682 I print_info: n_embd_head_k    = 128
0.00.040.682 I print_info: n_embd_head_v    = 128
0.00.040.682 I print_info: n_gqa            = 1
0.00.040.683 I print_info: n_embd_k_gqa     = 2048
0.00.040.684 I print_info: n_embd_v_gqa     = 2048
0.00.040.684 I print_info: f_norm_eps       = 1.0e-05
0.00.040.685 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.685 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.685 I print_info: f_logit_scale    = 0.0e+00
0.00.040.686 I print_info: n_ff             = 8192
0.00.040.686 I print_info: n_expert         = 0
0.00.040.686 I print_info: n_expert_used    = 0
0.00.040.686 I print_info: causal attn      = 1
0.00.040.688 I print_info: pooling type     = 0
0.00.040.688 I print_info: rope type        = 2
0.00.040.688 I print_info: rope scaling     = linear
0.00.040.688 I print_info: freq_base_train  = 10000.0
0.00.040.689 I print_info: freq_scale_train = 1
0.00.040.689 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.689 I print_info: rope_finetuned   = unknown
0.00.040.689 I print_info: ssm_d_conv       = 0
0.00.040.689 I print_info: ssm_d_inner      = 0
0.00.040.689 I print_info: ssm_d_state      = 0
0.00.040.689 I print_info: ssm_dt_rank      = 0
0.00.040.690 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.690 I print_info: model type       = 1.4B
0.00.040.690 I print_info: model params     = 1.41 B
0.00.040.690 I print_info: general.name     = 1.4B
0.00.040.691 I print_info: vocab type       = BPE
0.00.040.691 I print_info: n_vocab          = 50304
0.00.040.691 I print_info: n_merges         = 50009
0.00.040.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.691 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.691 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.692 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.693 I print_info: LF token         = 187 'Ċ'
0.00.040.693 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.695 I print_info: max token length = 1024
0.00.040.695 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.516.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.793 I load_tensors: offloading output layer to GPU
0.00.516.794 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.828 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.829 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.518.563 I llama_init_from_model: n_seq_max     = 1
0.00.518.566 I llama_init_from_model: n_ctx         = 2048
0.00.518.567 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.518.567 I llama_init_from_model: n_batch       = 2048
0.00.518.567 I llama_init_from_model: n_ubatch      = 512
0.00.518.568 I llama_init_from_model: flash_attn    = 0
0.00.518.571 I llama_init_from_model: freq_base     = 10000.0
0.00.518.571 I llama_init_from_model: freq_scale    = 1
0.00.518.575 I ggml_metal_init: allocating
0.00.518.645 I ggml_metal_init: found device: Apple M4
0.00.518.657 I ggml_metal_init: picking default device: Apple M4
0.00.520.463 I ggml_metal_init: using embedded metal library
0.00.527.047 I ggml_metal_init: GPU name:   Apple M4
0.00.527.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.053 I ggml_metal_init: simdgroup reduction   = true
0.00.527.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.053 I ggml_metal_init: has residency sets    = true
0.00.527.054 I ggml_metal_init: has bfloat            = true
0.00.527.054 I ggml_metal_init: use bfloat            = true
0.00.527.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.066 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.489 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.819 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.611.828 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.611.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.103 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.616.105 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.616.105 I llama_init_from_model: graph nodes  = 967
0.00.616.106 I llama_init_from_model: graph splits = 2
0.00.616.112 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.616.246 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.616.247 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.353 I main: llama threadpool init, n_threads = 4
0.00.671.405 I 
0.00.671.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.433 I 
0.00.671.603 I sampler seed: 1234
0.00.671.608 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.671.623 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.671.626 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.671.626 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.432.546 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49719.89 tokens per second)
0.01.432.547 I llama_perf_context_print:        load time =     660.32 ms
0.01.432.549 I llama_perf_context_print: prompt eval time =      47.30 ms /     7 tokens (    6.76 ms per token,   147.98 tokens per second)
0.01.432.549 I llama_perf_context_print:        eval time =     710.68 ms /    63 runs   (   11.28 ms per token,    88.65 tokens per second)
0.01.432.551 I llama_perf_context_print:       total time =     761.93 ms /    70 tokens
0.01.432.849 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.111s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.493 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.386 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.391 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.201 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.959 I llama_model_loader: - type  f32:  194 tensors
0.00.025.959 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.959 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.960 I print_info: file format = GGUF V3 (latest)
0.00.025.960 I print_info: file type   = Q5_K - Medium
0.00.025.961 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.842 I load: special tokens cache size = 25
0.00.040.292 I load: token to piece cache size = 0.2984 MB
0.00.040.306 I print_info: arch             = gptneox
0.00.040.307 I print_info: vocab_only       = 0
0.00.040.308 I print_info: n_ctx_train      = 2048
0.00.040.308 I print_info: n_embd           = 2048
0.00.040.308 I print_info: n_layer          = 24
0.00.040.311 I print_info: n_head           = 16
0.00.040.312 I print_info: n_head_kv        = 16
0.00.040.312 I print_info: n_rot            = 32
0.00.040.312 I print_info: n_swa            = 0
0.00.040.312 I print_info: n_embd_head_k    = 128
0.00.040.312 I print_info: n_embd_head_v    = 128
0.00.040.313 I print_info: n_gqa            = 1
0.00.040.314 I print_info: n_embd_k_gqa     = 2048
0.00.040.314 I print_info: n_embd_v_gqa     = 2048
0.00.040.317 I print_info: f_norm_eps       = 1.0e-05
0.00.040.317 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.317 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.317 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.317 I print_info: f_logit_scale    = 0.0e+00
0.00.040.318 I print_info: n_ff             = 8192
0.00.040.318 I print_info: n_expert         = 0
0.00.040.318 I print_info: n_expert_used    = 0
0.00.040.318 I print_info: causal attn      = 1
0.00.040.318 I print_info: pooling type     = 0
0.00.040.320 I print_info: rope type        = 2
0.00.040.321 I print_info: rope scaling     = linear
0.00.040.322 I print_info: freq_base_train  = 10000.0
0.00.040.322 I print_info: freq_scale_train = 1
0.00.040.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.322 I print_info: rope_finetuned   = unknown
0.00.040.322 I print_info: ssm_d_conv       = 0
0.00.040.322 I print_info: ssm_d_inner      = 0
0.00.040.322 I print_info: ssm_d_state      = 0
0.00.040.322 I print_info: ssm_dt_rank      = 0
0.00.040.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.323 I print_info: model type       = 1.4B
0.00.040.326 I print_info: model params     = 1.41 B
0.00.040.326 I print_info: general.name     = 1.4B
0.00.040.327 I print_info: vocab type       = BPE
0.00.040.327 I print_info: n_vocab          = 50304
0.00.040.328 I print_info: n_merges         = 50009
0.00.040.329 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.329 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.329 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.329 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.329 I print_info: LF token         = 187 'Ċ'
0.00.040.330 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.330 I print_info: max token length = 1024
0.00.040.330 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.342 I load_tensors: offloading output layer to GPU
0.00.600.343 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.367 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.371 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.942 I llama_init_from_model: n_seq_max     = 1
0.00.601.945 I llama_init_from_model: n_ctx         = 2048
0.00.601.945 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.945 I llama_init_from_model: n_batch       = 2048
0.00.601.946 I llama_init_from_model: n_ubatch      = 512
0.00.601.947 I llama_init_from_model: flash_attn    = 0
0.00.601.948 I llama_init_from_model: freq_base     = 10000.0
0.00.601.948 I llama_init_from_model: freq_scale    = 1
0.00.601.949 I ggml_metal_init: allocating
0.00.601.964 I ggml_metal_init: found device: Apple M4
0.00.601.974 I ggml_metal_init: picking default device: Apple M4
0.00.603.522 I ggml_metal_init: using embedded metal library
0.00.609.768 I ggml_metal_init: GPU name:   Apple M4
0.00.609.771 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.772 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.773 I ggml_metal_init: simdgroup reduction   = true
0.00.609.773 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.774 I ggml_metal_init: has residency sets    = true
0.00.609.774 I ggml_metal_init: has bfloat            = true
0.00.609.774 I ggml_metal_init: use bfloat            = true
0.00.609.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.148 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.429 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.432 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.432 I llama_init_from_model: graph nodes  = 967
0.00.703.433 I llama_init_from_model: graph splits = 2
0.00.703.438 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.844 I main: llama threadpool init, n_threads = 4
0.00.768.889 I 
0.00.768.913 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.914 I 
0.00.769.080 I sampler seed: 1234
0.00.769.085 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.101 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.101 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.101 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.615.637 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.615.637 I llama_perf_context_print:        load time =     758.59 ms
0.01.615.638 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.35 tokens per second)
0.01.615.639 I llama_perf_context_print:        eval time =     790.63 ms /    63 runs   (   12.55 ms per token,    79.68 tokens per second)
0.01.615.639 I llama_perf_context_print:       total time =     847.55 ms /    70 tokens
0.01.615.887 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.112s
sys	0m0.218s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.139 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.943 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.638 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.639 I llama_model_loader: - type  f32:  194 tensors
0.00.027.640 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.640 I print_info: file format = GGUF V3 (latest)
0.00.027.641 I print_info: file type   = Q6_K
0.00.027.642 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.599 I load: special tokens cache size = 25
0.00.042.063 I load: token to piece cache size = 0.2984 MB
0.00.042.077 I print_info: arch             = gptneox
0.00.042.078 I print_info: vocab_only       = 0
0.00.042.079 I print_info: n_ctx_train      = 2048
0.00.042.079 I print_info: n_embd           = 2048
0.00.042.079 I print_info: n_layer          = 24
0.00.042.082 I print_info: n_head           = 16
0.00.042.083 I print_info: n_head_kv        = 16
0.00.042.083 I print_info: n_rot            = 32
0.00.042.083 I print_info: n_swa            = 0
0.00.042.083 I print_info: n_embd_head_k    = 128
0.00.042.084 I print_info: n_embd_head_v    = 128
0.00.042.084 I print_info: n_gqa            = 1
0.00.042.085 I print_info: n_embd_k_gqa     = 2048
0.00.042.086 I print_info: n_embd_v_gqa     = 2048
0.00.042.086 I print_info: f_norm_eps       = 1.0e-05
0.00.042.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.089 I print_info: f_logit_scale    = 0.0e+00
0.00.042.089 I print_info: n_ff             = 8192
0.00.042.090 I print_info: n_expert         = 0
0.00.042.090 I print_info: n_expert_used    = 0
0.00.042.090 I print_info: causal attn      = 1
0.00.042.090 I print_info: pooling type     = 0
0.00.042.090 I print_info: rope type        = 2
0.00.042.090 I print_info: rope scaling     = linear
0.00.042.090 I print_info: freq_base_train  = 10000.0
0.00.042.091 I print_info: freq_scale_train = 1
0.00.042.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.091 I print_info: rope_finetuned   = unknown
0.00.042.092 I print_info: ssm_d_conv       = 0
0.00.042.093 I print_info: ssm_d_inner      = 0
0.00.042.093 I print_info: ssm_d_state      = 0
0.00.042.093 I print_info: ssm_dt_rank      = 0
0.00.042.093 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.093 I print_info: model type       = 1.4B
0.00.042.093 I print_info: model params     = 1.41 B
0.00.042.093 I print_info: general.name     = 1.4B
0.00.042.094 I print_info: vocab type       = BPE
0.00.042.095 I print_info: n_vocab          = 50304
0.00.042.095 I print_info: n_merges         = 50009
0.00.042.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.095 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.099 I print_info: LF token         = 187 'Ċ'
0.00.042.099 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.099 I print_info: max token length = 1024
0.00.042.102 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.939 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.943 I load_tensors: offloading output layer to GPU
0.00.652.944 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.966 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.652.971 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.654.193 I llama_init_from_model: n_seq_max     = 1
0.00.654.195 I llama_init_from_model: n_ctx         = 2048
0.00.654.196 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.196 I llama_init_from_model: n_batch       = 2048
0.00.654.196 I llama_init_from_model: n_ubatch      = 512
0.00.654.197 I llama_init_from_model: flash_attn    = 0
0.00.654.198 I llama_init_from_model: freq_base     = 10000.0
0.00.654.198 I llama_init_from_model: freq_scale    = 1
0.00.654.199 I ggml_metal_init: allocating
0.00.654.214 I ggml_metal_init: found device: Apple M4
0.00.654.225 I ggml_metal_init: picking default device: Apple M4
0.00.655.613 I ggml_metal_init: using embedded metal library
0.00.661.479 I ggml_metal_init: GPU name:   Apple M4
0.00.661.482 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.484 I ggml_metal_init: simdgroup reduction   = true
0.00.661.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.484 I ggml_metal_init: has residency sets    = true
0.00.661.484 I ggml_metal_init: has bfloat            = true
0.00.661.485 I ggml_metal_init: use bfloat            = true
0.00.661.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.028 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.012 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.019 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.686 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.689 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.689 I llama_init_from_model: graph nodes  = 967
0.00.734.689 I llama_init_from_model: graph splits = 2
0.00.734.696 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.803 I main: llama threadpool init, n_threads = 4
0.00.803.850 I 
0.00.803.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.875 I 
0.00.804.036 I sampler seed: 1234
0.00.804.041 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.097 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.101 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.101 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.682.276 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.682.276 I llama_perf_context_print:        load time =     792.25 ms
0.01.682.277 I llama_perf_context_print: prompt eval time =      57.77 ms /     7 tokens (    8.25 ms per token,   121.16 tokens per second)
0.01.682.278 I llama_perf_context_print:        eval time =     817.46 ms /    63 runs   (   12.98 ms per token,    77.07 tokens per second)
0.01.682.278 I llama_perf_context_print:       total time =     879.22 ms /    70 tokens
0.01.682.514 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.621 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.815 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.316 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.328 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.330 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.347 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.348 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.905 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.594 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.595 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.595 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.595 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.596 I llama_model_loader: - type  f32:  194 tensors
0.00.053.597 I llama_model_loader: - type  f16:   98 tensors
0.00.053.598 I print_info: file format = GGUF V3 (latest)
0.00.053.598 I print_info: file type   = all F32 (guessed)
0.00.053.599 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.974 I load: special tokens cache size = 25
0.00.074.719 I load: token to piece cache size = 0.2984 MB
0.00.074.734 I print_info: arch             = gptneox
0.00.074.735 I print_info: vocab_only       = 0
0.00.074.735 I print_info: n_ctx_train      = 2048
0.00.074.735 I print_info: n_embd           = 2048
0.00.074.736 I print_info: n_layer          = 24
0.00.074.739 I print_info: n_head           = 16
0.00.074.740 I print_info: n_head_kv        = 16
0.00.074.740 I print_info: n_rot            = 32
0.00.074.740 I print_info: n_swa            = 0
0.00.074.740 I print_info: n_embd_head_k    = 128
0.00.074.740 I print_info: n_embd_head_v    = 128
0.00.074.741 I print_info: n_gqa            = 1
0.00.074.742 I print_info: n_embd_k_gqa     = 2048
0.00.074.743 I print_info: n_embd_v_gqa     = 2048
0.00.074.743 I print_info: f_norm_eps       = 1.0e-05
0.00.074.744 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.744 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.744 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.744 I print_info: f_logit_scale    = 0.0e+00
0.00.074.745 I print_info: n_ff             = 8192
0.00.074.745 I print_info: n_expert         = 0
0.00.074.745 I print_info: n_expert_used    = 0
0.00.074.745 I print_info: causal attn      = 1
0.00.074.745 I print_info: pooling type     = 0
0.00.074.746 I print_info: rope type        = 2
0.00.074.746 I print_info: rope scaling     = linear
0.00.074.746 I print_info: freq_base_train  = 10000.0
0.00.074.747 I print_info: freq_scale_train = 1
0.00.074.747 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.749 I print_info: rope_finetuned   = unknown
0.00.074.750 I print_info: ssm_d_conv       = 0
0.00.074.750 I print_info: ssm_d_inner      = 0
0.00.074.751 I print_info: ssm_d_state      = 0
0.00.074.751 I print_info: ssm_dt_rank      = 0
0.00.074.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.751 I print_info: model type       = 1.4B
0.00.074.752 I print_info: model params     = 1.41 B
0.00.074.752 I print_info: general.name     = 1.4B
0.00.074.752 I print_info: vocab type       = BPE
0.00.074.753 I print_info: n_vocab          = 50304
0.00.074.753 I print_info: n_merges         = 50009
0.00.074.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.753 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.753 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.754 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.754 I print_info: LF token         = 187 'Ċ'
0.00.074.756 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.756 I print_info: max token length = 1024
0.00.074.756 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.438.135 I load_tensors: offloading 24 repeating layers to GPU
0.01.438.141 I load_tensors: offloading output layer to GPU
0.01.438.142 I load_tensors: offloaded 25/25 layers to GPU
0.01.438.166 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.438.168 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.438.977 I llama_init_from_model: n_seq_max     = 1
0.01.438.978 I llama_init_from_model: n_ctx         = 128
0.01.438.979 I llama_init_from_model: n_ctx_per_seq = 128
0.01.438.979 I llama_init_from_model: n_batch       = 128
0.01.438.979 I llama_init_from_model: n_ubatch      = 128
0.01.438.979 I llama_init_from_model: flash_attn    = 0
0.01.438.980 I llama_init_from_model: freq_base     = 10000.0
0.01.438.980 I llama_init_from_model: freq_scale    = 1
0.01.438.980 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.438.982 I ggml_metal_init: allocating
0.01.439.015 I ggml_metal_init: found device: Apple M4
0.01.439.022 I ggml_metal_init: picking default device: Apple M4
0.01.440.190 I ggml_metal_init: using embedded metal library
0.01.444.218 I ggml_metal_init: GPU name:   Apple M4
0.01.444.220 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.444.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.444.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.444.221 I ggml_metal_init: simdgroup reduction   = true
0.01.444.221 I ggml_metal_init: simdgroup matrix mul. = true
0.01.444.221 I ggml_metal_init: has residency sets    = true
0.01.444.221 I ggml_metal_init: has bfloat            = true
0.01.444.222 I ggml_metal_init: use bfloat            = true
0.01.444.222 I ggml_metal_init: hasUnifiedMemory      = true
0.01.444.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.455.420 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.457.216 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.457.218 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.457.232 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.458.942 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.458.943 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.458.943 I llama_init_from_model: graph nodes  = 967
0.01.458.944 I llama_init_from_model: graph splits = 2
0.01.458.945 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.458.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.497.004 I 
0.01.497.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.497.092 I perplexity: tokenizing the input ..
0.01.502.973 I perplexity: tokenization took 5.878 ms
0.01.502.980 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.621.200 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.622.584 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.598 I llama_perf_context_print:        load time =    1474.18 ms
0.01.622.600 I llama_perf_context_print: prompt eval time =     117.89 ms /   128 tokens (    0.92 ms per token,  1085.79 tokens per second)
0.01.622.600 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.601 I llama_perf_context_print:       total time =     125.59 ms /   129 tokens
0.01.622.984 I ggml_metal_free: deallocating

real	0m1.813s
user	0m0.099s
sys	0m0.254s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.249 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.249 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.255 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.256 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.259 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.890 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.892 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.892 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.893 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.893 I llama_model_loader: - type  f32:  194 tensors
0.00.025.894 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.894 I print_info: file format = GGUF V3 (latest)
0.00.025.895 I print_info: file type   = Q8_0
0.00.025.896 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.044 I load: special tokens cache size = 25
0.00.040.154 I load: token to piece cache size = 0.2984 MB
0.00.040.171 I print_info: arch             = gptneox
0.00.040.172 I print_info: vocab_only       = 0
0.00.040.172 I print_info: n_ctx_train      = 2048
0.00.040.172 I print_info: n_embd           = 2048
0.00.040.172 I print_info: n_layer          = 24
0.00.040.176 I print_info: n_head           = 16
0.00.040.177 I print_info: n_head_kv        = 16
0.00.040.177 I print_info: n_rot            = 32
0.00.040.177 I print_info: n_swa            = 0
0.00.040.177 I print_info: n_embd_head_k    = 128
0.00.040.177 I print_info: n_embd_head_v    = 128
0.00.040.178 I print_info: n_gqa            = 1
0.00.040.178 I print_info: n_embd_k_gqa     = 2048
0.00.040.179 I print_info: n_embd_v_gqa     = 2048
0.00.040.179 I print_info: f_norm_eps       = 1.0e-05
0.00.040.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.180 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.180 I print_info: f_logit_scale    = 0.0e+00
0.00.040.181 I print_info: n_ff             = 8192
0.00.040.181 I print_info: n_expert         = 0
0.00.040.181 I print_info: n_expert_used    = 0
0.00.040.181 I print_info: causal attn      = 1
0.00.040.181 I print_info: pooling type     = 0
0.00.040.184 I print_info: rope type        = 2
0.00.040.184 I print_info: rope scaling     = linear
0.00.040.185 I print_info: freq_base_train  = 10000.0
0.00.040.185 I print_info: freq_scale_train = 1
0.00.040.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.185 I print_info: rope_finetuned   = unknown
0.00.040.186 I print_info: ssm_d_conv       = 0
0.00.040.186 I print_info: ssm_d_inner      = 0
0.00.040.186 I print_info: ssm_d_state      = 0
0.00.040.186 I print_info: ssm_dt_rank      = 0
0.00.040.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.188 I print_info: model type       = 1.4B
0.00.040.188 I print_info: model params     = 1.41 B
0.00.040.188 I print_info: general.name     = 1.4B
0.00.040.189 I print_info: vocab type       = BPE
0.00.040.189 I print_info: n_vocab          = 50304
0.00.040.189 I print_info: n_merges         = 50009
0.00.040.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.190 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.190 I print_info: LF token         = 187 'Ċ'
0.00.040.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.191 I print_info: max token length = 1024
0.00.040.191 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.731.855 I load_tensors: offloading 24 repeating layers to GPU
0.00.731.863 I load_tensors: offloading output layer to GPU
0.00.731.863 I load_tensors: offloaded 25/25 layers to GPU
0.00.731.893 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.731.894 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.733.308 I llama_init_from_model: n_seq_max     = 1
0.00.733.311 I llama_init_from_model: n_ctx         = 128
0.00.733.311 I llama_init_from_model: n_ctx_per_seq = 128
0.00.733.311 I llama_init_from_model: n_batch       = 128
0.00.733.312 I llama_init_from_model: n_ubatch      = 128
0.00.733.312 I llama_init_from_model: flash_attn    = 0
0.00.733.313 I llama_init_from_model: freq_base     = 10000.0
0.00.733.313 I llama_init_from_model: freq_scale    = 1
0.00.733.314 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.733.315 I ggml_metal_init: allocating
0.00.733.399 I ggml_metal_init: found device: Apple M4
0.00.733.409 I ggml_metal_init: picking default device: Apple M4
0.00.734.721 I ggml_metal_init: using embedded metal library
0.00.739.957 I ggml_metal_init: GPU name:   Apple M4
0.00.739.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.739.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.739.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.739.962 I ggml_metal_init: simdgroup reduction   = true
0.00.739.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.739.962 I ggml_metal_init: has residency sets    = true
0.00.739.963 I ggml_metal_init: has bfloat            = true
0.00.739.963 I ggml_metal_init: use bfloat            = true
0.00.739.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.739.968 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.347 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.725 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.758.729 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.758.753 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.761.902 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.761.904 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.761.905 I llama_init_from_model: graph nodes  = 967
0.00.761.905 I llama_init_from_model: graph splits = 2
0.00.761.908 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.761.908 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.756 I 
0.00.786.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.857 I perplexity: tokenizing the input ..
0.00.794.316 I perplexity: tokenization took 7.456 ms
0.00.794.324 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.919.057 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.920.412 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.920.431 I llama_perf_context_print:        load time =     776.69 ms
0.00.920.432 I llama_perf_context_print: prompt eval time =     124.11 ms /   128 tokens (    0.97 ms per token,  1031.37 tokens per second)
0.00.920.433 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.920.434 I llama_perf_context_print:       total time =     133.68 ms /   129 tokens
0.00.920.818 I ggml_metal_free: deallocating

real	0m0.937s
user	0m0.077s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.256 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.117 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.129 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.130 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.131 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.133 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.133 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.963 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.764 I llama_model_loader: - type  f32:  194 tensors
0.00.025.764 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.764 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.765 I print_info: file format = GGUF V3 (latest)
0.00.025.766 I print_info: file type   = Q4_0
0.00.025.767 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.183 I load: special tokens cache size = 25
0.00.040.332 I load: token to piece cache size = 0.2984 MB
0.00.040.348 I print_info: arch             = gptneox
0.00.040.349 I print_info: vocab_only       = 0
0.00.040.349 I print_info: n_ctx_train      = 2048
0.00.040.350 I print_info: n_embd           = 2048
0.00.040.350 I print_info: n_layer          = 24
0.00.040.354 I print_info: n_head           = 16
0.00.040.354 I print_info: n_head_kv        = 16
0.00.040.354 I print_info: n_rot            = 32
0.00.040.355 I print_info: n_swa            = 0
0.00.040.355 I print_info: n_embd_head_k    = 128
0.00.040.355 I print_info: n_embd_head_v    = 128
0.00.040.356 I print_info: n_gqa            = 1
0.00.040.356 I print_info: n_embd_k_gqa     = 2048
0.00.040.359 I print_info: n_embd_v_gqa     = 2048
0.00.040.360 I print_info: f_norm_eps       = 1.0e-05
0.00.040.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.361 I print_info: f_logit_scale    = 0.0e+00
0.00.040.361 I print_info: n_ff             = 8192
0.00.040.361 I print_info: n_expert         = 0
0.00.040.361 I print_info: n_expert_used    = 0
0.00.040.362 I print_info: causal attn      = 1
0.00.040.362 I print_info: pooling type     = 0
0.00.040.363 I print_info: rope type        = 2
0.00.040.364 I print_info: rope scaling     = linear
0.00.040.364 I print_info: freq_base_train  = 10000.0
0.00.040.364 I print_info: freq_scale_train = 1
0.00.040.364 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.365 I print_info: rope_finetuned   = unknown
0.00.040.365 I print_info: ssm_d_conv       = 0
0.00.040.365 I print_info: ssm_d_inner      = 0
0.00.040.365 I print_info: ssm_d_state      = 0
0.00.040.365 I print_info: ssm_dt_rank      = 0
0.00.040.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.365 I print_info: model type       = 1.4B
0.00.040.366 I print_info: model params     = 1.41 B
0.00.040.366 I print_info: general.name     = 1.4B
0.00.040.366 I print_info: vocab type       = BPE
0.00.040.367 I print_info: n_vocab          = 50304
0.00.040.367 I print_info: n_merges         = 50009
0.00.040.367 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.367 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.368 I print_info: LF token         = 187 'Ċ'
0.00.040.369 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: max token length = 1024
0.00.040.369 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.419 I load_tensors: offloading output layer to GPU
0.00.595.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.454 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.595.457 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.597.046 I llama_init_from_model: n_seq_max     = 1
0.00.597.049 I llama_init_from_model: n_ctx         = 128
0.00.597.050 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.050 I llama_init_from_model: n_batch       = 128
0.00.597.051 I llama_init_from_model: n_ubatch      = 128
0.00.597.051 I llama_init_from_model: flash_attn    = 0
0.00.597.053 I llama_init_from_model: freq_base     = 10000.0
0.00.597.054 I llama_init_from_model: freq_scale    = 1
0.00.597.054 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.057 I ggml_metal_init: allocating
0.00.597.132 I ggml_metal_init: found device: Apple M4
0.00.597.146 I ggml_metal_init: picking default device: Apple M4
0.00.598.922 I ggml_metal_init: using embedded metal library
0.00.604.500 I ggml_metal_init: GPU name:   Apple M4
0.00.604.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.514 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.515 I ggml_metal_init: simdgroup reduction   = true
0.00.604.515 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.516 I ggml_metal_init: has residency sets    = true
0.00.604.516 I ggml_metal_init: has bfloat            = true
0.00.604.516 I ggml_metal_init: use bfloat            = true
0.00.604.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.490 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.305 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.317 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.352 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.608 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.610 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.611 I llama_init_from_model: graph nodes  = 967
0.00.632.611 I llama_init_from_model: graph splits = 2
0.00.632.614 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.068 I 
0.00.661.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.184 I perplexity: tokenizing the input ..
0.00.668.009 I perplexity: tokenization took 6.823 ms
0.00.668.015 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.669 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.804.007 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.804.026 I llama_perf_context_print:        load time =     651.23 ms
0.00.804.028 I llama_perf_context_print: prompt eval time =     134.37 ms /   128 tokens (    1.05 ms per token,   952.63 tokens per second)
0.00.804.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.029 I llama_perf_context_print:       total time =     142.96 ms /   129 tokens
0.00.804.387 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.081s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.543 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.346 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.346 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.347 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.347 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.348 I llama_model_loader: - type  f32:  194 tensors
0.00.025.348 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.349 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.349 I print_info: file format = GGUF V3 (latest)
0.00.025.350 I print_info: file type   = Q4_1
0.00.025.351 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.465 I load: special tokens cache size = 25
0.00.039.604 I load: token to piece cache size = 0.2984 MB
0.00.039.621 I print_info: arch             = gptneox
0.00.039.622 I print_info: vocab_only       = 0
0.00.039.622 I print_info: n_ctx_train      = 2048
0.00.039.622 I print_info: n_embd           = 2048
0.00.039.623 I print_info: n_layer          = 24
0.00.039.626 I print_info: n_head           = 16
0.00.039.627 I print_info: n_head_kv        = 16
0.00.039.627 I print_info: n_rot            = 32
0.00.039.627 I print_info: n_swa            = 0
0.00.039.627 I print_info: n_embd_head_k    = 128
0.00.039.628 I print_info: n_embd_head_v    = 128
0.00.039.631 I print_info: n_gqa            = 1
0.00.039.631 I print_info: n_embd_k_gqa     = 2048
0.00.039.632 I print_info: n_embd_v_gqa     = 2048
0.00.039.632 I print_info: f_norm_eps       = 1.0e-05
0.00.039.633 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.633 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.633 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.633 I print_info: f_logit_scale    = 0.0e+00
0.00.039.634 I print_info: n_ff             = 8192
0.00.039.634 I print_info: n_expert         = 0
0.00.039.634 I print_info: n_expert_used    = 0
0.00.039.634 I print_info: causal attn      = 1
0.00.039.634 I print_info: pooling type     = 0
0.00.039.634 I print_info: rope type        = 2
0.00.039.634 I print_info: rope scaling     = linear
0.00.039.635 I print_info: freq_base_train  = 10000.0
0.00.039.635 I print_info: freq_scale_train = 1
0.00.039.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.635 I print_info: rope_finetuned   = unknown
0.00.039.635 I print_info: ssm_d_conv       = 0
0.00.039.636 I print_info: ssm_d_inner      = 0
0.00.039.636 I print_info: ssm_d_state      = 0
0.00.039.636 I print_info: ssm_dt_rank      = 0
0.00.039.636 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.636 I print_info: model type       = 1.4B
0.00.039.637 I print_info: model params     = 1.41 B
0.00.039.637 I print_info: general.name     = 1.4B
0.00.039.637 I print_info: vocab type       = BPE
0.00.039.637 I print_info: n_vocab          = 50304
0.00.039.638 I print_info: n_merges         = 50009
0.00.039.638 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.638 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.638 I print_info: LF token         = 187 'Ċ'
0.00.039.639 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.639 I print_info: max token length = 1024
0.00.039.639 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.564.380 I load_tensors: offloading 24 repeating layers to GPU
0.00.564.383 I load_tensors: offloading output layer to GPU
0.00.564.384 I load_tensors: offloaded 25/25 layers to GPU
0.00.564.402 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.564.403 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.565.360 I llama_init_from_model: n_seq_max     = 1
0.00.565.364 I llama_init_from_model: n_ctx         = 128
0.00.565.365 I llama_init_from_model: n_ctx_per_seq = 128
0.00.565.365 I llama_init_from_model: n_batch       = 128
0.00.565.365 I llama_init_from_model: n_ubatch      = 128
0.00.565.366 I llama_init_from_model: flash_attn    = 0
0.00.565.367 I llama_init_from_model: freq_base     = 10000.0
0.00.565.368 I llama_init_from_model: freq_scale    = 1
0.00.565.368 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.565.370 I ggml_metal_init: allocating
0.00.565.406 I ggml_metal_init: found device: Apple M4
0.00.565.416 I ggml_metal_init: picking default device: Apple M4
0.00.566.459 I ggml_metal_init: using embedded metal library
0.00.570.674 I ggml_metal_init: GPU name:   Apple M4
0.00.570.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.570.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.570.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.570.685 I ggml_metal_init: simdgroup reduction   = true
0.00.570.685 I ggml_metal_init: simdgroup matrix mul. = true
0.00.570.685 I ggml_metal_init: has residency sets    = true
0.00.570.685 I ggml_metal_init: has bfloat            = true
0.00.570.686 I ggml_metal_init: use bfloat            = true
0.00.570.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.570.689 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.587.307 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.588.918 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.588.922 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.588.937 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.590.541 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.590.543 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.590.543 I llama_init_from_model: graph nodes  = 967
0.00.590.543 I llama_init_from_model: graph splits = 2
0.00.590.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.868 I 
0.00.611.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.916 I perplexity: tokenizing the input ..
0.00.615.973 I perplexity: tokenization took 4.054 ms
0.00.615.976 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.024 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.752.419 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.752.434 I llama_perf_context_print:        load time =     602.32 ms
0.00.752.434 I llama_perf_context_print: prompt eval time =     134.81 ms /   128 tokens (    1.05 ms per token,   949.47 tokens per second)
0.00.752.435 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.437 I llama_perf_context_print:       total time =     140.57 ms /   129 tokens
0.00.752.797 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.072s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.291 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.698 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.700 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.701 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.701 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.702 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.703 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.704 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.705 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.491 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.492 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.493 I llama_model_loader: - type  f32:  194 tensors
0.00.027.494 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.494 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.495 I print_info: file format = GGUF V3 (latest)
0.00.027.495 I print_info: file type   = Q5_0
0.00.027.497 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.915 I load: special tokens cache size = 25
0.00.042.424 I load: token to piece cache size = 0.2984 MB
0.00.042.442 I print_info: arch             = gptneox
0.00.042.443 I print_info: vocab_only       = 0
0.00.042.443 I print_info: n_ctx_train      = 2048
0.00.042.444 I print_info: n_embd           = 2048
0.00.042.444 I print_info: n_layer          = 24
0.00.042.448 I print_info: n_head           = 16
0.00.042.449 I print_info: n_head_kv        = 16
0.00.042.449 I print_info: n_rot            = 32
0.00.042.449 I print_info: n_swa            = 0
0.00.042.449 I print_info: n_embd_head_k    = 128
0.00.042.450 I print_info: n_embd_head_v    = 128
0.00.042.450 I print_info: n_gqa            = 1
0.00.042.451 I print_info: n_embd_k_gqa     = 2048
0.00.042.451 I print_info: n_embd_v_gqa     = 2048
0.00.042.452 I print_info: f_norm_eps       = 1.0e-05
0.00.042.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.455 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.455 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.455 I print_info: f_logit_scale    = 0.0e+00
0.00.042.456 I print_info: n_ff             = 8192
0.00.042.456 I print_info: n_expert         = 0
0.00.042.456 I print_info: n_expert_used    = 0
0.00.042.456 I print_info: causal attn      = 1
0.00.042.461 I print_info: pooling type     = 0
0.00.042.461 I print_info: rope type        = 2
0.00.042.462 I print_info: rope scaling     = linear
0.00.042.462 I print_info: freq_base_train  = 10000.0
0.00.042.462 I print_info: freq_scale_train = 1
0.00.042.462 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.463 I print_info: rope_finetuned   = unknown
0.00.042.463 I print_info: ssm_d_conv       = 0
0.00.042.463 I print_info: ssm_d_inner      = 0
0.00.042.463 I print_info: ssm_d_state      = 0
0.00.042.463 I print_info: ssm_dt_rank      = 0
0.00.042.463 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.463 I print_info: model type       = 1.4B
0.00.042.464 I print_info: model params     = 1.41 B
0.00.042.464 I print_info: general.name     = 1.4B
0.00.042.465 I print_info: vocab type       = BPE
0.00.042.465 I print_info: n_vocab          = 50304
0.00.042.465 I print_info: n_merges         = 50009
0.00.042.465 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.465 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.465 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.465 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.466 I print_info: LF token         = 187 'Ċ'
0.00.042.466 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.466 I print_info: max token length = 1024
0.00.042.467 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.273.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.273.921 I load_tensors: offloading output layer to GPU
0.00.273.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.273.942 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.273.947 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.274.938 I llama_init_from_model: n_seq_max     = 1
0.00.274.942 I llama_init_from_model: n_ctx         = 128
0.00.274.943 I llama_init_from_model: n_ctx_per_seq = 128
0.00.274.943 I llama_init_from_model: n_batch       = 128
0.00.274.943 I llama_init_from_model: n_ubatch      = 128
0.00.274.944 I llama_init_from_model: flash_attn    = 0
0.00.274.945 I llama_init_from_model: freq_base     = 10000.0
0.00.274.946 I llama_init_from_model: freq_scale    = 1
0.00.274.946 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.274.948 I ggml_metal_init: allocating
0.00.275.004 I ggml_metal_init: found device: Apple M4
0.00.275.016 I ggml_metal_init: picking default device: Apple M4
0.00.276.070 I ggml_metal_init: using embedded metal library
0.00.280.284 I ggml_metal_init: GPU name:   Apple M4
0.00.280.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.280.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.280.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.280.292 I ggml_metal_init: simdgroup reduction   = true
0.00.280.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.280.293 I ggml_metal_init: has residency sets    = true
0.00.280.293 I ggml_metal_init: has bfloat            = true
0.00.280.293 I ggml_metal_init: use bfloat            = true
0.00.280.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.280.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.294.576 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.296.209 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.296.213 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.296.237 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.297.844 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.297.845 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.297.845 I llama_init_from_model: graph nodes  = 967
0.00.297.845 I llama_init_from_model: graph splits = 2
0.00.297.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.297.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.324.360 I 
0.00.324.394 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.324.408 I perplexity: tokenizing the input ..
0.00.328.288 I perplexity: tokenization took 3.879 ms
0.00.328.292 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.474.772 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.476.608 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.476.621 I llama_perf_context_print:        load time =     313.06 ms
0.00.476.622 I llama_perf_context_print: prompt eval time =     146.25 ms /   128 tokens (    1.14 ms per token,   875.19 tokens per second)
0.00.476.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.476.623 I llama_perf_context_print:       total time =     152.26 ms /   129 tokens
0.00.476.994 I ggml_metal_free: deallocating

real	0m0.492s
user	0m0.071s
sys	0m0.060s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.163 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.010 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.016 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.017 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.024 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.027 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.027 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.028 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.610 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.612 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.612 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.613 I llama_model_loader: - type  f32:  194 tensors
0.00.025.613 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.614 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.614 I print_info: file format = GGUF V3 (latest)
0.00.025.615 I print_info: file type   = Q5_1
0.00.025.619 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.585 I load: special tokens cache size = 25
0.00.039.869 I load: token to piece cache size = 0.2984 MB
0.00.039.883 I print_info: arch             = gptneox
0.00.039.884 I print_info: vocab_only       = 0
0.00.039.884 I print_info: n_ctx_train      = 2048
0.00.039.884 I print_info: n_embd           = 2048
0.00.039.885 I print_info: n_layer          = 24
0.00.039.887 I print_info: n_head           = 16
0.00.039.888 I print_info: n_head_kv        = 16
0.00.039.888 I print_info: n_rot            = 32
0.00.039.889 I print_info: n_swa            = 0
0.00.039.889 I print_info: n_embd_head_k    = 128
0.00.039.889 I print_info: n_embd_head_v    = 128
0.00.039.891 I print_info: n_gqa            = 1
0.00.039.892 I print_info: n_embd_k_gqa     = 2048
0.00.039.893 I print_info: n_embd_v_gqa     = 2048
0.00.039.893 I print_info: f_norm_eps       = 1.0e-05
0.00.039.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.895 I print_info: f_logit_scale    = 0.0e+00
0.00.039.896 I print_info: n_ff             = 8192
0.00.039.896 I print_info: n_expert         = 0
0.00.039.896 I print_info: n_expert_used    = 0
0.00.039.896 I print_info: causal attn      = 1
0.00.039.896 I print_info: pooling type     = 0
0.00.039.897 I print_info: rope type        = 2
0.00.039.897 I print_info: rope scaling     = linear
0.00.039.897 I print_info: freq_base_train  = 10000.0
0.00.039.897 I print_info: freq_scale_train = 1
0.00.039.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.897 I print_info: rope_finetuned   = unknown
0.00.039.898 I print_info: ssm_d_conv       = 0
0.00.039.898 I print_info: ssm_d_inner      = 0
0.00.039.898 I print_info: ssm_d_state      = 0
0.00.039.898 I print_info: ssm_dt_rank      = 0
0.00.039.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.899 I print_info: model type       = 1.4B
0.00.039.903 I print_info: model params     = 1.41 B
0.00.039.904 I print_info: general.name     = 1.4B
0.00.039.905 I print_info: vocab type       = BPE
0.00.039.905 I print_info: n_vocab          = 50304
0.00.039.905 I print_info: n_merges         = 50009
0.00.039.906 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.907 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.907 I print_info: LF token         = 187 'Ċ'
0.00.039.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: max token length = 1024
0.00.039.908 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.665.769 I load_tensors: offloading 24 repeating layers to GPU
0.00.665.780 I load_tensors: offloading output layer to GPU
0.00.665.781 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.813 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.665.846 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.667.452 I llama_init_from_model: n_seq_max     = 1
0.00.667.455 I llama_init_from_model: n_ctx         = 128
0.00.667.455 I llama_init_from_model: n_ctx_per_seq = 128
0.00.667.455 I llama_init_from_model: n_batch       = 128
0.00.667.456 I llama_init_from_model: n_ubatch      = 128
0.00.667.456 I llama_init_from_model: flash_attn    = 0
0.00.667.457 I llama_init_from_model: freq_base     = 10000.0
0.00.667.458 I llama_init_from_model: freq_scale    = 1
0.00.667.459 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.667.460 I ggml_metal_init: allocating
0.00.667.477 I ggml_metal_init: found device: Apple M4
0.00.667.487 I ggml_metal_init: picking default device: Apple M4
0.00.668.933 I ggml_metal_init: using embedded metal library
0.00.675.123 I ggml_metal_init: GPU name:   Apple M4
0.00.675.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.129 I ggml_metal_init: simdgroup reduction   = true
0.00.675.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.130 I ggml_metal_init: has residency sets    = true
0.00.675.130 I ggml_metal_init: has bfloat            = true
0.00.675.130 I ggml_metal_init: use bfloat            = true
0.00.675.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.101 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.439 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.443 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.825 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.827 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.828 I llama_init_from_model: graph nodes  = 967
0.00.698.828 I llama_init_from_model: graph splits = 2
0.00.698.831 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.226 I 
0.00.732.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.346 I perplexity: tokenizing the input ..
0.00.739.660 I perplexity: tokenization took 7.311 ms
0.00.739.668 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.883.599 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.884.923 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.884.940 I llama_perf_context_print:        load time =     723.05 ms
0.00.884.941 I llama_perf_context_print: prompt eval time =     142.99 ms /   128 tokens (    1.12 ms per token,   895.19 tokens per second)
0.00.884.941 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.884.941 I llama_perf_context_print:       total time =     152.72 ms /   129 tokens
0.00.885.322 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.080s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.752 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.795 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.281 I llama_model_loader: - type  f32:  194 tensors
0.00.025.281 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.281 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.282 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.282 I print_info: file format = GGUF V3 (latest)
0.00.025.283 I print_info: file type   = Q2_K - Medium
0.00.025.284 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.799 I load: special tokens cache size = 25
0.00.040.179 I load: token to piece cache size = 0.2984 MB
0.00.040.196 I print_info: arch             = gptneox
0.00.040.197 I print_info: vocab_only       = 0
0.00.040.197 I print_info: n_ctx_train      = 2048
0.00.040.197 I print_info: n_embd           = 2048
0.00.040.197 I print_info: n_layer          = 24
0.00.040.201 I print_info: n_head           = 16
0.00.040.202 I print_info: n_head_kv        = 16
0.00.040.202 I print_info: n_rot            = 32
0.00.040.202 I print_info: n_swa            = 0
0.00.040.202 I print_info: n_embd_head_k    = 128
0.00.040.202 I print_info: n_embd_head_v    = 128
0.00.040.203 I print_info: n_gqa            = 1
0.00.040.204 I print_info: n_embd_k_gqa     = 2048
0.00.040.204 I print_info: n_embd_v_gqa     = 2048
0.00.040.207 I print_info: f_norm_eps       = 1.0e-05
0.00.040.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.208 I print_info: f_logit_scale    = 0.0e+00
0.00.040.209 I print_info: n_ff             = 8192
0.00.040.209 I print_info: n_expert         = 0
0.00.040.209 I print_info: n_expert_used    = 0
0.00.040.209 I print_info: causal attn      = 1
0.00.040.209 I print_info: pooling type     = 0
0.00.040.209 I print_info: rope type        = 2
0.00.040.210 I print_info: rope scaling     = linear
0.00.040.210 I print_info: freq_base_train  = 10000.0
0.00.040.210 I print_info: freq_scale_train = 1
0.00.040.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.210 I print_info: rope_finetuned   = unknown
0.00.040.211 I print_info: ssm_d_conv       = 0
0.00.040.211 I print_info: ssm_d_inner      = 0
0.00.040.211 I print_info: ssm_d_state      = 0
0.00.040.211 I print_info: ssm_dt_rank      = 0
0.00.040.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.211 I print_info: model type       = 1.4B
0.00.040.212 I print_info: model params     = 1.41 B
0.00.040.212 I print_info: general.name     = 1.4B
0.00.040.212 I print_info: vocab type       = BPE
0.00.040.212 I print_info: n_vocab          = 50304
0.00.040.213 I print_info: n_merges         = 50009
0.00.040.213 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: LF token         = 187 'Ċ'
0.00.040.214 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.214 I print_info: max token length = 1024
0.00.040.214 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.371.960 I load_tensors: offloading 24 repeating layers to GPU
0.00.371.970 I load_tensors: offloading output layer to GPU
0.00.371.971 I load_tensors: offloaded 25/25 layers to GPU
0.00.372.006 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.372.007 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.373.800 I llama_init_from_model: n_seq_max     = 1
0.00.373.803 I llama_init_from_model: n_ctx         = 128
0.00.373.804 I llama_init_from_model: n_ctx_per_seq = 128
0.00.373.804 I llama_init_from_model: n_batch       = 128
0.00.373.805 I llama_init_from_model: n_ubatch      = 128
0.00.373.805 I llama_init_from_model: flash_attn    = 0
0.00.373.807 I llama_init_from_model: freq_base     = 10000.0
0.00.373.807 I llama_init_from_model: freq_scale    = 1
0.00.373.808 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.373.814 I ggml_metal_init: allocating
0.00.373.928 I ggml_metal_init: found device: Apple M4
0.00.373.942 I ggml_metal_init: picking default device: Apple M4
0.00.375.877 I ggml_metal_init: using embedded metal library
0.00.381.445 I ggml_metal_init: GPU name:   Apple M4
0.00.381.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.381.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.381.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.381.460 I ggml_metal_init: simdgroup reduction   = true
0.00.381.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.381.460 I ggml_metal_init: has residency sets    = true
0.00.381.461 I ggml_metal_init: has bfloat            = true
0.00.381.461 I ggml_metal_init: use bfloat            = true
0.00.381.463 I ggml_metal_init: hasUnifiedMemory      = true
0.00.381.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.402.460 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.406.061 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.406.069 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.406.104 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.409.339 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.409.340 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.409.341 I llama_init_from_model: graph nodes  = 967
0.00.409.341 I llama_init_from_model: graph splits = 2
0.00.409.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.409.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.095 I 
0.00.437.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.204 I perplexity: tokenizing the input ..
0.00.444.469 I perplexity: tokenization took 7.262 ms
0.00.444.477 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.555 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.578.890 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.578.904 I llama_perf_context_print:        load time =     427.33 ms
0.00.578.905 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.35 tokens per second)
0.00.578.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.578.906 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.579.292 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.082s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.850 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.836 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.585 I llama_model_loader: - type  f32:  194 tensors
0.00.024.585 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.586 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.586 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.586 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.587 I print_info: file format = GGUF V3 (latest)
0.00.024.587 I print_info: file type   = Q3_K - Medium
0.00.024.589 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.053 I load: special tokens cache size = 25
0.00.039.192 I load: token to piece cache size = 0.2984 MB
0.00.039.211 I print_info: arch             = gptneox
0.00.039.212 I print_info: vocab_only       = 0
0.00.039.212 I print_info: n_ctx_train      = 2048
0.00.039.212 I print_info: n_embd           = 2048
0.00.039.212 I print_info: n_layer          = 24
0.00.039.216 I print_info: n_head           = 16
0.00.039.217 I print_info: n_head_kv        = 16
0.00.039.217 I print_info: n_rot            = 32
0.00.039.217 I print_info: n_swa            = 0
0.00.039.217 I print_info: n_embd_head_k    = 128
0.00.039.218 I print_info: n_embd_head_v    = 128
0.00.039.218 I print_info: n_gqa            = 1
0.00.039.219 I print_info: n_embd_k_gqa     = 2048
0.00.039.220 I print_info: n_embd_v_gqa     = 2048
0.00.039.220 I print_info: f_norm_eps       = 1.0e-05
0.00.039.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.222 I print_info: f_logit_scale    = 0.0e+00
0.00.039.223 I print_info: n_ff             = 8192
0.00.039.223 I print_info: n_expert         = 0
0.00.039.223 I print_info: n_expert_used    = 0
0.00.039.224 I print_info: causal attn      = 1
0.00.039.224 I print_info: pooling type     = 0
0.00.039.224 I print_info: rope type        = 2
0.00.039.224 I print_info: rope scaling     = linear
0.00.039.224 I print_info: freq_base_train  = 10000.0
0.00.039.225 I print_info: freq_scale_train = 1
0.00.039.225 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.225 I print_info: rope_finetuned   = unknown
0.00.039.225 I print_info: ssm_d_conv       = 0
0.00.039.225 I print_info: ssm_d_inner      = 0
0.00.039.225 I print_info: ssm_d_state      = 0
0.00.039.225 I print_info: ssm_dt_rank      = 0
0.00.039.225 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.226 I print_info: model type       = 1.4B
0.00.039.226 I print_info: model params     = 1.41 B
0.00.039.226 I print_info: general.name     = 1.4B
0.00.039.227 I print_info: vocab type       = BPE
0.00.039.227 I print_info: n_vocab          = 50304
0.00.039.227 I print_info: n_merges         = 50009
0.00.039.227 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.227 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.227 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.227 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.228 I print_info: LF token         = 187 'Ċ'
0.00.039.228 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.228 I print_info: max token length = 1024
0.00.039.229 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.438.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.438.060 I load_tensors: offloading output layer to GPU
0.00.438.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.438.095 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.438.111 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.439.772 I llama_init_from_model: n_seq_max     = 1
0.00.439.775 I llama_init_from_model: n_ctx         = 128
0.00.439.776 I llama_init_from_model: n_ctx_per_seq = 128
0.00.439.776 I llama_init_from_model: n_batch       = 128
0.00.439.777 I llama_init_from_model: n_ubatch      = 128
0.00.439.777 I llama_init_from_model: flash_attn    = 0
0.00.439.780 I llama_init_from_model: freq_base     = 10000.0
0.00.439.780 I llama_init_from_model: freq_scale    = 1
0.00.439.783 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.439.785 I ggml_metal_init: allocating
0.00.439.871 I ggml_metal_init: found device: Apple M4
0.00.439.885 I ggml_metal_init: picking default device: Apple M4
0.00.441.663 I ggml_metal_init: using embedded metal library
0.00.447.922 I ggml_metal_init: GPU name:   Apple M4
0.00.447.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.933 I ggml_metal_init: simdgroup reduction   = true
0.00.447.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.934 I ggml_metal_init: has residency sets    = true
0.00.447.934 I ggml_metal_init: has bfloat            = true
0.00.447.934 I ggml_metal_init: use bfloat            = true
0.00.447.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.318 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.470.892 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.470.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.470.926 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.320 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.474.322 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.474.323 I llama_init_from_model: graph nodes  = 967
0.00.474.323 I llama_init_from_model: graph splits = 2
0.00.474.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.474.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.301 I 
0.00.503.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.405 I perplexity: tokenizing the input ..
0.00.510.360 I perplexity: tokenization took 6.952 ms
0.00.510.372 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.574 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.648.059 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.648.072 I llama_perf_context_print:        load time =     494.55 ms
0.00.648.073 I llama_perf_context_print: prompt eval time =     135.97 ms /   128 tokens (    1.06 ms per token,   941.36 tokens per second)
0.00.648.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.648.073 I llama_perf_context_print:       total time =     144.78 ms /   129 tokens
0.00.648.427 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.078s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.173 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.229 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.241 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.241 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.241 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.242 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.931 I llama_model_loader: - type  f32:  194 tensors
0.00.025.932 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.932 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.932 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.932 I print_info: file format = GGUF V3 (latest)
0.00.025.937 I print_info: file type   = Q4_K - Medium
0.00.025.939 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.341 I load: special tokens cache size = 25
0.00.040.515 I load: token to piece cache size = 0.2984 MB
0.00.040.528 I print_info: arch             = gptneox
0.00.040.529 I print_info: vocab_only       = 0
0.00.040.530 I print_info: n_ctx_train      = 2048
0.00.040.530 I print_info: n_embd           = 2048
0.00.040.530 I print_info: n_layer          = 24
0.00.040.534 I print_info: n_head           = 16
0.00.040.535 I print_info: n_head_kv        = 16
0.00.040.535 I print_info: n_rot            = 32
0.00.040.535 I print_info: n_swa            = 0
0.00.040.535 I print_info: n_embd_head_k    = 128
0.00.040.535 I print_info: n_embd_head_v    = 128
0.00.040.536 I print_info: n_gqa            = 1
0.00.040.536 I print_info: n_embd_k_gqa     = 2048
0.00.040.538 I print_info: n_embd_v_gqa     = 2048
0.00.040.539 I print_info: f_norm_eps       = 1.0e-05
0.00.040.539 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.539 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.539 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.539 I print_info: f_logit_scale    = 0.0e+00
0.00.040.540 I print_info: n_ff             = 8192
0.00.040.540 I print_info: n_expert         = 0
0.00.040.540 I print_info: n_expert_used    = 0
0.00.040.540 I print_info: causal attn      = 1
0.00.040.541 I print_info: pooling type     = 0
0.00.040.541 I print_info: rope type        = 2
0.00.040.541 I print_info: rope scaling     = linear
0.00.040.541 I print_info: freq_base_train  = 10000.0
0.00.040.542 I print_info: freq_scale_train = 1
0.00.040.542 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.542 I print_info: rope_finetuned   = unknown
0.00.040.542 I print_info: ssm_d_conv       = 0
0.00.040.542 I print_info: ssm_d_inner      = 0
0.00.040.542 I print_info: ssm_d_state      = 0
0.00.040.543 I print_info: ssm_dt_rank      = 0
0.00.040.543 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.543 I print_info: model type       = 1.4B
0.00.040.543 I print_info: model params     = 1.41 B
0.00.040.544 I print_info: general.name     = 1.4B
0.00.040.544 I print_info: vocab type       = BPE
0.00.040.544 I print_info: n_vocab          = 50304
0.00.040.544 I print_info: n_merges         = 50009
0.00.040.545 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.545 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.545 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.545 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.545 I print_info: LF token         = 187 'Ċ'
0.00.040.546 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.546 I print_info: max token length = 1024
0.00.040.546 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.395 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.410 I load_tensors: offloading output layer to GPU
0.00.547.411 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.445 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.451 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.549.106 I llama_init_from_model: n_seq_max     = 1
0.00.549.107 I llama_init_from_model: n_ctx         = 128
0.00.549.108 I llama_init_from_model: n_ctx_per_seq = 128
0.00.549.108 I llama_init_from_model: n_batch       = 128
0.00.549.109 I llama_init_from_model: n_ubatch      = 128
0.00.549.109 I llama_init_from_model: flash_attn    = 0
0.00.549.111 I llama_init_from_model: freq_base     = 10000.0
0.00.549.112 I llama_init_from_model: freq_scale    = 1
0.00.549.112 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.549.114 I ggml_metal_init: allocating
0.00.549.204 I ggml_metal_init: found device: Apple M4
0.00.549.218 I ggml_metal_init: picking default device: Apple M4
0.00.551.013 I ggml_metal_init: using embedded metal library
0.00.557.043 I ggml_metal_init: GPU name:   Apple M4
0.00.557.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.055 I ggml_metal_init: simdgroup reduction   = true
0.00.557.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.056 I ggml_metal_init: has residency sets    = true
0.00.557.056 I ggml_metal_init: has bfloat            = true
0.00.557.056 I ggml_metal_init: use bfloat            = true
0.00.557.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.689 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.580.368 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.580.372 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.580.401 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.583.626 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.583.628 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.583.629 I llama_init_from_model: graph nodes  = 967
0.00.583.629 I llama_init_from_model: graph splits = 2
0.00.583.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.583.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.085 I 
0.00.611.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.207 I perplexity: tokenizing the input ..
0.00.618.295 I perplexity: tokenization took 7.085 ms
0.00.618.303 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.087 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.753.436 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.753.449 I llama_perf_context_print:        load time =     600.90 ms
0.00.753.450 I llama_perf_context_print: prompt eval time =     132.83 ms /   128 tokens (    1.04 ms per token,   963.64 tokens per second)
0.00.753.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.451 I llama_perf_context_print:       total time =     142.37 ms /   129 tokens
0.00.753.864 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.081s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.322 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.335 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.232 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.106 I llama_model_loader: - type  f32:  194 tensors
0.00.025.107 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.107 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.108 I print_info: file format = GGUF V3 (latest)
0.00.025.108 I print_info: file type   = Q5_K - Medium
0.00.025.109 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.246 I load: special tokens cache size = 25
0.00.039.676 I load: token to piece cache size = 0.2984 MB
0.00.039.694 I print_info: arch             = gptneox
0.00.039.695 I print_info: vocab_only       = 0
0.00.039.695 I print_info: n_ctx_train      = 2048
0.00.039.695 I print_info: n_embd           = 2048
0.00.039.695 I print_info: n_layer          = 24
0.00.039.699 I print_info: n_head           = 16
0.00.039.700 I print_info: n_head_kv        = 16
0.00.039.704 I print_info: n_rot            = 32
0.00.039.704 I print_info: n_swa            = 0
0.00.039.705 I print_info: n_embd_head_k    = 128
0.00.039.705 I print_info: n_embd_head_v    = 128
0.00.039.705 I print_info: n_gqa            = 1
0.00.039.706 I print_info: n_embd_k_gqa     = 2048
0.00.039.706 I print_info: n_embd_v_gqa     = 2048
0.00.039.707 I print_info: f_norm_eps       = 1.0e-05
0.00.039.707 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.708 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.708 I print_info: f_logit_scale    = 0.0e+00
0.00.039.708 I print_info: n_ff             = 8192
0.00.039.709 I print_info: n_expert         = 0
0.00.039.709 I print_info: n_expert_used    = 0
0.00.039.709 I print_info: causal attn      = 1
0.00.039.709 I print_info: pooling type     = 0
0.00.039.709 I print_info: rope type        = 2
0.00.039.709 I print_info: rope scaling     = linear
0.00.039.709 I print_info: freq_base_train  = 10000.0
0.00.039.710 I print_info: freq_scale_train = 1
0.00.039.710 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.710 I print_info: rope_finetuned   = unknown
0.00.039.710 I print_info: ssm_d_conv       = 0
0.00.039.710 I print_info: ssm_d_inner      = 0
0.00.039.710 I print_info: ssm_d_state      = 0
0.00.039.710 I print_info: ssm_dt_rank      = 0
0.00.039.711 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.711 I print_info: model type       = 1.4B
0.00.039.711 I print_info: model params     = 1.41 B
0.00.039.711 I print_info: general.name     = 1.4B
0.00.039.712 I print_info: vocab type       = BPE
0.00.039.712 I print_info: n_vocab          = 50304
0.00.039.712 I print_info: n_merges         = 50009
0.00.039.712 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.712 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.713 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.713 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.713 I print_info: LF token         = 187 'Ċ'
0.00.039.713 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.713 I print_info: max token length = 1024
0.00.039.714 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.877 I load_tensors: offloading output layer to GPU
0.00.588.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.905 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.588.908 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.590.478 I llama_init_from_model: n_seq_max     = 1
0.00.590.480 I llama_init_from_model: n_ctx         = 128
0.00.590.481 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.481 I llama_init_from_model: n_batch       = 128
0.00.590.482 I llama_init_from_model: n_ubatch      = 128
0.00.590.482 I llama_init_from_model: flash_attn    = 0
0.00.590.483 I llama_init_from_model: freq_base     = 10000.0
0.00.590.484 I llama_init_from_model: freq_scale    = 1
0.00.590.485 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.486 I ggml_metal_init: allocating
0.00.590.538 I ggml_metal_init: found device: Apple M4
0.00.590.552 I ggml_metal_init: picking default device: Apple M4
0.00.592.072 I ggml_metal_init: using embedded metal library
0.00.598.284 I ggml_metal_init: GPU name:   Apple M4
0.00.598.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.288 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.289 I ggml_metal_init: simdgroup reduction   = true
0.00.598.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.290 I ggml_metal_init: has residency sets    = true
0.00.598.290 I ggml_metal_init: has bfloat            = true
0.00.598.290 I ggml_metal_init: use bfloat            = true
0.00.598.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.160 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.685 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.697 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.895 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.896 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.897 I llama_init_from_model: graph nodes  = 967
0.00.621.897 I llama_init_from_model: graph splits = 2
0.00.621.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.487 I 
0.00.657.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.590 I perplexity: tokenizing the input ..
0.00.665.074 I perplexity: tokenization took 7.481 ms
0.00.665.082 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.581 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.815.919 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.815.935 I llama_perf_context_print:        load time =     648.55 ms
0.00.815.936 I llama_perf_context_print: prompt eval time =     148.63 ms /   128 tokens (    1.16 ms per token,   861.17 tokens per second)
0.00.815.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.940 I llama_perf_context_print:       total time =     158.45 ms /   129 tokens
0.00.816.339 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.080s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.994 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.996 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.778 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.780 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.780 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.781 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.782 I llama_model_loader: - type  f32:  194 tensors
0.00.025.782 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.783 I print_info: file format = GGUF V3 (latest)
0.00.025.783 I print_info: file type   = Q6_K
0.00.025.785 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.182 I load: special tokens cache size = 25
0.00.040.275 I load: token to piece cache size = 0.2984 MB
0.00.040.292 I print_info: arch             = gptneox
0.00.040.293 I print_info: vocab_only       = 0
0.00.040.293 I print_info: n_ctx_train      = 2048
0.00.040.293 I print_info: n_embd           = 2048
0.00.040.293 I print_info: n_layer          = 24
0.00.040.297 I print_info: n_head           = 16
0.00.040.298 I print_info: n_head_kv        = 16
0.00.040.298 I print_info: n_rot            = 32
0.00.040.298 I print_info: n_swa            = 0
0.00.040.298 I print_info: n_embd_head_k    = 128
0.00.040.298 I print_info: n_embd_head_v    = 128
0.00.040.299 I print_info: n_gqa            = 1
0.00.040.299 I print_info: n_embd_k_gqa     = 2048
0.00.040.301 I print_info: n_embd_v_gqa     = 2048
0.00.040.302 I print_info: f_norm_eps       = 1.0e-05
0.00.040.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.303 I print_info: f_logit_scale    = 0.0e+00
0.00.040.304 I print_info: n_ff             = 8192
0.00.040.304 I print_info: n_expert         = 0
0.00.040.304 I print_info: n_expert_used    = 0
0.00.040.304 I print_info: causal attn      = 1
0.00.040.304 I print_info: pooling type     = 0
0.00.040.304 I print_info: rope type        = 2
0.00.040.306 I print_info: rope scaling     = linear
0.00.040.307 I print_info: freq_base_train  = 10000.0
0.00.040.307 I print_info: freq_scale_train = 1
0.00.040.307 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.307 I print_info: rope_finetuned   = unknown
0.00.040.307 I print_info: ssm_d_conv       = 0
0.00.040.308 I print_info: ssm_d_inner      = 0
0.00.040.308 I print_info: ssm_d_state      = 0
0.00.040.309 I print_info: ssm_dt_rank      = 0
0.00.040.309 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.310 I print_info: model type       = 1.4B
0.00.040.310 I print_info: model params     = 1.41 B
0.00.040.310 I print_info: general.name     = 1.4B
0.00.040.311 I print_info: vocab type       = BPE
0.00.040.311 I print_info: n_vocab          = 50304
0.00.040.311 I print_info: n_merges         = 50009
0.00.040.311 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.311 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.311 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: LF token         = 187 'Ċ'
0.00.040.312 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.312 I print_info: max token length = 1024
0.00.040.313 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.530.013 I load_tensors: offloading 24 repeating layers to GPU
0.00.530.020 I load_tensors: offloading output layer to GPU
0.00.530.021 I load_tensors: offloaded 25/25 layers to GPU
0.00.530.049 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.530.052 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.531.634 I llama_init_from_model: n_seq_max     = 1
0.00.531.636 I llama_init_from_model: n_ctx         = 128
0.00.531.637 I llama_init_from_model: n_ctx_per_seq = 128
0.00.531.637 I llama_init_from_model: n_batch       = 128
0.00.531.637 I llama_init_from_model: n_ubatch      = 128
0.00.531.638 I llama_init_from_model: flash_attn    = 0
0.00.531.639 I llama_init_from_model: freq_base     = 10000.0
0.00.531.639 I llama_init_from_model: freq_scale    = 1
0.00.531.640 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.531.641 I ggml_metal_init: allocating
0.00.531.693 I ggml_metal_init: found device: Apple M4
0.00.531.704 I ggml_metal_init: picking default device: Apple M4
0.00.533.086 I ggml_metal_init: using embedded metal library
0.00.538.761 I ggml_metal_init: GPU name:   Apple M4
0.00.538.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.767 I ggml_metal_init: simdgroup reduction   = true
0.00.538.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.767 I ggml_metal_init: has residency sets    = true
0.00.538.768 I ggml_metal_init: has bfloat            = true
0.00.538.768 I ggml_metal_init: use bfloat            = true
0.00.538.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.830 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.558.308 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.558.315 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.558.363 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.561.621 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.561.623 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.561.624 I llama_init_from_model: graph nodes  = 967
0.00.561.624 I llama_init_from_model: graph splits = 2
0.00.561.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.561.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.032 I 
0.00.595.108 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.135 I perplexity: tokenizing the input ..
0.00.602.003 I perplexity: tokenization took 6.866 ms
0.00.602.013 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.697 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.734.211 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.734.225 I llama_perf_context_print:        load time =     585.17 ms
0.00.734.226 I llama_perf_context_print: prompt eval time =     130.45 ms /   128 tokens (    1.02 ms per token,   981.20 tokens per second)
0.00.734.229 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.734.231 I llama_perf_context_print:       total time =     139.20 ms /   129 tokens
0.00.734.553 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.076s
sys	0m0.123s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.326 I build: 4828 (669912d9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.867 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.872 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.877 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.887 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.888 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.888 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.889 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.891 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.891 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.797 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.959 I llama_model_loader: - type  f32:  194 tensors
0.00.047.959 I llama_model_loader: - type  f16:   98 tensors
0.00.047.960 I print_info: file format = GGUF V3 (latest)
0.00.047.961 I print_info: file type   = all F32 (guessed)
0.00.047.962 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.059.562 I load: special tokens cache size = 25
0.00.067.344 I load: token to piece cache size = 0.2984 MB
0.00.067.359 I print_info: arch             = gptneox
0.00.067.360 I print_info: vocab_only       = 0
0.00.067.360 I print_info: n_ctx_train      = 2048
0.00.067.361 I print_info: n_embd           = 2048
0.00.067.361 I print_info: n_layer          = 24
0.00.067.364 I print_info: n_head           = 16
0.00.067.365 I print_info: n_head_kv        = 16
0.00.067.365 I print_info: n_rot            = 32
0.00.067.365 I print_info: n_swa            = 0
0.00.067.367 I print_info: n_embd_head_k    = 128
0.00.067.367 I print_info: n_embd_head_v    = 128
0.00.067.368 I print_info: n_gqa            = 1
0.00.067.369 I print_info: n_embd_k_gqa     = 2048
0.00.067.370 I print_info: n_embd_v_gqa     = 2048
0.00.067.370 I print_info: f_norm_eps       = 1.0e-05
0.00.067.371 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.371 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.371 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.371 I print_info: f_logit_scale    = 0.0e+00
0.00.067.372 I print_info: n_ff             = 8192
0.00.067.372 I print_info: n_expert         = 0
0.00.067.372 I print_info: n_expert_used    = 0
0.00.067.372 I print_info: causal attn      = 1
0.00.067.373 I print_info: pooling type     = 0
0.00.067.373 I print_info: rope type        = 2
0.00.067.373 I print_info: rope scaling     = linear
0.00.067.373 I print_info: freq_base_train  = 10000.0
0.00.067.374 I print_info: freq_scale_train = 1
0.00.067.374 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.374 I print_info: rope_finetuned   = unknown
0.00.067.374 I print_info: ssm_d_conv       = 0
0.00.067.376 I print_info: ssm_d_inner      = 0
0.00.067.376 I print_info: ssm_d_state      = 0
0.00.067.376 I print_info: ssm_dt_rank      = 0
0.00.067.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.377 I print_info: model type       = 1.4B
0.00.067.377 I print_info: model params     = 1.41 B
0.00.067.377 I print_info: general.name     = 1.4B
0.00.067.378 I print_info: vocab type       = BPE
0.00.067.378 I print_info: n_vocab          = 50304
0.00.067.378 I print_info: n_merges         = 50009
0.00.067.379 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.379 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.380 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.382 I print_info: LF token         = 187 'Ċ'
0.00.067.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.382 I print_info: max token length = 1024
0.00.067.383 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.259.946 I load_tensors: offloading 24 repeating layers to GPU
0.01.259.950 I load_tensors: offloading output layer to GPU
0.01.259.951 I load_tensors: offloaded 25/25 layers to GPU
0.01.259.972 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.259.974 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.260.878 I llama_init_from_model: n_seq_max     = 1
0.01.260.880 I llama_init_from_model: n_ctx         = 128
0.01.260.880 I llama_init_from_model: n_ctx_per_seq = 128
0.01.260.880 I llama_init_from_model: n_batch       = 128
0.01.260.880 I llama_init_from_model: n_ubatch      = 128
0.01.260.881 I llama_init_from_model: flash_attn    = 0
0.01.260.881 I llama_init_from_model: freq_base     = 10000.0
0.01.260.882 I llama_init_from_model: freq_scale    = 1
0.01.260.882 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.260.883 I ggml_metal_init: allocating
0.01.260.925 I ggml_metal_init: found device: Apple M4
0.01.260.931 I ggml_metal_init: picking default device: Apple M4
0.01.262.028 I ggml_metal_init: using embedded metal library
0.01.266.001 I ggml_metal_init: GPU name:   Apple M4
0.01.266.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.266.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.266.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.266.005 I ggml_metal_init: simdgroup reduction   = true
0.01.266.005 I ggml_metal_init: simdgroup matrix mul. = true
0.01.266.005 I ggml_metal_init: has residency sets    = true
0.01.266.005 I ggml_metal_init: has bfloat            = true
0.01.266.005 I ggml_metal_init: use bfloat            = true
0.01.266.006 I ggml_metal_init: hasUnifiedMemory      = true
0.01.266.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.277.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.278.774 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.278.776 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.278.791 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.280.508 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.280.510 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.280.511 I llama_init_from_model: graph nodes  = 967
0.01.280.511 I llama_init_from_model: graph splits = 2
0.01.280.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.280.512 I 
0.01.280.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.280.551 I compute_imatrix: tokenizing the input ..
0.01.284.608 I compute_imatrix: tokenization took 4.056 ms
0.01.284.610 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.549.559 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.551.905 I llama_perf_context_print:        load time =    1530.90 ms
0.01.551.906 I llama_perf_context_print: prompt eval time =     263.19 ms /   128 tokens (    2.06 ms per token,   486.34 tokens per second)
0.01.551.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.551.907 I llama_perf_context_print:       total time =    1533.25 ms /   129 tokens
0.01.552.405 I ggml_metal_free: deallocating

real	0m1.748s
user	0m0.122s
sys	0m0.236s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4828 (669912d9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x111707ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1117085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x111708ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x111709150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x111709700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x111709cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11170a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11170a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11170adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11170b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11170b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11170bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11170c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11170cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11170d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11170dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11170e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11170ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11170f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11170fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111710310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111710a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x111711150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1117119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111712110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1117123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1117129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111713650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111713b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x111713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1117142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1117145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111714e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x111715380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111715640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111715ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111715f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1117168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x111716d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111717200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1117176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111717b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x111717fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1117182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1117188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x111718ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1117197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x111719df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11171a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11171aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11171b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11171b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11171bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11171c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11171c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11171cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11171d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11171d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11171de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11171e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11171e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11171ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11171eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11171f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11171f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11171fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111720150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1117205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111720a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111720f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1117213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111721870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x111721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x111722310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x111722860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x111722db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x111723300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x111723850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x111723da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1117242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x111724840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x111724d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1117252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x111725830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x111725d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1117262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x111726820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x111726d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1117272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x111727810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x111727d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1117282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x111728800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x111728d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1117292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1117297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1117194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x111729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11172a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11172a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11172aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11172b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11172b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11172bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11172c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11172c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11172ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11172d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11172d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11172de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11172e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11172e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11172edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11172f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11172f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11172fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111730040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1117304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111730980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111730e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1117312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111731760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1117320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111732540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1117329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x111732e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111733320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1117337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x111733c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111734100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1117345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111734a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x111734ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x111735380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111735820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x111735cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x111736160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111736600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111736aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111736f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1117373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111737880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x111737d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1117381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x111738660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x111738b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x111738fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x111739440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1117398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x111739d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11173a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11173a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11173ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11173b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11173b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11173b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11173bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11173c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11173c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11173cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11173d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11173d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11173d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11173de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11173e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11173e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11173ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11173f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11173f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11173fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11173fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111740340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1117407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111740c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111741120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1117415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111741a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111741f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1117423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111742840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111742ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111743180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111743620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111743ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x111743f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111744400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1117448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111744d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1117451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111745680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111745b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x111746070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1117465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111746b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x111747060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111747320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111747930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x111747f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x111748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x111748d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1117491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1117494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x111749ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11174a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11174a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11174ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11174b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11174b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11174be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11174c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11174c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11174ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11174d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11174d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11174de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11174e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11174e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11174ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11174f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11174f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11174fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111750350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1117508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111750df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111751340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111751890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111751de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111752330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111752880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111752dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111753320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111753870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111753dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111754310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111754860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111754db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111755300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111755850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111755da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1117562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111756840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111756d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1117572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111757830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x111757d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1117582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x111758820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x111758d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1117592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x111759810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x111759d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11175a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11175a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11175ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11175b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11175b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11175bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11175c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11175c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11175cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11175d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11175d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11175dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11175e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11175e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11175ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11175f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11175f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11175fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11175fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111760380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111760820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111760cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111761160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111761600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111761aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111761f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1117623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111762880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111762d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1117631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x111763660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x111763b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x111763fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x111764440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1117648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x111764d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x111765220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1117656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x111765b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1117660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1117667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111766ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111767610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111767d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111767ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1117687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111768aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1117690b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.707.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1076056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1076075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107608120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107608c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1076093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107609c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10760a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10760aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10760b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10760b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10760bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10760c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10760cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10760d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10760dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10760e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10760e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10760e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10760ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10760f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10760f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10760fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10760ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1076106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107610b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1076118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107611d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107612190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107612a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107612ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107613350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1076137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107613c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1076140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107614980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107614df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107615260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1076156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107615b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107616e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1076184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10761a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10761a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10761acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10761b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10761b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10761ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10761be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10761c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10761c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10761cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10761d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10761d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10761d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10761dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10761e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10761e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10761ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10761ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10761f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10761f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10761fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1076209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1076212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1076228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1076231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1076250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1076259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1076262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1076278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1076281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1076297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10762a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10762a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10762a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10762ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10762b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10762b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10762bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10762bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10762c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10762c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10762cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10762d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10762d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10762da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10762dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10762e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10762e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10762ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10762f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10762f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10762f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10762fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1076306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1076325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1076337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1076344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107635e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1076363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1076375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107637a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107638300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107638be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1076394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10763a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10763a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10763aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10763af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10763b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10763b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10763bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10763c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10763c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10763ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10763ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10763d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10763d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10763dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10763e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10763e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10763e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10763ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10763f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10763f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10763fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1076400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1076409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107640e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107641290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1076417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107641cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1076430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107643c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1076441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1076447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107645330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1076458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107645eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107646470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1076475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107647b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107648130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1076486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107649270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107649830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107649df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10764a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10764a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10764af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10764b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10764bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10764c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10764c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10764cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10764d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10764d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10764dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10764e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10764e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10764ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10764f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10764f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10764ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107650570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1076510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1076516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107651c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107652230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1076527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107652db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107653370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107653ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1076544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107654a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107655030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1076555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107655bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107656730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107656cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1076571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1076576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1076580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1076585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107658ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1076594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1076599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107659ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10765a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10765a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10765adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10765b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10765b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10765bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10765c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10765c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10765cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10765d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10765d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10765daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10765dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10765e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10765e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10765f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10765fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107660240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107660960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107660c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107661410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1076616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107661ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1077044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1077056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1077063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1077078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1077083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10770a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10770a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10770b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10770b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10770bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10770c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10770cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10770d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10770db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10770de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10770e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10770e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10770e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10770ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10770f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10770f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10770fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10770ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1077107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1077110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1077119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1077138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1077141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1077157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1077160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1077185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10771a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10771a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10771a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10771adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10771b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10771b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10771bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10771bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10771c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10771c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10771ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10771d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10771d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10771da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10771de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10771e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10771e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10771ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10771f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10771f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10771f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10771fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1077213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107722500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107722a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107722fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107723580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107723b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1077240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107724690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107724c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1077251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1077257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107725d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107726300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1077268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107726e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1077279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107727ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1077283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1077288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107728dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1077292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1077297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107729cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10772a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10772a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10772abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10772b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10772b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10772bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10772bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10772c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10772c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10772cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10772d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10772d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10772ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10772e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10772e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10772ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10772f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10772f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10772fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1077300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1077305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107730ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1077314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1077319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107731ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1077323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1077328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1077332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1077337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107733cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1077341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1077346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107734bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1077350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1077355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107735ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107735fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1077364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1077369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107736ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1077373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1077378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107737dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1077382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1077387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107738cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1077391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1077396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107739bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10773a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10773a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10773aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10773afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10773b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10773b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10773bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10773c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10773c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10773cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10773d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10773d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10773dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10773e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10773e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10773ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10773f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10773f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10773fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10773ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1077404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1077409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107740f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107741520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107741ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107742080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107742ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1077432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107743aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107744200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107744810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107744e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107745610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107745ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107745f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1077463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1077470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107747640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1077480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107748630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107748b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1077490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107749b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10774a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10774a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10774ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10774b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10774b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10774bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10774c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10774c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10774cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10774d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10774d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10774db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10774e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10774e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10774eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10774f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10774f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10774fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107750060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1077505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107750b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107751050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1077515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107751af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107752040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107752590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107752ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107753030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107753580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107753ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107754020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107754570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107754ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107755010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107755560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107755ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107756000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107756550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107756aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107756ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107757540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107757a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107757fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107758530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107758a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107758fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107759520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1077599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107759e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10775a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10775a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10775ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10775b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10775b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10775ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10775bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10775c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10775c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10775cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10775d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10775d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10775da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10775df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10775e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10775e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10775ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10775f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10775f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10775fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10775ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x107760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1077608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107760e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107761530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107761c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107762370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107762a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107762d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107763540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107763800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107763e10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.774s
user	0m0.281s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4828 (669912d9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15260d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15260dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15260e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15260eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15260f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15260f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15260fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1526101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152610780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152611180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152611680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1526121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152612950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152613160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152613880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152613fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1526146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1526155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1526163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152616b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1526173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152617ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1526183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152619cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15261a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15261ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15261b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15261b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15261b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15261bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15261c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15261c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15261cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15261d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15261d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15261d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15261dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15261e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15261e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15261f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15261f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15261fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1526203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1526209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152620ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152621df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152622290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152622730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1526229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152623000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1526237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152623f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1526243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152624890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1526251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152625670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152625b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152625fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152626450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1526268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152626d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152627230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152627780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152627cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152628220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152628770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152628cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152629210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152629760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152629cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15262a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15262a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15262aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15262b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15262b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15262bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15262c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15262c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15262cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15262d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15262d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15262dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15262e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15262e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15262ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15262f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15261ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15262f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15262fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152630870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152630dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152631310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152631860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152631db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152632300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152632850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152632da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1526332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152633d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1526342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152634780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152634c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1526350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152635a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152635ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152636340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1526367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152636c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152637120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1526375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152637a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152637f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1526383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152638ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152639180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152639620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152639ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152639f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15263a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15263a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15263ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15263b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15263b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15263bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15263bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15263c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15263c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15263cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15263d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15263d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15263db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15263e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15263e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15263e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15263ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15263f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15263f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15263fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152640080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152640520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1526409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152640e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152641300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1526417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152641c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1526420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152642580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152642a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152643360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152643800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152643ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152644140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1526445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152644a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152644f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1526453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152645860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152645d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1526461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152646640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152646ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152646f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152647420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1526478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152647d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1526486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152648b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152648fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152649480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152649920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152649dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15264a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15264a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15264aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15264b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15264b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15264ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15264bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15264c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15264ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15264cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15264d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15264d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15264df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15264e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15264eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15264ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15264f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15264fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152650710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152650bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152651050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152651800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152651d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1526522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1526527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152652d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152653290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1526537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152653d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152654280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1526547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152654d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152655270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1526557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152655d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152656260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1526567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152656d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152657250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1526577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152657cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152658240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152658790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152658ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152659230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152659780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152659cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15265a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15265a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15265acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15265b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15265b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15265bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15265c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15265c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15265cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15265d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15265d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15265dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15265e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15265e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15265ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15265f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15265f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15265fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1526601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152660710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152660c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1526611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152661700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152661c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1526621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1526626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152662c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152663190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1526636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152663c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152664180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152664620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152664ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152664f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152665400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1526658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152665d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1526661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152666680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152666b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152666fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152667460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152667900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152668240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1526686e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x152668b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x152669020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1526694c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x152669960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152669e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15266a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15266a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15266abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15266b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15266b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15266ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15266c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15266c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15266cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15266d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15266d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15266e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15266e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15266ea70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.690 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1538053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1538069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1538072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1538090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15380a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15380a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15380ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15380b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15380bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15380c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15380cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15380d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15380d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15380e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15380e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15380e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15380eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15380ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15380f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15380f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15380fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1538101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1538111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1538123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1538130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1538139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1538142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1538158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1538161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1538170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1538186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15381a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15381a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15381aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15381aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15381b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15381b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15381bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15381c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15381c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15381c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15381cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15381d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15381d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15381db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15381df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15381e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15381e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15381ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15381f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15381f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15381fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15381fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1538214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1538233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1538245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1538252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1538264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1538283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1538299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15382a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15382a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15382abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15382b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15382b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15382b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15382bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15382c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15382c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15382cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15382cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15382d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15382d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15382dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15382e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15382e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15382e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15382ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15382f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15382f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15382fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1538308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1538311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1538327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1538330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1538339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1538377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1538380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1538396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15383a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15383a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15383ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15383b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15383b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15383ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15383bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15383c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15383c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15383cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15383d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15383d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15383d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15383ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15383e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15383e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15383eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15383ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15383f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15383f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15383fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1538402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1538425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1538439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1538450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1538467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1538495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15384a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15384a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15384acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15384b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15384b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15384be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15384c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15384c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15384cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15384d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15384dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15384e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15384e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15384ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15384f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15384f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15384fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1538508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1538536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1538564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15385a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15385a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15385ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15385b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15385b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15385ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15385bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15385c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15385c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15385ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15385d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15385d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15385dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15385e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15385e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15385f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15385f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15385ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153860700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1538609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1538611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153861470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153861a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15264d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15264f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15266e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15264cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15264dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152620ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152620690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152622cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15264f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15261eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15261f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15261e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1526212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152620080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152617050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15262f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15266dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15261a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15261a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15264fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15264e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152618660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152618920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152618be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15266eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15266f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15266f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15266f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15266f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15266fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15266ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152670210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1526704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152670790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152670a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152670d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152670fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152671290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152671550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152671810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152671ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152671d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152672050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152672310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1526725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152672890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152672b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152672e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1526730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152673390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152673650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152673910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152673bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152673e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152674150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152674410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1526746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152674990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152674c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152674f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1526751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152675490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152675750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152675a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152675cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152708230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1527086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152708b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152708f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1527093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152709860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152709cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15270a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15270a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15270aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15270ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15270b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15270b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15270bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15270c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15270c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15270c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15270cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15270d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15270d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15270daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15270df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15270e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15270e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15270ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15270f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15270f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15270fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15270fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1527102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152710750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152710bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152711450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152711b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152711ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152712460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1527128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152712d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1527131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152713620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152713a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152713f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152714370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1527147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152714c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1527150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152715530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1527159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152715e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152716280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1527166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152716b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152716fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152717440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1527178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152717d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152718190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152718600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152718a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152718ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152719350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1527197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152719c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15271a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15271a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15271a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15271adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15271b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15271b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15271bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15271bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15271c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15271c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15271cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15271d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15271d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15271da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15271dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15271e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15271e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15271ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15271f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15271f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15271f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15271fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152720240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1527206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152720b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152720f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152721400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152721870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152721ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152722150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1527225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152722a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152722ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152723310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152723780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152723bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152724060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1527244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152724940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152724db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152725220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152725690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152725b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152725f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1527263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152726850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152726cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152727130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1527275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152727a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152727e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1527282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152728760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152728bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152729040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1527294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152729920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152729d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15272a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15272a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15272aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15272af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15272b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15272b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15272bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15272c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15272c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15272c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15272ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15272d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15272d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15272dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15272e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15272e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15272e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15272ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15272f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15272f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15272fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152730040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152730b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152730e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152731110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152731580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1527319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152731e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1527322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152732740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152732bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152733020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152733490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152733900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152733d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1527341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152734650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152734ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152734f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1527353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152735810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152735c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1527360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152736560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1527369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152736e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1527372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152737720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152738000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152738470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1527388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152738d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1527391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152739630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152739aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152739f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15273a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15273a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15273ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15273b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15273b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15273b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15273be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15273c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15273c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15273cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15273cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15273d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15273d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15273dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15273e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15273e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15273ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15273eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15273f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15273f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15273fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1527400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152740520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152740990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152740e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152741270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1527416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152741b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152741fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152742430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1527428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152742d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152743180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1527435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152743a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152743ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152744340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1527447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x152744c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x152745090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x152745500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152745970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x152745de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x152746250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1527466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x152746b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x152746fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152747ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1527485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152748d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152749420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1527496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1527499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152749e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15274a280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.232s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
