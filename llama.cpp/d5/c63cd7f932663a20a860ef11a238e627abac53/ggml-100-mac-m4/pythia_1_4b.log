Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.9s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.168s
user	0m1.046s
sys	0m1.499s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-chat
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-chat
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-tokenizer-1-spm
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Built target test-log
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Built target test-quantize-fns
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target test-rope
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-gritlm
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-gguf-split
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-bench
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-stats
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-save-load-state
[ 83%] Built target llama-cli
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Built target llama-quantize
[ 85%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Built target llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-tts
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-speculative-simple
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-gen-docs
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.148s
user	0m6.471s
sys	0m9.878s

main: quantize time =  5371.48 ms
main:    total time =  5371.48 ms

main: quantize time =  1872.27 ms
main:    total time =  1872.27 ms

main: quantize time =  1416.86 ms
main:    total time =  1416.86 ms

main: quantize time =  2129.75 ms
main:    total time =  2129.75 ms

main: quantize time =  2944.24 ms
main:    total time =  2944.24 ms

main: quantize time =  5456.46 ms
main:    total time =  5456.46 ms

main: quantize time =  6123.95 ms
main:    total time =  6123.95 ms

main: quantize time =  6884.41 ms
main:    total time =  6884.41 ms

main: quantize time =  6454.06 ms
main:    total time =  6454.06 ms

main: quantize time =  4555.60 ms
main:    total time =  4555.60 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.139 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.314 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.093.973 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.106.267 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.106.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.106.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.106.285 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.106.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.106.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.106.286 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.106.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.106.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.106.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.106.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.106.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.106.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.106.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.106.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.106.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.106.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.113.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.115.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.121.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.121.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.122.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.122.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.122.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.122.002 I llama_model_loader: - type  f32:  194 tensors
0.00.122.003 I llama_model_loader: - type  f16:   98 tensors
0.00.122.005 I print_info: file format = GGUF V3 (latest)
0.00.122.006 I print_info: file type   = all F32 (guessed)
0.00.122.010 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.140.975 I load: special tokens cache size = 25
0.00.151.242 I load: token to piece cache size = 0.2984 MB
0.00.151.247 I print_info: arch             = gptneox
0.00.151.248 I print_info: vocab_only       = 0
0.00.151.248 I print_info: n_ctx_train      = 2048
0.00.151.248 I print_info: n_embd           = 2048
0.00.151.249 I print_info: n_layer          = 24
0.00.151.255 I print_info: n_head           = 16
0.00.151.256 I print_info: n_head_kv        = 16
0.00.151.256 I print_info: n_rot            = 32
0.00.151.257 I print_info: n_swa            = 0
0.00.151.257 I print_info: n_embd_head_k    = 128
0.00.151.257 I print_info: n_embd_head_v    = 128
0.00.151.258 I print_info: n_gqa            = 1
0.00.151.259 I print_info: n_embd_k_gqa     = 2048
0.00.151.260 I print_info: n_embd_v_gqa     = 2048
0.00.151.261 I print_info: f_norm_eps       = 1.0e-05
0.00.151.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.151.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.151.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.151.262 I print_info: f_logit_scale    = 0.0e+00
0.00.151.263 I print_info: n_ff             = 8192
0.00.151.263 I print_info: n_expert         = 0
0.00.151.263 I print_info: n_expert_used    = 0
0.00.151.264 I print_info: causal attn      = 1
0.00.151.264 I print_info: pooling type     = 0
0.00.151.267 I print_info: rope type        = 2
0.00.151.267 I print_info: rope scaling     = linear
0.00.151.268 I print_info: freq_base_train  = 10000.0
0.00.151.268 I print_info: freq_scale_train = 1
0.00.151.269 I print_info: n_ctx_orig_yarn  = 2048
0.00.151.269 I print_info: rope_finetuned   = unknown
0.00.151.269 I print_info: ssm_d_conv       = 0
0.00.151.269 I print_info: ssm_d_inner      = 0
0.00.151.269 I print_info: ssm_d_state      = 0
0.00.151.270 I print_info: ssm_dt_rank      = 0
0.00.151.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.151.270 I print_info: model type       = 1.4B
0.00.151.271 I print_info: model params     = 1.41 B
0.00.151.271 I print_info: general.name     = 1.4B
0.00.151.277 I print_info: vocab type       = BPE
0.00.151.277 I print_info: n_vocab          = 50304
0.00.151.277 I print_info: n_merges         = 50009
0.00.151.278 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.151.278 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.151.278 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.151.278 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.151.279 I print_info: LF token         = 187 'Ċ'
0.00.151.281 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.151.282 I print_info: max token length = 1024
0.00.151.282 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.222.113 I load_tensors: offloading 24 repeating layers to GPU
0.00.222.116 I load_tensors: offloading output layer to GPU
0.00.222.116 I load_tensors: offloaded 25/25 layers to GPU
0.00.222.144 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.222.146 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.222.827 I llama_init_from_model: n_seq_max     = 1
0.00.222.828 I llama_init_from_model: n_ctx         = 2048
0.00.222.828 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.222.829 I llama_init_from_model: n_batch       = 2048
0.00.222.829 I llama_init_from_model: n_ubatch      = 512
0.00.222.829 I llama_init_from_model: flash_attn    = 0
0.00.222.830 I llama_init_from_model: freq_base     = 10000.0
0.00.222.830 I llama_init_from_model: freq_scale    = 1
0.00.222.831 I ggml_metal_init: allocating
0.00.222.868 I ggml_metal_init: found device: Apple M4
0.00.222.873 I ggml_metal_init: picking default device: Apple M4
0.00.223.574 I ggml_metal_init: using embedded metal library
0.00.243.351 I ggml_metal_init: GPU name:   Apple M4
0.00.243.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.243.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.243.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.243.354 I ggml_metal_init: simdgroup reduction   = true
0.00.243.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.243.354 I ggml_metal_init: has residency sets    = true
0.00.243.354 I ggml_metal_init: has bfloat            = true
0.00.243.354 I ggml_metal_init: use bfloat            = true
0.00.243.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.243.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.307.399 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.339.714 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.339.719 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.339.765 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.343.364 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.343.366 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.343.366 I llama_init_from_model: graph nodes  = 967
0.00.343.366 I llama_init_from_model: graph splits = 2
0.00.343.373 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.343.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.343.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.409.108 I main: llama threadpool init, n_threads = 4
0.00.409.151 I 
0.00.409.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.409.183 I 
0.00.409.336 I sampler seed: 1234
0.00.409.340 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.409.364 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.409.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.409.365 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.242.205 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.02.242.206 I llama_perf_context_print:        load time =     314.21 ms
0.02.242.207 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.76 tokens per second)
0.02.242.207 I llama_perf_context_print:        eval time =    1786.42 ms /    63 runs   (   28.36 ms per token,    35.27 tokens per second)
0.02.242.207 I llama_perf_context_print:       total time =    1834.01 ms /    70 tokens
0.02.242.418 I ggml_metal_free: deallocating

real	0m2.559s
user	0m0.136s
sys	0m0.166s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.107 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.673 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.674 I llama_model_loader: - type  f32:  194 tensors
0.00.035.674 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.675 I print_info: file format = GGUF V3 (latest)
0.00.035.675 I print_info: file type   = Q8_0
0.00.035.677 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.865 I load: special tokens cache size = 25
0.00.051.337 I load: token to piece cache size = 0.2984 MB
0.00.051.342 I print_info: arch             = gptneox
0.00.051.342 I print_info: vocab_only       = 0
0.00.051.342 I print_info: n_ctx_train      = 2048
0.00.051.343 I print_info: n_embd           = 2048
0.00.051.343 I print_info: n_layer          = 24
0.00.051.348 I print_info: n_head           = 16
0.00.051.349 I print_info: n_head_kv        = 16
0.00.051.349 I print_info: n_rot            = 32
0.00.051.349 I print_info: n_swa            = 0
0.00.051.350 I print_info: n_embd_head_k    = 128
0.00.051.350 I print_info: n_embd_head_v    = 128
0.00.051.351 I print_info: n_gqa            = 1
0.00.051.351 I print_info: n_embd_k_gqa     = 2048
0.00.051.352 I print_info: n_embd_v_gqa     = 2048
0.00.051.353 I print_info: f_norm_eps       = 1.0e-05
0.00.051.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.355 I print_info: f_logit_scale    = 0.0e+00
0.00.051.356 I print_info: n_ff             = 8192
0.00.051.356 I print_info: n_expert         = 0
0.00.051.356 I print_info: n_expert_used    = 0
0.00.051.356 I print_info: causal attn      = 1
0.00.051.356 I print_info: pooling type     = 0
0.00.051.356 I print_info: rope type        = 2
0.00.051.356 I print_info: rope scaling     = linear
0.00.051.357 I print_info: freq_base_train  = 10000.0
0.00.051.357 I print_info: freq_scale_train = 1
0.00.051.357 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.357 I print_info: rope_finetuned   = unknown
0.00.051.357 I print_info: ssm_d_conv       = 0
0.00.051.358 I print_info: ssm_d_inner      = 0
0.00.051.358 I print_info: ssm_d_state      = 0
0.00.051.358 I print_info: ssm_dt_rank      = 0
0.00.051.358 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.358 I print_info: model type       = 1.4B
0.00.051.359 I print_info: model params     = 1.41 B
0.00.051.360 I print_info: general.name     = 1.4B
0.00.051.360 I print_info: vocab type       = BPE
0.00.051.362 I print_info: n_vocab          = 50304
0.00.051.362 I print_info: n_merges         = 50009
0.00.051.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.363 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.363 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.364 I print_info: LF token         = 187 'Ċ'
0.00.051.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.364 I print_info: max token length = 1024
0.00.051.365 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.052.865 I load_tensors: offloading 24 repeating layers to GPU
0.01.052.870 I load_tensors: offloading output layer to GPU
0.01.052.871 I load_tensors: offloaded 25/25 layers to GPU
0.01.052.889 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.052.892 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.053.670 I llama_init_from_model: n_seq_max     = 1
0.01.053.676 I llama_init_from_model: n_ctx         = 2048
0.01.053.676 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.053.676 I llama_init_from_model: n_batch       = 2048
0.01.053.677 I llama_init_from_model: n_ubatch      = 512
0.01.053.677 I llama_init_from_model: flash_attn    = 0
0.01.053.678 I llama_init_from_model: freq_base     = 10000.0
0.01.053.679 I llama_init_from_model: freq_scale    = 1
0.01.053.680 I ggml_metal_init: allocating
0.01.053.718 I ggml_metal_init: found device: Apple M4
0.01.053.728 I ggml_metal_init: picking default device: Apple M4
0.01.054.773 I ggml_metal_init: using embedded metal library
0.01.059.134 I ggml_metal_init: GPU name:   Apple M4
0.01.059.144 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.059.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.059.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.059.145 I ggml_metal_init: simdgroup reduction   = true
0.01.059.146 I ggml_metal_init: simdgroup matrix mul. = true
0.01.059.146 I ggml_metal_init: has residency sets    = true
0.01.059.146 I ggml_metal_init: has bfloat            = true
0.01.059.146 I ggml_metal_init: use bfloat            = true
0.01.059.148 I ggml_metal_init: hasUnifiedMemory      = true
0.01.059.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.072.691 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.105.594 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.105.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.105.634 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.110.020 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.110.022 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.110.022 I llama_init_from_model: graph nodes  = 967
0.01.110.022 I llama_init_from_model: graph splits = 2
0.01.110.027 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.110.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.110.157 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.165.853 I main: llama threadpool init, n_threads = 4
0.01.165.890 I 
0.01.165.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.165.908 I 
0.01.166.014 I sampler seed: 1234
0.01.166.019 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.166.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.166.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.166.031 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.271.770 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47301.80 tokens per second)
0.02.271.771 I llama_perf_context_print:        load time =    1155.02 ms
0.02.271.774 I llama_perf_context_print: prompt eval time =      46.91 ms /     7 tokens (    6.70 ms per token,   149.24 tokens per second)
0.02.271.775 I llama_perf_context_print:        eval time =    1056.02 ms /    63 runs   (   16.76 ms per token,    59.66 tokens per second)
0.02.271.776 I llama_perf_context_print:       total time =    1106.64 ms /    70 tokens
0.02.272.043 I ggml_metal_free: deallocating

real	0m2.292s
user	0m0.106s
sys	0m0.235s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.220 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.221 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.222 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.224 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.227 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.228 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.272 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.273 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.274 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.275 I llama_model_loader: - type  f32:  194 tensors
0.00.037.275 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.276 I print_info: file format = GGUF V3 (latest)
0.00.037.277 I print_info: file type   = Q4_0
0.00.037.277 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.217 I load: special tokens cache size = 25
0.00.053.212 I load: token to piece cache size = 0.2984 MB
0.00.053.216 I print_info: arch             = gptneox
0.00.053.216 I print_info: vocab_only       = 0
0.00.053.217 I print_info: n_ctx_train      = 2048
0.00.053.217 I print_info: n_embd           = 2048
0.00.053.217 I print_info: n_layer          = 24
0.00.053.222 I print_info: n_head           = 16
0.00.053.223 I print_info: n_head_kv        = 16
0.00.053.223 I print_info: n_rot            = 32
0.00.053.223 I print_info: n_swa            = 0
0.00.053.223 I print_info: n_embd_head_k    = 128
0.00.053.223 I print_info: n_embd_head_v    = 128
0.00.053.224 I print_info: n_gqa            = 1
0.00.053.225 I print_info: n_embd_k_gqa     = 2048
0.00.053.226 I print_info: n_embd_v_gqa     = 2048
0.00.053.226 I print_info: f_norm_eps       = 1.0e-05
0.00.053.227 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.227 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.227 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.227 I print_info: f_logit_scale    = 0.0e+00
0.00.053.228 I print_info: n_ff             = 8192
0.00.053.228 I print_info: n_expert         = 0
0.00.053.228 I print_info: n_expert_used    = 0
0.00.053.228 I print_info: causal attn      = 1
0.00.053.232 I print_info: pooling type     = 0
0.00.053.232 I print_info: rope type        = 2
0.00.053.232 I print_info: rope scaling     = linear
0.00.053.233 I print_info: freq_base_train  = 10000.0
0.00.053.233 I print_info: freq_scale_train = 1
0.00.053.233 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.233 I print_info: rope_finetuned   = unknown
0.00.053.234 I print_info: ssm_d_conv       = 0
0.00.053.234 I print_info: ssm_d_inner      = 0
0.00.053.234 I print_info: ssm_d_state      = 0
0.00.053.234 I print_info: ssm_dt_rank      = 0
0.00.053.234 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.234 I print_info: model type       = 1.4B
0.00.053.239 I print_info: model params     = 1.41 B
0.00.053.240 I print_info: general.name     = 1.4B
0.00.053.241 I print_info: vocab type       = BPE
0.00.053.241 I print_info: n_vocab          = 50304
0.00.053.241 I print_info: n_merges         = 50009
0.00.053.241 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.242 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.242 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.242 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.242 I print_info: LF token         = 187 'Ċ'
0.00.053.242 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.243 I print_info: max token length = 1024
0.00.053.243 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.871 I load_tensors: offloading output layer to GPU
0.00.639.872 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.904 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.639.905 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.641.103 I llama_init_from_model: n_seq_max     = 1
0.00.641.108 I llama_init_from_model: n_ctx         = 2048
0.00.641.108 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.109 I llama_init_from_model: n_batch       = 2048
0.00.641.109 I llama_init_from_model: n_ubatch      = 512
0.00.641.110 I llama_init_from_model: flash_attn    = 0
0.00.641.112 I llama_init_from_model: freq_base     = 10000.0
0.00.641.112 I llama_init_from_model: freq_scale    = 1
0.00.641.114 I ggml_metal_init: allocating
0.00.641.184 I ggml_metal_init: found device: Apple M4
0.00.641.198 I ggml_metal_init: picking default device: Apple M4
0.00.643.035 I ggml_metal_init: using embedded metal library
0.00.649.375 I ggml_metal_init: GPU name:   Apple M4
0.00.649.383 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.384 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.385 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.386 I ggml_metal_init: simdgroup reduction   = true
0.00.649.386 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.386 I ggml_metal_init: has residency sets    = true
0.00.649.387 I ggml_metal_init: has bfloat            = true
0.00.649.387 I ggml_metal_init: use bfloat            = true
0.00.649.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.082 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.670 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.677 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.727 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.291 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.293 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.293 I llama_init_from_model: graph nodes  = 967
0.00.734.293 I llama_init_from_model: graph splits = 2
0.00.734.299 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.427 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.888 I main: llama threadpool init, n_threads = 4
0.00.790.925 I 
0.00.790.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.946 I 
0.00.791.098 I sampler seed: 1234
0.00.791.103 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.791.136 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.791.139 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.791.139 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.487.034 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.487.035 I llama_perf_context_print:        load time =     779.25 ms
0.01.487.035 I llama_perf_context_print: prompt eval time =      49.37 ms /     7 tokens (    7.05 ms per token,   141.79 tokens per second)
0.01.487.037 I llama_perf_context_print:        eval time =     643.64 ms /    63 runs   (   10.22 ms per token,    97.88 tokens per second)
0.01.487.037 I llama_perf_context_print:       total time =     696.91 ms /    70 tokens
0.01.487.312 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.113s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.898 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.173 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.030.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.260 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.429 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.430 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.431 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.431 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.431 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.432 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.432 I llama_model_loader: - type  f32:  194 tensors
0.00.039.432 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.433 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.433 I print_info: file format = GGUF V3 (latest)
0.00.039.434 I print_info: file type   = Q4_1
0.00.039.434 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.477 I load: special tokens cache size = 25
0.00.056.327 I load: token to piece cache size = 0.2984 MB
0.00.056.330 I print_info: arch             = gptneox
0.00.056.330 I print_info: vocab_only       = 0
0.00.056.331 I print_info: n_ctx_train      = 2048
0.00.056.331 I print_info: n_embd           = 2048
0.00.056.331 I print_info: n_layer          = 24
0.00.056.333 I print_info: n_head           = 16
0.00.056.334 I print_info: n_head_kv        = 16
0.00.056.334 I print_info: n_rot            = 32
0.00.056.334 I print_info: n_swa            = 0
0.00.056.335 I print_info: n_embd_head_k    = 128
0.00.056.335 I print_info: n_embd_head_v    = 128
0.00.056.336 I print_info: n_gqa            = 1
0.00.056.336 I print_info: n_embd_k_gqa     = 2048
0.00.056.337 I print_info: n_embd_v_gqa     = 2048
0.00.056.338 I print_info: f_norm_eps       = 1.0e-05
0.00.056.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.340 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.340 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.340 I print_info: f_logit_scale    = 0.0e+00
0.00.056.341 I print_info: n_ff             = 8192
0.00.056.350 I print_info: n_expert         = 0
0.00.056.352 I print_info: n_expert_used    = 0
0.00.056.353 I print_info: causal attn      = 1
0.00.056.353 I print_info: pooling type     = 0
0.00.056.353 I print_info: rope type        = 2
0.00.056.354 I print_info: rope scaling     = linear
0.00.056.355 I print_info: freq_base_train  = 10000.0
0.00.056.355 I print_info: freq_scale_train = 1
0.00.056.355 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.357 I print_info: rope_finetuned   = unknown
0.00.056.357 I print_info: ssm_d_conv       = 0
0.00.056.357 I print_info: ssm_d_inner      = 0
0.00.056.357 I print_info: ssm_d_state      = 0
0.00.056.357 I print_info: ssm_dt_rank      = 0
0.00.056.358 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.358 I print_info: model type       = 1.4B
0.00.056.359 I print_info: model params     = 1.41 B
0.00.056.359 I print_info: general.name     = 1.4B
0.00.056.359 I print_info: vocab type       = BPE
0.00.056.359 I print_info: n_vocab          = 50304
0.00.056.360 I print_info: n_merges         = 50009
0.00.056.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.360 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.361 I print_info: LF token         = 187 'Ċ'
0.00.056.363 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.363 I print_info: max token length = 1024
0.00.056.364 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.412 I load_tensors: offloading output layer to GPU
0.00.608.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.446 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.608.462 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.609.878 I llama_init_from_model: n_seq_max     = 1
0.00.609.881 I llama_init_from_model: n_ctx         = 2048
0.00.609.882 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.609.882 I llama_init_from_model: n_batch       = 2048
0.00.609.883 I llama_init_from_model: n_ubatch      = 512
0.00.609.883 I llama_init_from_model: flash_attn    = 0
0.00.609.886 I llama_init_from_model: freq_base     = 10000.0
0.00.609.886 I llama_init_from_model: freq_scale    = 1
0.00.609.889 I ggml_metal_init: allocating
0.00.609.963 I ggml_metal_init: found device: Apple M4
0.00.609.976 I ggml_metal_init: picking default device: Apple M4
0.00.611.865 I ggml_metal_init: using embedded metal library
0.00.618.717 I ggml_metal_init: GPU name:   Apple M4
0.00.618.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.724 I ggml_metal_init: simdgroup reduction   = true
0.00.618.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.725 I ggml_metal_init: has residency sets    = true
0.00.618.725 I ggml_metal_init: has bfloat            = true
0.00.618.725 I ggml_metal_init: use bfloat            = true
0.00.618.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.891 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.913 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.921 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.969 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.055 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.697.057 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.697.057 I llama_init_from_model: graph nodes  = 967
0.00.697.057 I llama_init_from_model: graph splits = 2
0.00.697.069 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.184 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.438 I main: llama threadpool init, n_threads = 4
0.00.753.483 I 
0.00.753.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.505 I 
0.00.753.661 I sampler seed: 1234
0.00.753.666 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.677 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.679 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.679 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.488.349 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.488.349 I llama_perf_context_print:        load time =     739.78 ms
0.01.488.350 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.34 tokens per second)
0.01.488.351 I llama_perf_context_print:        eval time =     683.05 ms /    63 runs   (   10.84 ms per token,    92.23 tokens per second)
0.01.488.352 I llama_perf_context_print:       total time =     735.67 ms /    70 tokens
0.01.488.648 I ggml_metal_free: deallocating

real	0m1.505s
user	0m0.114s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.012.686 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.095 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.096 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.097 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.097 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.098 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.099 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.099 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.100 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.102 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.152 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.155 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.155 I llama_model_loader: - type  f32:  194 tensors
0.00.031.155 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.156 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.156 I print_info: file format = GGUF V3 (latest)
0.00.031.157 I print_info: file type   = Q5_0
0.00.031.157 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.041.442 I load: special tokens cache size = 25
0.00.049.201 I load: token to piece cache size = 0.2984 MB
0.00.049.204 I print_info: arch             = gptneox
0.00.049.204 I print_info: vocab_only       = 0
0.00.049.205 I print_info: n_ctx_train      = 2048
0.00.049.205 I print_info: n_embd           = 2048
0.00.049.205 I print_info: n_layer          = 24
0.00.049.207 I print_info: n_head           = 16
0.00.049.209 I print_info: n_head_kv        = 16
0.00.049.209 I print_info: n_rot            = 32
0.00.049.210 I print_info: n_swa            = 0
0.00.049.210 I print_info: n_embd_head_k    = 128
0.00.049.210 I print_info: n_embd_head_v    = 128
0.00.049.211 I print_info: n_gqa            = 1
0.00.049.214 I print_info: n_embd_k_gqa     = 2048
0.00.049.214 I print_info: n_embd_v_gqa     = 2048
0.00.049.215 I print_info: f_norm_eps       = 1.0e-05
0.00.049.215 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.215 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.216 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.216 I print_info: f_logit_scale    = 0.0e+00
0.00.049.221 I print_info: n_ff             = 8192
0.00.049.221 I print_info: n_expert         = 0
0.00.049.222 I print_info: n_expert_used    = 0
0.00.049.222 I print_info: causal attn      = 1
0.00.049.223 I print_info: pooling type     = 0
0.00.049.225 I print_info: rope type        = 2
0.00.049.225 I print_info: rope scaling     = linear
0.00.049.226 I print_info: freq_base_train  = 10000.0
0.00.049.226 I print_info: freq_scale_train = 1
0.00.049.226 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.227 I print_info: rope_finetuned   = unknown
0.00.049.227 I print_info: ssm_d_conv       = 0
0.00.049.227 I print_info: ssm_d_inner      = 0
0.00.049.227 I print_info: ssm_d_state      = 0
0.00.049.227 I print_info: ssm_dt_rank      = 0
0.00.049.229 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.230 I print_info: model type       = 1.4B
0.00.049.230 I print_info: model params     = 1.41 B
0.00.049.230 I print_info: general.name     = 1.4B
0.00.049.231 I print_info: vocab type       = BPE
0.00.049.231 I print_info: n_vocab          = 50304
0.00.049.231 I print_info: n_merges         = 50009
0.00.049.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.232 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.232 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.232 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.232 I print_info: LF token         = 187 'Ċ'
0.00.049.233 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.233 I print_info: max token length = 1024
0.00.049.234 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.497 I load_tensors: offloading output layer to GPU
0.00.664.498 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.532 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.664.534 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.665.857 I llama_init_from_model: n_seq_max     = 1
0.00.665.860 I llama_init_from_model: n_ctx         = 2048
0.00.665.860 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.665.861 I llama_init_from_model: n_batch       = 2048
0.00.665.861 I llama_init_from_model: n_ubatch      = 512
0.00.665.862 I llama_init_from_model: flash_attn    = 0
0.00.665.864 I llama_init_from_model: freq_base     = 10000.0
0.00.665.864 I llama_init_from_model: freq_scale    = 1
0.00.665.866 I ggml_metal_init: allocating
0.00.665.944 I ggml_metal_init: found device: Apple M4
0.00.665.957 I ggml_metal_init: picking default device: Apple M4
0.00.667.848 I ggml_metal_init: using embedded metal library
0.00.674.546 I ggml_metal_init: GPU name:   Apple M4
0.00.674.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.553 I ggml_metal_init: simdgroup reduction   = true
0.00.674.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.553 I ggml_metal_init: has residency sets    = true
0.00.674.554 I ggml_metal_init: has bfloat            = true
0.00.674.554 I ggml_metal_init: use bfloat            = true
0.00.674.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.074 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.914 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.751.920 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.955 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.601 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.603 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.604 I llama_init_from_model: graph nodes  = 967
0.00.756.604 I llama_init_from_model: graph splits = 2
0.00.756.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.741 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.231 I main: llama threadpool init, n_threads = 4
0.00.817.277 I 
0.00.817.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.299 I 
0.00.817.447 I sampler seed: 1234
0.00.817.452 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.817.462 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.817.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.817.463 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.610.697 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.610.697 I llama_perf_context_print:        load time =     803.82 ms
0.01.610.699 I llama_perf_context_print: prompt eval time =      52.95 ms /     7 tokens (    7.56 ms per token,   132.19 tokens per second)
0.01.610.700 I llama_perf_context_print:        eval time =     737.27 ms /    63 runs   (   11.70 ms per token,    85.45 tokens per second)
0.01.610.700 I llama_perf_context_print:       total time =     794.18 ms /    70 tokens
0.01.610.959 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.117s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.989 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.936 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.028.941 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.949 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.952 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.037.875 I llama_model_loader: - type  f32:  194 tensors
0.00.037.876 I llama_model_loader: - type q5_1:   97 tensors
0.00.037.876 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.876 I print_info: file format = GGUF V3 (latest)
0.00.037.877 I print_info: file type   = Q5_1
0.00.037.877 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.684 I load: special tokens cache size = 25
0.00.053.729 I load: token to piece cache size = 0.2984 MB
0.00.053.732 I print_info: arch             = gptneox
0.00.053.733 I print_info: vocab_only       = 0
0.00.053.733 I print_info: n_ctx_train      = 2048
0.00.053.733 I print_info: n_embd           = 2048
0.00.053.733 I print_info: n_layer          = 24
0.00.053.736 I print_info: n_head           = 16
0.00.053.737 I print_info: n_head_kv        = 16
0.00.053.737 I print_info: n_rot            = 32
0.00.053.737 I print_info: n_swa            = 0
0.00.053.737 I print_info: n_embd_head_k    = 128
0.00.053.738 I print_info: n_embd_head_v    = 128
0.00.053.738 I print_info: n_gqa            = 1
0.00.053.739 I print_info: n_embd_k_gqa     = 2048
0.00.053.740 I print_info: n_embd_v_gqa     = 2048
0.00.053.740 I print_info: f_norm_eps       = 1.0e-05
0.00.053.741 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.741 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.741 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.741 I print_info: f_logit_scale    = 0.0e+00
0.00.053.742 I print_info: n_ff             = 8192
0.00.053.742 I print_info: n_expert         = 0
0.00.053.742 I print_info: n_expert_used    = 0
0.00.053.742 I print_info: causal attn      = 1
0.00.053.743 I print_info: pooling type     = 0
0.00.053.743 I print_info: rope type        = 2
0.00.053.743 I print_info: rope scaling     = linear
0.00.053.743 I print_info: freq_base_train  = 10000.0
0.00.053.744 I print_info: freq_scale_train = 1
0.00.053.744 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.744 I print_info: rope_finetuned   = unknown
0.00.053.744 I print_info: ssm_d_conv       = 0
0.00.053.744 I print_info: ssm_d_inner      = 0
0.00.053.744 I print_info: ssm_d_state      = 0
0.00.053.746 I print_info: ssm_dt_rank      = 0
0.00.053.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.747 I print_info: model type       = 1.4B
0.00.053.747 I print_info: model params     = 1.41 B
0.00.053.747 I print_info: general.name     = 1.4B
0.00.053.748 I print_info: vocab type       = BPE
0.00.053.748 I print_info: n_vocab          = 50304
0.00.053.748 I print_info: n_merges         = 50009
0.00.053.748 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: LF token         = 187 'Ċ'
0.00.053.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.750 I print_info: max token length = 1024
0.00.053.750 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.768.387 I load_tensors: offloading 24 repeating layers to GPU
0.00.768.403 I load_tensors: offloading output layer to GPU
0.00.768.404 I load_tensors: offloaded 25/25 layers to GPU
0.00.768.442 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.768.443 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.770.064 I llama_init_from_model: n_seq_max     = 1
0.00.770.067 I llama_init_from_model: n_ctx         = 2048
0.00.770.068 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.770.068 I llama_init_from_model: n_batch       = 2048
0.00.770.068 I llama_init_from_model: n_ubatch      = 512
0.00.770.069 I llama_init_from_model: flash_attn    = 0
0.00.770.071 I llama_init_from_model: freq_base     = 10000.0
0.00.770.072 I llama_init_from_model: freq_scale    = 1
0.00.770.078 I ggml_metal_init: allocating
0.00.770.197 I ggml_metal_init: found device: Apple M4
0.00.770.210 I ggml_metal_init: picking default device: Apple M4
0.00.772.112 I ggml_metal_init: using embedded metal library
0.00.778.616 I ggml_metal_init: GPU name:   Apple M4
0.00.778.621 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.778.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.778.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.778.624 I ggml_metal_init: simdgroup reduction   = true
0.00.778.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.778.624 I ggml_metal_init: has residency sets    = true
0.00.778.625 I ggml_metal_init: has bfloat            = true
0.00.778.625 I ggml_metal_init: use bfloat            = true
0.00.778.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.778.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.797.122 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.849.949 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.849.956 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.849.999 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.854.129 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.854.131 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.854.132 I llama_init_from_model: graph nodes  = 967
0.00.854.132 I llama_init_from_model: graph splits = 2
0.00.854.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.854.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.854.254 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.911.123 I main: llama threadpool init, n_threads = 4
0.00.911.170 I 
0.00.911.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.911.193 I 
0.00.911.370 I sampler seed: 1234
0.00.911.375 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.911.396 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.911.396 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.911.396 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.743.610 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.743.611 I llama_perf_context_print:        load time =     901.39 ms
0.01.743.611 I llama_perf_context_print: prompt eval time =      41.95 ms /     7 tokens (    5.99 ms per token,   166.85 tokens per second)
0.01.743.612 I llama_perf_context_print:        eval time =     787.34 ms /    63 runs   (   12.50 ms per token,    80.02 tokens per second)
0.01.743.612 I llama_perf_context_print:       total time =     833.23 ms /    70 tokens
0.01.743.856 I ggml_metal_free: deallocating

real	0m1.765s
user	0m0.112s
sys	0m0.228s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.760 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.442 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.443 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.171 I llama_model_loader: - type  f32:  194 tensors
0.00.025.172 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.172 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.172 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.173 I print_info: file format = GGUF V3 (latest)
0.00.025.173 I print_info: file type   = Q2_K - Medium
0.00.025.178 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.164 I load: special tokens cache size = 25
0.00.039.243 I load: token to piece cache size = 0.2984 MB
0.00.039.246 I print_info: arch             = gptneox
0.00.039.246 I print_info: vocab_only       = 0
0.00.039.247 I print_info: n_ctx_train      = 2048
0.00.039.247 I print_info: n_embd           = 2048
0.00.039.247 I print_info: n_layer          = 24
0.00.039.250 I print_info: n_head           = 16
0.00.039.251 I print_info: n_head_kv        = 16
0.00.039.251 I print_info: n_rot            = 32
0.00.039.251 I print_info: n_swa            = 0
0.00.039.251 I print_info: n_embd_head_k    = 128
0.00.039.252 I print_info: n_embd_head_v    = 128
0.00.039.252 I print_info: n_gqa            = 1
0.00.039.253 I print_info: n_embd_k_gqa     = 2048
0.00.039.254 I print_info: n_embd_v_gqa     = 2048
0.00.039.254 I print_info: f_norm_eps       = 1.0e-05
0.00.039.255 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.255 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.255 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.255 I print_info: f_logit_scale    = 0.0e+00
0.00.039.256 I print_info: n_ff             = 8192
0.00.039.256 I print_info: n_expert         = 0
0.00.039.256 I print_info: n_expert_used    = 0
0.00.039.257 I print_info: causal attn      = 1
0.00.039.257 I print_info: pooling type     = 0
0.00.039.257 I print_info: rope type        = 2
0.00.039.257 I print_info: rope scaling     = linear
0.00.039.258 I print_info: freq_base_train  = 10000.0
0.00.039.258 I print_info: freq_scale_train = 1
0.00.039.258 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.258 I print_info: rope_finetuned   = unknown
0.00.039.258 I print_info: ssm_d_conv       = 0
0.00.039.259 I print_info: ssm_d_inner      = 0
0.00.039.259 I print_info: ssm_d_state      = 0
0.00.039.259 I print_info: ssm_dt_rank      = 0
0.00.039.261 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.262 I print_info: model type       = 1.4B
0.00.039.262 I print_info: model params     = 1.41 B
0.00.039.262 I print_info: general.name     = 1.4B
0.00.039.263 I print_info: vocab type       = BPE
0.00.039.263 I print_info: n_vocab          = 50304
0.00.039.263 I print_info: n_merges         = 50009
0.00.039.263 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.263 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.264 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.264 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: LF token         = 187 'Ċ'
0.00.039.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: max token length = 1024
0.00.039.269 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.400.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.400.172 I load_tensors: offloading output layer to GPU
0.00.400.172 I load_tensors: offloaded 25/25 layers to GPU
0.00.400.207 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.400.208 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.401.784 I llama_init_from_model: n_seq_max     = 1
0.00.401.787 I llama_init_from_model: n_ctx         = 2048
0.00.401.787 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.401.788 I llama_init_from_model: n_batch       = 2048
0.00.401.788 I llama_init_from_model: n_ubatch      = 512
0.00.401.788 I llama_init_from_model: flash_attn    = 0
0.00.401.790 I llama_init_from_model: freq_base     = 10000.0
0.00.401.791 I llama_init_from_model: freq_scale    = 1
0.00.401.793 I ggml_metal_init: allocating
0.00.401.870 I ggml_metal_init: found device: Apple M4
0.00.401.883 I ggml_metal_init: picking default device: Apple M4
0.00.403.761 I ggml_metal_init: using embedded metal library
0.00.409.461 I ggml_metal_init: GPU name:   Apple M4
0.00.409.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.409.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.409.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.409.474 I ggml_metal_init: simdgroup reduction   = true
0.00.409.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.409.474 I ggml_metal_init: has residency sets    = true
0.00.409.475 I ggml_metal_init: has bfloat            = true
0.00.409.475 I ggml_metal_init: use bfloat            = true
0.00.409.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.409.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.330 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.489.332 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.489.414 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.493.823 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.493.825 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.493.825 I llama_init_from_model: graph nodes  = 967
0.00.493.826 I llama_init_from_model: graph splits = 2
0.00.493.832 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.493.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.493.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.555.110 I main: llama threadpool init, n_threads = 4
0.00.555.163 I 
0.00.555.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.555.191 I 
0.00.555.371 I sampler seed: 1234
0.00.555.376 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.555.423 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.555.425 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.555.426 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.246.354 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51749.27 tokens per second)
0.01.246.357 I llama_perf_context_print:        load time =     544.60 ms
0.01.246.359 I llama_perf_context_print: prompt eval time =      45.32 ms /     7 tokens (    6.47 ms per token,   154.45 tokens per second)
0.01.246.360 I llama_perf_context_print:        eval time =     642.71 ms /    63 runs   (   10.20 ms per token,    98.02 tokens per second)
0.01.246.360 I llama_perf_context_print:       total time =     691.99 ms /    70 tokens
0.01.246.579 I ggml_metal_free: deallocating

real	0m1.265s
user	0m0.113s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.453 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.130 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.131 I llama_model_loader: - type  f32:  194 tensors
0.00.025.131 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.131 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.132 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.132 I print_info: file format = GGUF V3 (latest)
0.00.025.133 I print_info: file type   = Q3_K - Medium
0.00.025.134 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.043 I load: special tokens cache size = 25
0.00.039.025 I load: token to piece cache size = 0.2984 MB
0.00.039.028 I print_info: arch             = gptneox
0.00.039.028 I print_info: vocab_only       = 0
0.00.039.028 I print_info: n_ctx_train      = 2048
0.00.039.028 I print_info: n_embd           = 2048
0.00.039.028 I print_info: n_layer          = 24
0.00.039.031 I print_info: n_head           = 16
0.00.039.032 I print_info: n_head_kv        = 16
0.00.039.032 I print_info: n_rot            = 32
0.00.039.032 I print_info: n_swa            = 0
0.00.039.032 I print_info: n_embd_head_k    = 128
0.00.039.032 I print_info: n_embd_head_v    = 128
0.00.039.034 I print_info: n_gqa            = 1
0.00.039.034 I print_info: n_embd_k_gqa     = 2048
0.00.039.035 I print_info: n_embd_v_gqa     = 2048
0.00.039.036 I print_info: f_norm_eps       = 1.0e-05
0.00.039.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.037 I print_info: f_logit_scale    = 0.0e+00
0.00.039.037 I print_info: n_ff             = 8192
0.00.039.038 I print_info: n_expert         = 0
0.00.039.038 I print_info: n_expert_used    = 0
0.00.039.039 I print_info: causal attn      = 1
0.00.039.041 I print_info: pooling type     = 0
0.00.039.041 I print_info: rope type        = 2
0.00.039.041 I print_info: rope scaling     = linear
0.00.039.041 I print_info: freq_base_train  = 10000.0
0.00.039.042 I print_info: freq_scale_train = 1
0.00.039.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.042 I print_info: rope_finetuned   = unknown
0.00.039.042 I print_info: ssm_d_conv       = 0
0.00.039.042 I print_info: ssm_d_inner      = 0
0.00.039.042 I print_info: ssm_d_state      = 0
0.00.039.043 I print_info: ssm_dt_rank      = 0
0.00.039.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.044 I print_info: model type       = 1.4B
0.00.039.044 I print_info: model params     = 1.41 B
0.00.039.045 I print_info: general.name     = 1.4B
0.00.039.045 I print_info: vocab type       = BPE
0.00.039.046 I print_info: n_vocab          = 50304
0.00.039.046 I print_info: n_merges         = 50009
0.00.039.046 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.047 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.047 I print_info: LF token         = 187 'Ċ'
0.00.039.047 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.047 I print_info: max token length = 1024
0.00.039.048 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.517.581 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.599 I load_tensors: offloading output layer to GPU
0.00.517.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.649 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.517.651 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.519.259 I llama_init_from_model: n_seq_max     = 1
0.00.519.262 I llama_init_from_model: n_ctx         = 2048
0.00.519.263 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.263 I llama_init_from_model: n_batch       = 2048
0.00.519.263 I llama_init_from_model: n_ubatch      = 512
0.00.519.264 I llama_init_from_model: flash_attn    = 0
0.00.519.266 I llama_init_from_model: freq_base     = 10000.0
0.00.519.266 I llama_init_from_model: freq_scale    = 1
0.00.519.268 I ggml_metal_init: allocating
0.00.519.335 I ggml_metal_init: found device: Apple M4
0.00.519.348 I ggml_metal_init: picking default device: Apple M4
0.00.521.286 I ggml_metal_init: using embedded metal library
0.00.526.893 I ggml_metal_init: GPU name:   Apple M4
0.00.526.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.909 I ggml_metal_init: simdgroup reduction   = true
0.00.526.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.909 I ggml_metal_init: has residency sets    = true
0.00.526.910 I ggml_metal_init: has bfloat            = true
0.00.526.910 I ggml_metal_init: use bfloat            = true
0.00.526.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.285 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.607.998 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.804 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.612.806 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.612.806 I llama_init_from_model: graph nodes  = 967
0.00.612.806 I llama_init_from_model: graph splits = 2
0.00.612.811 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.612.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.612.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.099 I main: llama threadpool init, n_threads = 4
0.00.671.145 I 
0.00.671.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.168 I 
0.00.671.375 I sampler seed: 1234
0.00.671.384 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.671.403 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.671.405 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.671.405 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.425.684 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.425.685 I llama_perf_context_print:        load time =     661.55 ms
0.01.425.686 I llama_perf_context_print: prompt eval time =      50.16 ms /     7 tokens (    7.17 ms per token,   139.56 tokens per second)
0.01.425.686 I llama_perf_context_print:        eval time =     701.19 ms /    63 runs   (   11.13 ms per token,    89.85 tokens per second)
0.01.425.687 I llama_perf_context_print:       total time =     755.32 ms /    70 tokens
0.01.425.907 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.113s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.528 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.304 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.313 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.940 I llama_model_loader: - type  f32:  194 tensors
0.00.026.940 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.940 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.940 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.941 I print_info: file format = GGUF V3 (latest)
0.00.026.942 I print_info: file type   = Q4_K - Medium
0.00.026.943 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.870 I load: special tokens cache size = 25
0.00.040.942 I load: token to piece cache size = 0.2984 MB
0.00.040.945 I print_info: arch             = gptneox
0.00.040.945 I print_info: vocab_only       = 0
0.00.040.945 I print_info: n_ctx_train      = 2048
0.00.040.945 I print_info: n_embd           = 2048
0.00.040.946 I print_info: n_layer          = 24
0.00.040.949 I print_info: n_head           = 16
0.00.040.949 I print_info: n_head_kv        = 16
0.00.040.950 I print_info: n_rot            = 32
0.00.040.950 I print_info: n_swa            = 0
0.00.040.950 I print_info: n_embd_head_k    = 128
0.00.040.950 I print_info: n_embd_head_v    = 128
0.00.040.951 I print_info: n_gqa            = 1
0.00.040.952 I print_info: n_embd_k_gqa     = 2048
0.00.040.953 I print_info: n_embd_v_gqa     = 2048
0.00.040.954 I print_info: f_norm_eps       = 1.0e-05
0.00.040.958 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.958 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.959 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.959 I print_info: f_logit_scale    = 0.0e+00
0.00.040.960 I print_info: n_ff             = 8192
0.00.040.960 I print_info: n_expert         = 0
0.00.040.960 I print_info: n_expert_used    = 0
0.00.040.960 I print_info: causal attn      = 1
0.00.040.962 I print_info: pooling type     = 0
0.00.040.963 I print_info: rope type        = 2
0.00.040.964 I print_info: rope scaling     = linear
0.00.040.964 I print_info: freq_base_train  = 10000.0
0.00.040.964 I print_info: freq_scale_train = 1
0.00.040.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.965 I print_info: rope_finetuned   = unknown
0.00.040.965 I print_info: ssm_d_conv       = 0
0.00.040.965 I print_info: ssm_d_inner      = 0
0.00.040.965 I print_info: ssm_d_state      = 0
0.00.040.965 I print_info: ssm_dt_rank      = 0
0.00.040.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.966 I print_info: model type       = 1.4B
0.00.040.967 I print_info: model params     = 1.41 B
0.00.040.967 I print_info: general.name     = 1.4B
0.00.040.968 I print_info: vocab type       = BPE
0.00.040.968 I print_info: n_vocab          = 50304
0.00.040.968 I print_info: n_merges         = 50009
0.00.040.969 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.969 I print_info: LF token         = 187 'Ċ'
0.00.040.970 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.970 I print_info: max token length = 1024
0.00.040.970 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.519.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.346 I load_tensors: offloading output layer to GPU
0.00.519.347 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.378 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.379 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.521.027 I llama_init_from_model: n_seq_max     = 1
0.00.521.029 I llama_init_from_model: n_ctx         = 2048
0.00.521.030 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.521.030 I llama_init_from_model: n_batch       = 2048
0.00.521.031 I llama_init_from_model: n_ubatch      = 512
0.00.521.031 I llama_init_from_model: flash_attn    = 0
0.00.521.033 I llama_init_from_model: freq_base     = 10000.0
0.00.521.034 I llama_init_from_model: freq_scale    = 1
0.00.521.036 I ggml_metal_init: allocating
0.00.521.091 I ggml_metal_init: found device: Apple M4
0.00.521.103 I ggml_metal_init: picking default device: Apple M4
0.00.522.965 I ggml_metal_init: using embedded metal library
0.00.529.760 I ggml_metal_init: GPU name:   Apple M4
0.00.529.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.766 I ggml_metal_init: simdgroup reduction   = true
0.00.529.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.767 I ggml_metal_init: has residency sets    = true
0.00.529.767 I ggml_metal_init: has bfloat            = true
0.00.529.767 I ggml_metal_init: use bfloat            = true
0.00.529.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.547.324 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.227 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.605.235 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.605.282 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.698 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.609.700 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.609.701 I llama_init_from_model: graph nodes  = 967
0.00.609.701 I llama_init_from_model: graph splits = 2
0.00.609.707 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.609.836 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.609.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.438 I main: llama threadpool init, n_threads = 4
0.00.667.485 I 
0.00.667.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.506 I 
0.00.667.665 I sampler seed: 1234
0.00.667.670 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.680 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.681 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.681 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.430.585 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.430.586 I llama_perf_context_print:        load time =     656.14 ms
0.01.430.586 I llama_perf_context_print: prompt eval time =      57.71 ms /     7 tokens (    8.24 ms per token,   121.29 tokens per second)
0.01.430.587 I llama_perf_context_print:        eval time =     702.41 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.430.587 I llama_perf_context_print:       total time =     763.92 ms /    70 tokens
0.01.430.824 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.784 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.372 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.255 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.301 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.138 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.139 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.139 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.139 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.140 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.140 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.141 I llama_model_loader: - type  f32:  194 tensors
0.00.029.141 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.141 I llama_model_loader: - type q6_K:   37 tensors
0.00.029.142 I print_info: file format = GGUF V3 (latest)
0.00.029.142 I print_info: file type   = Q5_K - Medium
0.00.029.143 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.037.486 I load: special tokens cache size = 25
0.00.043.545 I load: token to piece cache size = 0.2984 MB
0.00.043.548 I print_info: arch             = gptneox
0.00.043.549 I print_info: vocab_only       = 0
0.00.043.549 I print_info: n_ctx_train      = 2048
0.00.043.549 I print_info: n_embd           = 2048
0.00.043.549 I print_info: n_layer          = 24
0.00.043.552 I print_info: n_head           = 16
0.00.043.553 I print_info: n_head_kv        = 16
0.00.043.553 I print_info: n_rot            = 32
0.00.043.553 I print_info: n_swa            = 0
0.00.043.553 I print_info: n_embd_head_k    = 128
0.00.043.554 I print_info: n_embd_head_v    = 128
0.00.043.554 I print_info: n_gqa            = 1
0.00.043.555 I print_info: n_embd_k_gqa     = 2048
0.00.043.556 I print_info: n_embd_v_gqa     = 2048
0.00.043.556 I print_info: f_norm_eps       = 1.0e-05
0.00.043.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.557 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.558 I print_info: f_logit_scale    = 0.0e+00
0.00.043.559 I print_info: n_ff             = 8192
0.00.043.559 I print_info: n_expert         = 0
0.00.043.559 I print_info: n_expert_used    = 0
0.00.043.559 I print_info: causal attn      = 1
0.00.043.560 I print_info: pooling type     = 0
0.00.043.561 I print_info: rope type        = 2
0.00.043.563 I print_info: rope scaling     = linear
0.00.043.563 I print_info: freq_base_train  = 10000.0
0.00.043.563 I print_info: freq_scale_train = 1
0.00.043.563 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.564 I print_info: rope_finetuned   = unknown
0.00.043.564 I print_info: ssm_d_conv       = 0
0.00.043.564 I print_info: ssm_d_inner      = 0
0.00.043.564 I print_info: ssm_d_state      = 0
0.00.043.564 I print_info: ssm_dt_rank      = 0
0.00.043.564 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.564 I print_info: model type       = 1.4B
0.00.043.565 I print_info: model params     = 1.41 B
0.00.043.565 I print_info: general.name     = 1.4B
0.00.043.566 I print_info: vocab type       = BPE
0.00.043.566 I print_info: n_vocab          = 50304
0.00.043.566 I print_info: n_merges         = 50009
0.00.043.566 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.566 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.567 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.567 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.567 I print_info: LF token         = 187 'Ċ'
0.00.043.567 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.567 I print_info: max token length = 1024
0.00.043.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.282 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.298 I load_tensors: offloading output layer to GPU
0.00.613.299 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.334 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.613.335 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.614.902 I llama_init_from_model: n_seq_max     = 1
0.00.614.904 I llama_init_from_model: n_ctx         = 2048
0.00.614.905 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.614.905 I llama_init_from_model: n_batch       = 2048
0.00.614.905 I llama_init_from_model: n_ubatch      = 512
0.00.614.906 I llama_init_from_model: flash_attn    = 0
0.00.614.907 I llama_init_from_model: freq_base     = 10000.0
0.00.614.907 I llama_init_from_model: freq_scale    = 1
0.00.614.909 I ggml_metal_init: allocating
0.00.614.920 I ggml_metal_init: found device: Apple M4
0.00.614.928 I ggml_metal_init: picking default device: Apple M4
0.00.616.461 I ggml_metal_init: using embedded metal library
0.00.622.795 I ggml_metal_init: GPU name:   Apple M4
0.00.622.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.800 I ggml_metal_init: simdgroup reduction   = true
0.00.622.800 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.801 I ggml_metal_init: has residency sets    = true
0.00.622.801 I ggml_metal_init: has bfloat            = true
0.00.622.801 I ggml_metal_init: use bfloat            = true
0.00.622.802 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.803 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.902 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.968 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.691.974 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.011 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.079 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.080 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.081 I llama_init_from_model: graph nodes  = 967
0.00.696.081 I llama_init_from_model: graph splits = 2
0.00.696.086 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.215 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.142 I main: llama threadpool init, n_threads = 4
0.00.758.187 I 
0.00.758.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.210 I 
0.00.758.377 I sampler seed: 1234
0.00.758.382 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.425 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.430 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.430 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.600.391 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.600.391 I llama_perf_context_print:        load time =     744.60 ms
0.01.600.392 I llama_perf_context_print: prompt eval time =      52.65 ms /     7 tokens (    7.52 ms per token,   132.95 tokens per second)
0.01.600.393 I llama_perf_context_print:        eval time =     786.57 ms /    63 runs   (   12.49 ms per token,    80.09 tokens per second)
0.01.600.393 I llama_perf_context_print:       total time =     843.00 ms /    70 tokens
0.01.600.653 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.109s
sys	0m0.221s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.867 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.037 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.044 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.044 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.045 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.663 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.664 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.664 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.665 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.665 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.665 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.666 I llama_model_loader: - type  f32:  194 tensors
0.00.025.666 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.667 I print_info: file format = GGUF V3 (latest)
0.00.025.667 I print_info: file type   = Q6_K
0.00.025.668 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.585 I load: special tokens cache size = 25
0.00.039.644 I load: token to piece cache size = 0.2984 MB
0.00.039.646 I print_info: arch             = gptneox
0.00.039.647 I print_info: vocab_only       = 0
0.00.039.647 I print_info: n_ctx_train      = 2048
0.00.039.647 I print_info: n_embd           = 2048
0.00.039.647 I print_info: n_layer          = 24
0.00.039.650 I print_info: n_head           = 16
0.00.039.651 I print_info: n_head_kv        = 16
0.00.039.651 I print_info: n_rot            = 32
0.00.039.651 I print_info: n_swa            = 0
0.00.039.652 I print_info: n_embd_head_k    = 128
0.00.039.652 I print_info: n_embd_head_v    = 128
0.00.039.652 I print_info: n_gqa            = 1
0.00.039.653 I print_info: n_embd_k_gqa     = 2048
0.00.039.654 I print_info: n_embd_v_gqa     = 2048
0.00.039.654 I print_info: f_norm_eps       = 1.0e-05
0.00.039.655 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.655 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.655 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.655 I print_info: f_logit_scale    = 0.0e+00
0.00.039.656 I print_info: n_ff             = 8192
0.00.039.658 I print_info: n_expert         = 0
0.00.039.658 I print_info: n_expert_used    = 0
0.00.039.658 I print_info: causal attn      = 1
0.00.039.658 I print_info: pooling type     = 0
0.00.039.659 I print_info: rope type        = 2
0.00.039.660 I print_info: rope scaling     = linear
0.00.039.662 I print_info: freq_base_train  = 10000.0
0.00.039.662 I print_info: freq_scale_train = 1
0.00.039.663 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.663 I print_info: rope_finetuned   = unknown
0.00.039.663 I print_info: ssm_d_conv       = 0
0.00.039.663 I print_info: ssm_d_inner      = 0
0.00.039.663 I print_info: ssm_d_state      = 0
0.00.039.663 I print_info: ssm_dt_rank      = 0
0.00.039.663 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.664 I print_info: model type       = 1.4B
0.00.039.664 I print_info: model params     = 1.41 B
0.00.039.664 I print_info: general.name     = 1.4B
0.00.039.668 I print_info: vocab type       = BPE
0.00.039.668 I print_info: n_vocab          = 50304
0.00.039.668 I print_info: n_merges         = 50009
0.00.039.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.670 I print_info: LF token         = 187 'Ċ'
0.00.039.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.670 I print_info: max token length = 1024
0.00.039.670 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.170 I load_tensors: offloading output layer to GPU
0.00.679.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.202 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.679.213 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.680.691 I llama_init_from_model: n_seq_max     = 1
0.00.680.695 I llama_init_from_model: n_ctx         = 2048
0.00.680.695 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.680.695 I llama_init_from_model: n_batch       = 2048
0.00.680.696 I llama_init_from_model: n_ubatch      = 512
0.00.680.696 I llama_init_from_model: flash_attn    = 0
0.00.680.697 I llama_init_from_model: freq_base     = 10000.0
0.00.680.698 I llama_init_from_model: freq_scale    = 1
0.00.680.699 I ggml_metal_init: allocating
0.00.680.715 I ggml_metal_init: found device: Apple M4
0.00.680.724 I ggml_metal_init: picking default device: Apple M4
0.00.682.163 I ggml_metal_init: using embedded metal library
0.00.688.715 I ggml_metal_init: GPU name:   Apple M4
0.00.688.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.720 I ggml_metal_init: simdgroup reduction   = true
0.00.688.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.721 I ggml_metal_init: has residency sets    = true
0.00.688.721 I ggml_metal_init: has bfloat            = true
0.00.688.721 I ggml_metal_init: use bfloat            = true
0.00.688.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.961 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.759.994 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.760.005 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.760.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.764.564 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.764.566 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.764.566 I llama_init_from_model: graph nodes  = 967
0.00.764.567 I llama_init_from_model: graph splits = 2
0.00.764.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.764.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.764.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.681 I main: llama threadpool init, n_threads = 4
0.00.827.728 I 
0.00.827.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.748 I 
0.00.827.906 I sampler seed: 1234
0.00.827.910 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.827.944 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.827.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.827.946 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.708.125 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48431.11 tokens per second)
0.01.708.126 I llama_perf_context_print:        load time =     818.10 ms
0.01.708.126 I llama_perf_context_print: prompt eval time =      57.81 ms /     7 tokens (    8.26 ms per token,   121.09 tokens per second)
0.01.708.127 I llama_perf_context_print:        eval time =     819.78 ms /    63 runs   (   13.01 ms per token,    76.85 tokens per second)
0.01.708.127 I llama_perf_context_print:       total time =     881.16 ms /    70 tokens
0.01.708.366 I ggml_metal_free: deallocating

real	0m1.726s
user	0m0.109s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.535 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.620 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.512 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.519 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.525 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.538 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.186 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.187 I llama_model_loader: - type  f32:  194 tensors
0.00.054.187 I llama_model_loader: - type  f16:   98 tensors
0.00.054.188 I print_info: file format = GGUF V3 (latest)
0.00.054.189 I print_info: file type   = all F32 (guessed)
0.00.054.190 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.906 I load: special tokens cache size = 25
0.00.075.040 I load: token to piece cache size = 0.2984 MB
0.00.075.043 I print_info: arch             = gptneox
0.00.075.043 I print_info: vocab_only       = 0
0.00.075.044 I print_info: n_ctx_train      = 2048
0.00.075.044 I print_info: n_embd           = 2048
0.00.075.044 I print_info: n_layer          = 24
0.00.075.047 I print_info: n_head           = 16
0.00.075.048 I print_info: n_head_kv        = 16
0.00.075.048 I print_info: n_rot            = 32
0.00.075.048 I print_info: n_swa            = 0
0.00.075.048 I print_info: n_embd_head_k    = 128
0.00.075.048 I print_info: n_embd_head_v    = 128
0.00.075.049 I print_info: n_gqa            = 1
0.00.075.052 I print_info: n_embd_k_gqa     = 2048
0.00.075.052 I print_info: n_embd_v_gqa     = 2048
0.00.075.053 I print_info: f_norm_eps       = 1.0e-05
0.00.075.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.054 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.054 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.054 I print_info: f_logit_scale    = 0.0e+00
0.00.075.055 I print_info: n_ff             = 8192
0.00.075.055 I print_info: n_expert         = 0
0.00.075.055 I print_info: n_expert_used    = 0
0.00.075.055 I print_info: causal attn      = 1
0.00.075.055 I print_info: pooling type     = 0
0.00.075.055 I print_info: rope type        = 2
0.00.075.056 I print_info: rope scaling     = linear
0.00.075.056 I print_info: freq_base_train  = 10000.0
0.00.075.056 I print_info: freq_scale_train = 1
0.00.075.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.057 I print_info: rope_finetuned   = unknown
0.00.075.058 I print_info: ssm_d_conv       = 0
0.00.075.058 I print_info: ssm_d_inner      = 0
0.00.075.059 I print_info: ssm_d_state      = 0
0.00.075.059 I print_info: ssm_dt_rank      = 0
0.00.075.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.059 I print_info: model type       = 1.4B
0.00.075.059 I print_info: model params     = 1.41 B
0.00.075.059 I print_info: general.name     = 1.4B
0.00.075.060 I print_info: vocab type       = BPE
0.00.075.060 I print_info: n_vocab          = 50304
0.00.075.060 I print_info: n_merges         = 50009
0.00.075.061 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.065 I print_info: LF token         = 187 'Ċ'
0.00.075.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.065 I print_info: max token length = 1024
0.00.075.066 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.383.256 I load_tensors: offloading 24 repeating layers to GPU
0.01.383.260 I load_tensors: offloading output layer to GPU
0.01.383.260 I load_tensors: offloaded 25/25 layers to GPU
0.01.383.281 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.383.283 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.384.353 I llama_init_from_model: n_seq_max     = 1
0.01.384.354 I llama_init_from_model: n_ctx         = 128
0.01.384.354 I llama_init_from_model: n_ctx_per_seq = 128
0.01.384.354 I llama_init_from_model: n_batch       = 128
0.01.384.355 I llama_init_from_model: n_ubatch      = 128
0.01.384.355 I llama_init_from_model: flash_attn    = 0
0.01.384.356 I llama_init_from_model: freq_base     = 10000.0
0.01.384.356 I llama_init_from_model: freq_scale    = 1
0.01.384.356 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.384.357 I ggml_metal_init: allocating
0.01.384.402 I ggml_metal_init: found device: Apple M4
0.01.384.408 I ggml_metal_init: picking default device: Apple M4
0.01.385.524 I ggml_metal_init: using embedded metal library
0.01.389.327 I ggml_metal_init: GPU name:   Apple M4
0.01.389.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.389.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.389.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.389.330 I ggml_metal_init: simdgroup reduction   = true
0.01.389.330 I ggml_metal_init: simdgroup matrix mul. = true
0.01.389.330 I ggml_metal_init: has residency sets    = true
0.01.389.331 I ggml_metal_init: has bfloat            = true
0.01.389.331 I ggml_metal_init: use bfloat            = true
0.01.389.331 I ggml_metal_init: hasUnifiedMemory      = true
0.01.389.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.400.240 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.401.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.401.938 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.401.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.403.594 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.403.595 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.403.596 I llama_init_from_model: graph nodes  = 967
0.01.403.596 I llama_init_from_model: graph splits = 2
0.01.403.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.403.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.439.286 I 
0.01.439.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.439.331 I perplexity: tokenizing the input ..
0.01.444.510 I perplexity: tokenization took 5.176 ms
0.01.444.534 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.562.951 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.564.487 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.564.503 I llama_perf_context_print:        load time =    1415.65 ms
0.01.564.504 I llama_perf_context_print: prompt eval time =     118.11 ms /   128 tokens (    0.92 ms per token,  1083.74 tokens per second)
0.01.564.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.564.505 I llama_perf_context_print:       total time =     125.22 ms /   129 tokens
0.01.564.862 I ggml_metal_free: deallocating

real	0m1.752s
user	0m0.098s
sys	0m0.269s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.257 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.072 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.983 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.994 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.998 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.837 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.746 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.748 I llama_model_loader: - type  f32:  194 tensors
0.00.025.748 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.749 I print_info: file format = GGUF V3 (latest)
0.00.025.750 I print_info: file type   = Q8_0
0.00.025.753 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.231 I load: special tokens cache size = 25
0.00.040.439 I load: token to piece cache size = 0.2984 MB
0.00.040.444 I print_info: arch             = gptneox
0.00.040.444 I print_info: vocab_only       = 0
0.00.040.444 I print_info: n_ctx_train      = 2048
0.00.040.444 I print_info: n_embd           = 2048
0.00.040.445 I print_info: n_layer          = 24
0.00.040.449 I print_info: n_head           = 16
0.00.040.450 I print_info: n_head_kv        = 16
0.00.040.450 I print_info: n_rot            = 32
0.00.040.450 I print_info: n_swa            = 0
0.00.040.450 I print_info: n_embd_head_k    = 128
0.00.040.451 I print_info: n_embd_head_v    = 128
0.00.040.451 I print_info: n_gqa            = 1
0.00.040.452 I print_info: n_embd_k_gqa     = 2048
0.00.040.453 I print_info: n_embd_v_gqa     = 2048
0.00.040.453 I print_info: f_norm_eps       = 1.0e-05
0.00.040.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.454 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.454 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.454 I print_info: f_logit_scale    = 0.0e+00
0.00.040.455 I print_info: n_ff             = 8192
0.00.040.455 I print_info: n_expert         = 0
0.00.040.455 I print_info: n_expert_used    = 0
0.00.040.455 I print_info: causal attn      = 1
0.00.040.456 I print_info: pooling type     = 0
0.00.040.456 I print_info: rope type        = 2
0.00.040.456 I print_info: rope scaling     = linear
0.00.040.456 I print_info: freq_base_train  = 10000.0
0.00.040.457 I print_info: freq_scale_train = 1
0.00.040.458 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.458 I print_info: rope_finetuned   = unknown
0.00.040.458 I print_info: ssm_d_conv       = 0
0.00.040.459 I print_info: ssm_d_inner      = 0
0.00.040.459 I print_info: ssm_d_state      = 0
0.00.040.459 I print_info: ssm_dt_rank      = 0
0.00.040.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.459 I print_info: model type       = 1.4B
0.00.040.460 I print_info: model params     = 1.41 B
0.00.040.460 I print_info: general.name     = 1.4B
0.00.040.460 I print_info: vocab type       = BPE
0.00.040.460 I print_info: n_vocab          = 50304
0.00.040.461 I print_info: n_merges         = 50009
0.00.040.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.461 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.461 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.462 I print_info: LF token         = 187 'Ċ'
0.00.040.462 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.462 I print_info: max token length = 1024
0.00.040.463 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.821.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.821.053 I load_tensors: offloading output layer to GPU
0.00.821.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.821.082 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.821.085 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.822.478 I llama_init_from_model: n_seq_max     = 1
0.00.822.480 I llama_init_from_model: n_ctx         = 128
0.00.822.480 I llama_init_from_model: n_ctx_per_seq = 128
0.00.822.481 I llama_init_from_model: n_batch       = 128
0.00.822.481 I llama_init_from_model: n_ubatch      = 128
0.00.822.481 I llama_init_from_model: flash_attn    = 0
0.00.822.482 I llama_init_from_model: freq_base     = 10000.0
0.00.822.482 I llama_init_from_model: freq_scale    = 1
0.00.822.483 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.822.484 I ggml_metal_init: allocating
0.00.822.528 I ggml_metal_init: found device: Apple M4
0.00.822.537 I ggml_metal_init: picking default device: Apple M4
0.00.824.123 I ggml_metal_init: using embedded metal library
0.00.829.839 I ggml_metal_init: GPU name:   Apple M4
0.00.829.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.829.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.829.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.829.846 I ggml_metal_init: simdgroup reduction   = true
0.00.829.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.829.846 I ggml_metal_init: has residency sets    = true
0.00.829.847 I ggml_metal_init: has bfloat            = true
0.00.829.847 I ggml_metal_init: use bfloat            = true
0.00.829.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.829.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.846.006 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.848.635 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.848.638 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.848.674 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.851.348 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.851.350 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.851.350 I llama_init_from_model: graph nodes  = 967
0.00.851.351 I llama_init_from_model: graph splits = 2
0.00.851.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.851.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.613 I 
0.00.879.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.703 I perplexity: tokenizing the input ..
0.00.887.399 I perplexity: tokenization took 7.693 ms
0.00.887.433 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.026.778 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.028.320 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.028.338 I llama_perf_context_print:        load time =     869.53 ms
0.01.028.339 I llama_perf_context_print: prompt eval time =     138.38 ms /   128 tokens (    1.08 ms per token,   924.96 tokens per second)
0.01.028.339 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.028.340 I llama_perf_context_print:       total time =     148.73 ms /   129 tokens
0.01.028.692 I ggml_metal_free: deallocating

real	0m1.046s
user	0m0.080s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.259 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.525 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.526 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.197 I llama_model_loader: - type  f32:  194 tensors
0.00.026.197 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.198 I print_info: file format = GGUF V3 (latest)
0.00.026.198 I print_info: file type   = Q4_0
0.00.026.199 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.746 I load: special tokens cache size = 25
0.00.040.849 I load: token to piece cache size = 0.2984 MB
0.00.040.853 I print_info: arch             = gptneox
0.00.040.854 I print_info: vocab_only       = 0
0.00.040.854 I print_info: n_ctx_train      = 2048
0.00.040.854 I print_info: n_embd           = 2048
0.00.040.854 I print_info: n_layer          = 24
0.00.040.859 I print_info: n_head           = 16
0.00.040.860 I print_info: n_head_kv        = 16
0.00.040.860 I print_info: n_rot            = 32
0.00.040.860 I print_info: n_swa            = 0
0.00.040.861 I print_info: n_embd_head_k    = 128
0.00.040.861 I print_info: n_embd_head_v    = 128
0.00.040.861 I print_info: n_gqa            = 1
0.00.040.862 I print_info: n_embd_k_gqa     = 2048
0.00.040.863 I print_info: n_embd_v_gqa     = 2048
0.00.040.863 I print_info: f_norm_eps       = 1.0e-05
0.00.040.864 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.864 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.864 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.864 I print_info: f_logit_scale    = 0.0e+00
0.00.040.865 I print_info: n_ff             = 8192
0.00.040.865 I print_info: n_expert         = 0
0.00.040.865 I print_info: n_expert_used    = 0
0.00.040.865 I print_info: causal attn      = 1
0.00.040.865 I print_info: pooling type     = 0
0.00.040.865 I print_info: rope type        = 2
0.00.040.865 I print_info: rope scaling     = linear
0.00.040.866 I print_info: freq_base_train  = 10000.0
0.00.040.866 I print_info: freq_scale_train = 1
0.00.040.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.869 I print_info: rope_finetuned   = unknown
0.00.040.869 I print_info: ssm_d_conv       = 0
0.00.040.870 I print_info: ssm_d_inner      = 0
0.00.040.870 I print_info: ssm_d_state      = 0
0.00.040.870 I print_info: ssm_dt_rank      = 0
0.00.040.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.870 I print_info: model type       = 1.4B
0.00.040.870 I print_info: model params     = 1.41 B
0.00.040.871 I print_info: general.name     = 1.4B
0.00.040.871 I print_info: vocab type       = BPE
0.00.040.872 I print_info: n_vocab          = 50304
0.00.040.873 I print_info: n_merges         = 50009
0.00.040.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: LF token         = 187 'Ċ'
0.00.040.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.874 I print_info: max token length = 1024
0.00.040.874 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.541.022 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.034 I load_tensors: offloading output layer to GPU
0.00.541.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.070 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.541.075 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.542.636 I llama_init_from_model: n_seq_max     = 1
0.00.542.639 I llama_init_from_model: n_ctx         = 128
0.00.542.639 I llama_init_from_model: n_ctx_per_seq = 128
0.00.542.640 I llama_init_from_model: n_batch       = 128
0.00.542.640 I llama_init_from_model: n_ubatch      = 128
0.00.542.641 I llama_init_from_model: flash_attn    = 0
0.00.542.643 I llama_init_from_model: freq_base     = 10000.0
0.00.542.644 I llama_init_from_model: freq_scale    = 1
0.00.542.645 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.542.647 I ggml_metal_init: allocating
0.00.542.727 I ggml_metal_init: found device: Apple M4
0.00.542.741 I ggml_metal_init: picking default device: Apple M4
0.00.544.604 I ggml_metal_init: using embedded metal library
0.00.550.153 I ggml_metal_init: GPU name:   Apple M4
0.00.550.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.550.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.550.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.550.165 I ggml_metal_init: simdgroup reduction   = true
0.00.550.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.550.166 I ggml_metal_init: has residency sets    = true
0.00.550.166 I ggml_metal_init: has bfloat            = true
0.00.550.166 I ggml_metal_init: use bfloat            = true
0.00.550.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.550.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.227 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.573.756 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.573.763 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.573.816 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.576.992 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.576.993 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.576.994 I llama_init_from_model: graph nodes  = 967
0.00.576.994 I llama_init_from_model: graph splits = 2
0.00.576.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.576.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.825 I 
0.00.604.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.908 I perplexity: tokenizing the input ..
0.00.612.038 I perplexity: tokenization took 7.126 ms
0.00.612.060 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.459 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.749.073 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.749.092 I llama_perf_context_print:        load time =     594.76 ms
0.00.749.092 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.73 tokens per second)
0.00.749.093 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.094 I llama_perf_context_print:       total time =     144.27 ms /   129 tokens
0.00.749.463 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.082s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.993 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.437 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.451 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.451 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.451 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.453 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.453 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.454 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.543 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.492 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.493 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.494 I llama_model_loader: - type  f32:  194 tensors
0.00.025.494 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.494 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.495 I print_info: file format = GGUF V3 (latest)
0.00.025.496 I print_info: file type   = Q4_1
0.00.025.496 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.786 I load: special tokens cache size = 25
0.00.039.594 I load: token to piece cache size = 0.2984 MB
0.00.039.598 I print_info: arch             = gptneox
0.00.039.598 I print_info: vocab_only       = 0
0.00.039.599 I print_info: n_ctx_train      = 2048
0.00.039.599 I print_info: n_embd           = 2048
0.00.039.599 I print_info: n_layer          = 24
0.00.039.603 I print_info: n_head           = 16
0.00.039.604 I print_info: n_head_kv        = 16
0.00.039.604 I print_info: n_rot            = 32
0.00.039.607 I print_info: n_swa            = 0
0.00.039.607 I print_info: n_embd_head_k    = 128
0.00.039.607 I print_info: n_embd_head_v    = 128
0.00.039.608 I print_info: n_gqa            = 1
0.00.039.609 I print_info: n_embd_k_gqa     = 2048
0.00.039.610 I print_info: n_embd_v_gqa     = 2048
0.00.039.610 I print_info: f_norm_eps       = 1.0e-05
0.00.039.611 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.611 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.611 I print_info: f_logit_scale    = 0.0e+00
0.00.039.613 I print_info: n_ff             = 8192
0.00.039.613 I print_info: n_expert         = 0
0.00.039.614 I print_info: n_expert_used    = 0
0.00.039.614 I print_info: causal attn      = 1
0.00.039.614 I print_info: pooling type     = 0
0.00.039.614 I print_info: rope type        = 2
0.00.039.615 I print_info: rope scaling     = linear
0.00.039.615 I print_info: freq_base_train  = 10000.0
0.00.039.615 I print_info: freq_scale_train = 1
0.00.039.615 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.616 I print_info: rope_finetuned   = unknown
0.00.039.616 I print_info: ssm_d_conv       = 0
0.00.039.616 I print_info: ssm_d_inner      = 0
0.00.039.616 I print_info: ssm_d_state      = 0
0.00.039.616 I print_info: ssm_dt_rank      = 0
0.00.039.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.617 I print_info: model type       = 1.4B
0.00.039.617 I print_info: model params     = 1.41 B
0.00.039.618 I print_info: general.name     = 1.4B
0.00.039.619 I print_info: vocab type       = BPE
0.00.039.619 I print_info: n_vocab          = 50304
0.00.039.619 I print_info: n_merges         = 50009
0.00.039.619 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.619 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: LF token         = 187 'Ċ'
0.00.039.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: max token length = 1024
0.00.039.622 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.485 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.504 I load_tensors: offloading output layer to GPU
0.00.587.505 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.548 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.587.549 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.589.281 I llama_init_from_model: n_seq_max     = 1
0.00.589.284 I llama_init_from_model: n_ctx         = 128
0.00.589.284 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.285 I llama_init_from_model: n_batch       = 128
0.00.589.285 I llama_init_from_model: n_ubatch      = 128
0.00.589.286 I llama_init_from_model: flash_attn    = 0
0.00.589.289 I llama_init_from_model: freq_base     = 10000.0
0.00.589.289 I llama_init_from_model: freq_scale    = 1
0.00.589.290 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.292 I ggml_metal_init: allocating
0.00.589.425 I ggml_metal_init: found device: Apple M4
0.00.589.439 I ggml_metal_init: picking default device: Apple M4
0.00.591.355 I ggml_metal_init: using embedded metal library
0.00.598.201 I ggml_metal_init: GPU name:   Apple M4
0.00.598.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.210 I ggml_metal_init: simdgroup reduction   = true
0.00.598.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.210 I ggml_metal_init: has residency sets    = true
0.00.598.210 I ggml_metal_init: has bfloat            = true
0.00.598.211 I ggml_metal_init: use bfloat            = true
0.00.598.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.219 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.583 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.094 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.098 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.143 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.418 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.420 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.420 I llama_init_from_model: graph nodes  = 967
0.00.623.420 I llama_init_from_model: graph splits = 2
0.00.623.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.005 I 
0.00.652.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.114 I perplexity: tokenizing the input ..
0.00.658.674 I perplexity: tokenization took 6.557 ms
0.00.658.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.671 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.796.176 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.796.193 I llama_perf_context_print:        load time =     643.00 ms
0.00.796.194 I llama_perf_context_print: prompt eval time =     135.44 ms /   128 tokens (    1.06 ms per token,   945.09 tokens per second)
0.00.796.195 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.195 I llama_perf_context_print:       total time =     144.19 ms /   129 tokens
0.00.796.599 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.082s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.132 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.144 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.145 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.800 I llama_model_loader: - type  f32:  194 tensors
0.00.025.800 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.800 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.801 I print_info: file format = GGUF V3 (latest)
0.00.025.802 I print_info: file type   = Q5_0
0.00.025.803 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.163 I load: special tokens cache size = 25
0.00.040.227 I load: token to piece cache size = 0.2984 MB
0.00.040.231 I print_info: arch             = gptneox
0.00.040.232 I print_info: vocab_only       = 0
0.00.040.232 I print_info: n_ctx_train      = 2048
0.00.040.232 I print_info: n_embd           = 2048
0.00.040.232 I print_info: n_layer          = 24
0.00.040.236 I print_info: n_head           = 16
0.00.040.237 I print_info: n_head_kv        = 16
0.00.040.237 I print_info: n_rot            = 32
0.00.040.237 I print_info: n_swa            = 0
0.00.040.238 I print_info: n_embd_head_k    = 128
0.00.040.238 I print_info: n_embd_head_v    = 128
0.00.040.238 I print_info: n_gqa            = 1
0.00.040.239 I print_info: n_embd_k_gqa     = 2048
0.00.040.240 I print_info: n_embd_v_gqa     = 2048
0.00.040.241 I print_info: f_norm_eps       = 1.0e-05
0.00.040.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.241 I print_info: f_logit_scale    = 0.0e+00
0.00.040.242 I print_info: n_ff             = 8192
0.00.040.242 I print_info: n_expert         = 0
0.00.040.242 I print_info: n_expert_used    = 0
0.00.040.242 I print_info: causal attn      = 1
0.00.040.242 I print_info: pooling type     = 0
0.00.040.243 I print_info: rope type        = 2
0.00.040.243 I print_info: rope scaling     = linear
0.00.040.243 I print_info: freq_base_train  = 10000.0
0.00.040.243 I print_info: freq_scale_train = 1
0.00.040.244 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.244 I print_info: rope_finetuned   = unknown
0.00.040.244 I print_info: ssm_d_conv       = 0
0.00.040.244 I print_info: ssm_d_inner      = 0
0.00.040.244 I print_info: ssm_d_state      = 0
0.00.040.244 I print_info: ssm_dt_rank      = 0
0.00.040.244 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.245 I print_info: model type       = 1.4B
0.00.040.248 I print_info: model params     = 1.41 B
0.00.040.248 I print_info: general.name     = 1.4B
0.00.040.248 I print_info: vocab type       = BPE
0.00.040.248 I print_info: n_vocab          = 50304
0.00.040.249 I print_info: n_merges         = 50009
0.00.040.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.252 I print_info: LF token         = 187 'Ċ'
0.00.040.252 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.252 I print_info: max token length = 1024
0.00.040.253 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.912 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.923 I load_tensors: offloading output layer to GPU
0.00.614.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.957 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.614.959 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.616.619 I llama_init_from_model: n_seq_max     = 1
0.00.616.622 I llama_init_from_model: n_ctx         = 128
0.00.616.622 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.623 I llama_init_from_model: n_batch       = 128
0.00.616.623 I llama_init_from_model: n_ubatch      = 128
0.00.616.623 I llama_init_from_model: flash_attn    = 0
0.00.616.626 I llama_init_from_model: freq_base     = 10000.0
0.00.616.626 I llama_init_from_model: freq_scale    = 1
0.00.616.627 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.629 I ggml_metal_init: allocating
0.00.616.711 I ggml_metal_init: found device: Apple M4
0.00.616.726 I ggml_metal_init: picking default device: Apple M4
0.00.618.499 I ggml_metal_init: using embedded metal library
0.00.625.356 I ggml_metal_init: GPU name:   Apple M4
0.00.625.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.365 I ggml_metal_init: simdgroup reduction   = true
0.00.625.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.366 I ggml_metal_init: has residency sets    = true
0.00.625.366 I ggml_metal_init: has bfloat            = true
0.00.625.366 I ggml_metal_init: use bfloat            = true
0.00.625.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.910 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.519 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.526 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.583 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.795 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.797 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.798 I llama_init_from_model: graph nodes  = 967
0.00.650.798 I llama_init_from_model: graph splits = 2
0.00.650.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.994 I 
0.00.680.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.082 I perplexity: tokenizing the input ..
0.00.686.908 I perplexity: tokenization took 6.823 ms
0.00.686.928 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.841 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.835.380 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.835.399 I llama_perf_context_print:        load time =     670.11 ms
0.00.835.400 I llama_perf_context_print: prompt eval time =     146.04 ms /   128 tokens (    1.14 ms per token,   876.48 tokens per second)
0.00.835.401 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.401 I llama_perf_context_print:       total time =     155.41 ms /   129 tokens
0.00.835.825 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.082s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.184 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.185 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.187 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.187 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.928 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.929 I llama_model_loader: - type  f32:  194 tensors
0.00.024.929 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.929 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.930 I print_info: file format = GGUF V3 (latest)
0.00.024.932 I print_info: file type   = Q5_1
0.00.024.933 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.114 I load: special tokens cache size = 25
0.00.039.229 I load: token to piece cache size = 0.2984 MB
0.00.039.233 I print_info: arch             = gptneox
0.00.039.233 I print_info: vocab_only       = 0
0.00.039.233 I print_info: n_ctx_train      = 2048
0.00.039.234 I print_info: n_embd           = 2048
0.00.039.234 I print_info: n_layer          = 24
0.00.039.238 I print_info: n_head           = 16
0.00.039.238 I print_info: n_head_kv        = 16
0.00.039.239 I print_info: n_rot            = 32
0.00.039.239 I print_info: n_swa            = 0
0.00.039.239 I print_info: n_embd_head_k    = 128
0.00.039.239 I print_info: n_embd_head_v    = 128
0.00.039.240 I print_info: n_gqa            = 1
0.00.039.246 I print_info: n_embd_k_gqa     = 2048
0.00.039.247 I print_info: n_embd_v_gqa     = 2048
0.00.039.247 I print_info: f_norm_eps       = 1.0e-05
0.00.039.247 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.248 I print_info: f_logit_scale    = 0.0e+00
0.00.039.248 I print_info: n_ff             = 8192
0.00.039.249 I print_info: n_expert         = 0
0.00.039.249 I print_info: n_expert_used    = 0
0.00.039.249 I print_info: causal attn      = 1
0.00.039.249 I print_info: pooling type     = 0
0.00.039.249 I print_info: rope type        = 2
0.00.039.250 I print_info: rope scaling     = linear
0.00.039.250 I print_info: freq_base_train  = 10000.0
0.00.039.250 I print_info: freq_scale_train = 1
0.00.039.251 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.251 I print_info: rope_finetuned   = unknown
0.00.039.251 I print_info: ssm_d_conv       = 0
0.00.039.251 I print_info: ssm_d_inner      = 0
0.00.039.251 I print_info: ssm_d_state      = 0
0.00.039.253 I print_info: ssm_dt_rank      = 0
0.00.039.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.254 I print_info: model type       = 1.4B
0.00.039.254 I print_info: model params     = 1.41 B
0.00.039.254 I print_info: general.name     = 1.4B
0.00.039.254 I print_info: vocab type       = BPE
0.00.039.254 I print_info: n_vocab          = 50304
0.00.039.255 I print_info: n_merges         = 50009
0.00.039.255 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.256 I print_info: LF token         = 187 'Ċ'
0.00.039.256 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.256 I print_info: max token length = 1024
0.00.039.256 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.676.655 I load_tensors: offloading 24 repeating layers to GPU
0.00.676.668 I load_tensors: offloading output layer to GPU
0.00.676.668 I load_tensors: offloaded 25/25 layers to GPU
0.00.676.700 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.676.702 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.678.385 I llama_init_from_model: n_seq_max     = 1
0.00.678.392 I llama_init_from_model: n_ctx         = 128
0.00.678.393 I llama_init_from_model: n_ctx_per_seq = 128
0.00.678.393 I llama_init_from_model: n_batch       = 128
0.00.678.394 I llama_init_from_model: n_ubatch      = 128
0.00.678.394 I llama_init_from_model: flash_attn    = 0
0.00.678.395 I llama_init_from_model: freq_base     = 10000.0
0.00.678.395 I llama_init_from_model: freq_scale    = 1
0.00.678.396 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.678.399 I ggml_metal_init: allocating
0.00.678.471 I ggml_metal_init: found device: Apple M4
0.00.678.485 I ggml_metal_init: picking default device: Apple M4
0.00.680.354 I ggml_metal_init: using embedded metal library
0.00.687.041 I ggml_metal_init: GPU name:   Apple M4
0.00.687.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.687.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.687.048 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.687.048 I ggml_metal_init: simdgroup reduction   = true
0.00.687.049 I ggml_metal_init: simdgroup matrix mul. = true
0.00.687.049 I ggml_metal_init: has residency sets    = true
0.00.687.049 I ggml_metal_init: has bfloat            = true
0.00.687.049 I ggml_metal_init: use bfloat            = true
0.00.687.050 I ggml_metal_init: hasUnifiedMemory      = true
0.00.687.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.061 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.783 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.708.790 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.708.855 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.349 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.712.351 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.712.352 I llama_init_from_model: graph nodes  = 967
0.00.712.352 I llama_init_from_model: graph splits = 2
0.00.712.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.712.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.436 I 
0.00.743.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.523 I perplexity: tokenizing the input ..
0.00.749.648 I perplexity: tokenization took 6.123 ms
0.00.749.673 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.883.760 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.885.569 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.885.586 I llama_perf_context_print:        load time =     734.59 ms
0.00.885.588 I llama_perf_context_print: prompt eval time =     133.75 ms /   128 tokens (    1.04 ms per token,   957.02 tokens per second)
0.00.885.590 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.885.590 I llama_perf_context_print:       total time =     142.15 ms /   129 tokens
0.00.885.948 I ggml_metal_free: deallocating

real	0m0.900s
user	0m0.079s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.170 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.293 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.995 I llama_model_loader: - type  f32:  194 tensors
0.00.025.996 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.996 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.996 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.997 I print_info: file format = GGUF V3 (latest)
0.00.026.001 I print_info: file type   = Q2_K - Medium
0.00.026.002 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.113 I load: special tokens cache size = 25
0.00.040.326 I load: token to piece cache size = 0.2984 MB
0.00.040.330 I print_info: arch             = gptneox
0.00.040.331 I print_info: vocab_only       = 0
0.00.040.331 I print_info: n_ctx_train      = 2048
0.00.040.331 I print_info: n_embd           = 2048
0.00.040.331 I print_info: n_layer          = 24
0.00.040.335 I print_info: n_head           = 16
0.00.040.336 I print_info: n_head_kv        = 16
0.00.040.336 I print_info: n_rot            = 32
0.00.040.337 I print_info: n_swa            = 0
0.00.040.337 I print_info: n_embd_head_k    = 128
0.00.040.337 I print_info: n_embd_head_v    = 128
0.00.040.338 I print_info: n_gqa            = 1
0.00.040.338 I print_info: n_embd_k_gqa     = 2048
0.00.040.339 I print_info: n_embd_v_gqa     = 2048
0.00.040.340 I print_info: f_norm_eps       = 1.0e-05
0.00.040.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.340 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.340 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.341 I print_info: f_logit_scale    = 0.0e+00
0.00.040.341 I print_info: n_ff             = 8192
0.00.040.341 I print_info: n_expert         = 0
0.00.040.341 I print_info: n_expert_used    = 0
0.00.040.343 I print_info: causal attn      = 1
0.00.040.343 I print_info: pooling type     = 0
0.00.040.343 I print_info: rope type        = 2
0.00.040.343 I print_info: rope scaling     = linear
0.00.040.343 I print_info: freq_base_train  = 10000.0
0.00.040.344 I print_info: freq_scale_train = 1
0.00.040.344 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.344 I print_info: rope_finetuned   = unknown
0.00.040.346 I print_info: ssm_d_conv       = 0
0.00.040.346 I print_info: ssm_d_inner      = 0
0.00.040.346 I print_info: ssm_d_state      = 0
0.00.040.346 I print_info: ssm_dt_rank      = 0
0.00.040.346 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.347 I print_info: model type       = 1.4B
0.00.040.347 I print_info: model params     = 1.41 B
0.00.040.347 I print_info: general.name     = 1.4B
0.00.040.347 I print_info: vocab type       = BPE
0.00.040.348 I print_info: n_vocab          = 50304
0.00.040.348 I print_info: n_merges         = 50009
0.00.040.348 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.349 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.349 I print_info: LF token         = 187 'Ċ'
0.00.040.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.349 I print_info: max token length = 1024
0.00.040.350 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.392.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.392.536 I load_tensors: offloading output layer to GPU
0.00.392.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.392.573 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.392.575 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.394.190 I llama_init_from_model: n_seq_max     = 1
0.00.394.193 I llama_init_from_model: n_ctx         = 128
0.00.394.193 I llama_init_from_model: n_ctx_per_seq = 128
0.00.394.194 I llama_init_from_model: n_batch       = 128
0.00.394.194 I llama_init_from_model: n_ubatch      = 128
0.00.394.195 I llama_init_from_model: flash_attn    = 0
0.00.394.197 I llama_init_from_model: freq_base     = 10000.0
0.00.394.197 I llama_init_from_model: freq_scale    = 1
0.00.394.198 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.394.200 I ggml_metal_init: allocating
0.00.394.282 I ggml_metal_init: found device: Apple M4
0.00.394.297 I ggml_metal_init: picking default device: Apple M4
0.00.396.058 I ggml_metal_init: using embedded metal library
0.00.401.480 I ggml_metal_init: GPU name:   Apple M4
0.00.401.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.401.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.401.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.401.495 I ggml_metal_init: simdgroup reduction   = true
0.00.401.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.401.496 I ggml_metal_init: has residency sets    = true
0.00.401.496 I ggml_metal_init: has bfloat            = true
0.00.401.497 I ggml_metal_init: use bfloat            = true
0.00.401.499 I ggml_metal_init: hasUnifiedMemory      = true
0.00.401.503 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.424.156 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.427.787 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.427.792 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.427.834 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.431.326 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.431.328 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.431.329 I llama_init_from_model: graph nodes  = 967
0.00.431.330 I llama_init_from_model: graph splits = 2
0.00.431.333 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.431.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.460.576 I 
0.00.460.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.460.664 I perplexity: tokenizing the input ..
0.00.467.768 I perplexity: tokenization took 7.1 ms
0.00.467.791 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.652 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.601.188 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.601.201 I llama_perf_context_print:        load time =     450.40 ms
0.00.601.202 I llama_perf_context_print: prompt eval time =     131.11 ms /   128 tokens (    1.02 ms per token,   976.30 tokens per second)
0.00.601.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.601.204 I llama_perf_context_print:       total time =     140.63 ms /   129 tokens
0.00.601.585 I ggml_metal_free: deallocating

real	0m0.617s
user	0m0.084s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.270 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.764 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.775 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.776 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.776 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.777 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.779 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.577 I llama_model_loader: - type  f32:  194 tensors
0.00.025.577 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.578 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.578 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.579 I print_info: file format = GGUF V3 (latest)
0.00.025.579 I print_info: file type   = Q3_K - Medium
0.00.025.581 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.024 I load: special tokens cache size = 25
0.00.040.168 I load: token to piece cache size = 0.2984 MB
0.00.040.173 I print_info: arch             = gptneox
0.00.040.173 I print_info: vocab_only       = 0
0.00.040.174 I print_info: n_ctx_train      = 2048
0.00.040.174 I print_info: n_embd           = 2048
0.00.040.174 I print_info: n_layer          = 24
0.00.040.179 I print_info: n_head           = 16
0.00.040.179 I print_info: n_head_kv        = 16
0.00.040.180 I print_info: n_rot            = 32
0.00.040.180 I print_info: n_swa            = 0
0.00.040.180 I print_info: n_embd_head_k    = 128
0.00.040.180 I print_info: n_embd_head_v    = 128
0.00.040.181 I print_info: n_gqa            = 1
0.00.040.182 I print_info: n_embd_k_gqa     = 2048
0.00.040.183 I print_info: n_embd_v_gqa     = 2048
0.00.040.183 I print_info: f_norm_eps       = 1.0e-05
0.00.040.184 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.184 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.184 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.184 I print_info: f_logit_scale    = 0.0e+00
0.00.040.185 I print_info: n_ff             = 8192
0.00.040.185 I print_info: n_expert         = 0
0.00.040.185 I print_info: n_expert_used    = 0
0.00.040.185 I print_info: causal attn      = 1
0.00.040.185 I print_info: pooling type     = 0
0.00.040.185 I print_info: rope type        = 2
0.00.040.185 I print_info: rope scaling     = linear
0.00.040.186 I print_info: freq_base_train  = 10000.0
0.00.040.186 I print_info: freq_scale_train = 1
0.00.040.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.186 I print_info: rope_finetuned   = unknown
0.00.040.187 I print_info: ssm_d_conv       = 0
0.00.040.187 I print_info: ssm_d_inner      = 0
0.00.040.187 I print_info: ssm_d_state      = 0
0.00.040.187 I print_info: ssm_dt_rank      = 0
0.00.040.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.187 I print_info: model type       = 1.4B
0.00.040.188 I print_info: model params     = 1.41 B
0.00.040.188 I print_info: general.name     = 1.4B
0.00.040.188 I print_info: vocab type       = BPE
0.00.040.188 I print_info: n_vocab          = 50304
0.00.040.189 I print_info: n_merges         = 50009
0.00.040.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.189 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.189 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.191 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.192 I print_info: LF token         = 187 'Ċ'
0.00.040.192 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.192 I print_info: max token length = 1024
0.00.040.192 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.522.679 I load_tensors: offloading 24 repeating layers to GPU
0.00.522.693 I load_tensors: offloading output layer to GPU
0.00.522.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.522.726 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.522.728 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.524.310 I llama_init_from_model: n_seq_max     = 1
0.00.524.312 I llama_init_from_model: n_ctx         = 128
0.00.524.313 I llama_init_from_model: n_ctx_per_seq = 128
0.00.524.313 I llama_init_from_model: n_batch       = 128
0.00.524.314 I llama_init_from_model: n_ubatch      = 128
0.00.524.314 I llama_init_from_model: flash_attn    = 0
0.00.524.316 I llama_init_from_model: freq_base     = 10000.0
0.00.524.316 I llama_init_from_model: freq_scale    = 1
0.00.524.317 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.524.319 I ggml_metal_init: allocating
0.00.524.421 I ggml_metal_init: found device: Apple M4
0.00.524.435 I ggml_metal_init: picking default device: Apple M4
0.00.526.280 I ggml_metal_init: using embedded metal library
0.00.532.231 I ggml_metal_init: GPU name:   Apple M4
0.00.532.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.532.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.532.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.532.251 I ggml_metal_init: simdgroup reduction   = true
0.00.532.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.532.252 I ggml_metal_init: has residency sets    = true
0.00.532.253 I ggml_metal_init: has bfloat            = true
0.00.532.253 I ggml_metal_init: use bfloat            = true
0.00.532.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.532.266 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.112 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.781 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.556.786 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.556.841 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.560.093 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.560.095 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.560.095 I llama_init_from_model: graph nodes  = 967
0.00.560.096 I llama_init_from_model: graph splits = 2
0.00.560.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.560.099 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.580 I 
0.00.589.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.670 I perplexity: tokenizing the input ..
0.00.596.167 I perplexity: tokenization took 6.496 ms
0.00.596.183 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.191 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.741.706 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.741.722 I llama_perf_context_print:        load time =     580.30 ms
0.00.741.723 I llama_perf_context_print: prompt eval time =     143.72 ms /   128 tokens (    1.12 ms per token,   890.63 tokens per second)
0.00.741.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.724 I llama_perf_context_print:       total time =     152.15 ms /   129 tokens
0.00.742.106 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.082s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.349 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.606 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.610 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.614 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.616 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.616 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.520 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.393 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.394 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.394 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.394 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.395 I llama_model_loader: - type  f32:  194 tensors
0.00.025.395 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.396 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.396 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.397 I print_info: file format = GGUF V3 (latest)
0.00.025.397 I print_info: file type   = Q4_K - Medium
0.00.025.398 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.534 I load: special tokens cache size = 25
0.00.039.664 I load: token to piece cache size = 0.2984 MB
0.00.039.669 I print_info: arch             = gptneox
0.00.039.669 I print_info: vocab_only       = 0
0.00.039.669 I print_info: n_ctx_train      = 2048
0.00.039.669 I print_info: n_embd           = 2048
0.00.039.670 I print_info: n_layer          = 24
0.00.039.674 I print_info: n_head           = 16
0.00.039.675 I print_info: n_head_kv        = 16
0.00.039.675 I print_info: n_rot            = 32
0.00.039.678 I print_info: n_swa            = 0
0.00.039.678 I print_info: n_embd_head_k    = 128
0.00.039.678 I print_info: n_embd_head_v    = 128
0.00.039.679 I print_info: n_gqa            = 1
0.00.039.680 I print_info: n_embd_k_gqa     = 2048
0.00.039.680 I print_info: n_embd_v_gqa     = 2048
0.00.039.681 I print_info: f_norm_eps       = 1.0e-05
0.00.039.681 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.681 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.682 I print_info: f_logit_scale    = 0.0e+00
0.00.039.682 I print_info: n_ff             = 8192
0.00.039.683 I print_info: n_expert         = 0
0.00.039.683 I print_info: n_expert_used    = 0
0.00.039.684 I print_info: causal attn      = 1
0.00.039.684 I print_info: pooling type     = 0
0.00.039.684 I print_info: rope type        = 2
0.00.039.684 I print_info: rope scaling     = linear
0.00.039.684 I print_info: freq_base_train  = 10000.0
0.00.039.685 I print_info: freq_scale_train = 1
0.00.039.685 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.685 I print_info: rope_finetuned   = unknown
0.00.039.686 I print_info: ssm_d_conv       = 0
0.00.039.686 I print_info: ssm_d_inner      = 0
0.00.039.686 I print_info: ssm_d_state      = 0
0.00.039.686 I print_info: ssm_dt_rank      = 0
0.00.039.686 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.686 I print_info: model type       = 1.4B
0.00.039.687 I print_info: model params     = 1.41 B
0.00.039.687 I print_info: general.name     = 1.4B
0.00.039.687 I print_info: vocab type       = BPE
0.00.039.687 I print_info: n_vocab          = 50304
0.00.039.687 I print_info: n_merges         = 50009
0.00.039.688 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.688 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.689 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.690 I print_info: LF token         = 187 'Ċ'
0.00.039.690 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.690 I print_info: max token length = 1024
0.00.039.691 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.516.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.947 I load_tensors: offloading output layer to GPU
0.00.516.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.984 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.985 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.518.403 I llama_init_from_model: n_seq_max     = 1
0.00.518.405 I llama_init_from_model: n_ctx         = 128
0.00.518.406 I llama_init_from_model: n_ctx_per_seq = 128
0.00.518.406 I llama_init_from_model: n_batch       = 128
0.00.518.407 I llama_init_from_model: n_ubatch      = 128
0.00.518.407 I llama_init_from_model: flash_attn    = 0
0.00.518.410 I llama_init_from_model: freq_base     = 10000.0
0.00.518.410 I llama_init_from_model: freq_scale    = 1
0.00.518.411 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.518.413 I ggml_metal_init: allocating
0.00.518.543 I ggml_metal_init: found device: Apple M4
0.00.518.558 I ggml_metal_init: picking default device: Apple M4
0.00.520.526 I ggml_metal_init: using embedded metal library
0.00.526.443 I ggml_metal_init: GPU name:   Apple M4
0.00.526.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.453 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.455 I ggml_metal_init: simdgroup reduction   = true
0.00.526.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.456 I ggml_metal_init: has residency sets    = true
0.00.526.456 I ggml_metal_init: has bfloat            = true
0.00.526.456 I ggml_metal_init: use bfloat            = true
0.00.526.457 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.462 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.800 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.300 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.549.304 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.549.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.552.669 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.552.671 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.552.671 I llama_init_from_model: graph nodes  = 967
0.00.552.671 I llama_init_from_model: graph splits = 2
0.00.552.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.552.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.817 I 
0.00.583.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.905 I perplexity: tokenizing the input ..
0.00.591.094 I perplexity: tokenization took 7.187 ms
0.00.591.121 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.028 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.739.643 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.739.661 I llama_perf_context_print:        load time =     574.46 ms
0.00.739.662 I llama_perf_context_print: prompt eval time =     145.91 ms /   128 tokens (    1.14 ms per token,   877.27 tokens per second)
0.00.739.663 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.663 I llama_perf_context_print:       total time =     155.85 ms /   129 tokens
0.00.740.016 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.081s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.002 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.004 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.004 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.005 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.006 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.825 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.826 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.826 I llama_model_loader: - type  f32:  194 tensors
0.00.025.827 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.827 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.828 I print_info: file format = GGUF V3 (latest)
0.00.025.828 I print_info: file type   = Q5_K - Medium
0.00.025.829 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.040 I load: special tokens cache size = 25
0.00.040.158 I load: token to piece cache size = 0.2984 MB
0.00.040.162 I print_info: arch             = gptneox
0.00.040.162 I print_info: vocab_only       = 0
0.00.040.163 I print_info: n_ctx_train      = 2048
0.00.040.163 I print_info: n_embd           = 2048
0.00.040.163 I print_info: n_layer          = 24
0.00.040.167 I print_info: n_head           = 16
0.00.040.168 I print_info: n_head_kv        = 16
0.00.040.168 I print_info: n_rot            = 32
0.00.040.168 I print_info: n_swa            = 0
0.00.040.169 I print_info: n_embd_head_k    = 128
0.00.040.169 I print_info: n_embd_head_v    = 128
0.00.040.169 I print_info: n_gqa            = 1
0.00.040.170 I print_info: n_embd_k_gqa     = 2048
0.00.040.173 I print_info: n_embd_v_gqa     = 2048
0.00.040.174 I print_info: f_norm_eps       = 1.0e-05
0.00.040.174 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.174 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.174 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.174 I print_info: f_logit_scale    = 0.0e+00
0.00.040.175 I print_info: n_ff             = 8192
0.00.040.175 I print_info: n_expert         = 0
0.00.040.176 I print_info: n_expert_used    = 0
0.00.040.176 I print_info: causal attn      = 1
0.00.040.176 I print_info: pooling type     = 0
0.00.040.176 I print_info: rope type        = 2
0.00.040.177 I print_info: rope scaling     = linear
0.00.040.177 I print_info: freq_base_train  = 10000.0
0.00.040.177 I print_info: freq_scale_train = 1
0.00.040.177 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.178 I print_info: rope_finetuned   = unknown
0.00.040.178 I print_info: ssm_d_conv       = 0
0.00.040.178 I print_info: ssm_d_inner      = 0
0.00.040.178 I print_info: ssm_d_state      = 0
0.00.040.178 I print_info: ssm_dt_rank      = 0
0.00.040.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.178 I print_info: model type       = 1.4B
0.00.040.179 I print_info: model params     = 1.41 B
0.00.040.179 I print_info: general.name     = 1.4B
0.00.040.179 I print_info: vocab type       = BPE
0.00.040.179 I print_info: n_vocab          = 50304
0.00.040.180 I print_info: n_merges         = 50009
0.00.040.180 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: LF token         = 187 'Ċ'
0.00.040.181 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.181 I print_info: max token length = 1024
0.00.040.181 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.309 I load_tensors: offloading output layer to GPU
0.00.586.309 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.346 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.348 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.587.930 I llama_init_from_model: n_seq_max     = 1
0.00.587.933 I llama_init_from_model: n_ctx         = 128
0.00.587.934 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.934 I llama_init_from_model: n_batch       = 128
0.00.587.935 I llama_init_from_model: n_ubatch      = 128
0.00.587.935 I llama_init_from_model: flash_attn    = 0
0.00.587.938 I llama_init_from_model: freq_base     = 10000.0
0.00.587.938 I llama_init_from_model: freq_scale    = 1
0.00.587.939 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.941 I ggml_metal_init: allocating
0.00.587.998 I ggml_metal_init: found device: Apple M4
0.00.588.011 I ggml_metal_init: picking default device: Apple M4
0.00.589.621 I ggml_metal_init: using embedded metal library
0.00.597.182 I ggml_metal_init: GPU name:   Apple M4
0.00.597.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.193 I ggml_metal_init: simdgroup reduction   = true
0.00.597.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.193 I ggml_metal_init: has residency sets    = true
0.00.597.193 I ggml_metal_init: has bfloat            = true
0.00.597.194 I ggml_metal_init: use bfloat            = true
0.00.597.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.006 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.597 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.601 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.646 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.717 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.719 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.719 I llama_init_from_model: graph nodes  = 967
0.00.621.720 I llama_init_from_model: graph splits = 2
0.00.621.723 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.891 I 
0.00.651.979 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.986 I perplexity: tokenizing the input ..
0.00.657.430 I perplexity: tokenization took 5.442 ms
0.00.657.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.262 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.794.798 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.794.813 I llama_perf_context_print:        load time =     642.04 ms
0.00.794.814 I llama_perf_context_print: prompt eval time =     135.59 ms /   128 tokens (    1.06 ms per token,   944.04 tokens per second)
0.00.794.814 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.815 I llama_perf_context_print:       total time =     142.93 ms /   129 tokens
0.00.795.204 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.079s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.815 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.825 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.828 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.832 I llama_model_loader: - type  f32:  194 tensors
0.00.026.833 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.833 I print_info: file format = GGUF V3 (latest)
0.00.026.834 I print_info: file type   = Q6_K
0.00.026.835 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.484 I load: special tokens cache size = 25
0.00.041.518 I load: token to piece cache size = 0.2984 MB
0.00.041.522 I print_info: arch             = gptneox
0.00.041.522 I print_info: vocab_only       = 0
0.00.041.522 I print_info: n_ctx_train      = 2048
0.00.041.523 I print_info: n_embd           = 2048
0.00.041.523 I print_info: n_layer          = 24
0.00.041.527 I print_info: n_head           = 16
0.00.041.527 I print_info: n_head_kv        = 16
0.00.041.528 I print_info: n_rot            = 32
0.00.041.528 I print_info: n_swa            = 0
0.00.041.532 I print_info: n_embd_head_k    = 128
0.00.041.532 I print_info: n_embd_head_v    = 128
0.00.041.532 I print_info: n_gqa            = 1
0.00.041.533 I print_info: n_embd_k_gqa     = 2048
0.00.041.534 I print_info: n_embd_v_gqa     = 2048
0.00.041.534 I print_info: f_norm_eps       = 1.0e-05
0.00.041.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.535 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.535 I print_info: f_logit_scale    = 0.0e+00
0.00.041.536 I print_info: n_ff             = 8192
0.00.041.536 I print_info: n_expert         = 0
0.00.041.536 I print_info: n_expert_used    = 0
0.00.041.536 I print_info: causal attn      = 1
0.00.041.536 I print_info: pooling type     = 0
0.00.041.537 I print_info: rope type        = 2
0.00.041.537 I print_info: rope scaling     = linear
0.00.041.537 I print_info: freq_base_train  = 10000.0
0.00.041.537 I print_info: freq_scale_train = 1
0.00.041.538 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.538 I print_info: rope_finetuned   = unknown
0.00.041.538 I print_info: ssm_d_conv       = 0
0.00.041.539 I print_info: ssm_d_inner      = 0
0.00.041.540 I print_info: ssm_d_state      = 0
0.00.041.540 I print_info: ssm_dt_rank      = 0
0.00.041.540 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.540 I print_info: model type       = 1.4B
0.00.041.540 I print_info: model params     = 1.41 B
0.00.041.541 I print_info: general.name     = 1.4B
0.00.041.542 I print_info: vocab type       = BPE
0.00.041.542 I print_info: n_vocab          = 50304
0.00.041.542 I print_info: n_merges         = 50009
0.00.041.543 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.543 I print_info: LF token         = 187 'Ċ'
0.00.041.544 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.544 I print_info: max token length = 1024
0.00.041.548 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.762 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.778 I load_tensors: offloading output layer to GPU
0.00.598.778 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.815 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.598.816 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.600.376 I llama_init_from_model: n_seq_max     = 1
0.00.600.380 I llama_init_from_model: n_ctx         = 128
0.00.600.380 I llama_init_from_model: n_ctx_per_seq = 128
0.00.600.381 I llama_init_from_model: n_batch       = 128
0.00.600.381 I llama_init_from_model: n_ubatch      = 128
0.00.600.382 I llama_init_from_model: flash_attn    = 0
0.00.600.384 I llama_init_from_model: freq_base     = 10000.0
0.00.600.385 I llama_init_from_model: freq_scale    = 1
0.00.600.385 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.388 I ggml_metal_init: allocating
0.00.600.460 I ggml_metal_init: found device: Apple M4
0.00.600.474 I ggml_metal_init: picking default device: Apple M4
0.00.602.035 I ggml_metal_init: using embedded metal library
0.00.608.310 I ggml_metal_init: GPU name:   Apple M4
0.00.608.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.316 I ggml_metal_init: simdgroup reduction   = true
0.00.608.316 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.317 I ggml_metal_init: has residency sets    = true
0.00.608.317 I ggml_metal_init: has bfloat            = true
0.00.608.317 I ggml_metal_init: use bfloat            = true
0.00.608.318 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.851 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.195 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.201 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.250 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.323 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.324 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.325 I llama_init_from_model: graph nodes  = 967
0.00.632.325 I llama_init_from_model: graph splits = 2
0.00.632.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.082 I 
0.00.670.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.171 I perplexity: tokenizing the input ..
0.00.676.852 I perplexity: tokenization took 6.677 ms
0.00.676.877 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.228 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.810.772 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.810.795 I llama_perf_context_print:        load time =     661.26 ms
0.00.810.796 I llama_perf_context_print: prompt eval time =     131.41 ms /   128 tokens (    1.03 ms per token,   974.02 tokens per second)
0.00.810.797 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.797 I llama_perf_context_print:       total time =     140.72 ms /   129 tokens
0.00.811.183 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.081s
sys	0m0.131s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.288 I build: 4805 (d5c63cd7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.387 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.257 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.849 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.854 I llama_model_loader: - type  f32:  194 tensors
0.00.055.854 I llama_model_loader: - type  f16:   98 tensors
0.00.055.855 I print_info: file format = GGUF V3 (latest)
0.00.055.856 I print_info: file type   = all F32 (guessed)
0.00.055.857 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.612 I load: special tokens cache size = 25
0.00.076.896 I load: token to piece cache size = 0.2984 MB
0.00.076.899 I print_info: arch             = gptneox
0.00.076.899 I print_info: vocab_only       = 0
0.00.076.899 I print_info: n_ctx_train      = 2048
0.00.076.899 I print_info: n_embd           = 2048
0.00.076.900 I print_info: n_layer          = 24
0.00.076.903 I print_info: n_head           = 16
0.00.076.904 I print_info: n_head_kv        = 16
0.00.076.904 I print_info: n_rot            = 32
0.00.076.904 I print_info: n_swa            = 0
0.00.076.904 I print_info: n_embd_head_k    = 128
0.00.076.904 I print_info: n_embd_head_v    = 128
0.00.076.905 I print_info: n_gqa            = 1
0.00.076.906 I print_info: n_embd_k_gqa     = 2048
0.00.076.907 I print_info: n_embd_v_gqa     = 2048
0.00.076.907 I print_info: f_norm_eps       = 1.0e-05
0.00.076.908 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.908 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.909 I print_info: f_logit_scale    = 0.0e+00
0.00.076.910 I print_info: n_ff             = 8192
0.00.076.910 I print_info: n_expert         = 0
0.00.076.910 I print_info: n_expert_used    = 0
0.00.076.910 I print_info: causal attn      = 1
0.00.076.910 I print_info: pooling type     = 0
0.00.076.910 I print_info: rope type        = 2
0.00.076.912 I print_info: rope scaling     = linear
0.00.076.913 I print_info: freq_base_train  = 10000.0
0.00.076.913 I print_info: freq_scale_train = 1
0.00.076.913 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.913 I print_info: rope_finetuned   = unknown
0.00.076.913 I print_info: ssm_d_conv       = 0
0.00.076.913 I print_info: ssm_d_inner      = 0
0.00.076.914 I print_info: ssm_d_state      = 0
0.00.076.914 I print_info: ssm_dt_rank      = 0
0.00.076.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.914 I print_info: model type       = 1.4B
0.00.076.915 I print_info: model params     = 1.41 B
0.00.076.915 I print_info: general.name     = 1.4B
0.00.076.915 I print_info: vocab type       = BPE
0.00.076.915 I print_info: n_vocab          = 50304
0.00.076.916 I print_info: n_merges         = 50009
0.00.076.916 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.916 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.917 I print_info: LF token         = 187 'Ċ'
0.00.076.917 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.917 I print_info: max token length = 1024
0.00.076.918 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.295.610 I load_tensors: offloading 24 repeating layers to GPU
0.01.295.614 I load_tensors: offloading output layer to GPU
0.01.295.615 I load_tensors: offloaded 25/25 layers to GPU
0.01.295.644 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.295.645 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.296.882 I llama_init_from_model: n_seq_max     = 1
0.01.296.884 I llama_init_from_model: n_ctx         = 128
0.01.296.884 I llama_init_from_model: n_ctx_per_seq = 128
0.01.296.884 I llama_init_from_model: n_batch       = 128
0.01.296.884 I llama_init_from_model: n_ubatch      = 128
0.01.296.885 I llama_init_from_model: flash_attn    = 0
0.01.296.885 I llama_init_from_model: freq_base     = 10000.0
0.01.296.886 I llama_init_from_model: freq_scale    = 1
0.01.296.886 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.296.890 I ggml_metal_init: allocating
0.01.296.986 I ggml_metal_init: found device: Apple M4
0.01.297.000 I ggml_metal_init: picking default device: Apple M4
0.01.298.228 I ggml_metal_init: using embedded metal library
0.01.302.139 I ggml_metal_init: GPU name:   Apple M4
0.01.302.142 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.302.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.302.143 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.302.143 I ggml_metal_init: simdgroup reduction   = true
0.01.302.143 I ggml_metal_init: simdgroup matrix mul. = true
0.01.302.143 I ggml_metal_init: has residency sets    = true
0.01.302.143 I ggml_metal_init: has bfloat            = true
0.01.302.144 I ggml_metal_init: use bfloat            = true
0.01.302.144 I ggml_metal_init: hasUnifiedMemory      = true
0.01.302.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.313.149 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.314.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.314.863 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.314.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.316.558 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.316.559 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.316.559 I llama_init_from_model: graph nodes  = 967
0.01.316.560 I llama_init_from_model: graph splits = 2
0.01.316.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.316.561 I 
0.01.316.596 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.316.597 I compute_imatrix: tokenizing the input ..
0.01.320.743 I compute_imatrix: tokenization took 4.145 ms
0.01.320.745 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.584.105 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.587.579 I llama_perf_context_print:        load time =    1560.85 ms
0.01.587.580 I llama_perf_context_print: prompt eval time =     261.62 ms /   128 tokens (    2.04 ms per token,   489.26 tokens per second)
0.01.587.581 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.587.581 I llama_perf_context_print:       total time =    1564.33 ms /   129 tokens
0.01.588.085 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.129s
sys	0m0.264s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4805 (d5c63cd7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ea058d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ea05f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ea063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ea09030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ea094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ea09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ea09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ea0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ea0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ea0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ea0b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ea0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ea0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ea0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ea0d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ea0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ea0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ea0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ea0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ea0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ea0ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ea10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ea10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ea11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ea11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ea12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ea12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ea132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ea137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ea13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ea13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ea14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ea14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ea14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ea152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ea15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ea15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ea16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ea16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ea169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ea16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ea17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ea177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ea17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ea17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ea18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ea18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ea19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ea19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ea1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ea1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ea1ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ea1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ea1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ea1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ea1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ea1c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ea1cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ea1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ea1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ea1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ea1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ea1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ea1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ea1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ea1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ea1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ea1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ea20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ea206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ea20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ea21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ea214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ea21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ea21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ea224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ea22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ea22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ea234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ea23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ea23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ea244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ea249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ea24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ea25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ea259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ea25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ea26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ea269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ea26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ea27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ea279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ea27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ea28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ea289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ea28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ea29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ea19130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ea298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ea2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ea2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ea2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ea2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ea2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ea2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ea2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ea2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ea2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ea2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ea2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ea2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ea2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ea2e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ea2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ea2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ea2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ea2f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ea2fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ea30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ea305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ea30a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ea30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ea313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ea31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ea31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ea321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ea32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ea32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ea32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ea33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ea338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ea33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ea34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ea346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ea34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ea34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ea35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ea35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ea35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ea36260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ea36700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ea36ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ea37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ea374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ea37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ea37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ea382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ea38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ea38c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ea390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ea39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ea399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ea39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ea3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ea3a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ea3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ea3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ea3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ea3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ea3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ea3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ea3c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ea3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ea3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ea3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ea3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ea3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ea3e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ea3e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ea3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ea3f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ea3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ea3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ea3ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ea40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ea408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ea40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ea41220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ea416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ea41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ea42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ea424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ea42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ea42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ea43280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ea43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ea43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ea44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ea44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ea449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ea44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ea452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ea45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ea45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ea46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ea46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ea46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ea46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ea47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ea47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ea481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ea489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ea48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ea49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ea49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ea49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ea4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ea4a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ea4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ea4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ea4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ea4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ea4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ea4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ea4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ea4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ea4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ea4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ea4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ea4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ea4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ea4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ea4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ea4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ea50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ea50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ea50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ea514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ea51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ea51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ea524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ea52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ea52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ea534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ea53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ea53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ea544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ea54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ea54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ea554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ea55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ea55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ea564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ea569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ea56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ea57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ea579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ea57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ea58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ea589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ea58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ea59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ea599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ea59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ea5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ea5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ea5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ea5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ea5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ea5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ea5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ea5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ea5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ea5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ea5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ea5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ea5e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ea5e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ea5ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ea5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ea5f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ea5fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ea5ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ea60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ea60920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ea60dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ea61260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ea61700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ea61ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ea62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ea624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ea62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ea62e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ea632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ea63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ea63c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ea640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ea64540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ea649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ea64e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ea65320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ea657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ea65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ea66430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ea66b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ea67270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ea67990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ea67c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ea68440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ea68700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ea68d10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.673.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.673.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e804bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e805030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e8054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e805910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e805d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e8061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e806660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e806ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e806f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e8073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e807820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e807ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e808a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e8091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e8099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e80a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e80a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e80af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e80b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e80be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e80c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e80cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e80d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e80da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e80e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e80e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e80e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e80eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e80f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e80f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e80f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e80fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e810290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e810550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e8109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e810e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e8112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e811710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e811b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e811ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e812460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e8128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e812d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e8131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e813620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e813a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e813f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e814370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e8147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e814c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e8150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e815530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e8159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e815e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e816280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e8166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e816c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e817160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e8175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e817a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e817eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e818320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e818790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e818c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e819070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e8194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e819950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e819dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e81a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e81a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e81ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e81af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e81b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e81b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e81bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e81c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e81c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e81ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e81ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e81d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e81d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e81dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e81e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e81e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e81e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e81eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e81f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e81f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e81faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e81ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e8203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e820840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e820cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e821120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e821590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e821a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e821e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e8222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e822750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e822bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e823030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e8234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e823910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e823d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e8241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e824660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e824ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e824f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e8253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e825c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e826100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e826570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e8269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e826e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e8272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e827730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e827ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e828010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e828480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e8288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e828d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e8291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e829640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e829ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e829f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e82a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e82a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e82ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e82b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e82b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e82b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e82be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e82c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e82c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e82cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e82cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e82d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e82d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e82dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e82e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e82e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e82ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e82ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e82f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e82f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e82fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e8300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e830530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e8309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e830e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e831280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e8316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e831b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e831fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e832440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e8328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e832d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e833190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e833600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e833a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e833ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e834350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e8347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e834c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e8350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e835cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e835f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e836250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e8366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e836b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e836fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e837410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e837880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e837cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e838160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e8385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e838a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e838eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e839320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e839790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e839c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e83a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e83a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e83a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e83adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e83b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e83b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e83bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e83bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e83c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e83c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e83ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e83d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e83d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e83da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e83de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e83e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e83e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e83ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e83f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e83f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e83fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e83ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e8403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e840810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e840c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e8410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e841610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e841b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e842690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e842950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e842f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e8434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e843a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e844050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e844610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e844bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e845190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e845750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e845d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e8462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e846890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e846e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e847410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e8479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e847f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e848550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e848b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e8490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e849690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e849c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e84a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e84a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e84ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e84b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e84b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e84bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e84c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e84ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e84d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e84d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e84db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e84e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e84e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e84ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e84f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e84f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e84fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e8503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e850990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e850f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e851510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e851ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e852090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e852c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e8531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e853790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e853d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e854310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e8548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e854e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e855450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e855a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e855fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e856590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e856b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e857050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e857550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e857a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e857f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e858450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e858950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e858e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e859350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e859850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e859d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e85a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e85a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e85ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e85b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e85b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e85bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e85c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e85c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e85ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e85cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e85d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e85d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e85de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e85e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e85e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e85f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e85f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e8600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e8607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e860a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e861270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e861530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e861b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ea689c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ea47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ea47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ea47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ea1af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ea1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ea1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ea122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ea18de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ea19700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ea19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ea187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ea1b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ea112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ea1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ea1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ea29b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ea67f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ea144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ea14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ea48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ea12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ea12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ea12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ea69170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ea69430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ea696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ea699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ea69c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ea69f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ea6a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ea6a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ea6a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ea6aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ea6acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ea6afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ea6b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ea6b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ea6b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ea6bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ea6bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ea6c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ea6c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ea6c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ea6c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ea6cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ea6cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ea6d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ea6d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ea6d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ea6d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ea6dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ea6de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ea6e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ea6e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ea6e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ea6e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ea6ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ea6eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ea6f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ea6f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ea6f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ea6f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ea6fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ea6ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ea70230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ea704f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ea707b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ea70a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ea70d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ea70ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ea712b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ea71570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ea71830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ea71af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ea71db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ea72070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ea72330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ea725f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ea728b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ea72b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ea72e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ea730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ea733b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ea73670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ea73930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ea73bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ea73eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ea74170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ea74430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ea746f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ea749b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ea74c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ea74f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ea751f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ea754b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ea75770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ea75a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ea75cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ea75fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ea76270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ea76530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ea767f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ea76ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ea76d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ea77030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ea772f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ea775b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ea77870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ea77b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ea77df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ea780b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ea78370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ea78630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ea788f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ea78bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ea78e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ea79130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ea793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ea796b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ea79970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ea79c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ea79ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ea7a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ea7a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ea7a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ea7a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ea7acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ea7af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ea7b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ea7b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ea7b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ea7ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ea7bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ea7bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ea7c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ea7c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ea7c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ea7caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ea7cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ea7d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ea7d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ea7d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ea7d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ea7db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ea7de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ea7e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ea7e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ea7e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ea7e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ea7ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ea7eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ea7f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ea7f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ea7f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ea7f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ea7fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ea7ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ea801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ea804b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ea80770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ea80a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ea80cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ea80fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ea81270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ea81530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ea817f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ea81ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ea81d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ea82030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ea822f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ea825b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ea82870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ea82b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ea82df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ea830b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ea83370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ea83630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ea838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ea83bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ea83e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ea84130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ea843f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ea846b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ea84970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ea84c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ea84ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ea851b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ea85470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ea85730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ea859f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ea85cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ea85f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ea86230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ea864f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ea867b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ea86a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ea86d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ea86ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ea872b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ea87570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ea87830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ea87af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ea87db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ea88070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ea88330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ea885f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ea888b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ea88b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ea88e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ea89400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ea896c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ea89980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ea89c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ea8a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ea8a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ea8ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ea8b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ea8b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ea8bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ea8c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ea8c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ea8cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ea8d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ea8d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ea8dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ea8e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ea8e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ea8ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ea8f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ea8f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ea8fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ea90130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ea90680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ea90bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ea91120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ea91670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ea91bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ea92110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ea92660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ea92bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ea93100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ea93650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ea93ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ea940f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ea94640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ea94b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ea950e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ea95630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ea95b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ea960d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ea96620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ea96b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ea970c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ea97610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ea97b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ea980b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ea98600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ea98b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ea990a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ea995f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ea99b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ea9a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ea9a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ea9ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ea9b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ea9b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ea9b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ea9bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ea9be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ea9c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ea9c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ea9cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ea9cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ea9d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ea9d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ea9dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ea9e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ea9e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ea9ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ea9eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ea9f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ea9f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ea9fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13eaa00a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13eaa0510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13eaa0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13eaa0df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13eaa1260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13eaa16d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13eaa1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13eaa1fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13eaa2420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13eaa2e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13eaa35a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13eaa3cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13eaa43e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13eaa46a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13eaa4b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13eaa5110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13eaa5720 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.734s
user	0m0.276s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4805 (d5c63cd7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132f0da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132f0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132f0e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132f0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132f0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132f103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132f10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132f10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132f11370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132f11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132f12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132f12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132f13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132f14190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132f148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132f14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132f157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132f15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132f16d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132f175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132f17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132f17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132f18590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132f19200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132f19740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132f19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132f19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132f1a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132f1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132f1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132f1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132f1bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132f1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132f1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132f1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132f1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132f1d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132f1db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132f1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132f1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x132f1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132f1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132f1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132f1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132f205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132f20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132f211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132f217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132f21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132f22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132f22be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132f239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132f23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132f24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132f245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132f24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132f24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132f253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132f25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132f25d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132f261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132f26640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132f26ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132f27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132f27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132f27ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132f28410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132f28960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132f28eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132f29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132f29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132f29ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132f2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132f2a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132f2ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132f2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132f2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132f2be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132f2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132f2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132f2d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132f2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132f2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132f2e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132f2e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132f2ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132f2f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132f1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132f2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132f2ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132f30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132f30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132f31500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132f31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132f324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132f32a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132f32f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132f334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x132f33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132f33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132f344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132f34970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132f34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132f352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132f35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132f35bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132f36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132f36530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132f369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132f36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132f37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132f377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132f37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132f380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132f38590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132f38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132f38ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132f39370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132f39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132f3a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132f3a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132f3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132f3af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132f3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132f3bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132f3c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132f3c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132f3caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132f3cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132f3d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132f3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132f3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132f3e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132f3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132f3eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132f3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132f3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132f3f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132f3fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132f40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132f40710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132f40bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132f41050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132f414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132f41990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132f41e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132f422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132f42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132f42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132f430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132f43550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132f439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132f43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132f44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132f447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132f44c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132f45110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132f455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132f45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132f45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132f46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132f46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132f46cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132f47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132f47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132f47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132f47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132f48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132f48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132f491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132f49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132f49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132f49fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132f4a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132f4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132f4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132f4b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132f4b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132f4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132f4c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132f4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132f4cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132f4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132f4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132f4daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132f4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132f4e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132f4ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132f4f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132f4f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132f4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132f50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132f50900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132f50da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132f51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132f519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132f529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x132f52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132f53480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132f539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x132f53f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132f54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132f549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132f54f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132f55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132f559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132f55f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132f56450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132f569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132f56ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132f57990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132f57ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132f58430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132f58980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132f58ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132f59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132f59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132f59ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132f5a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132f5a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132f5aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132f5b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132f5b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132f5bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132f5c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132f5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132f5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132f5d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132f5d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132f5de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132f5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132f5e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132f5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132f5f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132f5f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132f5fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132f603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132f60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132f60e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132f613a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132f618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132f61e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132f62390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132f628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132f62e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132f63380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132f638d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x132f63e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132f64370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132f64810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132f64cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132f65150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132f655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132f65a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132f65f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132f663d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132f66870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132f66d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132f671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132f67650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132f67af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132f67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132f68430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132f688d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x132f68d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x132f69210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x132f696b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x132f69b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x132f69ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x132f6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x132f6a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x132f6add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x132f6b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x132f6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132f6bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132f6c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132f6caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132f6d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132f6d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132f6dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132f6e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132f6e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132f6ec60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.104.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132e06730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132e06ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132e07010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132e095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132e09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132e09ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132e0a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132e0a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132e0ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132e0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132e0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132e0bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132e0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132e0cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132e0d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132e0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132e0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132e0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132e0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132e0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132e10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132e109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132e110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132e117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132e11f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132e121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132e12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132e12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132e12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132e131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132e13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132e13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132e142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132e14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132e14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132e15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132e15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132e158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132e15d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132e161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132e16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132e16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132e16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132e17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132e177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x132e17c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132e180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132e18540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132e189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132e18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132e19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132e19700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132e19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132e19fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132e1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132e1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132e1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132e1b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132e1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132e1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132e1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132e1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132e1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132e1cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132e1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132e1d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132e1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132e1df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132e1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132e1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132e1ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132e1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132e1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132e1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132e1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132e20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132e20780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132e20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132e21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132e214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132e21940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132e21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132e22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132e22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132e22b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132e22f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132e233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132e23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132e23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132e24130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132e245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132e24a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132e24e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132e252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132e25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132e25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132e26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132e264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132e26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132e26d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132e27200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132e27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132e27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132e27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132e283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132e28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132e28ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132e29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132e29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x132e299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132e29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132e2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132e2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132e2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132e2b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132e2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132e2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132e2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132e2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132e2c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132e2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132e2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132e2d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132e2d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132e2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132e2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132e2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132e2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132e2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132e2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132e2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132e2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132e30000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132e30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132e308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132e30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132e311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132e31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132e31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132e31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132e32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132e327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132e32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132e330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132e33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132e339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132e33e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132e34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132e34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132e34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132e34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132e35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132e358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132e35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132e361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132e36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132e36a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132e36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132e37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132e377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132e37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132e380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132e38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132e38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132e38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132e39a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132e39cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132e39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132e3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132e3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132e3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132e3b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132e3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132e3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132e3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132e3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132e3c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132e3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132e3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132e3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132e3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132e3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132e3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132e3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132e3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132e3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132e3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132e3fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132e40150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132e405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132e40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132e40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132e41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132e41780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132e41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132e42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132e424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132e42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132e42db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132e43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132e43780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132e43c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132e44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132e44570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132e449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132e44e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132e45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132e45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132e463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132e466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132e46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132e47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x132e477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132e47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132e48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x132e48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132e48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132e494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132e49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132e4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132e4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132e4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132e4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132e4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132e4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132e4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132e4c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132e4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132e4d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132e4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132e4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132e4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132e4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132e4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132e4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132e501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132e507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132e50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132e51330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132e518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132e51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132e52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132e52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132e52ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132e535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132e53b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132e54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132e546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132e54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132e55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132e55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132e55df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132e563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132e56970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132e56f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132e574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132e57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132e58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132e58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132e58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132e591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132e59770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x132e59d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132e5a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132e5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132e5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132e5b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132e5b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132e5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132e5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132e5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132e5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132e5d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132e5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132e5dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132e5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132e5e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132e5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132e5eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x132e5f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x132e5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x132e5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x132e602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x132e607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x132e60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x132e611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x132e616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x132e61bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x132e620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132e625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132e62fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132e636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132e63e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132e64520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132e647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132e64fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132e65290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132e658a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1263044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126304dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126305230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1263056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126305b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126305f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1263063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126306860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126306dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126307240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1263078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1263083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126308b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1263093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126309ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12630a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12630a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12630b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12630b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12630bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12630c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12630cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12630d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12630db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12630de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12630e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12630e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12630e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12630ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12630f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12630f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12630fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12630ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1263103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126310810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126310c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1263110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126311560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1263119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126311e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1263122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126312720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126312b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126313000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126313470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1263138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126313d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1263141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126314630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126314aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126314f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126315380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1263157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126315c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1263160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126316640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126316b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126316fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126317420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126317890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126317d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126318170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1263185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126318a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126318ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126319330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1263197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126319c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12631a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12631a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12631a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12631add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12631b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12631b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12631bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12631bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12631c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12631c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12631cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12631d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12631d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12631da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12631dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12631e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12631e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12631ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12631f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12631f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12631f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12631fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126320220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126320690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126320b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126320f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1263213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126321850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126321cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126322520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126322a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126322ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1263235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126323b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126324100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1263246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126324c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126325210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1263257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126325d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126326320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1263268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126326e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126327430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1263279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126327ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1263283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1263288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126328de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1263292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1263297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126329ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12632a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12632a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12632abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12632b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12632b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12632bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12632bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12632c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12632c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12632cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12632d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12632d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12632dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12632e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12632e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12632ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12632f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12632f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12632fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1263300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1263305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126330ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126330fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1263314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1263319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126331ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1263323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1263328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126332de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1263332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1263337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126333ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1263341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1263346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126334be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1263350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1263355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126335ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126335fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1263364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1263369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126336ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1263373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1263378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126337de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1263382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1263387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126338ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1263391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1263396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126339be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12633a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12633a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12633aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12633afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12633b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12633b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12633bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12633c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12633c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12633cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12633d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12633d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12633dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12633e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12633e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12633ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12633f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12633f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12633fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12633ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1263404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1263409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126340f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126341540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126341af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1263420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1263426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126342cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1263432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126343ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126343f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126344220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126344830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126344e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126345630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126345ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126345f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126346410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126346bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126347110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126347660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126347bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126348100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126348650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126348ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1263490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126349640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126349b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12634a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12634a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12634ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12634b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12634b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12634bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12634c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12634c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12634cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12634d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12634d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12634db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12634e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12634e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12634eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12634f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12634f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12634fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126350080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1263505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126350b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126351070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1263515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126351b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126352060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1263525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126352b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126353050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1263535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126353af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126354040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126354590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126354ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126355030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126355580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126355ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126356020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126356570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126356ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126357010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126357560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126357ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126358000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126358550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126358aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126358ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126359540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1263599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126359e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12635a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12635a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12635ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12635b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12635b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12635ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12635bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12635c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12635c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12635ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12635d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12635d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12635daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12635df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12635e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12635e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12635ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12635f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12635f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12635fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12635ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x126360440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1263608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126360e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126361550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126361c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126362390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126362ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126362d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126363560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126363820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126363e30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.231s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
