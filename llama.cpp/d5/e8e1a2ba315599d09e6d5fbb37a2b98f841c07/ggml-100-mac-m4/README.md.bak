### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.17 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.29 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.91 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.97 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.13 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.88 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.10 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 252.88 sec*proc (29 tests)

Total Test time (real) = 252.90 sec

real	4m12.988s
user	8m35.286s
sys	0m7.137s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.04 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.86 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.48 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.52 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.88 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.38 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.10 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.00 sec*proc (29 tests)

Total Test time (real) =  55.01 sec

real	0m55.024s
user	1m17.583s
sys	0m6.376s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.136 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.816 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.660 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.668 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.670 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.670 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.671 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.675 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.676 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.676 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.677 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.677 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.680 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.680 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.681 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.681 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.682 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.683 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.683 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.302 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.304 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.305 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.305 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.306 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.307 I llama_model_loader: - type  f32:  124 tensors
0.00.026.307 I llama_model_loader: - type  f16:   73 tensors
0.00.026.308 I print_info: file format = GGUF V3 (latest)
0.00.026.309 I print_info: file type   = F16
0.00.026.310 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.335 I load: special tokens cache size = 5
0.00.032.409 I load: token to piece cache size = 0.2032 MB
0.00.032.436 I print_info: arch             = bert
0.00.032.437 I print_info: vocab_only       = 0
0.00.032.437 I print_info: n_ctx_train      = 512
0.00.032.437 I print_info: n_embd           = 384
0.00.032.438 I print_info: n_layer          = 12
0.00.032.440 I print_info: n_head           = 12
0.00.032.441 I print_info: n_head_kv        = 12
0.00.032.441 I print_info: n_rot            = 32
0.00.032.441 I print_info: n_swa            = 0
0.00.032.447 I print_info: n_embd_head_k    = 32
0.00.032.448 I print_info: n_embd_head_v    = 32
0.00.032.449 I print_info: n_gqa            = 1
0.00.032.449 I print_info: n_embd_k_gqa     = 384
0.00.032.450 I print_info: n_embd_v_gqa     = 384
0.00.032.451 I print_info: f_norm_eps       = 1.0e-12
0.00.032.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.032.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.032.452 I print_info: f_max_alibi_bias = 0.0e+00
0.00.032.452 I print_info: f_logit_scale    = 0.0e+00
0.00.032.453 I print_info: n_ff             = 1536
0.00.032.453 I print_info: n_expert         = 0
0.00.032.453 I print_info: n_expert_used    = 0
0.00.032.454 I print_info: causal attn      = 0
0.00.032.454 I print_info: pooling type     = 2
0.00.032.454 I print_info: rope type        = 2
0.00.032.454 I print_info: rope scaling     = linear
0.00.032.455 I print_info: freq_base_train  = 10000.0
0.00.032.457 I print_info: freq_scale_train = 1
0.00.032.458 I print_info: n_ctx_orig_yarn  = 512
0.00.032.458 I print_info: rope_finetuned   = unknown
0.00.032.458 I print_info: ssm_d_conv       = 0
0.00.032.458 I print_info: ssm_d_inner      = 0
0.00.032.458 I print_info: ssm_d_state      = 0
0.00.032.458 I print_info: ssm_dt_rank      = 0
0.00.032.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.459 I print_info: model type       = 33M
0.00.032.459 I print_info: model params     = 33.21 M
0.00.032.460 I print_info: general.name     = Bge Small
0.00.032.460 I print_info: vocab type       = WPM
0.00.032.460 I print_info: n_vocab          = 30522
0.00.032.461 I print_info: n_merges         = 0
0.00.032.461 I print_info: BOS token        = 101 '[CLS]'
0.00.032.461 I print_info: UNK token        = 100 '[UNK]'
0.00.032.461 I print_info: SEP token        = 102 '[SEP]'
0.00.032.465 I print_info: PAD token        = 0 '[PAD]'
0.00.032.465 I print_info: MASK token       = 103 '[MASK]'
0.00.032.466 I print_info: LF token         = 0 '[PAD]'
0.00.032.466 I print_info: max token length = 21
0.00.032.466 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.035.523 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.525 I load_tensors: offloading output layer to GPU
0.00.035.525 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.550 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.552 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.866 I llama_context: n_seq_max     = 1
0.00.035.867 I llama_context: n_ctx         = 512
0.00.035.868 I llama_context: n_ctx_per_seq = 512
0.00.035.868 I llama_context: n_batch       = 2048
0.00.035.868 I llama_context: n_ubatch      = 2048
0.00.035.868 I llama_context: flash_attn    = 0
0.00.035.869 I llama_context: freq_base     = 10000.0
0.00.035.869 I llama_context: freq_scale    = 1
0.00.035.870 I ggml_metal_init: allocating
0.00.035.880 I ggml_metal_init: found device: Apple M4
0.00.035.886 I ggml_metal_init: picking default device: Apple M4
0.00.036.624 I ggml_metal_init: using embedded metal library
0.00.040.740 I ggml_metal_init: GPU name:   Apple M4
0.00.040.742 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.743 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.744 I ggml_metal_init: simdgroup reduction   = true
0.00.040.744 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.744 I ggml_metal_init: has residency sets    = true
0.00.040.744 I ggml_metal_init: has bfloat            = true
0.00.040.745 I ggml_metal_init: use bfloat            = true
0.00.040.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.544 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.052.547 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.053.235 I init:      Metal KV buffer size =     9.00 MiB
0.00.053.237 I llama_context_kv_self: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.474 I init:      Metal compute buffer size =    16.00 MiB
0.00.054.476 I init:        CPU compute buffer size =     2.51 MiB
0.00.054.476 I init: graph nodes  = 429
0.00.054.476 I init: graph splits = 2
0.00.054.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.790 I 
0.00.059.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.450 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.498 I llama_perf_context_print:        load time =      43.97 ms
0.00.065.499 I llama_perf_context_print: prompt eval time =       4.92 ms /     9 tokens (    0.55 ms per token,  1830.01 tokens per second)
0.00.065.500 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.501 I llama_perf_context_print:       total time =       5.71 ms /    10 tokens
0.00.065.691 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.048s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.419 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.087 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.092 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.093 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.093 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.093 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.094 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.094 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.095 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.095 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.095 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.097 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.098 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.098 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.099 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.099 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.099 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.479 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.101 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.102 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.102 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.103 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.103 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.104 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.104 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.104 I llama_model_loader: - type  f32:  124 tensors
0.00.015.105 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.105 I print_info: file format = GGUF V3 (latest)
0.00.015.106 I print_info: file type   = Q8_0
0.00.015.107 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.601 I load: special tokens cache size = 5
0.00.018.872 I load: token to piece cache size = 0.2032 MB
0.00.018.881 I print_info: arch             = bert
0.00.018.881 I print_info: vocab_only       = 0
0.00.018.882 I print_info: n_ctx_train      = 512
0.00.018.882 I print_info: n_embd           = 384
0.00.018.882 I print_info: n_layer          = 12
0.00.018.885 I print_info: n_head           = 12
0.00.018.886 I print_info: n_head_kv        = 12
0.00.018.886 I print_info: n_rot            = 32
0.00.018.886 I print_info: n_swa            = 0
0.00.018.887 I print_info: n_embd_head_k    = 32
0.00.018.887 I print_info: n_embd_head_v    = 32
0.00.018.887 I print_info: n_gqa            = 1
0.00.018.888 I print_info: n_embd_k_gqa     = 384
0.00.018.889 I print_info: n_embd_v_gqa     = 384
0.00.018.894 I print_info: f_norm_eps       = 1.0e-12
0.00.018.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.895 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.895 I print_info: f_logit_scale    = 0.0e+00
0.00.018.896 I print_info: n_ff             = 1536
0.00.018.896 I print_info: n_expert         = 0
0.00.018.896 I print_info: n_expert_used    = 0
0.00.018.896 I print_info: causal attn      = 0
0.00.018.896 I print_info: pooling type     = 2
0.00.018.896 I print_info: rope type        = 2
0.00.018.896 I print_info: rope scaling     = linear
0.00.018.897 I print_info: freq_base_train  = 10000.0
0.00.018.897 I print_info: freq_scale_train = 1
0.00.018.897 I print_info: n_ctx_orig_yarn  = 512
0.00.018.897 I print_info: rope_finetuned   = unknown
0.00.018.898 I print_info: ssm_d_conv       = 0
0.00.018.898 I print_info: ssm_d_inner      = 0
0.00.018.898 I print_info: ssm_d_state      = 0
0.00.018.898 I print_info: ssm_dt_rank      = 0
0.00.018.898 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.898 I print_info: model type       = 33M
0.00.018.899 I print_info: model params     = 33.21 M
0.00.018.899 I print_info: general.name     = Bge Small
0.00.018.899 I print_info: vocab type       = WPM
0.00.018.900 I print_info: n_vocab          = 30522
0.00.018.900 I print_info: n_merges         = 0
0.00.018.900 I print_info: BOS token        = 101 '[CLS]'
0.00.018.900 I print_info: UNK token        = 100 '[UNK]'
0.00.018.900 I print_info: SEP token        = 102 '[SEP]'
0.00.018.901 I print_info: PAD token        = 0 '[PAD]'
0.00.018.901 I print_info: MASK token       = 103 '[MASK]'
0.00.018.901 I print_info: LF token         = 0 '[PAD]'
0.00.018.902 I print_info: max token length = 21
0.00.018.902 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.516 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.517 I load_tensors: offloading output layer to GPU
0.00.020.517 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.523 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.523 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.691 I llama_context: n_seq_max     = 1
0.00.020.692 I llama_context: n_ctx         = 512
0.00.020.692 I llama_context: n_ctx_per_seq = 512
0.00.020.692 I llama_context: n_batch       = 2048
0.00.020.692 I llama_context: n_ubatch      = 2048
0.00.020.693 I llama_context: flash_attn    = 0
0.00.020.693 I llama_context: freq_base     = 10000.0
0.00.020.693 I llama_context: freq_scale    = 1
0.00.020.694 I ggml_metal_init: allocating
0.00.020.697 I ggml_metal_init: found device: Apple M4
0.00.020.701 I ggml_metal_init: picking default device: Apple M4
0.00.021.190 I ggml_metal_init: using embedded metal library
0.00.023.788 I ggml_metal_init: GPU name:   Apple M4
0.00.023.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.791 I ggml_metal_init: simdgroup reduction   = true
0.00.023.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.791 I ggml_metal_init: has residency sets    = true
0.00.023.791 I ggml_metal_init: has bfloat            = true
0.00.023.791 I ggml_metal_init: use bfloat            = true
0.00.023.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.967 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.033.970 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.556 I init:      Metal KV buffer size =     9.00 MiB
0.00.034.558 I llama_context_kv_self: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.503 I init:      Metal compute buffer size =    16.00 MiB
0.00.035.504 I init:        CPU compute buffer size =     2.51 MiB
0.00.035.504 I init: graph nodes  = 429
0.00.035.505 I init: graph splits = 2
0.00.035.506 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.429 I 
0.00.039.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.979 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.458 I llama_perf_context_print:        load time =      30.01 ms
0.00.044.460 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2069.44 tokens per second)
0.00.044.460 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.461 I llama_perf_context_print:       total time =       5.03 ms /    10 tokens
0.00.044.715 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.182 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.087 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.571 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.576 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.022.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.577 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.022.580 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.022.580 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.022.581 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.022.582 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.022.582 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.022.582 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.022.583 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.022.585 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.586 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.586 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.022.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.587 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.026.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.028.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.030.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.030.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.030.988 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.030.989 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.030.989 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.030.989 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.990 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.030.990 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.030.990 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.030.991 I llama_model_loader: - type  f32:   40 tensors
0.00.030.992 I llama_model_loader: - type  f16:   30 tensors
0.00.030.993 I print_info: file format = GGUF V3 (latest)
0.00.030.994 I print_info: file type   = F16
0.00.030.995 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.033.762 W load: empty token at index 5
0.00.037.330 W load: model vocab missing newline token, using special_pad_id instead
0.00.038.435 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.038.468 I load: special tokens cache size = 5
0.00.307.286 I load: token to piece cache size = 1.5060 MB
0.00.307.317 I print_info: arch             = jina-bert-v2
0.00.307.319 I print_info: vocab_only       = 0
0.00.307.319 I print_info: n_ctx_train      = 8192
0.00.307.319 I print_info: n_embd           = 384
0.00.307.319 I print_info: n_layer          = 4
0.00.307.323 I print_info: n_head           = 12
0.00.307.323 I print_info: n_head_kv        = 12
0.00.307.324 I print_info: n_rot            = 32
0.00.307.324 I print_info: n_swa            = 0
0.00.307.324 I print_info: n_embd_head_k    = 32
0.00.307.324 I print_info: n_embd_head_v    = 32
0.00.307.324 I print_info: n_gqa            = 1
0.00.307.325 I print_info: n_embd_k_gqa     = 384
0.00.307.325 I print_info: n_embd_v_gqa     = 384
0.00.307.326 I print_info: f_norm_eps       = 1.0e-12
0.00.307.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.307.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.307.327 I print_info: f_max_alibi_bias = 8.0e+00
0.00.307.327 I print_info: f_logit_scale    = 0.0e+00
0.00.307.327 I print_info: n_ff             = 1536
0.00.307.327 I print_info: n_expert         = 0
0.00.307.328 I print_info: n_expert_used    = 0
0.00.307.328 I print_info: causal attn      = 0
0.00.307.328 I print_info: pooling type     = -1
0.00.307.328 I print_info: rope type        = -1
0.00.307.328 I print_info: rope scaling     = linear
0.00.307.328 I print_info: freq_base_train  = 10000.0
0.00.307.329 I print_info: freq_scale_train = 1
0.00.307.329 I print_info: n_ctx_orig_yarn  = 8192
0.00.307.329 I print_info: rope_finetuned   = unknown
0.00.307.329 I print_info: ssm_d_conv       = 0
0.00.307.329 I print_info: ssm_d_inner      = 0
0.00.307.329 I print_info: ssm_d_state      = 0
0.00.307.329 I print_info: ssm_dt_rank      = 0
0.00.307.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.307.330 I print_info: model type       = 33M
0.00.307.330 I print_info: model params     = 32.90 M
0.00.307.331 I print_info: general.name     = Jina Bert Implementation
0.00.307.331 I print_info: vocab type       = BPE
0.00.307.331 I print_info: n_vocab          = 61056
0.00.307.331 I print_info: n_merges         = 39382
0.00.307.332 I print_info: BOS token        = 0 '<s>'
0.00.307.332 I print_info: EOS token        = 2 '</s>'
0.00.307.332 I print_info: UNK token        = 3 '<unk>'
0.00.307.332 I print_info: SEP token        = 2 '</s>'
0.00.307.333 I print_info: PAD token        = 1 '<pad>'
0.00.307.333 I print_info: MASK token       = 4 '<mask>'
0.00.307.333 I print_info: EOG token        = 2 '</s>'
0.00.307.334 I print_info: max token length = 45
0.00.307.334 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.308.459 I load_tensors: offloading 4 repeating layers to GPU
0.00.308.460 I load_tensors: offloading output layer to GPU
0.00.308.460 I load_tensors: offloaded 5/5 layers to GPU
0.00.308.480 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.308.481 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.308.650 I llama_context: n_seq_max     = 1
0.00.308.651 I llama_context: n_ctx         = 8192
0.00.308.651 I llama_context: n_ctx_per_seq = 8192
0.00.308.651 I llama_context: n_batch       = 2048
0.00.308.651 I llama_context: n_ubatch      = 2048
0.00.308.651 I llama_context: flash_attn    = 0
0.00.308.652 I llama_context: freq_base     = 10000.0
0.00.308.652 I llama_context: freq_scale    = 1
0.00.308.652 I ggml_metal_init: allocating
0.00.308.657 I ggml_metal_init: found device: Apple M4
0.00.308.660 I ggml_metal_init: picking default device: Apple M4
0.00.309.207 I ggml_metal_init: using embedded metal library
0.00.311.740 I ggml_metal_init: GPU name:   Apple M4
0.00.311.742 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.311.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.311.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.311.743 I ggml_metal_init: simdgroup reduction   = true
0.00.311.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.311.743 I ggml_metal_init: has residency sets    = true
0.00.311.743 I ggml_metal_init: has bfloat            = true
0.00.311.743 I ggml_metal_init: use bfloat            = true
0.00.311.744 I ggml_metal_init: hasUnifiedMemory      = true
0.00.311.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.322.155 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.322.158 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.325.153 I init:      Metal KV buffer size =    48.00 MiB
0.00.325.155 I llama_context_kv_self: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.331.388 I init:      Metal compute buffer size =   220.01 MiB
0.00.331.389 I init:        CPU compute buffer size =    22.02 MiB
0.00.331.390 I init: graph nodes  = 154
0.00.331.390 I init: graph splits = 2
0.00.331.391 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.331.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.336.679 I 
0.00.336.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.336.804 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.336.805 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.336.810 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.336.810 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.336.813 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.336.813 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.337.304 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.340.691 I llama_perf_context_print:        load time =     320.59 ms
0.00.340.692 I llama_perf_context_print: prompt eval time =       3.38 ms /    62 tokens (    0.05 ms per token, 18343.20 tokens per second)
0.00.340.693 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.340.693 I llama_perf_context_print:       total time =       4.01 ms /    63 tokens
0.00.341.032 I ggml_metal_free: deallocating

real	0m1.041s
user	0m0.319s
sys	0m0.036s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.367 I main: llama backend init
0.00.000.374 I main: load the model and apply lora adapter, if any
0.00.078.713 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.091.071 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.091.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.091.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.091.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.091.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.091.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.091.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.091.099 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.091.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.091.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.091.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.091.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.091.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.091.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.091.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.091.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.091.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.097.871 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.100.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.107.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.107.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.107.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.107.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.107.617 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.107.618 I llama_model_loader: - type  f32:  194 tensors
0.00.107.619 I llama_model_loader: - type  f16:   98 tensors
0.00.107.631 I print_info: file format = GGUF V3 (latest)
0.00.107.632 I print_info: file type   = all F32 (guessed)
0.00.107.635 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.125.753 I load: special tokens cache size = 25
0.00.135.990 I load: token to piece cache size = 0.2984 MB
0.00.136.019 I print_info: arch             = gptneox
0.00.136.021 I print_info: vocab_only       = 0
0.00.136.021 I print_info: n_ctx_train      = 2048
0.00.136.021 I print_info: n_embd           = 2048
0.00.136.022 I print_info: n_layer          = 24
0.00.136.027 I print_info: n_head           = 16
0.00.136.028 I print_info: n_head_kv        = 16
0.00.136.028 I print_info: n_rot            = 32
0.00.136.028 I print_info: n_swa            = 0
0.00.136.030 I print_info: n_embd_head_k    = 128
0.00.136.030 I print_info: n_embd_head_v    = 128
0.00.136.031 I print_info: n_gqa            = 1
0.00.136.032 I print_info: n_embd_k_gqa     = 2048
0.00.136.033 I print_info: n_embd_v_gqa     = 2048
0.00.136.034 I print_info: f_norm_eps       = 1.0e-05
0.00.136.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.136.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.136.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.136.038 I print_info: f_logit_scale    = 0.0e+00
0.00.136.039 I print_info: n_ff             = 8192
0.00.136.039 I print_info: n_expert         = 0
0.00.136.039 I print_info: n_expert_used    = 0
0.00.136.039 I print_info: causal attn      = 1
0.00.136.039 I print_info: pooling type     = 0
0.00.136.040 I print_info: rope type        = 2
0.00.136.040 I print_info: rope scaling     = linear
0.00.136.041 I print_info: freq_base_train  = 10000.0
0.00.136.041 I print_info: freq_scale_train = 1
0.00.136.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.136.042 I print_info: rope_finetuned   = unknown
0.00.136.042 I print_info: ssm_d_conv       = 0
0.00.136.042 I print_info: ssm_d_inner      = 0
0.00.136.042 I print_info: ssm_d_state      = 0
0.00.136.042 I print_info: ssm_dt_rank      = 0
0.00.136.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.136.043 I print_info: model type       = 1.4B
0.00.136.043 I print_info: model params     = 1.41 B
0.00.136.044 I print_info: general.name     = 1.4B
0.00.136.044 I print_info: vocab type       = BPE
0.00.136.045 I print_info: n_vocab          = 50304
0.00.136.045 I print_info: n_merges         = 50009
0.00.136.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.136.047 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.136.048 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.136.048 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.136.048 I print_info: LF token         = 187 'Ċ'
0.00.136.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.136.049 I print_info: max token length = 1024
0.00.136.049 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.208.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.208.735 I load_tensors: offloading output layer to GPU
0.00.208.735 I load_tensors: offloaded 25/25 layers to GPU
0.00.208.758 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.208.759 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.209.256 I llama_context: n_seq_max     = 1
0.00.209.257 I llama_context: n_ctx         = 2048
0.00.209.257 I llama_context: n_ctx_per_seq = 2048
0.00.209.257 I llama_context: n_batch       = 2048
0.00.209.257 I llama_context: n_ubatch      = 512
0.00.209.258 I llama_context: flash_attn    = 0
0.00.209.258 I llama_context: freq_base     = 10000.0
0.00.209.258 I llama_context: freq_scale    = 1
0.00.209.259 I ggml_metal_init: allocating
0.00.209.293 I ggml_metal_init: found device: Apple M4
0.00.209.299 I ggml_metal_init: picking default device: Apple M4
0.00.209.900 I ggml_metal_init: using embedded metal library
0.00.219.125 I ggml_metal_init: GPU name:   Apple M4
0.00.219.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.219.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.219.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.219.128 I ggml_metal_init: simdgroup reduction   = true
0.00.219.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.219.128 I ggml_metal_init: has residency sets    = true
0.00.219.128 I ggml_metal_init: has bfloat            = true
0.00.219.128 I ggml_metal_init: use bfloat            = true
0.00.219.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.219.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.243.809 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.243.811 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.272.203 I init:      Metal KV buffer size =   384.00 MiB
0.00.272.208 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.276.252 I init:      Metal compute buffer size =   102.25 MiB
0.00.276.254 I init:        CPU compute buffer size =     8.01 MiB
0.00.276.254 I init: graph nodes  = 967
0.00.276.254 I init: graph splits = 2
0.00.276.259 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.276.389 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.276.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.340.132 I main: llama threadpool init, n_threads = 4
0.00.340.171 I 
0.00.340.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.340.202 I 
0.00.340.379 I sampler seed: 1234
0.00.340.383 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.340.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.340.409 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.340.409 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.181.120 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.02.181.125 I llama_perf_context_print:        load time =     260.54 ms
0.02.181.126 I llama_perf_context_print: prompt eval time =      43.57 ms /     7 tokens (    6.22 ms per token,   160.66 tokens per second)
0.02.181.126 I llama_perf_context_print:        eval time =    1794.25 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.181.127 I llama_perf_context_print:       total time =    1841.86 ms /    70 tokens
0.02.185.137 I ggml_metal_free: deallocating

real	0m2.481s
user	0m0.133s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.689 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.474 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.938 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.954 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.581 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.585 I llama_model_loader: - type  f32:  194 tensors
0.00.055.586 I llama_model_loader: - type  f16:   98 tensors
0.00.055.587 I print_info: file format = GGUF V3 (latest)
0.00.055.588 I print_info: file type   = all F32 (guessed)
0.00.055.589 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.891 I load: special tokens cache size = 25
0.00.075.728 I load: token to piece cache size = 0.2984 MB
0.00.075.743 I print_info: arch             = gptneox
0.00.075.745 I print_info: vocab_only       = 0
0.00.075.745 I print_info: n_ctx_train      = 2048
0.00.075.745 I print_info: n_embd           = 2048
0.00.075.746 I print_info: n_layer          = 24
0.00.075.749 I print_info: n_head           = 16
0.00.075.750 I print_info: n_head_kv        = 16
0.00.075.750 I print_info: n_rot            = 32
0.00.075.750 I print_info: n_swa            = 0
0.00.075.750 I print_info: n_embd_head_k    = 128
0.00.075.750 I print_info: n_embd_head_v    = 128
0.00.075.751 I print_info: n_gqa            = 1
0.00.075.752 I print_info: n_embd_k_gqa     = 2048
0.00.075.753 I print_info: n_embd_v_gqa     = 2048
0.00.075.753 I print_info: f_norm_eps       = 1.0e-05
0.00.075.754 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.756 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.756 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.756 I print_info: f_logit_scale    = 0.0e+00
0.00.075.757 I print_info: n_ff             = 8192
0.00.075.757 I print_info: n_expert         = 0
0.00.075.757 I print_info: n_expert_used    = 0
0.00.075.757 I print_info: causal attn      = 1
0.00.075.757 I print_info: pooling type     = 0
0.00.075.757 I print_info: rope type        = 2
0.00.075.758 I print_info: rope scaling     = linear
0.00.075.759 I print_info: freq_base_train  = 10000.0
0.00.075.759 I print_info: freq_scale_train = 1
0.00.075.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.760 I print_info: rope_finetuned   = unknown
0.00.075.760 I print_info: ssm_d_conv       = 0
0.00.075.760 I print_info: ssm_d_inner      = 0
0.00.075.760 I print_info: ssm_d_state      = 0
0.00.075.760 I print_info: ssm_dt_rank      = 0
0.00.075.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.761 I print_info: model type       = 1.4B
0.00.075.763 I print_info: model params     = 1.41 B
0.00.075.763 I print_info: general.name     = 1.4B
0.00.075.763 I print_info: vocab type       = BPE
0.00.075.763 I print_info: n_vocab          = 50304
0.00.075.763 I print_info: n_merges         = 50009
0.00.075.764 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.764 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.764 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.764 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.765 I print_info: LF token         = 187 'Ċ'
0.00.075.765 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.766 I print_info: max token length = 1024
0.00.075.766 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.429.946 I load_tensors: offloading 24 repeating layers to GPU
0.01.429.950 I load_tensors: offloading output layer to GPU
0.01.429.950 I load_tensors: offloaded 25/25 layers to GPU
0.01.429.972 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.429.974 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.430.710 I llama_context: n_seq_max     = 1
0.01.430.711 I llama_context: n_ctx         = 128
0.01.430.712 I llama_context: n_ctx_per_seq = 128
0.01.430.712 I llama_context: n_batch       = 128
0.01.430.712 I llama_context: n_ubatch      = 128
0.01.430.712 I llama_context: flash_attn    = 0
0.01.430.713 I llama_context: freq_base     = 10000.0
0.01.430.713 I llama_context: freq_scale    = 1
0.01.430.714 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.430.716 I ggml_metal_init: allocating
0.01.430.767 I ggml_metal_init: found device: Apple M4
0.01.430.772 I ggml_metal_init: picking default device: Apple M4
0.01.431.842 I ggml_metal_init: using embedded metal library
0.01.435.528 I ggml_metal_init: GPU name:   Apple M4
0.01.435.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.435.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.435.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.435.532 I ggml_metal_init: simdgroup reduction   = true
0.01.435.532 I ggml_metal_init: simdgroup matrix mul. = true
0.01.435.532 I ggml_metal_init: has residency sets    = true
0.01.435.532 I ggml_metal_init: has bfloat            = true
0.01.435.532 I ggml_metal_init: use bfloat            = true
0.01.435.533 I ggml_metal_init: hasUnifiedMemory      = true
0.01.435.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.446.111 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.446.114 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.447.784 I init:      Metal KV buffer size =    24.00 MiB
0.01.447.786 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.449.392 I init:      Metal compute buffer size =    25.56 MiB
0.01.449.393 I init:        CPU compute buffer size =     1.06 MiB
0.01.449.394 I init: graph nodes  = 967
0.01.449.394 I init: graph splits = 2
0.01.449.395 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.449.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.483.612 I 
0.01.483.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.483.672 I perplexity: tokenizing the input ..
0.01.488.660 I perplexity: tokenization took 4.986 ms
0.01.488.682 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.607.788 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.609.199 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.209 I llama_perf_context_print:        load time =    1459.13 ms
0.01.609.210 I llama_perf_context_print: prompt eval time =     118.84 ms /   128 tokens (    0.93 ms per token,  1077.09 tokens per second)
0.01.609.212 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.213 I llama_perf_context_print:       total time =     125.60 ms /   129 tokens
0.01.609.825 I ggml_metal_free: deallocating

real	0m1.796s
user	0m0.096s
sys	0m0.252s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.331 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.681 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.526 I llama_model_loader: - type  f32:  194 tensors
0.00.033.527 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.528 I print_info: file format = GGUF V3 (latest)
0.00.033.528 I print_info: file type   = Q8_0
0.00.033.529 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.541 I load: special tokens cache size = 25
0.00.049.075 I load: token to piece cache size = 0.2984 MB
0.00.049.090 I print_info: arch             = gptneox
0.00.049.091 I print_info: vocab_only       = 0
0.00.049.091 I print_info: n_ctx_train      = 2048
0.00.049.091 I print_info: n_embd           = 2048
0.00.049.092 I print_info: n_layer          = 24
0.00.049.096 I print_info: n_head           = 16
0.00.049.097 I print_info: n_head_kv        = 16
0.00.049.097 I print_info: n_rot            = 32
0.00.049.098 I print_info: n_swa            = 0
0.00.049.098 I print_info: n_embd_head_k    = 128
0.00.049.098 I print_info: n_embd_head_v    = 128
0.00.049.099 I print_info: n_gqa            = 1
0.00.049.100 I print_info: n_embd_k_gqa     = 2048
0.00.049.100 I print_info: n_embd_v_gqa     = 2048
0.00.049.101 I print_info: f_norm_eps       = 1.0e-05
0.00.049.101 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.101 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.102 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.102 I print_info: f_logit_scale    = 0.0e+00
0.00.049.102 I print_info: n_ff             = 8192
0.00.049.102 I print_info: n_expert         = 0
0.00.049.103 I print_info: n_expert_used    = 0
0.00.049.103 I print_info: causal attn      = 1
0.00.049.103 I print_info: pooling type     = 0
0.00.049.103 I print_info: rope type        = 2
0.00.049.103 I print_info: rope scaling     = linear
0.00.049.106 I print_info: freq_base_train  = 10000.0
0.00.049.106 I print_info: freq_scale_train = 1
0.00.049.106 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.106 I print_info: rope_finetuned   = unknown
0.00.049.107 I print_info: ssm_d_conv       = 0
0.00.049.107 I print_info: ssm_d_inner      = 0
0.00.049.107 I print_info: ssm_d_state      = 0
0.00.049.107 I print_info: ssm_dt_rank      = 0
0.00.049.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.107 I print_info: model type       = 1.4B
0.00.049.107 I print_info: model params     = 1.41 B
0.00.049.108 I print_info: general.name     = 1.4B
0.00.049.108 I print_info: vocab type       = BPE
0.00.049.108 I print_info: n_vocab          = 50304
0.00.049.108 I print_info: n_merges         = 50009
0.00.049.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.109 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.109 I print_info: LF token         = 187 'Ċ'
0.00.049.109 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.110 I print_info: max token length = 1024
0.00.049.110 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.366.456 I load_tensors: offloading 24 repeating layers to GPU
0.01.366.460 I load_tensors: offloading output layer to GPU
0.01.366.462 I load_tensors: offloaded 25/25 layers to GPU
0.01.366.485 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.366.489 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.367.572 I llama_context: n_seq_max     = 1
0.01.367.574 I llama_context: n_ctx         = 2048
0.01.367.574 I llama_context: n_ctx_per_seq = 2048
0.01.367.574 I llama_context: n_batch       = 2048
0.01.367.575 I llama_context: n_ubatch      = 512
0.01.367.575 I llama_context: flash_attn    = 0
0.01.367.576 I llama_context: freq_base     = 10000.0
0.01.367.576 I llama_context: freq_scale    = 1
0.01.367.577 I ggml_metal_init: allocating
0.01.367.586 I ggml_metal_init: found device: Apple M4
0.01.367.593 I ggml_metal_init: picking default device: Apple M4
0.01.368.846 I ggml_metal_init: using embedded metal library
0.01.374.433 I ggml_metal_init: GPU name:   Apple M4
0.01.374.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.374.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.374.437 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.374.438 I ggml_metal_init: simdgroup reduction   = true
0.01.374.438 I ggml_metal_init: simdgroup matrix mul. = true
0.01.374.440 I ggml_metal_init: has residency sets    = true
0.01.374.441 I ggml_metal_init: has bfloat            = true
0.01.374.441 I ggml_metal_init: use bfloat            = true
0.01.374.442 I ggml_metal_init: hasUnifiedMemory      = true
0.01.374.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.390.963 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.390.967 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.442.085 I init:      Metal KV buffer size =   384.00 MiB
0.01.442.101 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.447.037 I init:      Metal compute buffer size =   102.25 MiB
0.01.447.039 I init:        CPU compute buffer size =     8.01 MiB
0.01.447.039 I init: graph nodes  = 967
0.01.447.040 I init: graph splits = 2
0.01.447.045 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.447.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.447.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.501.248 I main: llama threadpool init, n_threads = 4
0.01.501.298 I 
0.01.501.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.501.321 I 
0.01.501.499 I sampler seed: 1234
0.01.501.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.501.515 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.501.515 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.501.515 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.597.122 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.02.597.122 I llama_perf_context_print:        load time =    1491.21 ms
0.02.597.123 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.42 tokens per second)
0.02.597.124 I llama_perf_context_print:        eval time =    1043.95 ms /    63 runs   (   16.57 ms per token,    60.35 tokens per second)
0.02.597.124 I llama_perf_context_print:       total time =    1096.57 ms /    70 tokens
0.02.601.199 I ggml_metal_free: deallocating

real	0m2.621s
user	0m0.109s
sys	0m0.276s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.294 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.268 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.279 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.285 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.285 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.104 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.107 I llama_model_loader: - type  f32:  194 tensors
0.00.026.108 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.108 I print_info: file format = GGUF V3 (latest)
0.00.026.109 I print_info: file type   = Q8_0
0.00.026.110 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.583 I load: special tokens cache size = 25
0.00.040.488 I load: token to piece cache size = 0.2984 MB
0.00.040.505 I print_info: arch             = gptneox
0.00.040.506 I print_info: vocab_only       = 0
0.00.040.506 I print_info: n_ctx_train      = 2048
0.00.040.506 I print_info: n_embd           = 2048
0.00.040.507 I print_info: n_layer          = 24
0.00.040.510 I print_info: n_head           = 16
0.00.040.511 I print_info: n_head_kv        = 16
0.00.040.517 I print_info: n_rot            = 32
0.00.040.517 I print_info: n_swa            = 0
0.00.040.517 I print_info: n_embd_head_k    = 128
0.00.040.517 I print_info: n_embd_head_v    = 128
0.00.040.518 I print_info: n_gqa            = 1
0.00.040.518 I print_info: n_embd_k_gqa     = 2048
0.00.040.519 I print_info: n_embd_v_gqa     = 2048
0.00.040.519 I print_info: f_norm_eps       = 1.0e-05
0.00.040.519 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.520 I print_info: f_logit_scale    = 0.0e+00
0.00.040.520 I print_info: n_ff             = 8192
0.00.040.521 I print_info: n_expert         = 0
0.00.040.522 I print_info: n_expert_used    = 0
0.00.040.522 I print_info: causal attn      = 1
0.00.040.523 I print_info: pooling type     = 0
0.00.040.523 I print_info: rope type        = 2
0.00.040.523 I print_info: rope scaling     = linear
0.00.040.523 I print_info: freq_base_train  = 10000.0
0.00.040.523 I print_info: freq_scale_train = 1
0.00.040.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.524 I print_info: rope_finetuned   = unknown
0.00.040.524 I print_info: ssm_d_conv       = 0
0.00.040.524 I print_info: ssm_d_inner      = 0
0.00.040.524 I print_info: ssm_d_state      = 0
0.00.040.524 I print_info: ssm_dt_rank      = 0
0.00.040.524 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.524 I print_info: model type       = 1.4B
0.00.040.529 I print_info: model params     = 1.41 B
0.00.040.529 I print_info: general.name     = 1.4B
0.00.040.530 I print_info: vocab type       = BPE
0.00.040.531 I print_info: n_vocab          = 50304
0.00.040.531 I print_info: n_merges         = 50009
0.00.040.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.533 I print_info: LF token         = 187 'Ċ'
0.00.040.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.533 I print_info: max token length = 1024
0.00.040.534 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.925.456 I load_tensors: offloading 24 repeating layers to GPU
0.00.925.463 I load_tensors: offloading output layer to GPU
0.00.925.464 I load_tensors: offloaded 25/25 layers to GPU
0.00.925.495 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.925.497 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.926.991 I llama_context: n_seq_max     = 1
0.00.926.992 I llama_context: n_ctx         = 128
0.00.926.993 I llama_context: n_ctx_per_seq = 128
0.00.926.993 I llama_context: n_batch       = 128
0.00.926.994 I llama_context: n_ubatch      = 128
0.00.926.994 I llama_context: flash_attn    = 0
0.00.926.995 I llama_context: freq_base     = 10000.0
0.00.926.995 I llama_context: freq_scale    = 1
0.00.926.996 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.926.996 I ggml_metal_init: allocating
0.00.927.093 I ggml_metal_init: found device: Apple M4
0.00.927.102 I ggml_metal_init: picking default device: Apple M4
0.00.928.387 I ggml_metal_init: using embedded metal library
0.00.933.593 I ggml_metal_init: GPU name:   Apple M4
0.00.933.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.933.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.933.598 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.933.598 I ggml_metal_init: simdgroup reduction   = true
0.00.933.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.933.599 I ggml_metal_init: has residency sets    = true
0.00.933.599 I ggml_metal_init: has bfloat            = true
0.00.933.599 I ggml_metal_init: use bfloat            = true
0.00.933.600 I ggml_metal_init: hasUnifiedMemory      = true
0.00.933.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.948.816 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.948.819 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.952.154 I init:      Metal KV buffer size =    24.00 MiB
0.00.952.163 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.955.138 I init:      Metal compute buffer size =    25.56 MiB
0.00.955.140 I init:        CPU compute buffer size =     1.06 MiB
0.00.955.140 I init: graph nodes  = 967
0.00.955.141 I init: graph splits = 2
0.00.955.144 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.955.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.981.467 I 
0.00.981.545 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.981.564 I perplexity: tokenizing the input ..
0.00.988.631 I perplexity: tokenization took 7.064 ms
0.00.988.658 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.114.141 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.115.452 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.115.465 I llama_perf_context_print:        load time =     971.36 ms
0.01.115.466 I llama_perf_context_print: prompt eval time =     124.52 ms /   128 tokens (    0.97 ms per token,  1027.96 tokens per second)
0.01.115.467 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.115.467 I llama_perf_context_print:       total time =     134.00 ms /   129 tokens
0.01.116.064 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.077s
sys	0m0.175s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.011.044 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.330 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.348 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.906 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.906 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.907 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.908 I llama_model_loader: - type  f32:  194 tensors
0.00.027.908 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.910 I print_info: file format = GGUF V3 (latest)
0.00.027.910 I print_info: file type   = Q4_0
0.00.027.912 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.037.435 I load: special tokens cache size = 25
0.00.043.680 I load: token to piece cache size = 0.2984 MB
0.00.043.703 I print_info: arch             = gptneox
0.00.043.704 I print_info: vocab_only       = 0
0.00.043.704 I print_info: n_ctx_train      = 2048
0.00.043.704 I print_info: n_embd           = 2048
0.00.043.704 I print_info: n_layer          = 24
0.00.043.710 I print_info: n_head           = 16
0.00.043.711 I print_info: n_head_kv        = 16
0.00.043.711 I print_info: n_rot            = 32
0.00.043.711 I print_info: n_swa            = 0
0.00.043.711 I print_info: n_embd_head_k    = 128
0.00.043.713 I print_info: n_embd_head_v    = 128
0.00.043.714 I print_info: n_gqa            = 1
0.00.043.714 I print_info: n_embd_k_gqa     = 2048
0.00.043.715 I print_info: n_embd_v_gqa     = 2048
0.00.043.715 I print_info: f_norm_eps       = 1.0e-05
0.00.043.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.716 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.716 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.716 I print_info: f_logit_scale    = 0.0e+00
0.00.043.717 I print_info: n_ff             = 8192
0.00.043.717 I print_info: n_expert         = 0
0.00.043.717 I print_info: n_expert_used    = 0
0.00.043.717 I print_info: causal attn      = 1
0.00.043.717 I print_info: pooling type     = 0
0.00.043.718 I print_info: rope type        = 2
0.00.043.718 I print_info: rope scaling     = linear
0.00.043.718 I print_info: freq_base_train  = 10000.0
0.00.043.719 I print_info: freq_scale_train = 1
0.00.043.719 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.719 I print_info: rope_finetuned   = unknown
0.00.043.719 I print_info: ssm_d_conv       = 0
0.00.043.719 I print_info: ssm_d_inner      = 0
0.00.043.719 I print_info: ssm_d_state      = 0
0.00.043.720 I print_info: ssm_dt_rank      = 0
0.00.043.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.720 I print_info: model type       = 1.4B
0.00.043.721 I print_info: model params     = 1.41 B
0.00.043.721 I print_info: general.name     = 1.4B
0.00.043.721 I print_info: vocab type       = BPE
0.00.043.722 I print_info: n_vocab          = 50304
0.00.043.722 I print_info: n_merges         = 50009
0.00.043.722 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.722 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.722 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.723 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.723 I print_info: LF token         = 187 'Ċ'
0.00.043.723 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.723 I print_info: max token length = 1024
0.00.043.724 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.106 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.121 I load_tensors: offloading output layer to GPU
0.00.621.122 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.166 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.621.167 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.622.700 I llama_context: n_seq_max     = 1
0.00.622.702 I llama_context: n_ctx         = 2048
0.00.622.703 I llama_context: n_ctx_per_seq = 2048
0.00.622.703 I llama_context: n_batch       = 2048
0.00.622.704 I llama_context: n_ubatch      = 512
0.00.622.704 I llama_context: flash_attn    = 0
0.00.622.707 I llama_context: freq_base     = 10000.0
0.00.622.707 I llama_context: freq_scale    = 1
0.00.622.709 I ggml_metal_init: allocating
0.00.622.839 I ggml_metal_init: found device: Apple M4
0.00.622.853 I ggml_metal_init: picking default device: Apple M4
0.00.624.784 I ggml_metal_init: using embedded metal library
0.00.631.751 I ggml_metal_init: GPU name:   Apple M4
0.00.631.760 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.761 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.762 I ggml_metal_init: simdgroup reduction   = true
0.00.631.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.762 I ggml_metal_init: has residency sets    = true
0.00.631.763 I ggml_metal_init: has bfloat            = true
0.00.631.763 I ggml_metal_init: use bfloat            = true
0.00.631.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.768 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.798 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.649.803 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.613 I init:      Metal KV buffer size =   384.00 MiB
0.00.705.621 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.218 I init:      Metal compute buffer size =   102.25 MiB
0.00.710.220 I init:        CPU compute buffer size =     8.01 MiB
0.00.710.221 I init: graph nodes  = 967
0.00.710.221 I init: graph splits = 2
0.00.710.226 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.710.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.710.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.591 I main: llama threadpool init, n_threads = 4
0.00.762.636 I 
0.00.762.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.662 I 
0.00.762.839 I sampler seed: 1234
0.00.762.843 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.864 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.864 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.864 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.436.863 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.436.864 I llama_perf_context_print:        load time =     750.83 ms
0.01.436.865 I llama_perf_context_print: prompt eval time =      39.44 ms /     7 tokens (    5.63 ms per token,   177.49 tokens per second)
0.01.436.868 I llama_perf_context_print:        eval time =     631.75 ms /    63 runs   (   10.03 ms per token,    99.72 tokens per second)
0.01.436.868 I llama_perf_context_print:       total time =     674.98 ms /    70 tokens
0.01.440.948 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.112s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.720 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.870 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.876 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.877 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.877 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.878 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.879 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.879 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.884 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.554 I llama_model_loader: - type  f32:  194 tensors
0.00.033.554 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.555 I print_info: file format = GGUF V3 (latest)
0.00.033.560 I print_info: file type   = Q4_0
0.00.033.561 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.041.939 I load: special tokens cache size = 25
0.00.048.118 I load: token to piece cache size = 0.2984 MB
0.00.048.135 I print_info: arch             = gptneox
0.00.048.136 I print_info: vocab_only       = 0
0.00.048.136 I print_info: n_ctx_train      = 2048
0.00.048.136 I print_info: n_embd           = 2048
0.00.048.136 I print_info: n_layer          = 24
0.00.048.142 I print_info: n_head           = 16
0.00.048.142 I print_info: n_head_kv        = 16
0.00.048.143 I print_info: n_rot            = 32
0.00.048.143 I print_info: n_swa            = 0
0.00.048.143 I print_info: n_embd_head_k    = 128
0.00.048.143 I print_info: n_embd_head_v    = 128
0.00.048.144 I print_info: n_gqa            = 1
0.00.048.144 I print_info: n_embd_k_gqa     = 2048
0.00.048.145 I print_info: n_embd_v_gqa     = 2048
0.00.048.145 I print_info: f_norm_eps       = 1.0e-05
0.00.048.146 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.146 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.146 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.146 I print_info: f_logit_scale    = 0.0e+00
0.00.048.147 I print_info: n_ff             = 8192
0.00.048.147 I print_info: n_expert         = 0
0.00.048.147 I print_info: n_expert_used    = 0
0.00.048.147 I print_info: causal attn      = 1
0.00.048.148 I print_info: pooling type     = 0
0.00.048.148 I print_info: rope type        = 2
0.00.048.150 I print_info: rope scaling     = linear
0.00.048.150 I print_info: freq_base_train  = 10000.0
0.00.048.151 I print_info: freq_scale_train = 1
0.00.048.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.151 I print_info: rope_finetuned   = unknown
0.00.048.151 I print_info: ssm_d_conv       = 0
0.00.048.151 I print_info: ssm_d_inner      = 0
0.00.048.151 I print_info: ssm_d_state      = 0
0.00.048.151 I print_info: ssm_dt_rank      = 0
0.00.048.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.152 I print_info: model type       = 1.4B
0.00.048.152 I print_info: model params     = 1.41 B
0.00.048.152 I print_info: general.name     = 1.4B
0.00.048.153 I print_info: vocab type       = BPE
0.00.048.153 I print_info: n_vocab          = 50304
0.00.048.153 I print_info: n_merges         = 50009
0.00.048.153 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.153 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.154 I print_info: LF token         = 187 'Ċ'
0.00.048.154 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.154 I print_info: max token length = 1024
0.00.048.155 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.578 I load_tensors: offloading output layer to GPU
0.00.607.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.619 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.607.621 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.609.028 I llama_context: n_seq_max     = 1
0.00.609.031 I llama_context: n_ctx         = 128
0.00.609.031 I llama_context: n_ctx_per_seq = 128
0.00.609.032 I llama_context: n_batch       = 128
0.00.609.032 I llama_context: n_ubatch      = 128
0.00.609.033 I llama_context: flash_attn    = 0
0.00.609.035 I llama_context: freq_base     = 10000.0
0.00.609.035 I llama_context: freq_scale    = 1
0.00.609.036 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.040 I ggml_metal_init: allocating
0.00.609.175 I ggml_metal_init: found device: Apple M4
0.00.609.190 I ggml_metal_init: picking default device: Apple M4
0.00.611.176 I ggml_metal_init: using embedded metal library
0.00.617.934 I ggml_metal_init: GPU name:   Apple M4
0.00.617.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.941 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.942 I ggml_metal_init: simdgroup reduction   = true
0.00.617.943 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.943 I ggml_metal_init: has residency sets    = true
0.00.617.943 I ggml_metal_init: has bfloat            = true
0.00.617.943 I ggml_metal_init: use bfloat            = true
0.00.617.945 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.969 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.635.974 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.495 I init:      Metal KV buffer size =    24.00 MiB
0.00.639.499 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.852 I init:      Metal compute buffer size =    25.56 MiB
0.00.642.854 I init:        CPU compute buffer size =     1.06 MiB
0.00.642.855 I init: graph nodes  = 967
0.00.642.855 I init: graph splits = 2
0.00.642.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.673 I 
0.00.671.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.783 I perplexity: tokenizing the input ..
0.00.679.049 I perplexity: tokenization took 7.262 ms
0.00.679.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.847 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.818.182 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.818.194 I llama_perf_context_print:        load time =     655.95 ms
0.00.818.197 I llama_perf_context_print: prompt eval time =     136.90 ms /   128 tokens (    1.07 ms per token,   934.98 tokens per second)
0.00.818.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.198 I llama_perf_context_print:       total time =     146.53 ms /   129 tokens
0.00.818.770 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.080s
sys	0m0.127s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.852 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.805 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.805 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.808 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.808 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.808 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.412 I llama_model_loader: - type  f32:  194 tensors
0.00.025.412 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.413 I print_info: file format = GGUF V3 (latest)
0.00.025.414 I print_info: file type   = Q4_1
0.00.025.414 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.678 I load: special tokens cache size = 25
0.00.039.636 I load: token to piece cache size = 0.2984 MB
0.00.039.651 I print_info: arch             = gptneox
0.00.039.652 I print_info: vocab_only       = 0
0.00.039.652 I print_info: n_ctx_train      = 2048
0.00.039.652 I print_info: n_embd           = 2048
0.00.039.652 I print_info: n_layer          = 24
0.00.039.655 I print_info: n_head           = 16
0.00.039.656 I print_info: n_head_kv        = 16
0.00.039.656 I print_info: n_rot            = 32
0.00.039.656 I print_info: n_swa            = 0
0.00.039.657 I print_info: n_embd_head_k    = 128
0.00.039.657 I print_info: n_embd_head_v    = 128
0.00.039.657 I print_info: n_gqa            = 1
0.00.039.658 I print_info: n_embd_k_gqa     = 2048
0.00.039.659 I print_info: n_embd_v_gqa     = 2048
0.00.039.660 I print_info: f_norm_eps       = 1.0e-05
0.00.039.660 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.660 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.660 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.660 I print_info: f_logit_scale    = 0.0e+00
0.00.039.661 I print_info: n_ff             = 8192
0.00.039.661 I print_info: n_expert         = 0
0.00.039.662 I print_info: n_expert_used    = 0
0.00.039.662 I print_info: causal attn      = 1
0.00.039.662 I print_info: pooling type     = 0
0.00.039.663 I print_info: rope type        = 2
0.00.039.664 I print_info: rope scaling     = linear
0.00.039.665 I print_info: freq_base_train  = 10000.0
0.00.039.665 I print_info: freq_scale_train = 1
0.00.039.665 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.665 I print_info: rope_finetuned   = unknown
0.00.039.665 I print_info: ssm_d_conv       = 0
0.00.039.666 I print_info: ssm_d_inner      = 0
0.00.039.666 I print_info: ssm_d_state      = 0
0.00.039.666 I print_info: ssm_dt_rank      = 0
0.00.039.666 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.666 I print_info: model type       = 1.4B
0.00.039.666 I print_info: model params     = 1.41 B
0.00.039.666 I print_info: general.name     = 1.4B
0.00.039.667 I print_info: vocab type       = BPE
0.00.039.668 I print_info: n_vocab          = 50304
0.00.039.672 I print_info: n_merges         = 50009
0.00.039.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: LF token         = 187 'Ċ'
0.00.039.673 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: max token length = 1024
0.00.039.674 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.336 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.346 I load_tensors: offloading output layer to GPU
0.00.639.347 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.376 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.639.377 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.640.900 I llama_context: n_seq_max     = 1
0.00.640.903 I llama_context: n_ctx         = 2048
0.00.640.903 I llama_context: n_ctx_per_seq = 2048
0.00.640.904 I llama_context: n_batch       = 2048
0.00.640.905 I llama_context: n_ubatch      = 512
0.00.640.905 I llama_context: flash_attn    = 0
0.00.640.907 I llama_context: freq_base     = 10000.0
0.00.640.907 I llama_context: freq_scale    = 1
0.00.640.909 I ggml_metal_init: allocating
0.00.640.969 I ggml_metal_init: found device: Apple M4
0.00.640.981 I ggml_metal_init: picking default device: Apple M4
0.00.642.725 I ggml_metal_init: using embedded metal library
0.00.649.317 I ggml_metal_init: GPU name:   Apple M4
0.00.649.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.323 I ggml_metal_init: simdgroup reduction   = true
0.00.649.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.324 I ggml_metal_init: has residency sets    = true
0.00.649.324 I ggml_metal_init: has bfloat            = true
0.00.649.324 I ggml_metal_init: use bfloat            = true
0.00.649.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.106 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.667.112 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.152 I init:      Metal KV buffer size =   384.00 MiB
0.00.723.158 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.212 I init:      Metal compute buffer size =   102.25 MiB
0.00.727.214 I init:        CPU compute buffer size =     8.01 MiB
0.00.727.215 I init: graph nodes  = 967
0.00.727.215 I init: graph splits = 2
0.00.727.221 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.344 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.912 I main: llama threadpool init, n_threads = 4
0.00.782.960 I 
0.00.782.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.984 I 
0.00.783.152 I sampler seed: 1234
0.00.783.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.168 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.168 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.169 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.506.392 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.506.393 I llama_perf_context_print:        load time =     773.37 ms
0.01.506.394 I llama_perf_context_print: prompt eval time =      47.70 ms /     7 tokens (    6.81 ms per token,   146.76 tokens per second)
0.01.506.395 I llama_perf_context_print:        eval time =     672.74 ms /    63 runs   (   10.68 ms per token,    93.65 tokens per second)
0.01.506.395 I llama_perf_context_print:       total time =     724.17 ms /    70 tokens
0.01.509.912 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.926 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.929 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.930 I llama_model_loader: - type  f32:  194 tensors
0.00.024.930 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.931 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.931 I print_info: file format = GGUF V3 (latest)
0.00.024.932 I print_info: file type   = Q4_1
0.00.024.933 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.972 I load: special tokens cache size = 25
0.00.039.180 I load: token to piece cache size = 0.2984 MB
0.00.039.197 I print_info: arch             = gptneox
0.00.039.198 I print_info: vocab_only       = 0
0.00.039.198 I print_info: n_ctx_train      = 2048
0.00.039.198 I print_info: n_embd           = 2048
0.00.039.198 I print_info: n_layer          = 24
0.00.039.202 I print_info: n_head           = 16
0.00.039.204 I print_info: n_head_kv        = 16
0.00.039.204 I print_info: n_rot            = 32
0.00.039.205 I print_info: n_swa            = 0
0.00.039.205 I print_info: n_embd_head_k    = 128
0.00.039.205 I print_info: n_embd_head_v    = 128
0.00.039.205 I print_info: n_gqa            = 1
0.00.039.206 I print_info: n_embd_k_gqa     = 2048
0.00.039.207 I print_info: n_embd_v_gqa     = 2048
0.00.039.207 I print_info: f_norm_eps       = 1.0e-05
0.00.039.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.208 I print_info: f_logit_scale    = 0.0e+00
0.00.039.209 I print_info: n_ff             = 8192
0.00.039.209 I print_info: n_expert         = 0
0.00.039.209 I print_info: n_expert_used    = 0
0.00.039.209 I print_info: causal attn      = 1
0.00.039.209 I print_info: pooling type     = 0
0.00.039.211 I print_info: rope type        = 2
0.00.039.212 I print_info: rope scaling     = linear
0.00.039.212 I print_info: freq_base_train  = 10000.0
0.00.039.212 I print_info: freq_scale_train = 1
0.00.039.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.213 I print_info: rope_finetuned   = unknown
0.00.039.213 I print_info: ssm_d_conv       = 0
0.00.039.213 I print_info: ssm_d_inner      = 0
0.00.039.213 I print_info: ssm_d_state      = 0
0.00.039.213 I print_info: ssm_dt_rank      = 0
0.00.039.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.213 I print_info: model type       = 1.4B
0.00.039.214 I print_info: model params     = 1.41 B
0.00.039.214 I print_info: general.name     = 1.4B
0.00.039.214 I print_info: vocab type       = BPE
0.00.039.214 I print_info: n_vocab          = 50304
0.00.039.215 I print_info: n_merges         = 50009
0.00.039.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: LF token         = 187 'Ċ'
0.00.039.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: max token length = 1024
0.00.039.216 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.760 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.775 I load_tensors: offloading output layer to GPU
0.00.664.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.808 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.664.809 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.666.561 I llama_context: n_seq_max     = 1
0.00.666.564 I llama_context: n_ctx         = 128
0.00.666.565 I llama_context: n_ctx_per_seq = 128
0.00.666.565 I llama_context: n_batch       = 128
0.00.666.565 I llama_context: n_ubatch      = 128
0.00.666.566 I llama_context: flash_attn    = 0
0.00.666.568 I llama_context: freq_base     = 10000.0
0.00.666.568 I llama_context: freq_scale    = 1
0.00.666.569 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.666.571 I ggml_metal_init: allocating
0.00.666.649 I ggml_metal_init: found device: Apple M4
0.00.666.662 I ggml_metal_init: picking default device: Apple M4
0.00.668.383 I ggml_metal_init: using embedded metal library
0.00.674.826 I ggml_metal_init: GPU name:   Apple M4
0.00.674.830 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.833 I ggml_metal_init: simdgroup reduction   = true
0.00.674.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.833 I ggml_metal_init: has residency sets    = true
0.00.674.834 I ggml_metal_init: has bfloat            = true
0.00.674.834 I ggml_metal_init: use bfloat            = true
0.00.674.835 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.658 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.692.663 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.212 I init:      Metal KV buffer size =    24.00 MiB
0.00.696.218 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.699.358 I init:      Metal compute buffer size =    25.56 MiB
0.00.699.360 I init:        CPU compute buffer size =     1.06 MiB
0.00.699.360 I init: graph nodes  = 967
0.00.699.361 I init: graph splits = 2
0.00.699.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.203 I 
0.00.727.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.293 I perplexity: tokenizing the input ..
0.00.734.185 I perplexity: tokenization took 6.888 ms
0.00.734.204 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.825 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.873.158 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.873.176 I llama_perf_context_print:        load time =     718.31 ms
0.00.873.177 I llama_perf_context_print: prompt eval time =     136.64 ms /   128 tokens (    1.07 ms per token,   936.76 tokens per second)
0.00.873.180 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.181 I llama_perf_context_print:       total time =     145.97 ms /   129 tokens
0.00.873.731 I ggml_metal_free: deallocating

real	0m0.888s
user	0m0.080s
sys	0m0.141s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.790 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.330 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.346 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.984 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.987 I llama_model_loader: - type  f32:  194 tensors
0.00.025.987 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.988 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.988 I print_info: file format = GGUF V3 (latest)
0.00.025.989 I print_info: file type   = Q5_0
0.00.025.990 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.220 I load: special tokens cache size = 25
0.00.040.312 I load: token to piece cache size = 0.2984 MB
0.00.040.326 I print_info: arch             = gptneox
0.00.040.327 I print_info: vocab_only       = 0
0.00.040.327 I print_info: n_ctx_train      = 2048
0.00.040.328 I print_info: n_embd           = 2048
0.00.040.328 I print_info: n_layer          = 24
0.00.040.330 I print_info: n_head           = 16
0.00.040.331 I print_info: n_head_kv        = 16
0.00.040.331 I print_info: n_rot            = 32
0.00.040.331 I print_info: n_swa            = 0
0.00.040.332 I print_info: n_embd_head_k    = 128
0.00.040.332 I print_info: n_embd_head_v    = 128
0.00.040.332 I print_info: n_gqa            = 1
0.00.040.333 I print_info: n_embd_k_gqa     = 2048
0.00.040.334 I print_info: n_embd_v_gqa     = 2048
0.00.040.334 I print_info: f_norm_eps       = 1.0e-05
0.00.040.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.335 I print_info: f_logit_scale    = 0.0e+00
0.00.040.336 I print_info: n_ff             = 8192
0.00.040.336 I print_info: n_expert         = 0
0.00.040.336 I print_info: n_expert_used    = 0
0.00.040.337 I print_info: causal attn      = 1
0.00.040.337 I print_info: pooling type     = 0
0.00.040.337 I print_info: rope type        = 2
0.00.040.338 I print_info: rope scaling     = linear
0.00.040.339 I print_info: freq_base_train  = 10000.0
0.00.040.339 I print_info: freq_scale_train = 1
0.00.040.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.339 I print_info: rope_finetuned   = unknown
0.00.040.340 I print_info: ssm_d_conv       = 0
0.00.040.340 I print_info: ssm_d_inner      = 0
0.00.040.340 I print_info: ssm_d_state      = 0
0.00.040.340 I print_info: ssm_dt_rank      = 0
0.00.040.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.340 I print_info: model type       = 1.4B
0.00.040.340 I print_info: model params     = 1.41 B
0.00.040.341 I print_info: general.name     = 1.4B
0.00.040.341 I print_info: vocab type       = BPE
0.00.040.341 I print_info: n_vocab          = 50304
0.00.040.341 I print_info: n_merges         = 50009
0.00.040.342 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.342 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.342 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.342 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.342 I print_info: LF token         = 187 'Ċ'
0.00.040.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.343 I print_info: max token length = 1024
0.00.040.343 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.715.529 I load_tensors: offloading 24 repeating layers to GPU
0.00.715.544 I load_tensors: offloading output layer to GPU
0.00.715.545 I load_tensors: offloaded 25/25 layers to GPU
0.00.715.582 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.715.583 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.716.982 I llama_context: n_seq_max     = 1
0.00.716.985 I llama_context: n_ctx         = 2048
0.00.716.986 I llama_context: n_ctx_per_seq = 2048
0.00.716.986 I llama_context: n_batch       = 2048
0.00.716.986 I llama_context: n_ubatch      = 512
0.00.716.987 I llama_context: flash_attn    = 0
0.00.716.988 I llama_context: freq_base     = 10000.0
0.00.716.988 I llama_context: freq_scale    = 1
0.00.716.989 I ggml_metal_init: allocating
0.00.717.001 I ggml_metal_init: found device: Apple M4
0.00.717.010 I ggml_metal_init: picking default device: Apple M4
0.00.718.470 I ggml_metal_init: using embedded metal library
0.00.724.821 I ggml_metal_init: GPU name:   Apple M4
0.00.724.824 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.724.825 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.724.826 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.724.826 I ggml_metal_init: simdgroup reduction   = true
0.00.724.826 I ggml_metal_init: simdgroup matrix mul. = true
0.00.724.827 I ggml_metal_init: has residency sets    = true
0.00.724.827 I ggml_metal_init: has bfloat            = true
0.00.724.827 I ggml_metal_init: use bfloat            = true
0.00.724.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.724.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.742.346 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.742.350 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.117 I init:      Metal KV buffer size =   384.00 MiB
0.00.796.124 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.800.607 I init:      Metal compute buffer size =   102.25 MiB
0.00.800.609 I init:        CPU compute buffer size =     8.01 MiB
0.00.800.610 I init: graph nodes  = 967
0.00.800.610 I init: graph splits = 2
0.00.800.616 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.800.745 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.800.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.835 I main: llama threadpool init, n_threads = 4
0.00.858.882 I 
0.00.858.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.908 I 
0.00.859.054 I sampler seed: 1234
0.00.859.059 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.859.077 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.859.077 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.859.077 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.647.948 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.01.647.949 I llama_perf_context_print:        load time =     848.29 ms
0.01.647.951 I llama_perf_context_print: prompt eval time =      52.79 ms /     7 tokens (    7.54 ms per token,   132.59 tokens per second)
0.01.647.951 I llama_perf_context_print:        eval time =     733.25 ms /    63 runs   (   11.64 ms per token,    85.92 tokens per second)
0.01.647.952 I llama_perf_context_print:       total time =     789.86 ms /    70 tokens
0.01.651.931 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.108s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.273 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.563 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.564 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.571 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.303 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.307 I llama_model_loader: - type  f32:  194 tensors
0.00.024.307 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.308 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.308 I print_info: file format = GGUF V3 (latest)
0.00.024.309 I print_info: file type   = Q5_0
0.00.024.310 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.493 I load: special tokens cache size = 25
0.00.038.588 I load: token to piece cache size = 0.2984 MB
0.00.038.606 I print_info: arch             = gptneox
0.00.038.607 I print_info: vocab_only       = 0
0.00.038.607 I print_info: n_ctx_train      = 2048
0.00.038.607 I print_info: n_embd           = 2048
0.00.038.607 I print_info: n_layer          = 24
0.00.038.611 I print_info: n_head           = 16
0.00.038.613 I print_info: n_head_kv        = 16
0.00.038.613 I print_info: n_rot            = 32
0.00.038.613 I print_info: n_swa            = 0
0.00.038.613 I print_info: n_embd_head_k    = 128
0.00.038.613 I print_info: n_embd_head_v    = 128
0.00.038.614 I print_info: n_gqa            = 1
0.00.038.615 I print_info: n_embd_k_gqa     = 2048
0.00.038.615 I print_info: n_embd_v_gqa     = 2048
0.00.038.616 I print_info: f_norm_eps       = 1.0e-05
0.00.038.616 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.616 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.616 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.616 I print_info: f_logit_scale    = 0.0e+00
0.00.038.617 I print_info: n_ff             = 8192
0.00.038.617 I print_info: n_expert         = 0
0.00.038.617 I print_info: n_expert_used    = 0
0.00.038.617 I print_info: causal attn      = 1
0.00.038.618 I print_info: pooling type     = 0
0.00.038.618 I print_info: rope type        = 2
0.00.038.618 I print_info: rope scaling     = linear
0.00.038.618 I print_info: freq_base_train  = 10000.0
0.00.038.619 I print_info: freq_scale_train = 1
0.00.038.619 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.619 I print_info: rope_finetuned   = unknown
0.00.038.619 I print_info: ssm_d_conv       = 0
0.00.038.619 I print_info: ssm_d_inner      = 0
0.00.038.619 I print_info: ssm_d_state      = 0
0.00.038.619 I print_info: ssm_dt_rank      = 0
0.00.038.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.620 I print_info: model type       = 1.4B
0.00.038.620 I print_info: model params     = 1.41 B
0.00.038.620 I print_info: general.name     = 1.4B
0.00.038.622 I print_info: vocab type       = BPE
0.00.038.622 I print_info: n_vocab          = 50304
0.00.038.622 I print_info: n_merges         = 50009
0.00.038.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: LF token         = 187 'Ċ'
0.00.038.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: max token length = 1024
0.00.038.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.690.673 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.690 I load_tensors: offloading output layer to GPU
0.00.690.691 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.724 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.690.725 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.692.515 I llama_context: n_seq_max     = 1
0.00.692.518 I llama_context: n_ctx         = 128
0.00.692.518 I llama_context: n_ctx_per_seq = 128
0.00.692.519 I llama_context: n_batch       = 128
0.00.692.519 I llama_context: n_ubatch      = 128
0.00.692.519 I llama_context: flash_attn    = 0
0.00.692.522 I llama_context: freq_base     = 10000.0
0.00.692.523 I llama_context: freq_scale    = 1
0.00.692.523 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.692.525 I ggml_metal_init: allocating
0.00.692.591 I ggml_metal_init: found device: Apple M4
0.00.692.603 I ggml_metal_init: picking default device: Apple M4
0.00.694.150 I ggml_metal_init: using embedded metal library
0.00.700.522 I ggml_metal_init: GPU name:   Apple M4
0.00.700.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.700.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.700.528 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.700.528 I ggml_metal_init: simdgroup reduction   = true
0.00.700.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.700.529 I ggml_metal_init: has residency sets    = true
0.00.700.529 I ggml_metal_init: has bfloat            = true
0.00.700.529 I ggml_metal_init: use bfloat            = true
0.00.700.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.700.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.538 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.717.543 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.964 I init:      Metal KV buffer size =    24.00 MiB
0.00.720.970 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.724.085 I init:      Metal compute buffer size =    25.56 MiB
0.00.724.087 I init:        CPU compute buffer size =     1.06 MiB
0.00.724.088 I init: graph nodes  = 967
0.00.724.088 I init: graph splits = 2
0.00.724.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.724.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.262 I 
0.00.754.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.366 I perplexity: tokenizing the input ..
0.00.761.897 I perplexity: tokenization took 7.527 ms
0.00.761.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.907.502 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.908.844 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.908.863 I llama_perf_context_print:        load time =     745.98 ms
0.00.908.864 I llama_perf_context_print: prompt eval time =     144.67 ms /   128 tokens (    1.13 ms per token,   884.79 tokens per second)
0.00.908.865 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.908.865 I llama_perf_context_print:       total time =     154.60 ms /   129 tokens
0.00.909.465 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.080s
sys	0m0.131s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.650 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.292 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.313 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.316 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.954 I llama_model_loader: - type  f32:  194 tensors
0.00.024.954 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.954 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.955 I print_info: file format = GGUF V3 (latest)
0.00.024.955 I print_info: file type   = Q5_1
0.00.024.956 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.138 I load: special tokens cache size = 25
0.00.039.174 I load: token to piece cache size = 0.2984 MB
0.00.039.188 I print_info: arch             = gptneox
0.00.039.189 I print_info: vocab_only       = 0
0.00.039.190 I print_info: n_ctx_train      = 2048
0.00.039.190 I print_info: n_embd           = 2048
0.00.039.190 I print_info: n_layer          = 24
0.00.039.193 I print_info: n_head           = 16
0.00.039.193 I print_info: n_head_kv        = 16
0.00.039.193 I print_info: n_rot            = 32
0.00.039.194 I print_info: n_swa            = 0
0.00.039.194 I print_info: n_embd_head_k    = 128
0.00.039.194 I print_info: n_embd_head_v    = 128
0.00.039.195 I print_info: n_gqa            = 1
0.00.039.195 I print_info: n_embd_k_gqa     = 2048
0.00.039.196 I print_info: n_embd_v_gqa     = 2048
0.00.039.197 I print_info: f_norm_eps       = 1.0e-05
0.00.039.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.197 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.197 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.198 I print_info: f_logit_scale    = 0.0e+00
0.00.039.198 I print_info: n_ff             = 8192
0.00.039.198 I print_info: n_expert         = 0
0.00.039.199 I print_info: n_expert_used    = 0
0.00.039.199 I print_info: causal attn      = 1
0.00.039.199 I print_info: pooling type     = 0
0.00.039.199 I print_info: rope type        = 2
0.00.039.199 I print_info: rope scaling     = linear
0.00.039.201 I print_info: freq_base_train  = 10000.0
0.00.039.201 I print_info: freq_scale_train = 1
0.00.039.201 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.202 I print_info: rope_finetuned   = unknown
0.00.039.202 I print_info: ssm_d_conv       = 0
0.00.039.202 I print_info: ssm_d_inner      = 0
0.00.039.202 I print_info: ssm_d_state      = 0
0.00.039.202 I print_info: ssm_dt_rank      = 0
0.00.039.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.203 I print_info: model type       = 1.4B
0.00.039.203 I print_info: model params     = 1.41 B
0.00.039.203 I print_info: general.name     = 1.4B
0.00.039.204 I print_info: vocab type       = BPE
0.00.039.204 I print_info: n_vocab          = 50304
0.00.039.204 I print_info: n_merges         = 50009
0.00.039.204 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.205 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.205 I print_info: LF token         = 187 'Ċ'
0.00.039.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.205 I print_info: max token length = 1024
0.00.039.206 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.640.567 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.572 I load_tensors: offloading output layer to GPU
0.00.640.573 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.596 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.640.598 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.642.073 I llama_context: n_seq_max     = 1
0.00.642.075 I llama_context: n_ctx         = 2048
0.00.642.076 I llama_context: n_ctx_per_seq = 2048
0.00.642.076 I llama_context: n_batch       = 2048
0.00.642.076 I llama_context: n_ubatch      = 512
0.00.642.077 I llama_context: flash_attn    = 0
0.00.642.078 I llama_context: freq_base     = 10000.0
0.00.642.079 I llama_context: freq_scale    = 1
0.00.642.080 I ggml_metal_init: allocating
0.00.642.095 I ggml_metal_init: found device: Apple M4
0.00.642.104 I ggml_metal_init: picking default device: Apple M4
0.00.643.594 I ggml_metal_init: using embedded metal library
0.00.649.451 I ggml_metal_init: GPU name:   Apple M4
0.00.649.455 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.457 I ggml_metal_init: simdgroup reduction   = true
0.00.649.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.458 I ggml_metal_init: has residency sets    = true
0.00.649.458 I ggml_metal_init: has bfloat            = true
0.00.649.458 I ggml_metal_init: use bfloat            = true
0.00.649.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.602 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.666.606 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.241 I init:      Metal KV buffer size =   384.00 MiB
0.00.718.249 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.591 I init:      Metal compute buffer size =   102.25 MiB
0.00.722.593 I init:        CPU compute buffer size =     8.01 MiB
0.00.722.594 I init: graph nodes  = 967
0.00.722.594 I init: graph splits = 2
0.00.722.599 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.679 I main: llama threadpool init, n_threads = 4
0.00.783.724 I 
0.00.783.747 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.747 I 
0.00.783.927 I sampler seed: 1234
0.00.783.931 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.965 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.969 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.970 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.620.736 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.620.736 I llama_perf_context_print:        load time =     774.32 ms
0.01.620.737 I llama_perf_context_print: prompt eval time =      51.42 ms /     7 tokens (    7.35 ms per token,   136.13 tokens per second)
0.01.620.738 I llama_perf_context_print:        eval time =     782.46 ms /    63 runs   (   12.42 ms per token,    80.51 tokens per second)
0.01.620.738 I llama_perf_context_print:       total time =     837.76 ms /    70 tokens
0.01.624.567 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.107s
sys	0m0.236s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.970 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.950 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.845 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.846 I llama_model_loader: - type  f32:  194 tensors
0.00.024.847 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.848 I print_info: file format = GGUF V3 (latest)
0.00.024.848 I print_info: file type   = Q5_1
0.00.024.850 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.152 I load: special tokens cache size = 25
0.00.039.190 I load: token to piece cache size = 0.2984 MB
0.00.039.206 I print_info: arch             = gptneox
0.00.039.207 I print_info: vocab_only       = 0
0.00.039.207 I print_info: n_ctx_train      = 2048
0.00.039.207 I print_info: n_embd           = 2048
0.00.039.207 I print_info: n_layer          = 24
0.00.039.211 I print_info: n_head           = 16
0.00.039.212 I print_info: n_head_kv        = 16
0.00.039.212 I print_info: n_rot            = 32
0.00.039.213 I print_info: n_swa            = 0
0.00.039.213 I print_info: n_embd_head_k    = 128
0.00.039.213 I print_info: n_embd_head_v    = 128
0.00.039.214 I print_info: n_gqa            = 1
0.00.039.214 I print_info: n_embd_k_gqa     = 2048
0.00.039.215 I print_info: n_embd_v_gqa     = 2048
0.00.039.216 I print_info: f_norm_eps       = 1.0e-05
0.00.039.216 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.216 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.217 I print_info: f_logit_scale    = 0.0e+00
0.00.039.217 I print_info: n_ff             = 8192
0.00.039.218 I print_info: n_expert         = 0
0.00.039.218 I print_info: n_expert_used    = 0
0.00.039.218 I print_info: causal attn      = 1
0.00.039.218 I print_info: pooling type     = 0
0.00.039.218 I print_info: rope type        = 2
0.00.039.218 I print_info: rope scaling     = linear
0.00.039.220 I print_info: freq_base_train  = 10000.0
0.00.039.221 I print_info: freq_scale_train = 1
0.00.039.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.221 I print_info: rope_finetuned   = unknown
0.00.039.221 I print_info: ssm_d_conv       = 0
0.00.039.221 I print_info: ssm_d_inner      = 0
0.00.039.221 I print_info: ssm_d_state      = 0
0.00.039.223 I print_info: ssm_dt_rank      = 0
0.00.039.223 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.223 I print_info: model type       = 1.4B
0.00.039.223 I print_info: model params     = 1.41 B
0.00.039.223 I print_info: general.name     = 1.4B
0.00.039.224 I print_info: vocab type       = BPE
0.00.039.224 I print_info: n_vocab          = 50304
0.00.039.224 I print_info: n_merges         = 50009
0.00.039.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.225 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.225 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.225 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.225 I print_info: LF token         = 187 'Ċ'
0.00.039.225 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.226 I print_info: max token length = 1024
0.00.039.226 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.724 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.728 I load_tensors: offloading output layer to GPU
0.00.613.729 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.754 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.613.756 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.615.303 I llama_context: n_seq_max     = 1
0.00.615.305 I llama_context: n_ctx         = 128
0.00.615.306 I llama_context: n_ctx_per_seq = 128
0.00.615.306 I llama_context: n_batch       = 128
0.00.615.306 I llama_context: n_ubatch      = 128
0.00.615.307 I llama_context: flash_attn    = 0
0.00.615.308 I llama_context: freq_base     = 10000.0
0.00.615.308 I llama_context: freq_scale    = 1
0.00.615.309 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.310 I ggml_metal_init: allocating
0.00.615.346 I ggml_metal_init: found device: Apple M4
0.00.615.358 I ggml_metal_init: picking default device: Apple M4
0.00.616.830 I ggml_metal_init: using embedded metal library
0.00.622.795 I ggml_metal_init: GPU name:   Apple M4
0.00.622.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.801 I ggml_metal_init: simdgroup reduction   = true
0.00.622.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.801 I ggml_metal_init: has residency sets    = true
0.00.622.802 I ggml_metal_init: has bfloat            = true
0.00.622.802 I ggml_metal_init: use bfloat            = true
0.00.622.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.786 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.639.791 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.643.176 I init:      Metal KV buffer size =    24.00 MiB
0.00.643.179 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.139 I init:      Metal compute buffer size =    25.56 MiB
0.00.646.140 I init:        CPU compute buffer size =     1.06 MiB
0.00.646.141 I init: graph nodes  = 967
0.00.646.141 I init: graph splits = 2
0.00.646.144 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.067 I 
0.00.678.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.172 I perplexity: tokenizing the input ..
0.00.685.380 I perplexity: tokenization took 7.204 ms
0.00.685.411 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.577 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.836.141 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.836.167 I llama_perf_context_print:        load time =     669.12 ms
0.00.836.168 I llama_perf_context_print: prompt eval time =     148.24 ms /   128 tokens (    1.16 ms per token,   863.46 tokens per second)
0.00.836.169 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.169 I llama_perf_context_print:       total time =     158.11 ms /   129 tokens
0.00.836.725 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.078s
sys	0m0.146s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.664 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.171 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.672 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.674 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.674 I llama_model_loader: - type  f32:  194 tensors
0.00.024.675 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.675 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.675 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.675 I print_info: file format = GGUF V3 (latest)
0.00.024.676 I print_info: file type   = Q2_K - Medium
0.00.024.678 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.550 I load: special tokens cache size = 25
0.00.038.416 I load: token to piece cache size = 0.2984 MB
0.00.038.430 I print_info: arch             = gptneox
0.00.038.431 I print_info: vocab_only       = 0
0.00.038.432 I print_info: n_ctx_train      = 2048
0.00.038.432 I print_info: n_embd           = 2048
0.00.038.432 I print_info: n_layer          = 24
0.00.038.435 I print_info: n_head           = 16
0.00.038.435 I print_info: n_head_kv        = 16
0.00.038.436 I print_info: n_rot            = 32
0.00.038.436 I print_info: n_swa            = 0
0.00.038.436 I print_info: n_embd_head_k    = 128
0.00.038.436 I print_info: n_embd_head_v    = 128
0.00.038.437 I print_info: n_gqa            = 1
0.00.038.438 I print_info: n_embd_k_gqa     = 2048
0.00.038.438 I print_info: n_embd_v_gqa     = 2048
0.00.038.440 I print_info: f_norm_eps       = 1.0e-05
0.00.038.441 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.441 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.441 I print_info: f_logit_scale    = 0.0e+00
0.00.038.442 I print_info: n_ff             = 8192
0.00.038.442 I print_info: n_expert         = 0
0.00.038.442 I print_info: n_expert_used    = 0
0.00.038.442 I print_info: causal attn      = 1
0.00.038.442 I print_info: pooling type     = 0
0.00.038.442 I print_info: rope type        = 2
0.00.038.443 I print_info: rope scaling     = linear
0.00.038.443 I print_info: freq_base_train  = 10000.0
0.00.038.443 I print_info: freq_scale_train = 1
0.00.038.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.443 I print_info: rope_finetuned   = unknown
0.00.038.444 I print_info: ssm_d_conv       = 0
0.00.038.444 I print_info: ssm_d_inner      = 0
0.00.038.444 I print_info: ssm_d_state      = 0
0.00.038.445 I print_info: ssm_dt_rank      = 0
0.00.038.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.445 I print_info: model type       = 1.4B
0.00.038.445 I print_info: model params     = 1.41 B
0.00.038.445 I print_info: general.name     = 1.4B
0.00.038.446 I print_info: vocab type       = BPE
0.00.038.446 I print_info: n_vocab          = 50304
0.00.038.446 I print_info: n_merges         = 50009
0.00.038.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.447 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.447 I print_info: LF token         = 187 'Ċ'
0.00.038.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: max token length = 1024
0.00.038.449 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.351.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.164 I load_tensors: offloading output layer to GPU
0.00.351.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.200 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.201 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.488 I llama_context: n_seq_max     = 1
0.00.352.493 I llama_context: n_ctx         = 2048
0.00.352.493 I llama_context: n_ctx_per_seq = 2048
0.00.352.494 I llama_context: n_batch       = 2048
0.00.352.494 I llama_context: n_ubatch      = 512
0.00.352.495 I llama_context: flash_attn    = 0
0.00.352.496 I llama_context: freq_base     = 10000.0
0.00.352.497 I llama_context: freq_scale    = 1
0.00.352.498 I ggml_metal_init: allocating
0.00.352.571 I ggml_metal_init: found device: Apple M4
0.00.352.585 I ggml_metal_init: picking default device: Apple M4
0.00.354.592 I ggml_metal_init: using embedded metal library
0.00.361.180 I ggml_metal_init: GPU name:   Apple M4
0.00.361.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.194 I ggml_metal_init: simdgroup reduction   = true
0.00.361.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.194 I ggml_metal_init: has residency sets    = true
0.00.361.194 I ggml_metal_init: has bfloat            = true
0.00.361.195 I ggml_metal_init: use bfloat            = true
0.00.361.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.382.831 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.382.837 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.663 I init:      Metal KV buffer size =   384.00 MiB
0.00.438.672 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.442.561 I init:      Metal compute buffer size =   102.25 MiB
0.00.442.564 I init:        CPU compute buffer size =     8.01 MiB
0.00.442.564 I init: graph nodes  = 967
0.00.442.564 I init: graph splits = 2
0.00.442.569 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.442.695 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.442.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.759 I main: llama threadpool init, n_threads = 4
0.00.500.800 I 
0.00.500.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.822 I 
0.00.501.001 I sampler seed: 1234
0.00.501.005 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.017 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.017 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.017 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.165.519 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.165.520 I llama_perf_context_print:        load time =     490.37 ms
0.01.165.520 I llama_perf_context_print: prompt eval time =      35.57 ms /     7 tokens (    5.08 ms per token,   196.77 tokens per second)
0.01.165.521 I llama_perf_context_print:        eval time =     626.58 ms /    63 runs   (    9.95 ms per token,   100.55 tokens per second)
0.01.165.522 I llama_perf_context_print:       total time =     665.48 ms /    70 tokens
0.01.168.200 I ggml_metal_free: deallocating

real	0m1.184s
user	0m0.112s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.264 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.367 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.373 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.373 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.373 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.202 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.203 I llama_model_loader: - type  f32:  194 tensors
0.00.026.203 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.204 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.204 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.204 I print_info: file format = GGUF V3 (latest)
0.00.026.206 I print_info: file type   = Q2_K - Medium
0.00.026.208 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.499 I load: special tokens cache size = 25
0.00.040.508 I load: token to piece cache size = 0.2984 MB
0.00.040.523 I print_info: arch             = gptneox
0.00.040.524 I print_info: vocab_only       = 0
0.00.040.525 I print_info: n_ctx_train      = 2048
0.00.040.525 I print_info: n_embd           = 2048
0.00.040.525 I print_info: n_layer          = 24
0.00.040.529 I print_info: n_head           = 16
0.00.040.529 I print_info: n_head_kv        = 16
0.00.040.530 I print_info: n_rot            = 32
0.00.040.530 I print_info: n_swa            = 0
0.00.040.530 I print_info: n_embd_head_k    = 128
0.00.040.530 I print_info: n_embd_head_v    = 128
0.00.040.531 I print_info: n_gqa            = 1
0.00.040.531 I print_info: n_embd_k_gqa     = 2048
0.00.040.532 I print_info: n_embd_v_gqa     = 2048
0.00.040.532 I print_info: f_norm_eps       = 1.0e-05
0.00.040.532 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.534 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.534 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.535 I print_info: f_logit_scale    = 0.0e+00
0.00.040.535 I print_info: n_ff             = 8192
0.00.040.535 I print_info: n_expert         = 0
0.00.040.535 I print_info: n_expert_used    = 0
0.00.040.536 I print_info: causal attn      = 1
0.00.040.536 I print_info: pooling type     = 0
0.00.040.536 I print_info: rope type        = 2
0.00.040.536 I print_info: rope scaling     = linear
0.00.040.536 I print_info: freq_base_train  = 10000.0
0.00.040.538 I print_info: freq_scale_train = 1
0.00.040.538 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.538 I print_info: rope_finetuned   = unknown
0.00.040.538 I print_info: ssm_d_conv       = 0
0.00.040.539 I print_info: ssm_d_inner      = 0
0.00.040.539 I print_info: ssm_d_state      = 0
0.00.040.539 I print_info: ssm_dt_rank      = 0
0.00.040.539 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.539 I print_info: model type       = 1.4B
0.00.040.539 I print_info: model params     = 1.41 B
0.00.040.540 I print_info: general.name     = 1.4B
0.00.040.541 I print_info: vocab type       = BPE
0.00.040.541 I print_info: n_vocab          = 50304
0.00.040.541 I print_info: n_merges         = 50009
0.00.040.541 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: LF token         = 187 'Ċ'
0.00.040.543 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: max token length = 1024
0.00.040.544 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.356.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.356.148 I load_tensors: offloading output layer to GPU
0.00.356.149 I load_tensors: offloaded 25/25 layers to GPU
0.00.356.183 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.356.184 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.357.874 I llama_context: n_seq_max     = 1
0.00.357.884 I llama_context: n_ctx         = 128
0.00.357.884 I llama_context: n_ctx_per_seq = 128
0.00.357.885 I llama_context: n_batch       = 128
0.00.357.885 I llama_context: n_ubatch      = 128
0.00.357.886 I llama_context: flash_attn    = 0
0.00.357.889 I llama_context: freq_base     = 10000.0
0.00.357.893 I llama_context: freq_scale    = 1
0.00.357.894 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.357.896 I ggml_metal_init: allocating
0.00.357.990 I ggml_metal_init: found device: Apple M4
0.00.358.004 I ggml_metal_init: picking default device: Apple M4
0.00.359.963 I ggml_metal_init: using embedded metal library
0.00.367.201 I ggml_metal_init: GPU name:   Apple M4
0.00.367.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.367.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.367.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.367.216 I ggml_metal_init: simdgroup reduction   = true
0.00.367.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.367.217 I ggml_metal_init: has residency sets    = true
0.00.367.217 I ggml_metal_init: has bfloat            = true
0.00.367.217 I ggml_metal_init: use bfloat            = true
0.00.367.219 I ggml_metal_init: hasUnifiedMemory      = true
0.00.367.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.389.333 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.389.342 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.393.271 I init:      Metal KV buffer size =    24.00 MiB
0.00.393.280 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.397.100 I init:      Metal compute buffer size =    25.56 MiB
0.00.397.102 I init:        CPU compute buffer size =     1.06 MiB
0.00.397.103 I init: graph nodes  = 967
0.00.397.103 I init: graph splits = 2
0.00.397.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.397.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.136 I 
0.00.429.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.429.229 I perplexity: tokenizing the input ..
0.00.434.352 I perplexity: tokenization took 5.121 ms
0.00.434.364 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.565.735 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.567.125 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.567.138 I llama_perf_context_print:        load time =     418.86 ms
0.00.567.140 I llama_perf_context_print: prompt eval time =     131.14 ms /   128 tokens (    1.02 ms per token,   976.07 tokens per second)
0.00.567.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.567.141 I llama_perf_context_print:       total time =     138.01 ms /   129 tokens
0.00.567.646 I ggml_metal_free: deallocating

real	0m0.583s
user	0m0.079s
sys	0m0.105s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.923 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.413 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.413 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.414 I llama_model_loader: - type  f32:  194 tensors
0.00.026.414 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.415 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.415 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.415 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.416 I print_info: file format = GGUF V3 (latest)
0.00.026.416 I print_info: file type   = Q3_K - Medium
0.00.026.417 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.402 I load: special tokens cache size = 25
0.00.040.649 I load: token to piece cache size = 0.2984 MB
0.00.040.664 I print_info: arch             = gptneox
0.00.040.664 I print_info: vocab_only       = 0
0.00.040.664 I print_info: n_ctx_train      = 2048
0.00.040.664 I print_info: n_embd           = 2048
0.00.040.664 I print_info: n_layer          = 24
0.00.040.668 I print_info: n_head           = 16
0.00.040.669 I print_info: n_head_kv        = 16
0.00.040.669 I print_info: n_rot            = 32
0.00.040.669 I print_info: n_swa            = 0
0.00.040.669 I print_info: n_embd_head_k    = 128
0.00.040.670 I print_info: n_embd_head_v    = 128
0.00.040.670 I print_info: n_gqa            = 1
0.00.040.671 I print_info: n_embd_k_gqa     = 2048
0.00.040.671 I print_info: n_embd_v_gqa     = 2048
0.00.040.672 I print_info: f_norm_eps       = 1.0e-05
0.00.040.672 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.673 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.673 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.673 I print_info: f_logit_scale    = 0.0e+00
0.00.040.673 I print_info: n_ff             = 8192
0.00.040.674 I print_info: n_expert         = 0
0.00.040.674 I print_info: n_expert_used    = 0
0.00.040.674 I print_info: causal attn      = 1
0.00.040.674 I print_info: pooling type     = 0
0.00.040.674 I print_info: rope type        = 2
0.00.040.674 I print_info: rope scaling     = linear
0.00.040.675 I print_info: freq_base_train  = 10000.0
0.00.040.675 I print_info: freq_scale_train = 1
0.00.040.675 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.675 I print_info: rope_finetuned   = unknown
0.00.040.675 I print_info: ssm_d_conv       = 0
0.00.040.675 I print_info: ssm_d_inner      = 0
0.00.040.676 I print_info: ssm_d_state      = 0
0.00.040.676 I print_info: ssm_dt_rank      = 0
0.00.040.676 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.676 I print_info: model type       = 1.4B
0.00.040.676 I print_info: model params     = 1.41 B
0.00.040.676 I print_info: general.name     = 1.4B
0.00.040.677 I print_info: vocab type       = BPE
0.00.040.677 I print_info: n_vocab          = 50304
0.00.040.678 I print_info: n_merges         = 50009
0.00.040.678 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.678 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.678 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.678 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: LF token         = 187 'Ċ'
0.00.040.689 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.689 I print_info: max token length = 1024
0.00.040.689 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.988 I load_tensors: offloading output layer to GPU
0.00.457.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.458.024 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.458.025 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.459.444 I llama_context: n_seq_max     = 1
0.00.459.449 I llama_context: n_ctx         = 2048
0.00.459.450 I llama_context: n_ctx_per_seq = 2048
0.00.459.450 I llama_context: n_batch       = 2048
0.00.459.451 I llama_context: n_ubatch      = 512
0.00.459.451 I llama_context: flash_attn    = 0
0.00.459.453 I llama_context: freq_base     = 10000.0
0.00.459.453 I llama_context: freq_scale    = 1
0.00.459.455 I ggml_metal_init: allocating
0.00.459.534 I ggml_metal_init: found device: Apple M4
0.00.459.557 I ggml_metal_init: picking default device: Apple M4
0.00.461.456 I ggml_metal_init: using embedded metal library
0.00.467.124 I ggml_metal_init: GPU name:   Apple M4
0.00.467.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.139 I ggml_metal_init: simdgroup reduction   = true
0.00.467.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.140 I ggml_metal_init: has residency sets    = true
0.00.467.140 I ggml_metal_init: has bfloat            = true
0.00.467.140 I ggml_metal_init: use bfloat            = true
0.00.467.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.487.325 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.487.329 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.383 I init:      Metal KV buffer size =   384.00 MiB
0.00.546.390 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.550.863 I init:      Metal compute buffer size =   102.25 MiB
0.00.550.866 I init:        CPU compute buffer size =     8.01 MiB
0.00.550.866 I init: graph nodes  = 967
0.00.550.866 I init: graph splits = 2
0.00.550.873 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.551.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.551.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.578 I main: llama threadpool init, n_threads = 4
0.00.608.620 I 
0.00.608.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.643 I 
0.00.608.814 I sampler seed: 1234
0.00.608.819 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.608.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.608.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.608.830 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.361.663 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48497.27 tokens per second)
0.01.361.664 I llama_perf_context_print:        load time =     598.94 ms
0.01.361.665 I llama_perf_context_print: prompt eval time =      49.70 ms /     7 tokens (    7.10 ms per token,   140.85 tokens per second)
0.01.361.665 I llama_perf_context_print:        eval time =     700.50 ms /    63 runs   (   11.12 ms per token,    89.94 tokens per second)
0.01.361.665 I llama_perf_context_print:       total time =     753.80 ms /    70 tokens
0.01.364.290 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.110s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.005 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.168 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.294 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.344 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.344 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.345 I llama_model_loader: - type  f32:  194 tensors
0.00.027.345 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.345 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.346 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.346 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.346 I print_info: file format = GGUF V3 (latest)
0.00.027.347 I print_info: file type   = Q3_K - Medium
0.00.027.353 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.710 I load: special tokens cache size = 25
0.00.041.913 I load: token to piece cache size = 0.2984 MB
0.00.041.929 I print_info: arch             = gptneox
0.00.041.930 I print_info: vocab_only       = 0
0.00.041.930 I print_info: n_ctx_train      = 2048
0.00.041.930 I print_info: n_embd           = 2048
0.00.041.930 I print_info: n_layer          = 24
0.00.041.933 I print_info: n_head           = 16
0.00.041.934 I print_info: n_head_kv        = 16
0.00.041.934 I print_info: n_rot            = 32
0.00.041.934 I print_info: n_swa            = 0
0.00.041.934 I print_info: n_embd_head_k    = 128
0.00.041.935 I print_info: n_embd_head_v    = 128
0.00.041.937 I print_info: n_gqa            = 1
0.00.041.938 I print_info: n_embd_k_gqa     = 2048
0.00.041.938 I print_info: n_embd_v_gqa     = 2048
0.00.041.939 I print_info: f_norm_eps       = 1.0e-05
0.00.041.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.940 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.941 I print_info: f_logit_scale    = 0.0e+00
0.00.041.941 I print_info: n_ff             = 8192
0.00.041.941 I print_info: n_expert         = 0
0.00.041.941 I print_info: n_expert_used    = 0
0.00.041.942 I print_info: causal attn      = 1
0.00.041.942 I print_info: pooling type     = 0
0.00.041.942 I print_info: rope type        = 2
0.00.041.942 I print_info: rope scaling     = linear
0.00.041.942 I print_info: freq_base_train  = 10000.0
0.00.041.943 I print_info: freq_scale_train = 1
0.00.041.943 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.943 I print_info: rope_finetuned   = unknown
0.00.041.943 I print_info: ssm_d_conv       = 0
0.00.041.943 I print_info: ssm_d_inner      = 0
0.00.041.943 I print_info: ssm_d_state      = 0
0.00.041.943 I print_info: ssm_dt_rank      = 0
0.00.041.944 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.944 I print_info: model type       = 1.4B
0.00.041.944 I print_info: model params     = 1.41 B
0.00.041.944 I print_info: general.name     = 1.4B
0.00.041.945 I print_info: vocab type       = BPE
0.00.041.945 I print_info: n_vocab          = 50304
0.00.041.945 I print_info: n_merges         = 50009
0.00.041.946 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.947 I print_info: LF token         = 187 'Ċ'
0.00.041.947 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.949 I print_info: max token length = 1024
0.00.041.949 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.484.747 I load_tensors: offloading 24 repeating layers to GPU
0.00.484.755 I load_tensors: offloading output layer to GPU
0.00.484.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.484.773 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.484.774 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.485.685 I llama_context: n_seq_max     = 1
0.00.485.689 I llama_context: n_ctx         = 128
0.00.485.689 I llama_context: n_ctx_per_seq = 128
0.00.485.690 I llama_context: n_batch       = 128
0.00.485.690 I llama_context: n_ubatch      = 128
0.00.485.691 I llama_context: flash_attn    = 0
0.00.485.692 I llama_context: freq_base     = 10000.0
0.00.485.692 I llama_context: freq_scale    = 1
0.00.485.693 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.485.694 I ggml_metal_init: allocating
0.00.485.739 I ggml_metal_init: found device: Apple M4
0.00.485.751 I ggml_metal_init: picking default device: Apple M4
0.00.486.777 I ggml_metal_init: using embedded metal library
0.00.490.817 I ggml_metal_init: GPU name:   Apple M4
0.00.490.824 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.490.825 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.490.825 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.490.826 I ggml_metal_init: simdgroup reduction   = true
0.00.490.826 I ggml_metal_init: simdgroup matrix mul. = true
0.00.490.826 I ggml_metal_init: has residency sets    = true
0.00.490.827 I ggml_metal_init: has bfloat            = true
0.00.490.827 I ggml_metal_init: use bfloat            = true
0.00.490.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.490.831 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.506.469 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.506.471 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.508.074 I init:      Metal KV buffer size =    24.00 MiB
0.00.508.076 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.509.682 I init:      Metal compute buffer size =    25.56 MiB
0.00.509.683 I init:        CPU compute buffer size =     1.06 MiB
0.00.509.683 I init: graph nodes  = 967
0.00.509.683 I init: graph splits = 2
0.00.509.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.509.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.659 I 
0.00.531.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.707 I perplexity: tokenizing the input ..
0.00.535.713 I perplexity: tokenization took 4.004 ms
0.00.535.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.744 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.892 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.914 I llama_perf_context_print:        load time =     522.65 ms
0.00.667.915 I llama_perf_context_print: prompt eval time =     130.79 ms /   128 tokens (    1.02 ms per token,   978.68 tokens per second)
0.00.667.915 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.916 I llama_perf_context_print:       total time =     136.25 ms /   129 tokens
0.00.668.440 I ggml_metal_free: deallocating

real	0m0.681s
user	0m0.071s
sys	0m0.082s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.127 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.480 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.487 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.610 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.547 I llama_model_loader: - type  f32:  194 tensors
0.00.028.549 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.549 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.549 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.550 I print_info: file format = GGUF V3 (latest)
0.00.028.551 I print_info: file type   = Q4_K - Medium
0.00.028.552 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.883 I load: special tokens cache size = 25
0.00.042.753 I load: token to piece cache size = 0.2984 MB
0.00.042.770 I print_info: arch             = gptneox
0.00.042.772 I print_info: vocab_only       = 0
0.00.042.772 I print_info: n_ctx_train      = 2048
0.00.042.772 I print_info: n_embd           = 2048
0.00.042.772 I print_info: n_layer          = 24
0.00.042.777 I print_info: n_head           = 16
0.00.042.778 I print_info: n_head_kv        = 16
0.00.042.778 I print_info: n_rot            = 32
0.00.042.778 I print_info: n_swa            = 0
0.00.042.778 I print_info: n_embd_head_k    = 128
0.00.042.778 I print_info: n_embd_head_v    = 128
0.00.042.779 I print_info: n_gqa            = 1
0.00.042.779 I print_info: n_embd_k_gqa     = 2048
0.00.042.780 I print_info: n_embd_v_gqa     = 2048
0.00.042.781 I print_info: f_norm_eps       = 1.0e-05
0.00.042.781 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.783 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.783 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.783 I print_info: f_logit_scale    = 0.0e+00
0.00.042.784 I print_info: n_ff             = 8192
0.00.042.784 I print_info: n_expert         = 0
0.00.042.784 I print_info: n_expert_used    = 0
0.00.042.784 I print_info: causal attn      = 1
0.00.042.784 I print_info: pooling type     = 0
0.00.042.784 I print_info: rope type        = 2
0.00.042.784 I print_info: rope scaling     = linear
0.00.042.785 I print_info: freq_base_train  = 10000.0
0.00.042.785 I print_info: freq_scale_train = 1
0.00.042.785 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.785 I print_info: rope_finetuned   = unknown
0.00.042.785 I print_info: ssm_d_conv       = 0
0.00.042.786 I print_info: ssm_d_inner      = 0
0.00.042.786 I print_info: ssm_d_state      = 0
0.00.042.786 I print_info: ssm_dt_rank      = 0
0.00.042.786 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.786 I print_info: model type       = 1.4B
0.00.042.787 I print_info: model params     = 1.41 B
0.00.042.787 I print_info: general.name     = 1.4B
0.00.042.787 I print_info: vocab type       = BPE
0.00.042.787 I print_info: n_vocab          = 50304
0.00.042.788 I print_info: n_merges         = 50009
0.00.042.788 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.788 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.790 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.790 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.790 I print_info: LF token         = 187 'Ċ'
0.00.042.791 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.791 I print_info: max token length = 1024
0.00.042.791 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.564.369 I load_tensors: offloading 24 repeating layers to GPU
0.00.564.379 I load_tensors: offloading output layer to GPU
0.00.564.379 I load_tensors: offloaded 25/25 layers to GPU
0.00.564.402 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.564.405 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.565.252 I llama_context: n_seq_max     = 1
0.00.565.255 I llama_context: n_ctx         = 2048
0.00.565.256 I llama_context: n_ctx_per_seq = 2048
0.00.565.256 I llama_context: n_batch       = 2048
0.00.565.257 I llama_context: n_ubatch      = 512
0.00.565.257 I llama_context: flash_attn    = 0
0.00.565.258 I llama_context: freq_base     = 10000.0
0.00.565.259 I llama_context: freq_scale    = 1
0.00.565.260 I ggml_metal_init: allocating
0.00.565.302 I ggml_metal_init: found device: Apple M4
0.00.565.314 I ggml_metal_init: picking default device: Apple M4
0.00.566.387 I ggml_metal_init: using embedded metal library
0.00.574.609 I ggml_metal_init: GPU name:   Apple M4
0.00.574.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.574.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.574.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.574.618 I ggml_metal_init: simdgroup reduction   = true
0.00.574.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.574.619 I ggml_metal_init: has residency sets    = true
0.00.574.619 I ggml_metal_init: has bfloat            = true
0.00.574.620 I ggml_metal_init: use bfloat            = true
0.00.574.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.574.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.592.294 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.592.297 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.865 I init:      Metal KV buffer size =   384.00 MiB
0.00.623.871 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.628.352 I init:      Metal compute buffer size =   102.25 MiB
0.00.628.354 I init:        CPU compute buffer size =     8.01 MiB
0.00.628.354 I init: graph nodes  = 967
0.00.628.354 I init: graph splits = 2
0.00.628.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.628.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.628.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.454 I main: llama threadpool init, n_threads = 4
0.00.683.496 I 
0.00.683.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.522 I 
0.00.683.697 I sampler seed: 1234
0.00.683.701 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.713 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.713 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.442.416 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.442.417 I llama_perf_context_print:        load time =     673.61 ms
0.01.442.419 I llama_perf_context_print: prompt eval time =      46.80 ms /     7 tokens (    6.69 ms per token,   149.56 tokens per second)
0.01.442.419 I llama_perf_context_print:        eval time =     708.94 ms /    63 runs   (   11.25 ms per token,    88.86 tokens per second)
0.01.442.420 I llama_perf_context_print:       total time =     759.68 ms /    70 tokens
0.01.446.267 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.106s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.663 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.836 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.714 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.717 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.717 I llama_model_loader: - type  f32:  194 tensors
0.00.024.718 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.718 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.718 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.719 I print_info: file format = GGUF V3 (latest)
0.00.024.719 I print_info: file type   = Q4_K - Medium
0.00.024.720 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.810 I load: special tokens cache size = 25
0.00.038.617 I load: token to piece cache size = 0.2984 MB
0.00.038.631 I print_info: arch             = gptneox
0.00.038.632 I print_info: vocab_only       = 0
0.00.038.632 I print_info: n_ctx_train      = 2048
0.00.038.632 I print_info: n_embd           = 2048
0.00.038.633 I print_info: n_layer          = 24
0.00.038.635 I print_info: n_head           = 16
0.00.038.636 I print_info: n_head_kv        = 16
0.00.038.636 I print_info: n_rot            = 32
0.00.038.636 I print_info: n_swa            = 0
0.00.038.636 I print_info: n_embd_head_k    = 128
0.00.038.637 I print_info: n_embd_head_v    = 128
0.00.038.637 I print_info: n_gqa            = 1
0.00.038.638 I print_info: n_embd_k_gqa     = 2048
0.00.038.639 I print_info: n_embd_v_gqa     = 2048
0.00.038.639 I print_info: f_norm_eps       = 1.0e-05
0.00.038.640 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.642 I print_info: f_logit_scale    = 0.0e+00
0.00.038.642 I print_info: n_ff             = 8192
0.00.038.643 I print_info: n_expert         = 0
0.00.038.643 I print_info: n_expert_used    = 0
0.00.038.643 I print_info: causal attn      = 1
0.00.038.643 I print_info: pooling type     = 0
0.00.038.643 I print_info: rope type        = 2
0.00.038.643 I print_info: rope scaling     = linear
0.00.038.644 I print_info: freq_base_train  = 10000.0
0.00.038.644 I print_info: freq_scale_train = 1
0.00.038.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.644 I print_info: rope_finetuned   = unknown
0.00.038.644 I print_info: ssm_d_conv       = 0
0.00.038.644 I print_info: ssm_d_inner      = 0
0.00.038.644 I print_info: ssm_d_state      = 0
0.00.038.644 I print_info: ssm_dt_rank      = 0
0.00.038.645 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.645 I print_info: model type       = 1.4B
0.00.038.645 I print_info: model params     = 1.41 B
0.00.038.645 I print_info: general.name     = 1.4B
0.00.038.646 I print_info: vocab type       = BPE
0.00.038.646 I print_info: n_vocab          = 50304
0.00.038.646 I print_info: n_merges         = 50009
0.00.038.646 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.647 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.647 I print_info: LF token         = 187 'Ċ'
0.00.038.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: max token length = 1024
0.00.038.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.554.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.554.389 I load_tensors: offloading output layer to GPU
0.00.554.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.554.426 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.554.427 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.556.219 I llama_context: n_seq_max     = 1
0.00.556.221 I llama_context: n_ctx         = 128
0.00.556.222 I llama_context: n_ctx_per_seq = 128
0.00.556.223 I llama_context: n_batch       = 128
0.00.556.223 I llama_context: n_ubatch      = 128
0.00.556.224 I llama_context: flash_attn    = 0
0.00.556.226 I llama_context: freq_base     = 10000.0
0.00.556.226 I llama_context: freq_scale    = 1
0.00.556.227 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.556.229 I ggml_metal_init: allocating
0.00.556.339 I ggml_metal_init: found device: Apple M4
0.00.556.352 I ggml_metal_init: picking default device: Apple M4
0.00.558.299 I ggml_metal_init: using embedded metal library
0.00.565.359 I ggml_metal_init: GPU name:   Apple M4
0.00.565.364 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.565.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.565.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.565.366 I ggml_metal_init: simdgroup reduction   = true
0.00.565.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.565.367 I ggml_metal_init: has residency sets    = true
0.00.565.367 I ggml_metal_init: has bfloat            = true
0.00.565.367 I ggml_metal_init: use bfloat            = true
0.00.565.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.565.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.583.320 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.583.325 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.586.882 I init:      Metal KV buffer size =    24.00 MiB
0.00.586.885 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.590.095 I init:      Metal compute buffer size =    25.56 MiB
0.00.590.096 I init:        CPU compute buffer size =     1.06 MiB
0.00.590.097 I init: graph nodes  = 967
0.00.590.097 I init: graph splits = 2
0.00.590.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.901 I 
0.00.617.976 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.995 I perplexity: tokenizing the input ..
0.00.625.573 I perplexity: tokenization took 7.575 ms
0.00.625.594 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.090 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.762.455 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.762.471 I llama_perf_context_print:        load time =     609.23 ms
0.00.762.471 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.81 tokens per second)
0.00.762.472 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.473 I llama_perf_context_print:       total time =     144.57 ms /   129 tokens
0.00.763.033 I ggml_metal_free: deallocating

real	0m0.779s
user	0m0.080s
sys	0m0.157s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.920 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.542 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.201 I llama_model_loader: - type  f32:  194 tensors
0.00.026.202 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.202 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.203 I print_info: file format = GGUF V3 (latest)
0.00.026.203 I print_info: file type   = Q5_K - Medium
0.00.026.204 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.483 I load: special tokens cache size = 25
0.00.040.154 I load: token to piece cache size = 0.2984 MB
0.00.040.168 I print_info: arch             = gptneox
0.00.040.169 I print_info: vocab_only       = 0
0.00.040.169 I print_info: n_ctx_train      = 2048
0.00.040.169 I print_info: n_embd           = 2048
0.00.040.169 I print_info: n_layer          = 24
0.00.040.172 I print_info: n_head           = 16
0.00.040.173 I print_info: n_head_kv        = 16
0.00.040.173 I print_info: n_rot            = 32
0.00.040.173 I print_info: n_swa            = 0
0.00.040.173 I print_info: n_embd_head_k    = 128
0.00.040.173 I print_info: n_embd_head_v    = 128
0.00.040.174 I print_info: n_gqa            = 1
0.00.040.175 I print_info: n_embd_k_gqa     = 2048
0.00.040.176 I print_info: n_embd_v_gqa     = 2048
0.00.040.176 I print_info: f_norm_eps       = 1.0e-05
0.00.040.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.177 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.177 I print_info: f_logit_scale    = 0.0e+00
0.00.040.178 I print_info: n_ff             = 8192
0.00.040.178 I print_info: n_expert         = 0
0.00.040.178 I print_info: n_expert_used    = 0
0.00.040.180 I print_info: causal attn      = 1
0.00.040.180 I print_info: pooling type     = 0
0.00.040.180 I print_info: rope type        = 2
0.00.040.181 I print_info: rope scaling     = linear
0.00.040.181 I print_info: freq_base_train  = 10000.0
0.00.040.181 I print_info: freq_scale_train = 1
0.00.040.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.181 I print_info: rope_finetuned   = unknown
0.00.040.181 I print_info: ssm_d_conv       = 0
0.00.040.182 I print_info: ssm_d_inner      = 0
0.00.040.182 I print_info: ssm_d_state      = 0
0.00.040.182 I print_info: ssm_dt_rank      = 0
0.00.040.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.182 I print_info: model type       = 1.4B
0.00.040.182 I print_info: model params     = 1.41 B
0.00.040.182 I print_info: general.name     = 1.4B
0.00.040.183 I print_info: vocab type       = BPE
0.00.040.183 I print_info: n_vocab          = 50304
0.00.040.183 I print_info: n_merges         = 50009
0.00.040.183 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: LF token         = 187 'Ċ'
0.00.040.184 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.185 I print_info: max token length = 1024
0.00.040.185 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.953 I load_tensors: offloading output layer to GPU
0.00.596.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.977 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.979 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.598.184 I llama_context: n_seq_max     = 1
0.00.598.186 I llama_context: n_ctx         = 2048
0.00.598.186 I llama_context: n_ctx_per_seq = 2048
0.00.598.187 I llama_context: n_batch       = 2048
0.00.598.187 I llama_context: n_ubatch      = 512
0.00.598.187 I llama_context: flash_attn    = 0
0.00.598.188 I llama_context: freq_base     = 10000.0
0.00.598.189 I llama_context: freq_scale    = 1
0.00.598.190 I ggml_metal_init: allocating
0.00.598.207 I ggml_metal_init: found device: Apple M4
0.00.598.216 I ggml_metal_init: picking default device: Apple M4
0.00.599.682 I ggml_metal_init: using embedded metal library
0.00.605.874 I ggml_metal_init: GPU name:   Apple M4
0.00.605.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.880 I ggml_metal_init: simdgroup reduction   = true
0.00.605.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.881 I ggml_metal_init: has residency sets    = true
0.00.605.881 I ggml_metal_init: has bfloat            = true
0.00.605.881 I ggml_metal_init: use bfloat            = true
0.00.605.882 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.526 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.622.529 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.474 I init:      Metal KV buffer size =   384.00 MiB
0.00.675.481 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.679.781 I init:      Metal compute buffer size =   102.25 MiB
0.00.679.783 I init:        CPU compute buffer size =     8.01 MiB
0.00.679.783 I init: graph nodes  = 967
0.00.679.783 I init: graph splits = 2
0.00.679.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.830 I main: llama threadpool init, n_threads = 4
0.00.739.876 I 
0.00.739.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.900 I 
0.00.740.053 I sampler seed: 1234
0.00.740.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.099 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.589.715 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.589.715 I llama_perf_context_print:        load time =     729.18 ms
0.01.589.716 I llama_perf_context_print: prompt eval time =      51.44 ms /     7 tokens (    7.35 ms per token,   136.09 tokens per second)
0.01.589.717 I llama_perf_context_print:        eval time =     795.34 ms /    63 runs   (   12.62 ms per token,    79.21 tokens per second)
0.01.589.717 I llama_perf_context_print:       total time =     850.61 ms /    70 tokens
0.01.593.662 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.107s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.068 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.070 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.071 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.071 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.071 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.072 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.079 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.963 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.082 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.082 I llama_model_loader: - type  f32:  194 tensors
0.00.026.083 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.083 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.084 I print_info: file format = GGUF V3 (latest)
0.00.026.090 I print_info: file type   = Q5_K - Medium
0.00.026.091 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.508 I load: special tokens cache size = 25
0.00.040.722 I load: token to piece cache size = 0.2984 MB
0.00.040.739 I print_info: arch             = gptneox
0.00.040.740 I print_info: vocab_only       = 0
0.00.040.740 I print_info: n_ctx_train      = 2048
0.00.040.740 I print_info: n_embd           = 2048
0.00.040.741 I print_info: n_layer          = 24
0.00.040.744 I print_info: n_head           = 16
0.00.040.745 I print_info: n_head_kv        = 16
0.00.040.745 I print_info: n_rot            = 32
0.00.040.745 I print_info: n_swa            = 0
0.00.040.746 I print_info: n_embd_head_k    = 128
0.00.040.746 I print_info: n_embd_head_v    = 128
0.00.040.746 I print_info: n_gqa            = 1
0.00.040.747 I print_info: n_embd_k_gqa     = 2048
0.00.040.748 I print_info: n_embd_v_gqa     = 2048
0.00.040.748 I print_info: f_norm_eps       = 1.0e-05
0.00.040.749 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.749 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.749 I print_info: f_logit_scale    = 0.0e+00
0.00.040.750 I print_info: n_ff             = 8192
0.00.040.750 I print_info: n_expert         = 0
0.00.040.750 I print_info: n_expert_used    = 0
0.00.040.750 I print_info: causal attn      = 1
0.00.040.750 I print_info: pooling type     = 0
0.00.040.750 I print_info: rope type        = 2
0.00.040.750 I print_info: rope scaling     = linear
0.00.040.751 I print_info: freq_base_train  = 10000.0
0.00.040.751 I print_info: freq_scale_train = 1
0.00.040.751 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.751 I print_info: rope_finetuned   = unknown
0.00.040.751 I print_info: ssm_d_conv       = 0
0.00.040.752 I print_info: ssm_d_inner      = 0
0.00.040.752 I print_info: ssm_d_state      = 0
0.00.040.752 I print_info: ssm_dt_rank      = 0
0.00.040.752 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.752 I print_info: model type       = 1.4B
0.00.040.753 I print_info: model params     = 1.41 B
0.00.040.753 I print_info: general.name     = 1.4B
0.00.040.753 I print_info: vocab type       = BPE
0.00.040.754 I print_info: n_vocab          = 50304
0.00.040.754 I print_info: n_merges         = 50009
0.00.040.754 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.754 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.754 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.754 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.757 I print_info: LF token         = 187 'Ċ'
0.00.040.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.758 I print_info: max token length = 1024
0.00.040.758 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.343 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.351 I load_tensors: offloading output layer to GPU
0.00.594.352 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.377 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.378 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.595.998 I llama_context: n_seq_max     = 1
0.00.596.000 I llama_context: n_ctx         = 128
0.00.596.000 I llama_context: n_ctx_per_seq = 128
0.00.596.001 I llama_context: n_batch       = 128
0.00.596.001 I llama_context: n_ubatch      = 128
0.00.596.001 I llama_context: flash_attn    = 0
0.00.596.003 I llama_context: freq_base     = 10000.0
0.00.596.003 I llama_context: freq_scale    = 1
0.00.596.004 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.005 I ggml_metal_init: allocating
0.00.596.105 I ggml_metal_init: found device: Apple M4
0.00.596.119 I ggml_metal_init: picking default device: Apple M4
0.00.597.692 I ggml_metal_init: using embedded metal library
0.00.603.796 I ggml_metal_init: GPU name:   Apple M4
0.00.603.800 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.801 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.802 I ggml_metal_init: simdgroup reduction   = true
0.00.603.802 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.802 I ggml_metal_init: has residency sets    = true
0.00.603.802 I ggml_metal_init: has bfloat            = true
0.00.603.803 I ggml_metal_init: use bfloat            = true
0.00.603.804 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.063 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.620.066 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.536 I init:      Metal KV buffer size =    24.00 MiB
0.00.623.540 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.626.919 I init:      Metal compute buffer size =    25.56 MiB
0.00.626.921 I init:        CPU compute buffer size =     1.06 MiB
0.00.626.922 I init: graph nodes  = 967
0.00.626.922 I init: graph splits = 2
0.00.626.925 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.273 I 
0.00.656.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.377 I perplexity: tokenizing the input ..
0.00.661.563 I perplexity: tokenization took 5.184 ms
0.00.661.574 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.247 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.584 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.603 I llama_perf_context_print:        load time =     646.41 ms
0.00.802.603 I llama_perf_context_print: prompt eval time =     139.44 ms /   128 tokens (    1.09 ms per token,   917.95 tokens per second)
0.00.802.604 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.604 I llama_perf_context_print:       total time =     146.33 ms /   129 tokens
0.00.803.205 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.076s
sys	0m0.135s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.575 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.931 I llama_model_loader: - type  f32:  194 tensors
0.00.024.931 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.932 I print_info: file format = GGUF V3 (latest)
0.00.024.932 I print_info: file type   = Q6_K
0.00.024.933 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.791 I load: special tokens cache size = 25
0.00.038.845 I load: token to piece cache size = 0.2984 MB
0.00.038.860 I print_info: arch             = gptneox
0.00.038.861 I print_info: vocab_only       = 0
0.00.038.861 I print_info: n_ctx_train      = 2048
0.00.038.861 I print_info: n_embd           = 2048
0.00.038.862 I print_info: n_layer          = 24
0.00.038.865 I print_info: n_head           = 16
0.00.038.865 I print_info: n_head_kv        = 16
0.00.038.866 I print_info: n_rot            = 32
0.00.038.866 I print_info: n_swa            = 0
0.00.038.866 I print_info: n_embd_head_k    = 128
0.00.038.866 I print_info: n_embd_head_v    = 128
0.00.038.867 I print_info: n_gqa            = 1
0.00.038.868 I print_info: n_embd_k_gqa     = 2048
0.00.038.868 I print_info: n_embd_v_gqa     = 2048
0.00.038.869 I print_info: f_norm_eps       = 1.0e-05
0.00.038.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.870 I print_info: f_logit_scale    = 0.0e+00
0.00.038.872 I print_info: n_ff             = 8192
0.00.038.872 I print_info: n_expert         = 0
0.00.038.872 I print_info: n_expert_used    = 0
0.00.038.872 I print_info: causal attn      = 1
0.00.038.872 I print_info: pooling type     = 0
0.00.038.874 I print_info: rope type        = 2
0.00.038.875 I print_info: rope scaling     = linear
0.00.038.876 I print_info: freq_base_train  = 10000.0
0.00.038.877 I print_info: freq_scale_train = 1
0.00.038.877 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.877 I print_info: rope_finetuned   = unknown
0.00.038.877 I print_info: ssm_d_conv       = 0
0.00.038.877 I print_info: ssm_d_inner      = 0
0.00.038.877 I print_info: ssm_d_state      = 0
0.00.038.878 I print_info: ssm_dt_rank      = 0
0.00.038.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.878 I print_info: model type       = 1.4B
0.00.038.878 I print_info: model params     = 1.41 B
0.00.038.878 I print_info: general.name     = 1.4B
0.00.038.882 I print_info: vocab type       = BPE
0.00.038.882 I print_info: n_vocab          = 50304
0.00.038.883 I print_info: n_merges         = 50009
0.00.038.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: LF token         = 187 'Ċ'
0.00.038.884 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.886 I print_info: max token length = 1024
0.00.038.886 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.485 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.497 I load_tensors: offloading output layer to GPU
0.00.639.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.531 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.639.538 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.641.146 I llama_context: n_seq_max     = 1
0.00.641.148 I llama_context: n_ctx         = 2048
0.00.641.149 I llama_context: n_ctx_per_seq = 2048
0.00.641.149 I llama_context: n_batch       = 2048
0.00.641.150 I llama_context: n_ubatch      = 512
0.00.641.150 I llama_context: flash_attn    = 0
0.00.641.152 I llama_context: freq_base     = 10000.0
0.00.641.152 I llama_context: freq_scale    = 1
0.00.641.153 I ggml_metal_init: allocating
0.00.641.174 I ggml_metal_init: found device: Apple M4
0.00.641.184 I ggml_metal_init: picking default device: Apple M4
0.00.642.633 I ggml_metal_init: using embedded metal library
0.00.648.730 I ggml_metal_init: GPU name:   Apple M4
0.00.648.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.736 I ggml_metal_init: simdgroup reduction   = true
0.00.648.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.736 I ggml_metal_init: has residency sets    = true
0.00.648.737 I ggml_metal_init: has bfloat            = true
0.00.648.737 I ggml_metal_init: use bfloat            = true
0.00.648.738 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.739 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.821 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.665.825 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.958 I init:      Metal KV buffer size =   384.00 MiB
0.00.719.967 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.126 I init:      Metal compute buffer size =   102.25 MiB
0.00.724.128 I init:        CPU compute buffer size =     8.01 MiB
0.00.724.128 I init: graph nodes  = 967
0.00.724.129 I init: graph splits = 2
0.00.724.134 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.862 I main: llama threadpool init, n_threads = 4
0.00.792.916 I 
0.00.792.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.944 I 
0.00.793.098 I sampler seed: 1234
0.00.793.103 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.151 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.675.688 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.675.689 I llama_perf_context_print:        load time =     783.57 ms
0.01.675.689 I llama_perf_context_print: prompt eval time =      54.27 ms /     7 tokens (    7.75 ms per token,   128.98 tokens per second)
0.01.675.690 I llama_perf_context_print:        eval time =     825.37 ms /    63 runs   (   13.10 ms per token,    76.33 tokens per second)
0.01.675.690 I llama_perf_context_print:       total time =     883.54 ms /    70 tokens
0.01.679.457 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.108s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4733 (d5e8e1a2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.887 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.901 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.903 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.903 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.833 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.707 I llama_model_loader: - type  f32:  194 tensors
0.00.024.707 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.708 I print_info: file format = GGUF V3 (latest)
0.00.024.709 I print_info: file type   = Q6_K
0.00.024.710 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.326 I load: special tokens cache size = 25
0.00.039.437 I load: token to piece cache size = 0.2984 MB
0.00.039.454 I print_info: arch             = gptneox
0.00.039.455 I print_info: vocab_only       = 0
0.00.039.455 I print_info: n_ctx_train      = 2048
0.00.039.455 I print_info: n_embd           = 2048
0.00.039.455 I print_info: n_layer          = 24
0.00.039.459 I print_info: n_head           = 16
0.00.039.460 I print_info: n_head_kv        = 16
0.00.039.460 I print_info: n_rot            = 32
0.00.039.460 I print_info: n_swa            = 0
0.00.039.460 I print_info: n_embd_head_k    = 128
0.00.039.461 I print_info: n_embd_head_v    = 128
0.00.039.461 I print_info: n_gqa            = 1
0.00.039.462 I print_info: n_embd_k_gqa     = 2048
0.00.039.462 I print_info: n_embd_v_gqa     = 2048
0.00.039.463 I print_info: f_norm_eps       = 1.0e-05
0.00.039.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.463 I print_info: f_logit_scale    = 0.0e+00
0.00.039.464 I print_info: n_ff             = 8192
0.00.039.466 I print_info: n_expert         = 0
0.00.039.466 I print_info: n_expert_used    = 0
0.00.039.466 I print_info: causal attn      = 1
0.00.039.468 I print_info: pooling type     = 0
0.00.039.468 I print_info: rope type        = 2
0.00.039.468 I print_info: rope scaling     = linear
0.00.039.468 I print_info: freq_base_train  = 10000.0
0.00.039.469 I print_info: freq_scale_train = 1
0.00.039.469 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.469 I print_info: rope_finetuned   = unknown
0.00.039.469 I print_info: ssm_d_conv       = 0
0.00.039.469 I print_info: ssm_d_inner      = 0
0.00.039.469 I print_info: ssm_d_state      = 0
0.00.039.469 I print_info: ssm_dt_rank      = 0
0.00.039.469 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.471 I print_info: model type       = 1.4B
0.00.039.471 I print_info: model params     = 1.41 B
0.00.039.472 I print_info: general.name     = 1.4B
0.00.039.472 I print_info: vocab type       = BPE
0.00.039.472 I print_info: n_vocab          = 50304
0.00.039.472 I print_info: n_merges         = 50009
0.00.039.473 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.473 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.474 I print_info: LF token         = 187 'Ċ'
0.00.039.474 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.474 I print_info: max token length = 1024
0.00.039.474 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.740 I load_tensors: offloading output layer to GPU
0.00.587.741 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.776 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.587.777 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.589.539 I llama_context: n_seq_max     = 1
0.00.589.541 I llama_context: n_ctx         = 128
0.00.589.542 I llama_context: n_ctx_per_seq = 128
0.00.589.542 I llama_context: n_batch       = 128
0.00.589.543 I llama_context: n_ubatch      = 128
0.00.589.543 I llama_context: flash_attn    = 0
0.00.589.544 I llama_context: freq_base     = 10000.0
0.00.589.544 I llama_context: freq_scale    = 1
0.00.589.545 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.547 I ggml_metal_init: allocating
0.00.589.582 I ggml_metal_init: found device: Apple M4
0.00.589.592 I ggml_metal_init: picking default device: Apple M4
0.00.591.085 I ggml_metal_init: using embedded metal library
0.00.597.350 I ggml_metal_init: GPU name:   Apple M4
0.00.597.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.356 I ggml_metal_init: simdgroup reduction   = true
0.00.597.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.357 I ggml_metal_init: has residency sets    = true
0.00.597.357 I ggml_metal_init: has bfloat            = true
0.00.597.357 I ggml_metal_init: use bfloat            = true
0.00.597.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.477 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.614.483 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.947 I init:      Metal KV buffer size =    24.00 MiB
0.00.617.950 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.247 I init:      Metal compute buffer size =    25.56 MiB
0.00.621.248 I init:        CPU compute buffer size =     1.06 MiB
0.00.621.249 I init: graph nodes  = 967
0.00.621.249 I init: graph splits = 2
0.00.621.252 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.253 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.497 I 
0.00.654.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.610 I perplexity: tokenizing the input ..
0.00.661.714 I perplexity: tokenization took 7.1 ms
0.00.661.737 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.838 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.803.249 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.803.266 I llama_perf_context_print:        load time =     645.60 ms
0.00.803.267 I llama_perf_context_print: prompt eval time =     139.35 ms /   128 tokens (    1.09 ms per token,   918.56 tokens per second)
0.00.803.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.269 I llama_perf_context_print:       total time =     148.77 ms /   129 tokens
0.00.803.876 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.078s
sys	0m0.133s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4733 (d5e8e1a2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143a07b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143a08020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143a08490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143a08900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143a08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143a091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143a09650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143a09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143a09f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143a0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143a0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143a0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143a0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143a0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143a0c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143a0d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143a0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143a0def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143a0ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143a0f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143a0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143a10340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143a10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143a11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143a115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143a11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143a11cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143a12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143a12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143a12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143a13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143a13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143a14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143a147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143a14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143a150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143a15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143a15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143a15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143a16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143a166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143a16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143a16fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143a17430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143a178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143a18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143a184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143a18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143a18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143a191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143a19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143a19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143a1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143a1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143a1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143a1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143a1b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143a1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143a1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143a1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143a1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143a1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143a1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143a1d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143a1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143a1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143a1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143a1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143a1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143a1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143a1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143a1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143a202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143a20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143a20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143a213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143a21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143a21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143a224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143a22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143a23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143a235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143a23b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143a24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143a246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143a24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143a25250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143a25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143a25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143a26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143a26910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143a26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143a27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143a27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143a17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143a28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143a285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143a28a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143a29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143a295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143a29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143a2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143a2a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143a2ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143a2b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143a2b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143a2bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143a2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143a2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143a2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143a2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143a2d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143a2de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143a2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143a2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143a2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143a2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143a2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143a30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143a30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143a30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143a31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143a31550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143a31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143a31f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143a32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143a32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143a32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143a33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143a33850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143a33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143a34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143a34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143a34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143a35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143a35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143a35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143a36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143a36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143a36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143a36f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143a37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143a37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143a37e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143a38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143a38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143a38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143a39250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143a39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143a39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143a3a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143a3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143a3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143a3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143a3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143a3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143a3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143a3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143a3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143a3ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143a3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143a3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143a3dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143a3e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143a3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143a3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143a3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143a3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143a3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143a40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143a40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143a40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143a40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143a41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143a41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143a41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143a42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143a42850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143a42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143a43250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143a43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143a43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143a44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143a44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143a44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143a45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143a45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143a45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143a45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143a46450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143a46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143a46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143a47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143a47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143a48120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143a48730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143a48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143a49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143a499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143a49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143a4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143a4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143a4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143a4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143a4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143a4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143a4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143a4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143a4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143a4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143a4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143a4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143a4e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143a4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143a4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143a4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143a4fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143a500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143a505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143a50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143a51090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143a515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143a51b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143a52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143a525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143a52b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143a53070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143a535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143a53b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143a54060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143a545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143a54b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143a55050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143a555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143a55af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143a56040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143a56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143a56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143a57030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143a57580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143a57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143a58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143a58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143a58ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143a59010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143a59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143a59ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143a5a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143a5a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143a5aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143a5aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143a5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143a5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143a5bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143a5c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143a5ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143a5cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143a5d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143a5da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143a5dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143a5e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143a5ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143a5efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143a5f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143a5f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143a5fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143a60230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143a606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143a60b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143a61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143a614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143a61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143a61df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143a62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143a62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143a62bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143a63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143a63510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143a63a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143a64180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143a648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143a64fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143a656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143a659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143a66190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143a66450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143a66a60 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.755.314 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.755.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143a1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143a25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143a1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143a27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143a24f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143a2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143a2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143a2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143a27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143a21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143a29e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143a46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143a26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143a21680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143a249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143a232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143a29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143a46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143a2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143a26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143a210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143a24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143a22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143a292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143a2af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143a26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143a20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143a23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143a28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143a2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143a25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143a238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143a2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143a66710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143a47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143a489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143a4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143a10880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143a65c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143a19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143a27ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143a4ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143a49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143a11fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143a66ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143a67180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143a67440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143a67700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143a679c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143a67c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143a67f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143a68200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143a684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143a68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143a68a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143a68d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143a68fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143a69280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143a69540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143a69800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143a69ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143a69d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143a6a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143a6a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143a6a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143a6a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143a6ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143a6ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143a6b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143a6b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143a6b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143a6b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143a6bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143a6be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143a6c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143a6c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143a6c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143a6c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143a6cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143a6cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143a6d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143a6d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143a6d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143a6da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143a6dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143a6df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143a6e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143a6e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143a6e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143a6ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143a6ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143a6f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143a6f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143a6f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143a6f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143a6fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143a6fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143a70080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143a70340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143a70600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143a708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143a70b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143a70e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143a71100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143a713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143a71680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143a71940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143a71c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143a71ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143a72180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143a72440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143a72700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143a729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143a72c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143a72f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143a73200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143a734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143a73780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143a73a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143a73d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143a73fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143a74280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143a74540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143a74800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143a74ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143a74d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143a75040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143a75300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143a755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143a75880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143a75b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143a75e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143a760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143a76380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143a76640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143a76900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143a76bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143a76e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143a77140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143a77400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143a776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143a77980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143a77c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143a77f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143a781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143a78480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143a78740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143a78a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143a78cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143a78f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143a79240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143a79500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143a797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143a79a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143a79d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143a7a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143a7a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143a7a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143a7a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143a7ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143a7adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143a7b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143a7b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143a7b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143a7b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143a7bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143a7be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143a7c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143a7c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143a7c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143a7c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143a7cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143a7cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143a7d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143a7d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143a7d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143a7d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143a7dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143a7df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143a7e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143a7e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143a7e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143a7ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143a7ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143a7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143a7f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143a7f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143a7fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143a803c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143a808c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143a80dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143a812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143a817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143a81cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143a82270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143a82820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143a82dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143a83380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143a83990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143a83fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143a845b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143a84da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143a85240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143a85500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143a85b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143a86120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143a86910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143a86db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143a87250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143a876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143a87ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143a883f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143a88940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143a88e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143a893e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143a89930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143a89e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143a8a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143a8a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143a8ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143a8b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143a8b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143a8be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143a8c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143a8c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143a8ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143a8d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143a8d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143a8de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143a8e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143a8e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143a8ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143a8f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143a8f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143a8fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143a90370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143a908c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143a90e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143a91360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143a918b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143a91e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143a92350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143a928a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143a92df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143a93340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143a93890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143a93de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143a94330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143a94880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143a94dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143a95320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143a95870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143a95dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143a96310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143a96860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143a96db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143a97300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143a97850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143a97da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143a982f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143a98840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143a98d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143a992e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143a99830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143a99d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143a9a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143a9a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143a9acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143a9b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143a9b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143a9baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143a9bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143a9c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143a9c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143a9cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143a9d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143a9d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143a9db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143a9dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143a9e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143a9e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143a9ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143a9f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143a9f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143aa0110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143aa0830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143aa0f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143aa1210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143aa1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143aa1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143aa22d0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104c046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104c04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104c04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104c05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104c058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104c05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104c06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104c065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104c06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104c06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104c07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104c07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104c08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104c08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104c09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104c09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104c0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104c0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104c0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104c0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104c0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104c0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104c0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104c0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104c0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104c0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104c0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104c0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104c0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104c0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104c0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104c0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104c0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104c10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104c104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104c10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104c10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104c111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104c11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104c11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104c11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104c123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104c12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104c12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104c13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104c13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104c139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104c13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104c142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104c14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104c14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104c15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104c15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104c158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104c15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104c16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104c16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104c170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104c17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104c17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104c17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104c18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104c186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104c18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104c18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104c19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104c198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104c19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104c1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104c1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104c1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104c1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104c1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104c1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104c1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104c1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104c1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104c1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104c1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104c1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104c1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104c1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104c1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104c1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104c1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104c1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104c1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104c1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104c1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104c1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104c20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104c20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104c20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104c21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104c214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104c21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104c22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104c226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104c22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104c22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104c233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104c23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104c23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104c243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104c24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104c24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104c25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104c25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104c259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104c25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104c262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104c26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104c26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104c27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104c27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104c278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104c27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104c281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104c28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104c28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104c28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104c29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104c29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104c29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104c2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104c2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104c2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104c2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104c2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104c2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104c2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104c2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104c2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104c2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104c2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104c2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104c2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104c2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104c2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104c2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104c2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104c2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104c2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104c2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104c2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104c2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104c30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104c306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104c30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104c30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104c31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104c318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104c31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104c32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104c32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104c32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104c32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104c33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104c337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104c33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104c340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104c34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104c34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104c34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104c35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104c356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104c35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104c35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104c36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104c36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104c36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104c37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104c375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104c37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104c37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104c38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104c387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104c38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104c39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104c394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104c39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104c39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104c3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104c3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104c3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104c3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104c3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104c3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104c3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104c3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104c3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104c3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104c3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104c3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104c3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104c3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104c3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104c3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104c3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104c3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104c3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104c3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104c3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104c3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104c403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104c40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104c40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104c41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104c41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104c41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104c42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104c426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104c42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104c42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104c433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104c43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104c43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104c44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104c445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104c44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104c44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104c45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104c45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104c45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104c46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104c464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104c46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104c46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104c47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104c47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104c47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104c47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104c483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104c48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104c48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104c49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104c49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104c49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104c49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104c4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104c4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104c4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104c4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104c4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104c4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104c4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104c4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104c4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104c4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104c4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104c4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104c4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104c4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104c4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104c4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104c4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104c4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104c4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104c4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104c4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104c50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104c50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104c508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104c50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104c511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104c51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104c51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104c51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104c52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104c52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104c52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104c530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104c53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104c539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104c53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104c542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104c54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104c54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104c54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104c55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104c558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104c56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104c56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104c57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104c578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104c57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104c57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104c585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104c58be0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.807s
user	0m0.275s
sys	0m0.339s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4733 (d5e8e1a2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13160ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13160c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13160c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13160cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13160d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13160d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13160dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13160e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13160e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13160ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13160f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13160f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131610320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131610ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1316112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131611a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131612120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131612840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131612f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131613730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131614570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131614c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131615530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131615c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131615f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131616520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131617190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1316176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131617990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131617e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1316180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131618980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131618ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131619180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131619620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131619ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13161a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13161a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13161ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13161b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13161b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13161bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13161bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13161c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13161ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13161d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13161d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13161df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13161e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13161eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13161f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13161f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13161ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131620410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1316208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131620b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131621180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131621970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131621c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1316220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131622570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131622eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131623350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1316237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131623c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131624130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1316245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131624a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131624f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1316253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131625900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131625e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1316263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1316268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131626e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131627390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1316278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131627e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131628380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1316288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131628e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1316298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131629e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13162a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13162a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13162ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13162b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13162b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13162bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13162c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13162c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13162cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13162d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13161d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13162d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13162df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13162e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13162e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13162ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13162f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13162f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13162ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131630480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1316309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131630f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131631470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1316319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131631f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131632460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131632900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131632da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131633240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1316336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131633b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131634020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1316344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131634e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1316352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131635740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131636080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131636520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1316369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131636e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131637300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1316377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131637c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1316380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131638580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131638a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131638ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131639360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131639800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131639ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13163a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13163a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13163aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13163af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13163b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13163b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13163bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13163c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13163c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13163cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13163cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13163d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13163d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13163dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13163e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13163e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13163eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13163efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13163f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13163f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13163fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131640260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131640700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131640ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131641040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1316414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131641980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131641e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1316422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131642760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131642c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1316430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131643540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1316439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131643e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131644320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1316447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131645100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1316455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131645a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131645ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131646380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131646820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131646cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131647160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131647600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131647aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1316483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131648880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131648d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1316491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131649660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131649bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13164a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13164a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13164aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13164ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13164b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13164ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13164c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13164c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13164cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13164cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13164d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13164dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13164e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13164e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13164ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13164f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13164f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13164fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131650420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131650970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131650ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131651960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131651eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131652400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131652950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131652ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1316533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131653940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131653e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1316543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131654930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131654e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1316553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131655920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131655e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1316563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131656910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131656e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1316573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131657900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131657e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1316583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1316588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131658e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131659390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1316598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131659e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13165a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13165a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13165ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13165b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13165b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13165be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13165c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13165c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13165ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13165d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13165d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13165ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13165e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13165e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13165ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13165f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13165f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13165fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131660320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131660870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131660dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131661310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131661860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131661db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131662300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1316627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131662c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1316630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131663580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131663a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131663ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131664360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131664800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131664ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131665140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1316655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131665a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131665f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1316663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131666860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131666db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1316674d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131667bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131668310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131668a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131668cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1316694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1316697a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131669db0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.099.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133005e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1330062d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133006740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133006bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133007020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133007490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133007900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133007d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1330081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133008650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133008ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1330091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133009ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13300a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13300aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13300b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13300bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13300c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13300c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13300d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13300d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13300de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13300e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13300ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13300f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13300f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13300f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13300fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133010250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1330106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133010b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133011060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1330114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133011790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133011c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133012070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1330124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133012950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133012dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133013230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1330136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133013b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133013f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1330143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133014860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133014cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133015140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1330155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133015a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133015e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133016300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133016770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133016be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133017050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1330174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133017930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133017ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1330183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133018810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133018c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1330190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133019560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1330199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133019e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13301a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13301a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13301ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13301b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13301b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131708f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1317097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131709c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13170a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13170a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13170a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13170ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13170b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13170b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13170bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13170bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13170c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13170c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13170cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13170d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13170d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13170da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13170def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13170e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13170e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13170ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13170f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13170f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13170f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13170fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131710270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1317106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131710b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131710fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131711430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1317118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131711d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131712180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1317125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131712a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131712ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131713340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1317137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131713c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131714090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131714500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131714970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131714de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131715250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131715790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131715c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131716070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1317164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131716950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131716dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131717230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1317176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131717b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1317183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131718860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131718cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131719140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1317195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131719a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131719e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13171a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13171a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13171abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13171b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13171b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13171b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13171bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13171c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13171c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13171caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13171cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13171d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13171d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13171dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13171e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13171e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13171ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13171ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13171f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13171f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13171fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131720030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1317204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131720910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131720d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1317211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131721660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131721ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131721f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1317223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131722820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131723100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131723570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1317239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131724100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131724610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131724b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131725010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131725510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131725a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131725f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131726410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131726910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131727310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131727810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131727d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131728210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131728710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131728c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131729110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131729610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131729b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13172a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13172a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13172aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13172af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13172b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13172b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13172be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13172c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13172c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13172cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13172d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13172d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13172ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13172e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13172eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13172f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13172f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13172fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131730050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131730660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131730c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131731460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131731900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131731da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131732240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1317329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131732f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131733490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1317339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131734480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1317349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131734f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131735470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1317359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131735f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1317369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131736f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131737450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1317379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131737ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131738440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131738990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131738ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131739430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131739980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131739ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13173a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13173a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13173aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13173b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13173b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13173beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13173c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13173c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13173cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13173d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13173d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13173de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13173e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13173e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13173ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13173f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13173f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13173fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1317403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131740910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131740e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1317413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131741900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1317423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1317428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131742e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131743390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1317438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131743e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131744380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1317448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131744e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131745370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131745810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131745cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131746150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1317465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131746a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131746f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1317473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131747870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131747d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1317481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131748650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131748af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131748f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131749430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1317498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131749e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13174a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13174ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13174b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13174baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13174bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13174c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13174c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13174ce20 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133009480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133005a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13300cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13301b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13301bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13301bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13301c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13301c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13301c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13301ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13301cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13301cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13301d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13301db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13301e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13301e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13301e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13301eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13301f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13301fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133020130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133020670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133020bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1330210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133021630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133021b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133021e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1330220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1330223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133022670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133022930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133022bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133022eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133023430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1330236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1330239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133023c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133023f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1330241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1330244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133024770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133024a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133024cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133024fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133025270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133025530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1330257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133025ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133025d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133026030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1330262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1330265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133026870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133026b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133026df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1330270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133027370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133027630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1330278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133027e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133028130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1330283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1330287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133028ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133028fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1330294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1330299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133029eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13302a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13302a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13302adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13302b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13302b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13302be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13302c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13302c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13302cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13302d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13302da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13302e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13302e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13302eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13302f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13302f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13302fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133030250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133030800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133030db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133031360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133031910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133031ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133032470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133032a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133032fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133033580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133033b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1330340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133034690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1330351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1330357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133035d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133036300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1330368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133036e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133037410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1330379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133037f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133038520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133038ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133039080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133039b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13303a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13303a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13303aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13303af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13303b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13303b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13303be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13303c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13303c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13303cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13303d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13303d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13303dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13303e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13303e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13303eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13303f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13303f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13303fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13303ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133040930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133040e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133041330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133041830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133041d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133042230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133042730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133042c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133043130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133043630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133043b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133044030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133044530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133044a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133044f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133045430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133045930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133045e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133046330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133046830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133046d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133047230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133047730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133047c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133048130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133048630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133048b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133049030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133049530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133049a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133049f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13304a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13304a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13304ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13304b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13304b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13304bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13304c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13304c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13304cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13304d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13304d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13304db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13304e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13304e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13304ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13304ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13304f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13304f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13304fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133050330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133050830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133050d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133051230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133051730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133051c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133052130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133052630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133052be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133053190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133053740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133053cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133054300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133054910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133054f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133055710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133055bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133055e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133056480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133057280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133057720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133057bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133058060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133058810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133058d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1330592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133059800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133059d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13305a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13305a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13305ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13305b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13305b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13305bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13305c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13305c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13305cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13305d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13305d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13305dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13305e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13305e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13305ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13305f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13305f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13305fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133060240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133060790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133060ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133061230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133061780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133061cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133062220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133062770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133062cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133063210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133063760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133063cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133064200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133064750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133064ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1330651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133065740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133065c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1330661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133066730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133066c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1330671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133067720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133067c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1330681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133068710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133068c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1330691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133069700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133069c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13306a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13306a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13306ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13306b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13306b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13306bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13306bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13306c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13306c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13306cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13306d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13306d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13306db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13306dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13306e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13306e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13306edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13306f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13306f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13306fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133070360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133070a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1330711a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1330718c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133071b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133072370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133072630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133072c40 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.962s
user	0m0.230s
sys	0m0.164s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.51 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.93 sec*proc (2 tests)

Total Test time (real) =   1.95 sec
        1.97 real         0.51 user         0.22 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.09 sys
```
