### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.12 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.30 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  178.84 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.88 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   26.04 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 221.16 sec*proc (27 tests)

Total Test time (real) = 221.17 sec

real	3m41.203s
user	7m41.681s
sys	0m6.167s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.20 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.77 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.37 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.02 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.22 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.44 sec*proc (27 tests)

Total Test time (real) =  51.45 sec

real	0m51.461s
user	1m12.636s
sys	0m5.851s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.065 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.371 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.545 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.554 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.556 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.557 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.557 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.559 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.559 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.560 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.560 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.561 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.564 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.564 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.565 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.566 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.566 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.567 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.567 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.914 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.916 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.917 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.917 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.918 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.918 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.919 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.920 I llama_model_loader: - type  f32:  124 tensors
0.00.025.920 I llama_model_loader: - type  f16:   73 tensors
0.00.030.496 I llm_load_vocab: special tokens cache size = 5
0.00.032.881 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.032.885 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.032.886 I llm_load_print_meta: arch             = bert
0.00.032.886 I llm_load_print_meta: vocab type       = WPM
0.00.032.886 I llm_load_print_meta: n_vocab          = 30522
0.00.032.887 I llm_load_print_meta: n_merges         = 0
0.00.032.887 I llm_load_print_meta: vocab_only       = 0
0.00.032.887 I llm_load_print_meta: n_ctx_train      = 512
0.00.032.888 I llm_load_print_meta: n_embd           = 384
0.00.032.888 I llm_load_print_meta: n_layer          = 12
0.00.032.903 I llm_load_print_meta: n_head           = 12
0.00.032.904 I llm_load_print_meta: n_head_kv        = 12
0.00.032.904 I llm_load_print_meta: n_rot            = 32
0.00.032.904 I llm_load_print_meta: n_swa            = 0
0.00.032.905 I llm_load_print_meta: n_embd_head_k    = 32
0.00.032.905 I llm_load_print_meta: n_embd_head_v    = 32
0.00.032.906 I llm_load_print_meta: n_gqa            = 1
0.00.032.906 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.032.907 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.032.908 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.032.909 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.032.909 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.032.909 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.032.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.032.910 I llm_load_print_meta: n_ff             = 1536
0.00.032.911 I llm_load_print_meta: n_expert         = 0
0.00.032.911 I llm_load_print_meta: n_expert_used    = 0
0.00.032.911 I llm_load_print_meta: causal attn      = 0
0.00.032.911 I llm_load_print_meta: pooling type     = 2
0.00.032.911 I llm_load_print_meta: rope type        = 2
0.00.032.912 I llm_load_print_meta: rope scaling     = linear
0.00.032.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.032.913 I llm_load_print_meta: freq_scale_train = 1
0.00.032.913 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.032.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.032.915 I llm_load_print_meta: ssm_d_conv       = 0
0.00.032.915 I llm_load_print_meta: ssm_d_inner      = 0
0.00.032.915 I llm_load_print_meta: ssm_d_state      = 0
0.00.032.917 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.032.917 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.032.918 I llm_load_print_meta: model type       = 33M
0.00.032.918 I llm_load_print_meta: model ftype      = F16
0.00.032.919 I llm_load_print_meta: model params     = 33.21 M
0.00.032.920 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.032.920 I llm_load_print_meta: general.name     = Bge Small
0.00.032.920 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.032.921 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.032.921 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.032.921 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.032.923 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.032.923 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.032.924 I llm_load_print_meta: max token length = 21
0.00.034.694 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.699 I llm_load_tensors: offloading output layer to GPU
0.00.034.700 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.725 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.727 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.265 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.266 I llama_new_context_with_model: n_ctx         = 512
0.00.035.266 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.266 I llama_new_context_with_model: n_batch       = 2048
0.00.035.267 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.267 I llama_new_context_with_model: flash_attn    = 0
0.00.035.268 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.268 I llama_new_context_with_model: freq_scale    = 1
0.00.035.269 I ggml_metal_init: allocating
0.00.035.273 I ggml_metal_init: found device: Apple M4
0.00.035.276 I ggml_metal_init: picking default device: Apple M4
0.00.036.152 I ggml_metal_init: using embedded metal library
0.00.040.514 I ggml_metal_init: GPU name:   Apple M4
0.00.040.517 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.518 I ggml_metal_init: simdgroup reduction   = true
0.00.040.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.519 I ggml_metal_init: has bfloat            = true
0.00.040.519 I ggml_metal_init: use bfloat            = true
0.00.040.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.750 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.753 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.754 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.595 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.054.597 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.054.597 I llama_new_context_with_model: graph nodes  = 429
0.00.054.597 I llama_new_context_with_model: graph splits = 2
0.00.054.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.932 I 
0.00.061.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.643 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.754 I llama_perf_context_print:        load time =      46.56 ms
0.00.067.755 I llama_perf_context_print: prompt eval time =       4.95 ms /     9 tokens (    0.55 ms per token,  1818.55 tokens per second)
0.00.067.756 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.756 I llama_perf_context_print:       total time =       5.82 ms /    10 tokens
0.00.067.891 I ggml_metal_free: deallocating

real	0m0.245s
user	0m0.049s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.336 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.476 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.482 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.482 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.483 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.483 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.484 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.484 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.485 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.485 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.485 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.488 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.488 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.488 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.489 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.489 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.489 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.489 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.603 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.604 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.604 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.604 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.605 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.605 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.605 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.606 I llama_model_loader: - type  f32:  124 tensors
0.00.014.606 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.262 I llm_load_vocab: special tokens cache size = 5
0.00.018.631 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.636 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.636 I llm_load_print_meta: arch             = bert
0.00.018.636 I llm_load_print_meta: vocab type       = WPM
0.00.018.638 I llm_load_print_meta: n_vocab          = 30522
0.00.018.638 I llm_load_print_meta: n_merges         = 0
0.00.018.640 I llm_load_print_meta: vocab_only       = 0
0.00.018.640 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.640 I llm_load_print_meta: n_embd           = 384
0.00.018.640 I llm_load_print_meta: n_layer          = 12
0.00.018.649 I llm_load_print_meta: n_head           = 12
0.00.018.649 I llm_load_print_meta: n_head_kv        = 12
0.00.018.650 I llm_load_print_meta: n_rot            = 32
0.00.018.650 I llm_load_print_meta: n_swa            = 0
0.00.018.650 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.650 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.650 I llm_load_print_meta: n_gqa            = 1
0.00.018.651 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.652 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.652 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.653 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.653 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.653 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.653 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.654 I llm_load_print_meta: n_ff             = 1536
0.00.018.654 I llm_load_print_meta: n_expert         = 0
0.00.018.654 I llm_load_print_meta: n_expert_used    = 0
0.00.018.654 I llm_load_print_meta: causal attn      = 0
0.00.018.654 I llm_load_print_meta: pooling type     = 2
0.00.018.655 I llm_load_print_meta: rope type        = 2
0.00.018.655 I llm_load_print_meta: rope scaling     = linear
0.00.018.655 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.656 I llm_load_print_meta: freq_scale_train = 1
0.00.018.656 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.656 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.656 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.656 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.656 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.657 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.657 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.657 I llm_load_print_meta: model type       = 33M
0.00.018.657 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.658 I llm_load_print_meta: model params     = 33.21 M
0.00.018.660 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.660 I llm_load_print_meta: general.name     = Bge Small
0.00.018.661 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.661 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.661 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.661 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.662 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.662 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.662 I llm_load_print_meta: max token length = 21
0.00.020.009 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.009 I llm_load_tensors: offloading output layer to GPU
0.00.020.013 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.020 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.022 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.388 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.389 I llama_new_context_with_model: n_ctx         = 512
0.00.020.389 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.389 I llama_new_context_with_model: n_batch       = 2048
0.00.020.389 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.390 I llama_new_context_with_model: flash_attn    = 0
0.00.020.390 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.390 I llama_new_context_with_model: freq_scale    = 1
0.00.020.391 I ggml_metal_init: allocating
0.00.020.393 I ggml_metal_init: found device: Apple M4
0.00.020.395 I ggml_metal_init: picking default device: Apple M4
0.00.021.034 I ggml_metal_init: using embedded metal library
0.00.023.582 I ggml_metal_init: GPU name:   Apple M4
0.00.023.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.585 I ggml_metal_init: simdgroup reduction   = true
0.00.023.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.586 I ggml_metal_init: has bfloat            = true
0.00.023.586 I ggml_metal_init: use bfloat            = true
0.00.023.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.287 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.289 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.291 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.891 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.892 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.892 I llama_new_context_with_model: graph nodes  = 429
0.00.034.893 I llama_new_context_with_model: graph splits = 2
0.00.034.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.026 I 
0.00.040.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.598 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.112 I llama_perf_context_print:        load time =      30.69 ms
0.00.045.113 I llama_perf_context_print: prompt eval time =       4.39 ms /     9 tokens (    0.49 ms per token,  2051.05 tokens per second)
0.00.045.114 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.115 I llama_perf_context_print:       total time =       5.08 ms /    10 tokens
0.00.045.299 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.159 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.977 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.252 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.260 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.262 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.263 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.263 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.264 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.266 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.266 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.267 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.268 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.271 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.272 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.273 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.274 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.364 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.367 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.367 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.368 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.368 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.368 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.369 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.369 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.369 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.370 I llama_model_loader: - type  f32:   41 tensors
0.00.051.370 I llama_model_loader: - type  f16:   29 tensors
0.00.070.128 W llm_load_vocab: empty token at index 5
0.00.074.871 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.076.196 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.076.223 I llm_load_vocab: special tokens cache size = 5
0.00.342.964 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.342.970 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.342.970 I llm_load_print_meta: arch             = jina-bert-v2
0.00.342.972 I llm_load_print_meta: vocab type       = BPE
0.00.342.972 I llm_load_print_meta: n_vocab          = 61056
0.00.342.972 I llm_load_print_meta: n_merges         = 39382
0.00.342.972 I llm_load_print_meta: vocab_only       = 0
0.00.342.975 I llm_load_print_meta: n_ctx_train      = 8192
0.00.342.975 I llm_load_print_meta: n_embd           = 384
0.00.342.975 I llm_load_print_meta: n_layer          = 4
0.00.343.008 I llm_load_print_meta: n_head           = 12
0.00.343.009 I llm_load_print_meta: n_head_kv        = 12
0.00.343.009 I llm_load_print_meta: n_rot            = 32
0.00.343.009 I llm_load_print_meta: n_swa            = 0
0.00.343.010 I llm_load_print_meta: n_embd_head_k    = 32
0.00.343.010 I llm_load_print_meta: n_embd_head_v    = 32
0.00.343.010 I llm_load_print_meta: n_gqa            = 1
0.00.343.011 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.343.011 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.343.012 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.343.013 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.343.013 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.343.013 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.343.017 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.343.018 I llm_load_print_meta: n_ff             = 1536
0.00.343.018 I llm_load_print_meta: n_expert         = 0
0.00.343.018 I llm_load_print_meta: n_expert_used    = 0
0.00.343.019 I llm_load_print_meta: causal attn      = 0
0.00.343.019 I llm_load_print_meta: pooling type     = -1
0.00.343.020 I llm_load_print_meta: rope type        = -1
0.00.343.020 I llm_load_print_meta: rope scaling     = linear
0.00.343.020 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.343.020 I llm_load_print_meta: freq_scale_train = 1
0.00.343.021 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.343.021 I llm_load_print_meta: rope_finetuned   = unknown
0.00.343.021 I llm_load_print_meta: ssm_d_conv       = 0
0.00.343.021 I llm_load_print_meta: ssm_d_inner      = 0
0.00.343.022 I llm_load_print_meta: ssm_d_state      = 0
0.00.343.022 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.343.022 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.343.023 I llm_load_print_meta: model type       = 33M
0.00.343.023 I llm_load_print_meta: model ftype      = F16
0.00.343.023 I llm_load_print_meta: model params     = 32.90 M
0.00.343.024 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.343.024 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.343.029 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.343.030 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.343.030 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.343.030 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.343.031 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.343.031 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.343.031 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.343.031 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.343.031 I llm_load_print_meta: max token length = 45
0.00.344.276 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.344.276 I llm_load_tensors: offloading output layer to GPU
0.00.344.277 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.344.305 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.344.306 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.345.188 I llama_new_context_with_model: n_seq_max     = 1
0.00.345.188 I llama_new_context_with_model: n_ctx         = 8192
0.00.345.189 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.345.189 I llama_new_context_with_model: n_batch       = 2048
0.00.345.189 I llama_new_context_with_model: n_ubatch      = 2048
0.00.345.190 I llama_new_context_with_model: flash_attn    = 0
0.00.345.190 I llama_new_context_with_model: freq_base     = 10000.0
0.00.345.190 I llama_new_context_with_model: freq_scale    = 1
0.00.345.191 I ggml_metal_init: allocating
0.00.345.194 I ggml_metal_init: found device: Apple M4
0.00.345.197 I ggml_metal_init: picking default device: Apple M4
0.00.346.265 I ggml_metal_init: using embedded metal library
0.00.348.860 I ggml_metal_init: GPU name:   Apple M4
0.00.348.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.862 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.862 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.863 I ggml_metal_init: simdgroup reduction   = true
0.00.348.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.863 I ggml_metal_init: has bfloat            = true
0.00.348.863 I ggml_metal_init: use bfloat            = true
0.00.348.863 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.864 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.361.011 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.361.013 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.361.016 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.361.641 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.361.642 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.361.642 I llama_new_context_with_model: graph nodes  = 154
0.00.361.642 I llama_new_context_with_model: graph splits = 2
0.00.361.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.361.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.374.228 I 
0.00.374.265 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.374.508 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.374.509 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.374.519 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.374.519 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.374.525 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.374.525 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.375.020 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.378.851 I llama_perf_context_print:        load time =     349.24 ms
0.00.378.852 I llama_perf_context_print: prompt eval time =       3.82 ms /    62 tokens (    0.06 ms per token, 16217.63 tokens per second)
0.00.378.856 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.378.856 I llama_perf_context_print:       total time =       4.62 ms /    63 tokens
0.00.379.034 I ggml_metal_free: deallocating

real	0m1.070s
user	0m0.348s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.105 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.227 I main: llama backend init
0.00.000.234 I main: load the model and apply lora adapter, if any
0.00.029.259 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.922 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.952 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.953 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.958 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.959 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.882 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.891 I llama_model_loader: - type  f32:  194 tensors
0.00.059.891 I llama_model_loader: - type  f16:   98 tensors
0.00.095.257 I llm_load_vocab: special tokens cache size = 25
0.00.102.685 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.102.688 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.102.688 I llm_load_print_meta: arch             = gptneox
0.00.102.689 I llm_load_print_meta: vocab type       = BPE
0.00.102.689 I llm_load_print_meta: n_vocab          = 50304
0.00.102.689 I llm_load_print_meta: n_merges         = 50009
0.00.102.689 I llm_load_print_meta: vocab_only       = 0
0.00.102.689 I llm_load_print_meta: n_ctx_train      = 2048
0.00.102.689 I llm_load_print_meta: n_embd           = 2048
0.00.102.690 I llm_load_print_meta: n_layer          = 24
0.00.102.704 I llm_load_print_meta: n_head           = 16
0.00.102.706 I llm_load_print_meta: n_head_kv        = 16
0.00.102.706 I llm_load_print_meta: n_rot            = 32
0.00.102.706 I llm_load_print_meta: n_swa            = 0
0.00.102.706 I llm_load_print_meta: n_embd_head_k    = 128
0.00.102.706 I llm_load_print_meta: n_embd_head_v    = 128
0.00.102.707 I llm_load_print_meta: n_gqa            = 1
0.00.102.708 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.102.708 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.102.709 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.102.709 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.102.709 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.102.709 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.102.710 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.102.711 I llm_load_print_meta: n_ff             = 8192
0.00.102.711 I llm_load_print_meta: n_expert         = 0
0.00.102.711 I llm_load_print_meta: n_expert_used    = 0
0.00.102.711 I llm_load_print_meta: causal attn      = 1
0.00.102.711 I llm_load_print_meta: pooling type     = 0
0.00.102.711 I llm_load_print_meta: rope type        = 2
0.00.102.712 I llm_load_print_meta: rope scaling     = linear
0.00.102.712 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.102.712 I llm_load_print_meta: freq_scale_train = 1
0.00.102.712 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.102.713 I llm_load_print_meta: rope_finetuned   = unknown
0.00.102.715 I llm_load_print_meta: ssm_d_conv       = 0
0.00.102.715 I llm_load_print_meta: ssm_d_inner      = 0
0.00.102.715 I llm_load_print_meta: ssm_d_state      = 0
0.00.102.715 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.102.715 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.102.716 I llm_load_print_meta: model type       = 1.4B
0.00.102.716 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.102.717 I llm_load_print_meta: model params     = 1.41 B
0.00.102.717 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.102.717 I llm_load_print_meta: general.name     = 1.4B
0.00.102.717 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.102.718 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.102.718 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.102.718 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.102.722 I llm_load_print_meta: LF token         = 128 ''
0.00.102.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.102.723 I llm_load_print_meta: max token length = 1024
0.00.105.448 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.448 I llm_load_tensors: offloading output layer to GPU
0.00.105.448 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.467 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.469 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.106.478 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.479 I llama_new_context_with_model: n_ctx         = 2048
0.00.106.479 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.106.479 I llama_new_context_with_model: n_batch       = 2048
0.00.106.479 I llama_new_context_with_model: n_ubatch      = 512
0.00.106.479 I llama_new_context_with_model: flash_attn    = 0
0.00.106.480 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.480 I llama_new_context_with_model: freq_scale    = 1
0.00.106.481 I ggml_metal_init: allocating
0.00.106.490 I ggml_metal_init: found device: Apple M4
0.00.106.493 I ggml_metal_init: picking default device: Apple M4
0.00.107.201 I ggml_metal_init: using embedded metal library
0.00.116.814 I ggml_metal_init: GPU name:   Apple M4
0.00.116.816 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.817 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.817 I ggml_metal_init: simdgroup reduction   = true
0.00.116.818 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.818 I ggml_metal_init: has bfloat            = true
0.00.116.818 I ggml_metal_init: use bfloat            = true
0.00.116.818 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.163.592 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.163.599 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.163.620 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.164.570 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.164.572 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.164.572 I llama_new_context_with_model: graph nodes  = 967
0.00.164.573 I llama_new_context_with_model: graph splits = 2
0.00.164.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.164.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.164.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.256.235 I main: llama threadpool init, n_threads = 4
0.00.256.277 I 
0.00.256.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.256.311 I 
0.00.256.381 I sampler seed: 1234
0.00.256.385 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.256.419 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.256.421 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.256.421 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.095.782 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.095.783 I llama_perf_context_print:        load time =     226.97 ms
0.02.095.784 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.24 tokens per second)
0.02.095.785 I llama_perf_context_print:        eval time =    1792.80 ms /    63 runs   (   28.46 ms per token,    35.14 tokens per second)
0.02.095.785 I llama_perf_context_print:       total time =    1839.55 ms /    70 tokens
0.02.095.982 I ggml_metal_free: deallocating

real	0m2.380s
user	0m0.149s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.629 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.892 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.800 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.821 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.706 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.707 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.707 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.709 I llama_model_loader: - type  f32:  194 tensors
0.00.057.709 I llama_model_loader: - type  f16:   98 tensors
0.00.088.212 I llm_load_vocab: special tokens cache size = 25
0.00.095.022 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.024 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.024 I llm_load_print_meta: arch             = gptneox
0.00.095.025 I llm_load_print_meta: vocab type       = BPE
0.00.095.025 I llm_load_print_meta: n_vocab          = 50304
0.00.095.025 I llm_load_print_meta: n_merges         = 50009
0.00.095.025 I llm_load_print_meta: vocab_only       = 0
0.00.095.025 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.026 I llm_load_print_meta: n_embd           = 2048
0.00.095.026 I llm_load_print_meta: n_layer          = 24
0.00.095.041 I llm_load_print_meta: n_head           = 16
0.00.095.042 I llm_load_print_meta: n_head_kv        = 16
0.00.095.042 I llm_load_print_meta: n_rot            = 32
0.00.095.042 I llm_load_print_meta: n_swa            = 0
0.00.095.042 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.042 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.043 I llm_load_print_meta: n_gqa            = 1
0.00.095.044 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.044 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.045 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.045 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.045 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.045 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.046 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.046 I llm_load_print_meta: n_ff             = 8192
0.00.095.046 I llm_load_print_meta: n_expert         = 0
0.00.095.047 I llm_load_print_meta: n_expert_used    = 0
0.00.095.047 I llm_load_print_meta: causal attn      = 1
0.00.095.047 I llm_load_print_meta: pooling type     = 0
0.00.095.047 I llm_load_print_meta: rope type        = 2
0.00.095.047 I llm_load_print_meta: rope scaling     = linear
0.00.095.048 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.048 I llm_load_print_meta: freq_scale_train = 1
0.00.095.048 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.049 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.049 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.051 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.051 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.052 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.052 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.052 I llm_load_print_meta: model type       = 1.4B
0.00.095.053 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.053 I llm_load_print_meta: model params     = 1.41 B
0.00.095.054 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.054 I llm_load_print_meta: general.name     = 1.4B
0.00.095.054 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.054 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.054 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.054 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.055 I llm_load_print_meta: LF token         = 128 ''
0.00.095.055 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.055 I llm_load_print_meta: max token length = 1024
0.00.096.991 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.991 I llm_load_tensors: offloading output layer to GPU
0.00.096.992 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.002 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.003 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.906 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.907 I llama_new_context_with_model: n_ctx         = 128
0.00.097.907 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.907 I llama_new_context_with_model: n_batch       = 128
0.00.097.907 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.907 I llama_new_context_with_model: flash_attn    = 0
0.00.097.908 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.908 I llama_new_context_with_model: freq_scale    = 1
0.00.097.908 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.909 I ggml_metal_init: allocating
0.00.097.915 I ggml_metal_init: found device: Apple M4
0.00.097.917 I ggml_metal_init: picking default device: Apple M4
0.00.098.583 I ggml_metal_init: using embedded metal library
0.00.101.241 I ggml_metal_init: GPU name:   Apple M4
0.00.101.242 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.243 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.243 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.243 I ggml_metal_init: simdgroup reduction   = true
0.00.101.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.244 I ggml_metal_init: has bfloat            = true
0.00.101.244 I ggml_metal_init: use bfloat            = true
0.00.101.244 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.245 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.111.937 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.111.952 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.875 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.112.876 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.112.876 I llama_new_context_with_model: graph nodes  = 967
0.00.112.877 I llama_new_context_with_model: graph splits = 2
0.00.112.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.112.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.084.393 I 
0.01.084.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.084.524 I perplexity: tokenizing the input ..
0.01.097.839 I perplexity: tokenization took 13.312 ms
0.01.097.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.232.014 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.233.687 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.233.705 I llama_perf_context_print:        load time =    1058.49 ms
0.01.233.707 I llama_perf_context_print: prompt eval time =     133.28 ms /   128 tokens (    1.04 ms per token,   960.41 tokens per second)
0.01.233.714 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.233.715 I llama_perf_context_print:       total time =     149.31 ms /   129 tokens
0.01.234.409 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.128s
sys	0m0.215s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.905 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.013 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.018 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.027 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.028 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.028 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.028 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.034 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.838 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.828 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.829 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.830 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.832 I llama_model_loader: - type  f32:  194 tensors
0.00.031.832 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.814 I llm_load_vocab: special tokens cache size = 25
0.00.059.882 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.886 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.886 I llm_load_print_meta: arch             = gptneox
0.00.059.887 I llm_load_print_meta: vocab type       = BPE
0.00.059.889 I llm_load_print_meta: n_vocab          = 50304
0.00.059.889 I llm_load_print_meta: n_merges         = 50009
0.00.059.889 I llm_load_print_meta: vocab_only       = 0
0.00.059.890 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.890 I llm_load_print_meta: n_embd           = 2048
0.00.059.892 I llm_load_print_meta: n_layer          = 24
0.00.059.905 I llm_load_print_meta: n_head           = 16
0.00.059.905 I llm_load_print_meta: n_head_kv        = 16
0.00.059.906 I llm_load_print_meta: n_rot            = 32
0.00.059.906 I llm_load_print_meta: n_swa            = 0
0.00.059.906 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.906 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.907 I llm_load_print_meta: n_gqa            = 1
0.00.059.908 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.908 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.909 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.909 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.910 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.913 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.914 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.915 I llm_load_print_meta: n_ff             = 8192
0.00.059.915 I llm_load_print_meta: n_expert         = 0
0.00.059.916 I llm_load_print_meta: n_expert_used    = 0
0.00.059.916 I llm_load_print_meta: causal attn      = 1
0.00.059.916 I llm_load_print_meta: pooling type     = 0
0.00.059.916 I llm_load_print_meta: rope type        = 2
0.00.059.916 I llm_load_print_meta: rope scaling     = linear
0.00.059.916 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.917 I llm_load_print_meta: freq_scale_train = 1
0.00.059.917 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.917 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.917 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.917 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.918 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.918 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.918 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.918 I llm_load_print_meta: model type       = 1.4B
0.00.059.919 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.919 I llm_load_print_meta: model params     = 1.41 B
0.00.059.920 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.920 I llm_load_print_meta: general.name     = 1.4B
0.00.059.920 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.920 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.920 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.921 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.921 I llm_load_print_meta: LF token         = 128 ''
0.00.059.922 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.922 I llm_load_print_meta: max token length = 1024
0.00.062.078 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.078 I llm_load_tensors: offloading output layer to GPU
0.00.062.078 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.085 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.086 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.059 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.060 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.060 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.060 I llama_new_context_with_model: n_batch       = 2048
0.00.063.061 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.061 I llama_new_context_with_model: flash_attn    = 0
0.00.063.062 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.062 I llama_new_context_with_model: freq_scale    = 1
0.00.063.062 I ggml_metal_init: allocating
0.00.063.071 I ggml_metal_init: found device: Apple M4
0.00.063.073 I ggml_metal_init: picking default device: Apple M4
0.00.063.787 I ggml_metal_init: using embedded metal library
0.00.066.391 I ggml_metal_init: GPU name:   Apple M4
0.00.066.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.394 I ggml_metal_init: simdgroup reduction   = true
0.00.066.394 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.394 I ggml_metal_init: has bfloat            = true
0.00.066.394 I ggml_metal_init: use bfloat            = true
0.00.066.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.363 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.374 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.411 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.536 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.538 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.538 I llama_new_context_with_model: graph nodes  = 967
0.00.103.538 I llama_new_context_with_model: graph splits = 2
0.00.103.566 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.347.127 I main: llama threadpool init, n_threads = 4
0.01.347.174 I 
0.01.347.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.347.210 I 
0.01.347.446 I sampler seed: 1234
0.01.347.451 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.347.466 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.347.469 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.347.469 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.444.250 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.02.444.251 I llama_perf_context_print:        load time =    1337.22 ms
0.02.444.252 I llama_perf_context_print: prompt eval time =      43.48 ms /     7 tokens (    6.21 ms per token,   160.98 tokens per second)
0.02.444.254 I llama_perf_context_print:        eval time =    1050.46 ms /    63 runs   (   16.67 ms per token,    59.97 tokens per second)
0.02.444.254 I llama_perf_context_print:       total time =    1097.13 ms /    70 tokens
0.02.444.431 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.114s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.126 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.182 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.863 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.459 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.460 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.460 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.461 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.462 I llama_model_loader: - type  f32:  194 tensors
0.00.030.463 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.436 I llm_load_vocab: special tokens cache size = 25
0.00.061.599 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.602 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.602 I llm_load_print_meta: arch             = gptneox
0.00.061.603 I llm_load_print_meta: vocab type       = BPE
0.00.061.603 I llm_load_print_meta: n_vocab          = 50304
0.00.061.603 I llm_load_print_meta: n_merges         = 50009
0.00.061.604 I llm_load_print_meta: vocab_only       = 0
0.00.061.604 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.604 I llm_load_print_meta: n_embd           = 2048
0.00.061.604 I llm_load_print_meta: n_layer          = 24
0.00.061.622 I llm_load_print_meta: n_head           = 16
0.00.061.623 I llm_load_print_meta: n_head_kv        = 16
0.00.061.624 I llm_load_print_meta: n_rot            = 32
0.00.061.624 I llm_load_print_meta: n_swa            = 0
0.00.061.624 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.625 I llm_load_print_meta: n_gqa            = 1
0.00.061.625 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.626 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.629 I llm_load_print_meta: n_ff             = 8192
0.00.061.629 I llm_load_print_meta: n_expert         = 0
0.00.061.630 I llm_load_print_meta: n_expert_used    = 0
0.00.061.632 I llm_load_print_meta: causal attn      = 1
0.00.061.632 I llm_load_print_meta: pooling type     = 0
0.00.061.632 I llm_load_print_meta: rope type        = 2
0.00.061.633 I llm_load_print_meta: rope scaling     = linear
0.00.061.633 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.633 I llm_load_print_meta: freq_scale_train = 1
0.00.061.633 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.635 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.635 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.635 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.635 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.636 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.636 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.636 I llm_load_print_meta: model type       = 1.4B
0.00.061.636 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.637 I llm_load_print_meta: model params     = 1.41 B
0.00.061.638 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.638 I llm_load_print_meta: general.name     = 1.4B
0.00.061.638 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.638 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.639 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.639 I llm_load_print_meta: LF token         = 128 ''
0.00.061.639 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.639 I llm_load_print_meta: max token length = 1024
0.00.064.122 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.123 I llm_load_tensors: offloading output layer to GPU
0.00.064.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.134 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.135 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.114 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.114 I llama_new_context_with_model: n_ctx         = 128
0.00.065.115 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.115 I llama_new_context_with_model: n_batch       = 128
0.00.065.115 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.115 I llama_new_context_with_model: flash_attn    = 0
0.00.065.115 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.116 I llama_new_context_with_model: freq_scale    = 1
0.00.065.116 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.116 I ggml_metal_init: allocating
0.00.065.120 I ggml_metal_init: found device: Apple M4
0.00.065.122 I ggml_metal_init: picking default device: Apple M4
0.00.065.841 I ggml_metal_init: using embedded metal library
0.00.068.512 I ggml_metal_init: GPU name:   Apple M4
0.00.068.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.515 I ggml_metal_init: simdgroup reduction   = true
0.00.068.515 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.515 I ggml_metal_init: has bfloat            = true
0.00.068.515 I ggml_metal_init: use bfloat            = true
0.00.068.516 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.297 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.303 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.318 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.433 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.434 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.435 I llama_new_context_with_model: graph nodes  = 967
0.00.081.435 I llama_new_context_with_model: graph splits = 2
0.00.081.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.962.479 I 
0.00.962.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.962.529 I perplexity: tokenizing the input ..
0.00.971.764 I perplexity: tokenization took 9.232 ms
0.00.971.769 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.095.933 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.097.351 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.097.364 I llama_perf_context_print:        load time =     951.29 ms
0.01.097.365 I llama_perf_context_print: prompt eval time =     123.92 ms /   128 tokens (    0.97 ms per token,  1032.93 tokens per second)
0.01.097.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.097.366 I llama_perf_context_print:       total time =     134.88 ms /   129 tokens
0.01.097.713 I ggml_metal_free: deallocating

real	0m1.114s
user	0m0.092s
sys	0m0.168s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.014.091 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.395 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.418 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.418 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.419 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.419 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.420 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.421 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.421 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.422 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.424 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.004 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.005 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.006 I llama_model_loader: - type  f32:  194 tensors
0.00.040.006 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.994 I llm_load_vocab: special tokens cache size = 25
0.00.082.163 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.167 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.168 I llm_load_print_meta: arch             = gptneox
0.00.082.168 I llm_load_print_meta: vocab type       = BPE
0.00.082.168 I llm_load_print_meta: n_vocab          = 50304
0.00.082.169 I llm_load_print_meta: n_merges         = 50009
0.00.082.169 I llm_load_print_meta: vocab_only       = 0
0.00.082.169 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.169 I llm_load_print_meta: n_embd           = 2048
0.00.082.170 I llm_load_print_meta: n_layer          = 24
0.00.082.187 I llm_load_print_meta: n_head           = 16
0.00.082.188 I llm_load_print_meta: n_head_kv        = 16
0.00.082.188 I llm_load_print_meta: n_rot            = 32
0.00.082.188 I llm_load_print_meta: n_swa            = 0
0.00.082.188 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.191 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.192 I llm_load_print_meta: n_gqa            = 1
0.00.082.193 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.194 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.195 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.196 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.196 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.196 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.196 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.199 I llm_load_print_meta: n_ff             = 8192
0.00.082.199 I llm_load_print_meta: n_expert         = 0
0.00.082.199 I llm_load_print_meta: n_expert_used    = 0
0.00.082.199 I llm_load_print_meta: causal attn      = 1
0.00.082.200 I llm_load_print_meta: pooling type     = 0
0.00.082.200 I llm_load_print_meta: rope type        = 2
0.00.082.200 I llm_load_print_meta: rope scaling     = linear
0.00.082.201 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.201 I llm_load_print_meta: freq_scale_train = 1
0.00.082.202 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.202 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.202 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.203 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.203 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.203 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.203 I llm_load_print_meta: model type       = 1.4B
0.00.082.206 I llm_load_print_meta: model ftype      = Q4_0
0.00.082.206 I llm_load_print_meta: model params     = 1.41 B
0.00.082.207 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.082.207 I llm_load_print_meta: general.name     = 1.4B
0.00.082.208 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.208 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.208 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.208 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.209 I llm_load_print_meta: LF token         = 128 ''
0.00.082.209 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.210 I llm_load_print_meta: max token length = 1024
0.00.085.291 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.292 I llm_load_tensors: offloading output layer to GPU
0.00.085.292 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.303 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.085.305 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.086.767 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.768 I llama_new_context_with_model: n_ctx         = 2048
0.00.086.769 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.086.769 I llama_new_context_with_model: n_batch       = 2048
0.00.086.769 I llama_new_context_with_model: n_ubatch      = 512
0.00.086.769 I llama_new_context_with_model: flash_attn    = 0
0.00.086.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.770 I llama_new_context_with_model: freq_scale    = 1
0.00.086.771 I ggml_metal_init: allocating
0.00.086.775 I ggml_metal_init: found device: Apple M4
0.00.086.778 I ggml_metal_init: picking default device: Apple M4
0.00.087.758 I ggml_metal_init: using embedded metal library
0.00.091.667 I ggml_metal_init: GPU name:   Apple M4
0.00.091.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.672 I ggml_metal_init: simdgroup reduction   = true
0.00.091.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.672 I ggml_metal_init: has bfloat            = true
0.00.091.672 I ggml_metal_init: use bfloat            = true
0.00.091.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.131.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.804 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.838 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.132.887 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.132.889 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.132.889 I llama_new_context_with_model: graph nodes  = 967
0.00.132.890 I llama_new_context_with_model: graph splits = 2
0.00.132.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.133.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.360 I main: llama threadpool init, n_threads = 4
0.00.858.434 I 
0.00.858.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.500 I 
0.00.858.883 I sampler seed: 1234
0.00.858.888 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.927 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.929 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.545.750 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.545.751 I llama_perf_context_print:        load time =     844.25 ms
0.01.545.752 I llama_perf_context_print: prompt eval time =      45.37 ms /     7 tokens (    6.48 ms per token,   154.28 tokens per second)
0.01.545.752 I llama_perf_context_print:        eval time =     638.43 ms /    63 runs   (   10.13 ms per token,    98.68 tokens per second)
0.01.545.753 I llama_perf_context_print:       total time =     687.40 ms /    70 tokens
0.01.545.943 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.139s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.044 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.969 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.970 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.972 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.972 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.864 I llama_model_loader: - type  f32:  194 tensors
0.00.024.865 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.975 I llm_load_vocab: special tokens cache size = 25
0.00.052.134 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.139 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.139 I llm_load_print_meta: arch             = gptneox
0.00.052.140 I llm_load_print_meta: vocab type       = BPE
0.00.052.140 I llm_load_print_meta: n_vocab          = 50304
0.00.052.140 I llm_load_print_meta: n_merges         = 50009
0.00.052.140 I llm_load_print_meta: vocab_only       = 0
0.00.052.141 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.141 I llm_load_print_meta: n_embd           = 2048
0.00.052.141 I llm_load_print_meta: n_layer          = 24
0.00.052.159 I llm_load_print_meta: n_head           = 16
0.00.052.160 I llm_load_print_meta: n_head_kv        = 16
0.00.052.160 I llm_load_print_meta: n_rot            = 32
0.00.052.161 I llm_load_print_meta: n_swa            = 0
0.00.052.161 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.161 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.161 I llm_load_print_meta: n_gqa            = 1
0.00.052.162 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.163 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.163 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.164 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.165 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.165 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.166 I llm_load_print_meta: n_ff             = 8192
0.00.052.166 I llm_load_print_meta: n_expert         = 0
0.00.052.169 I llm_load_print_meta: n_expert_used    = 0
0.00.052.169 I llm_load_print_meta: causal attn      = 1
0.00.052.169 I llm_load_print_meta: pooling type     = 0
0.00.052.169 I llm_load_print_meta: rope type        = 2
0.00.052.169 I llm_load_print_meta: rope scaling     = linear
0.00.052.170 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.170 I llm_load_print_meta: freq_scale_train = 1
0.00.052.170 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.170 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.172 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.172 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.172 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.172 I llm_load_print_meta: model type       = 1.4B
0.00.052.173 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.173 I llm_load_print_meta: model params     = 1.41 B
0.00.052.174 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.174 I llm_load_print_meta: general.name     = 1.4B
0.00.052.174 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.175 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.175 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.176 I llm_load_print_meta: LF token         = 128 ''
0.00.052.176 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.176 I llm_load_print_meta: max token length = 1024
0.00.054.144 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.144 I llm_load_tensors: offloading output layer to GPU
0.00.054.144 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.155 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.156 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.045 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.046 I llama_new_context_with_model: n_ctx         = 128
0.00.055.046 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.046 I llama_new_context_with_model: n_batch       = 128
0.00.055.046 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.047 I llama_new_context_with_model: flash_attn    = 0
0.00.055.047 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.047 I llama_new_context_with_model: freq_scale    = 1
0.00.055.048 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.048 I ggml_metal_init: allocating
0.00.055.052 I ggml_metal_init: found device: Apple M4
0.00.055.054 I ggml_metal_init: picking default device: Apple M4
0.00.055.691 I ggml_metal_init: using embedded metal library
0.00.058.044 I ggml_metal_init: GPU name:   Apple M4
0.00.058.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.046 I ggml_metal_init: simdgroup reduction   = true
0.00.058.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.047 I ggml_metal_init: has bfloat            = true
0.00.058.047 I ggml_metal_init: use bfloat            = true
0.00.058.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.876 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.878 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.895 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.977 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.978 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.978 I llama_new_context_with_model: graph nodes  = 967
0.00.070.979 I llama_new_context_with_model: graph splits = 2
0.00.070.991 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.278 I 
0.00.637.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.343 I perplexity: tokenizing the input ..
0.00.645.209 I perplexity: tokenization took 7.863 ms
0.00.645.213 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.767.066 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.768.509 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.768.529 I llama_perf_context_print:        load time =     627.23 ms
0.00.768.530 I llama_perf_context_print: prompt eval time =     121.61 ms /   128 tokens (    0.95 ms per token,  1052.58 tokens per second)
0.00.768.531 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.532 I llama_perf_context_print:       total time =     131.25 ms /   129 tokens
0.00.768.901 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.081s
sys	0m0.092s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.011.600 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.032.815 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.830 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.833 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.834 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.836 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.836 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.837 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.324 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.863 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.046.117 I llama_model_loader: - type  f32:  194 tensors
0.00.046.117 I llama_model_loader: - type q4_1:   97 tensors
0.00.046.118 I llama_model_loader: - type q6_K:    1 tensors
0.00.088.157 I llm_load_vocab: special tokens cache size = 25
0.00.097.626 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.629 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.629 I llm_load_print_meta: arch             = gptneox
0.00.097.630 I llm_load_print_meta: vocab type       = BPE
0.00.097.630 I llm_load_print_meta: n_vocab          = 50304
0.00.097.630 I llm_load_print_meta: n_merges         = 50009
0.00.097.631 I llm_load_print_meta: vocab_only       = 0
0.00.097.631 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.631 I llm_load_print_meta: n_embd           = 2048
0.00.097.631 I llm_load_print_meta: n_layer          = 24
0.00.097.646 I llm_load_print_meta: n_head           = 16
0.00.097.647 I llm_load_print_meta: n_head_kv        = 16
0.00.097.647 I llm_load_print_meta: n_rot            = 32
0.00.097.647 I llm_load_print_meta: n_swa            = 0
0.00.097.648 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.648 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.649 I llm_load_print_meta: n_gqa            = 1
0.00.097.650 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.650 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.651 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.651 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.651 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.652 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.653 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.654 I llm_load_print_meta: n_ff             = 8192
0.00.097.654 I llm_load_print_meta: n_expert         = 0
0.00.097.654 I llm_load_print_meta: n_expert_used    = 0
0.00.097.654 I llm_load_print_meta: causal attn      = 1
0.00.097.655 I llm_load_print_meta: pooling type     = 0
0.00.097.657 I llm_load_print_meta: rope type        = 2
0.00.097.657 I llm_load_print_meta: rope scaling     = linear
0.00.097.657 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.658 I llm_load_print_meta: freq_scale_train = 1
0.00.097.658 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.658 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.659 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.659 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.659 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.659 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.659 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.665 I llm_load_print_meta: model type       = 1.4B
0.00.097.665 I llm_load_print_meta: model ftype      = Q4_1
0.00.097.666 I llm_load_print_meta: model params     = 1.41 B
0.00.097.666 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.097.667 I llm_load_print_meta: general.name     = 1.4B
0.00.097.667 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.667 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.667 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.668 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.668 I llm_load_print_meta: LF token         = 128 ''
0.00.097.668 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.668 I llm_load_print_meta: max token length = 1024
0.00.100.242 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.242 I llm_load_tensors: offloading output layer to GPU
0.00.100.243 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.254 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.100.255 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.101.510 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.511 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.511 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.512 I llama_new_context_with_model: n_batch       = 2048
0.00.101.512 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.512 I llama_new_context_with_model: flash_attn    = 0
0.00.101.513 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.513 I llama_new_context_with_model: freq_scale    = 1
0.00.101.513 I ggml_metal_init: allocating
0.00.101.517 I ggml_metal_init: found device: Apple M4
0.00.101.520 I ggml_metal_init: picking default device: Apple M4
0.00.102.317 I ggml_metal_init: using embedded metal library
0.00.105.524 I ggml_metal_init: GPU name:   Apple M4
0.00.105.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.105.527 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.105.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.105.528 I ggml_metal_init: simdgroup reduction   = true
0.00.105.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.105.528 I ggml_metal_init: has bfloat            = true
0.00.105.528 I ggml_metal_init: use bfloat            = true
0.00.105.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.105.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.139.116 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.139.123 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.139.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.140.170 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.140.172 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.140.172 I llama_new_context_with_model: graph nodes  = 967
0.00.140.172 I llama_new_context_with_model: graph splits = 2
0.00.140.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.140.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.140.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.864.817 I main: llama threadpool init, n_threads = 4
0.00.864.904 I 
0.00.864.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.864.987 I 
0.00.865.483 I sampler seed: 1234
0.00.865.492 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.865.536 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.865.539 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.865.539 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.605.263 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.605.264 I llama_perf_context_print:        load time =     853.21 ms
0.01.605.265 I llama_perf_context_print: prompt eval time =      50.30 ms /     7 tokens (    7.19 ms per token,   139.17 tokens per second)
0.01.605.265 I llama_perf_context_print:        eval time =     686.57 ms /    63 runs   (   10.90 ms per token,    91.76 tokens per second)
0.01.605.266 I llama_perf_context_print:       total time =     740.45 ms /    70 tokens
0.01.605.454 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.156s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.813 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.660 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.666 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.667 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.671 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.301 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.301 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.302 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.302 I llama_model_loader: - type  f32:  194 tensors
0.00.023.302 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.303 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.311 I llm_load_vocab: special tokens cache size = 25
0.00.050.272 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.275 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.275 I llm_load_print_meta: arch             = gptneox
0.00.050.276 I llm_load_print_meta: vocab type       = BPE
0.00.050.276 I llm_load_print_meta: n_vocab          = 50304
0.00.050.276 I llm_load_print_meta: n_merges         = 50009
0.00.050.276 I llm_load_print_meta: vocab_only       = 0
0.00.050.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.277 I llm_load_print_meta: n_embd           = 2048
0.00.050.277 I llm_load_print_meta: n_layer          = 24
0.00.050.291 I llm_load_print_meta: n_head           = 16
0.00.050.292 I llm_load_print_meta: n_head_kv        = 16
0.00.050.292 I llm_load_print_meta: n_rot            = 32
0.00.050.292 I llm_load_print_meta: n_swa            = 0
0.00.050.292 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.293 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.293 I llm_load_print_meta: n_gqa            = 1
0.00.050.294 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.295 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.295 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.296 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.296 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.296 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.297 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.298 I llm_load_print_meta: n_ff             = 8192
0.00.050.298 I llm_load_print_meta: n_expert         = 0
0.00.050.298 I llm_load_print_meta: n_expert_used    = 0
0.00.050.299 I llm_load_print_meta: causal attn      = 1
0.00.050.299 I llm_load_print_meta: pooling type     = 0
0.00.050.299 I llm_load_print_meta: rope type        = 2
0.00.050.299 I llm_load_print_meta: rope scaling     = linear
0.00.050.300 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.300 I llm_load_print_meta: freq_scale_train = 1
0.00.050.300 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.300 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.300 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.301 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.301 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.302 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.302 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.303 I llm_load_print_meta: model type       = 1.4B
0.00.050.303 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.303 I llm_load_print_meta: model params     = 1.41 B
0.00.050.304 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.304 I llm_load_print_meta: general.name     = 1.4B
0.00.050.304 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.304 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.305 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.305 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.305 I llm_load_print_meta: LF token         = 128 ''
0.00.050.305 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.305 I llm_load_print_meta: max token length = 1024
0.00.052.298 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.298 I llm_load_tensors: offloading output layer to GPU
0.00.052.298 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.309 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.310 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.187 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.188 I llama_new_context_with_model: n_ctx         = 128
0.00.053.188 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.188 I llama_new_context_with_model: n_batch       = 128
0.00.053.188 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.189 I llama_new_context_with_model: flash_attn    = 0
0.00.053.189 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.189 I llama_new_context_with_model: freq_scale    = 1
0.00.053.189 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.190 I ggml_metal_init: allocating
0.00.053.193 I ggml_metal_init: found device: Apple M4
0.00.053.195 I ggml_metal_init: picking default device: Apple M4
0.00.053.778 I ggml_metal_init: using embedded metal library
0.00.056.176 I ggml_metal_init: GPU name:   Apple M4
0.00.056.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.178 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.178 I ggml_metal_init: simdgroup reduction   = true
0.00.056.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.179 I ggml_metal_init: has bfloat            = true
0.00.056.179 I ggml_metal_init: use bfloat            = true
0.00.056.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.657 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.660 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.675 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.553 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.554 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.555 I llama_new_context_with_model: graph nodes  = 967
0.00.068.555 I llama_new_context_with_model: graph splits = 2
0.00.068.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.441 I 
0.00.658.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.483 I perplexity: tokenizing the input ..
0.00.665.910 I perplexity: tokenization took 7.425 ms
0.00.665.914 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.872 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.790.130 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.790.146 I llama_perf_context_print:        load time =     649.63 ms
0.00.790.148 I llama_perf_context_print: prompt eval time =     122.73 ms /   128 tokens (    0.96 ms per token,  1042.98 tokens per second)
0.00.790.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.150 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.790.658 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.079s
sys	0m0.097s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.546 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.024.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.348 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.161 I llama_model_loader: - type  f32:  194 tensors
0.00.033.162 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.162 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.817 I llm_load_vocab: special tokens cache size = 25
0.00.061.767 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.769 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.770 I llm_load_print_meta: arch             = gptneox
0.00.061.770 I llm_load_print_meta: vocab type       = BPE
0.00.061.771 I llm_load_print_meta: n_vocab          = 50304
0.00.061.771 I llm_load_print_meta: n_merges         = 50009
0.00.061.771 I llm_load_print_meta: vocab_only       = 0
0.00.061.771 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.771 I llm_load_print_meta: n_embd           = 2048
0.00.061.772 I llm_load_print_meta: n_layer          = 24
0.00.061.781 I llm_load_print_meta: n_head           = 16
0.00.061.781 I llm_load_print_meta: n_head_kv        = 16
0.00.061.782 I llm_load_print_meta: n_rot            = 32
0.00.061.782 I llm_load_print_meta: n_swa            = 0
0.00.061.782 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.782 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.783 I llm_load_print_meta: n_gqa            = 1
0.00.061.784 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.786 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.787 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.787 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.787 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.787 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.788 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.788 I llm_load_print_meta: n_ff             = 8192
0.00.061.788 I llm_load_print_meta: n_expert         = 0
0.00.061.789 I llm_load_print_meta: n_expert_used    = 0
0.00.061.789 I llm_load_print_meta: causal attn      = 1
0.00.061.791 I llm_load_print_meta: pooling type     = 0
0.00.061.791 I llm_load_print_meta: rope type        = 2
0.00.061.791 I llm_load_print_meta: rope scaling     = linear
0.00.061.791 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.792 I llm_load_print_meta: freq_scale_train = 1
0.00.061.792 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.795 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.795 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.795 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.795 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.796 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.796 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.796 I llm_load_print_meta: model type       = 1.4B
0.00.061.796 I llm_load_print_meta: model ftype      = Q5_0
0.00.061.797 I llm_load_print_meta: model params     = 1.41 B
0.00.061.797 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.061.797 I llm_load_print_meta: general.name     = 1.4B
0.00.061.797 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.798 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.798 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.798 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.798 I llm_load_print_meta: LF token         = 128 ''
0.00.061.798 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.800 I llm_load_print_meta: max token length = 1024
0.00.063.649 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.649 I llm_load_tensors: offloading output layer to GPU
0.00.063.649 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.654 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.063.655 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.064.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.490 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.491 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.491 I llama_new_context_with_model: n_batch       = 2048
0.00.064.491 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.491 I llama_new_context_with_model: flash_attn    = 0
0.00.064.492 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.492 I llama_new_context_with_model: freq_scale    = 1
0.00.064.492 I ggml_metal_init: allocating
0.00.064.495 I ggml_metal_init: found device: Apple M4
0.00.064.497 I ggml_metal_init: picking default device: Apple M4
0.00.065.086 I ggml_metal_init: using embedded metal library
0.00.067.408 I ggml_metal_init: GPU name:   Apple M4
0.00.067.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.409 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.410 I ggml_metal_init: simdgroup reduction   = true
0.00.067.410 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.410 I ggml_metal_init: has bfloat            = true
0.00.067.410 I ggml_metal_init: use bfloat            = true
0.00.067.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.135 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.145 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.172 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.251 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.253 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.253 I llama_new_context_with_model: graph nodes  = 967
0.00.099.253 I llama_new_context_with_model: graph splits = 2
0.00.099.278 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.413 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.640 I main: llama threadpool init, n_threads = 4
0.00.858.677 I 
0.00.858.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.710 I 
0.00.858.856 I sampler seed: 1234
0.00.858.860 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.874 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.876 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.876 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.651.593 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.651.594 I llama_perf_context_print:        load time =     849.09 ms
0.01.651.594 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.46 tokens per second)
0.01.651.595 I llama_perf_context_print:        eval time =     746.76 ms /    63 runs   (   11.85 ms per token,    84.36 tokens per second)
0.01.651.595 I llama_perf_context_print:       total time =     792.95 ms /    70 tokens
0.01.651.797 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.111s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.550 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.150 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.160 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.025 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.956 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.958 I llama_model_loader: - type  f32:  194 tensors
0.00.024.958 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.959 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.057 I llm_load_vocab: special tokens cache size = 25
0.00.052.170 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.173 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.173 I llm_load_print_meta: arch             = gptneox
0.00.052.174 I llm_load_print_meta: vocab type       = BPE
0.00.052.174 I llm_load_print_meta: n_vocab          = 50304
0.00.052.174 I llm_load_print_meta: n_merges         = 50009
0.00.052.174 I llm_load_print_meta: vocab_only       = 0
0.00.052.174 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.174 I llm_load_print_meta: n_embd           = 2048
0.00.052.175 I llm_load_print_meta: n_layer          = 24
0.00.052.189 I llm_load_print_meta: n_head           = 16
0.00.052.192 I llm_load_print_meta: n_head_kv        = 16
0.00.052.192 I llm_load_print_meta: n_rot            = 32
0.00.052.193 I llm_load_print_meta: n_swa            = 0
0.00.052.193 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.193 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.194 I llm_load_print_meta: n_gqa            = 1
0.00.052.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.195 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.196 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.196 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.196 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.196 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.196 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.197 I llm_load_print_meta: n_ff             = 8192
0.00.052.197 I llm_load_print_meta: n_expert         = 0
0.00.052.197 I llm_load_print_meta: n_expert_used    = 0
0.00.052.197 I llm_load_print_meta: causal attn      = 1
0.00.052.198 I llm_load_print_meta: pooling type     = 0
0.00.052.198 I llm_load_print_meta: rope type        = 2
0.00.052.198 I llm_load_print_meta: rope scaling     = linear
0.00.052.199 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.199 I llm_load_print_meta: freq_scale_train = 1
0.00.052.199 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.199 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.200 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.200 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.201 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.201 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.201 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.202 I llm_load_print_meta: model type       = 1.4B
0.00.052.202 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.202 I llm_load_print_meta: model params     = 1.41 B
0.00.052.203 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.203 I llm_load_print_meta: general.name     = 1.4B
0.00.052.203 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.203 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: LF token         = 128 ''
0.00.052.204 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: max token length = 1024
0.00.054.273 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.273 I llm_load_tensors: offloading output layer to GPU
0.00.054.273 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.284 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.285 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.209 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.210 I llama_new_context_with_model: n_ctx         = 128
0.00.055.210 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.210 I llama_new_context_with_model: n_batch       = 128
0.00.055.211 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.211 I llama_new_context_with_model: flash_attn    = 0
0.00.055.211 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.212 I llama_new_context_with_model: freq_scale    = 1
0.00.055.212 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.213 I ggml_metal_init: allocating
0.00.055.222 I ggml_metal_init: found device: Apple M4
0.00.055.225 I ggml_metal_init: picking default device: Apple M4
0.00.055.768 I ggml_metal_init: using embedded metal library
0.00.058.136 I ggml_metal_init: GPU name:   Apple M4
0.00.058.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.139 I ggml_metal_init: simdgroup reduction   = true
0.00.058.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.139 I ggml_metal_init: has bfloat            = true
0.00.058.139 I ggml_metal_init: use bfloat            = true
0.00.058.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.762 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.769 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.784 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.673 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.674 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.675 I llama_new_context_with_model: graph nodes  = 967
0.00.069.675 I llama_new_context_with_model: graph splits = 2
0.00.069.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.294 I 
0.00.715.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.346 I perplexity: tokenizing the input ..
0.00.723.643 I perplexity: tokenization took 8.295 ms
0.00.723.646 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.519 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.859.710 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.859.722 I llama_perf_context_print:        load time =     704.74 ms
0.00.859.723 I llama_perf_context_print: prompt eval time =     134.65 ms /   128 tokens (    1.05 ms per token,   950.62 tokens per second)
0.00.859.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.728 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.860.271 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.079s
sys	0m0.107s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.619 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.418 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.419 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.420 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.158 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.159 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.159 I llama_model_loader: - type  f32:  194 tensors
0.00.026.160 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.285 I llm_load_vocab: special tokens cache size = 25
0.00.053.297 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.299 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.300 I llm_load_print_meta: arch             = gptneox
0.00.053.300 I llm_load_print_meta: vocab type       = BPE
0.00.053.300 I llm_load_print_meta: n_vocab          = 50304
0.00.053.300 I llm_load_print_meta: n_merges         = 50009
0.00.053.301 I llm_load_print_meta: vocab_only       = 0
0.00.053.301 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.301 I llm_load_print_meta: n_embd           = 2048
0.00.053.301 I llm_load_print_meta: n_layer          = 24
0.00.053.310 I llm_load_print_meta: n_head           = 16
0.00.053.311 I llm_load_print_meta: n_head_kv        = 16
0.00.053.311 I llm_load_print_meta: n_rot            = 32
0.00.053.312 I llm_load_print_meta: n_swa            = 0
0.00.053.312 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.312 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.313 I llm_load_print_meta: n_gqa            = 1
0.00.053.314 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.314 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.315 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.315 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.316 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.316 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.316 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.317 I llm_load_print_meta: n_ff             = 8192
0.00.053.317 I llm_load_print_meta: n_expert         = 0
0.00.053.317 I llm_load_print_meta: n_expert_used    = 0
0.00.053.317 I llm_load_print_meta: causal attn      = 1
0.00.053.317 I llm_load_print_meta: pooling type     = 0
0.00.053.317 I llm_load_print_meta: rope type        = 2
0.00.053.318 I llm_load_print_meta: rope scaling     = linear
0.00.053.320 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.321 I llm_load_print_meta: freq_scale_train = 1
0.00.053.321 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.321 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.321 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.322 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.322 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.322 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.322 I llm_load_print_meta: model type       = 1.4B
0.00.053.322 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.323 I llm_load_print_meta: model params     = 1.41 B
0.00.053.324 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.324 I llm_load_print_meta: general.name     = 1.4B
0.00.053.325 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.325 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.325 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.325 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.325 I llm_load_print_meta: LF token         = 128 ''
0.00.053.326 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.326 I llm_load_print_meta: max token length = 1024
0.00.055.106 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.106 I llm_load_tensors: offloading output layer to GPU
0.00.055.106 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.112 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.113 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.078 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.079 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.079 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.079 I llama_new_context_with_model: n_batch       = 2048
0.00.056.079 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.079 I llama_new_context_with_model: flash_attn    = 0
0.00.056.080 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.080 I llama_new_context_with_model: freq_scale    = 1
0.00.056.081 I ggml_metal_init: allocating
0.00.056.083 I ggml_metal_init: found device: Apple M4
0.00.056.085 I ggml_metal_init: picking default device: Apple M4
0.00.056.695 I ggml_metal_init: using embedded metal library
0.00.059.051 I ggml_metal_init: GPU name:   Apple M4
0.00.059.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.053 I ggml_metal_init: simdgroup reduction   = true
0.00.059.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.053 I ggml_metal_init: has bfloat            = true
0.00.059.054 I ggml_metal_init: use bfloat            = true
0.00.059.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.151 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.165 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.183 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.228 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.229 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.229 I llama_new_context_with_model: graph nodes  = 967
0.00.089.230 I llama_new_context_with_model: graph splits = 2
0.00.089.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.397 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.713 I main: llama threadpool init, n_threads = 4
0.00.709.754 I 
0.00.709.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.783 I 
0.00.710.004 I sampler seed: 1234
0.00.710.009 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.042 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.043 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.043 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.548.738 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.548.738 I llama_perf_context_print:        load time =     699.09 ms
0.01.548.739 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.38 tokens per second)
0.01.548.739 I llama_perf_context_print:        eval time =     793.35 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.548.740 I llama_perf_context_print:       total time =     839.03 ms /    70 tokens
0.01.548.936 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.788 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.692 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.693 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.694 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.696 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.696 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.697 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.697 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.704 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.684 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.539 I llama_model_loader: - type  f32:  194 tensors
0.00.023.540 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.540 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.476 I llm_load_vocab: special tokens cache size = 25
0.00.050.468 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.470 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.471 I llm_load_print_meta: arch             = gptneox
0.00.050.471 I llm_load_print_meta: vocab type       = BPE
0.00.050.471 I llm_load_print_meta: n_vocab          = 50304
0.00.050.471 I llm_load_print_meta: n_merges         = 50009
0.00.050.472 I llm_load_print_meta: vocab_only       = 0
0.00.050.472 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.472 I llm_load_print_meta: n_embd           = 2048
0.00.050.472 I llm_load_print_meta: n_layer          = 24
0.00.050.485 I llm_load_print_meta: n_head           = 16
0.00.050.486 I llm_load_print_meta: n_head_kv        = 16
0.00.050.486 I llm_load_print_meta: n_rot            = 32
0.00.050.486 I llm_load_print_meta: n_swa            = 0
0.00.050.489 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.489 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.490 I llm_load_print_meta: n_gqa            = 1
0.00.050.491 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.491 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.495 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.495 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.497 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.497 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.497 I llm_load_print_meta: n_ff             = 8192
0.00.050.498 I llm_load_print_meta: n_expert         = 0
0.00.050.498 I llm_load_print_meta: n_expert_used    = 0
0.00.050.498 I llm_load_print_meta: causal attn      = 1
0.00.050.498 I llm_load_print_meta: pooling type     = 0
0.00.050.498 I llm_load_print_meta: rope type        = 2
0.00.050.498 I llm_load_print_meta: rope scaling     = linear
0.00.050.499 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.499 I llm_load_print_meta: freq_scale_train = 1
0.00.050.499 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.500 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.501 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.501 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.501 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.501 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.502 I llm_load_print_meta: model type       = 1.4B
0.00.050.502 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.502 I llm_load_print_meta: model params     = 1.41 B
0.00.050.503 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.505 I llm_load_print_meta: general.name     = 1.4B
0.00.050.505 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: LF token         = 128 ''
0.00.050.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.507 I llm_load_print_meta: max token length = 1024
0.00.052.095 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.095 I llm_load_tensors: offloading output layer to GPU
0.00.052.096 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.106 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.107 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.909 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.910 I llama_new_context_with_model: n_ctx         = 128
0.00.052.910 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.911 I llama_new_context_with_model: n_batch       = 128
0.00.052.911 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.911 I llama_new_context_with_model: flash_attn    = 0
0.00.052.911 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.912 I llama_new_context_with_model: freq_scale    = 1
0.00.052.912 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.912 I ggml_metal_init: allocating
0.00.052.915 I ggml_metal_init: found device: Apple M4
0.00.052.917 I ggml_metal_init: picking default device: Apple M4
0.00.053.510 I ggml_metal_init: using embedded metal library
0.00.055.848 I ggml_metal_init: GPU name:   Apple M4
0.00.055.849 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.850 I ggml_metal_init: simdgroup reduction   = true
0.00.055.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.851 I ggml_metal_init: has bfloat            = true
0.00.055.851 I ggml_metal_init: use bfloat            = true
0.00.055.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.368 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.371 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.385 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.272 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.273 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.273 I llama_new_context_with_model: graph nodes  = 967
0.00.067.273 I llama_new_context_with_model: graph splits = 2
0.00.067.286 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.287 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.589 I 
0.00.667.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.699 I perplexity: tokenizing the input ..
0.00.675.702 I perplexity: tokenization took 8.001 ms
0.00.675.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.423 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.811.602 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.811.629 I llama_perf_context_print:        load time =     658.79 ms
0.00.811.630 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.72 tokens per second)
0.00.811.631 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.631 I llama_perf_context_print:       total time =     144.05 ms /   129 tokens
0.00.811.962 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.078s
sys	0m0.124s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.563 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.076 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.076 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.080 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.080 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.083 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.083 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.084 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.943 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.944 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.945 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.946 I llama_model_loader: - type  f32:  194 tensors
0.00.023.946 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.946 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.946 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.932 I llm_load_vocab: special tokens cache size = 25
0.00.051.027 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.029 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.030 I llm_load_print_meta: arch             = gptneox
0.00.051.030 I llm_load_print_meta: vocab type       = BPE
0.00.051.030 I llm_load_print_meta: n_vocab          = 50304
0.00.051.031 I llm_load_print_meta: n_merges         = 50009
0.00.051.031 I llm_load_print_meta: vocab_only       = 0
0.00.051.031 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.031 I llm_load_print_meta: n_embd           = 2048
0.00.051.031 I llm_load_print_meta: n_layer          = 24
0.00.051.046 I llm_load_print_meta: n_head           = 16
0.00.051.046 I llm_load_print_meta: n_head_kv        = 16
0.00.051.048 I llm_load_print_meta: n_rot            = 32
0.00.051.049 I llm_load_print_meta: n_swa            = 0
0.00.051.049 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.049 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.050 I llm_load_print_meta: n_gqa            = 1
0.00.051.051 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.051 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.052 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.052 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.052 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.052 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.053 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.053 I llm_load_print_meta: n_ff             = 8192
0.00.051.053 I llm_load_print_meta: n_expert         = 0
0.00.051.053 I llm_load_print_meta: n_expert_used    = 0
0.00.051.054 I llm_load_print_meta: causal attn      = 1
0.00.051.054 I llm_load_print_meta: pooling type     = 0
0.00.051.054 I llm_load_print_meta: rope type        = 2
0.00.051.054 I llm_load_print_meta: rope scaling     = linear
0.00.051.054 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.055 I llm_load_print_meta: freq_scale_train = 1
0.00.051.055 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.055 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.055 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.055 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.057 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.057 I llm_load_print_meta: model type       = 1.4B
0.00.051.057 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.058 I llm_load_print_meta: model params     = 1.41 B
0.00.051.058 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.058 I llm_load_print_meta: general.name     = 1.4B
0.00.051.058 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: LF token         = 128 ''
0.00.051.060 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: max token length = 1024
0.00.052.934 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.935 I llm_load_tensors: offloading output layer to GPU
0.00.052.935 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.945 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.947 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.849 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.850 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.850 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.850 I llama_new_context_with_model: n_batch       = 2048
0.00.053.850 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.851 I llama_new_context_with_model: flash_attn    = 0
0.00.053.851 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.851 I llama_new_context_with_model: freq_scale    = 1
0.00.053.852 I ggml_metal_init: allocating
0.00.053.855 I ggml_metal_init: found device: Apple M4
0.00.053.857 I ggml_metal_init: picking default device: Apple M4
0.00.054.437 I ggml_metal_init: using embedded metal library
0.00.056.773 I ggml_metal_init: GPU name:   Apple M4
0.00.056.775 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.775 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.776 I ggml_metal_init: simdgroup reduction   = true
0.00.056.776 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.776 I ggml_metal_init: has bfloat            = true
0.00.056.776 I ggml_metal_init: use bfloat            = true
0.00.056.777 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.777 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.821 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.827 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.845 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.888 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.889 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.890 I llama_new_context_with_model: graph nodes  = 967
0.00.087.890 I llama_new_context_with_model: graph splits = 2
0.00.087.915 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.057 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.057 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.613 I main: llama threadpool init, n_threads = 4
0.00.441.650 I 
0.00.441.680 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.682 I 
0.00.441.924 I sampler seed: 1234
0.00.441.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.979 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.983 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.983 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.108 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.120.109 I llama_perf_context_print:        load time =     432.04 ms
0.01.120.110 I llama_perf_context_print: prompt eval time =      35.94 ms /     7 tokens (    5.13 ms per token,   194.77 tokens per second)
0.01.120.111 I llama_perf_context_print:        eval time =     639.15 ms /    63 runs   (   10.15 ms per token,    98.57 tokens per second)
0.01.120.111 I llama_perf_context_print:       total time =     678.50 ms /    70 tokens
0.01.120.325 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.778 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.235 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.243 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.245 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.245 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.257 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.161 I llama_model_loader: - type  f32:  194 tensors
0.00.024.162 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.162 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.162 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.986 I llm_load_vocab: special tokens cache size = 25
0.00.051.072 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.075 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.075 I llm_load_print_meta: arch             = gptneox
0.00.051.076 I llm_load_print_meta: vocab type       = BPE
0.00.051.076 I llm_load_print_meta: n_vocab          = 50304
0.00.051.076 I llm_load_print_meta: n_merges         = 50009
0.00.051.076 I llm_load_print_meta: vocab_only       = 0
0.00.051.077 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.077 I llm_load_print_meta: n_embd           = 2048
0.00.051.077 I llm_load_print_meta: n_layer          = 24
0.00.051.091 I llm_load_print_meta: n_head           = 16
0.00.051.092 I llm_load_print_meta: n_head_kv        = 16
0.00.051.094 I llm_load_print_meta: n_rot            = 32
0.00.051.094 I llm_load_print_meta: n_swa            = 0
0.00.051.094 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.094 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.095 I llm_load_print_meta: n_gqa            = 1
0.00.051.096 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.096 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.097 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.097 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.097 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.098 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.098 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.099 I llm_load_print_meta: n_ff             = 8192
0.00.051.100 I llm_load_print_meta: n_expert         = 0
0.00.051.100 I llm_load_print_meta: n_expert_used    = 0
0.00.051.100 I llm_load_print_meta: causal attn      = 1
0.00.051.100 I llm_load_print_meta: pooling type     = 0
0.00.051.101 I llm_load_print_meta: rope type        = 2
0.00.051.101 I llm_load_print_meta: rope scaling     = linear
0.00.051.101 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.102 I llm_load_print_meta: freq_scale_train = 1
0.00.051.102 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.102 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.102 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.102 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.102 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.103 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.103 I llm_load_print_meta: model type       = 1.4B
0.00.051.103 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.103 I llm_load_print_meta: model params     = 1.41 B
0.00.051.104 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.104 I llm_load_print_meta: general.name     = 1.4B
0.00.051.104 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.104 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: LF token         = 128 ''
0.00.051.105 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: max token length = 1024
0.00.053.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.007 I llm_load_tensors: offloading output layer to GPU
0.00.053.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.018 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.019 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.891 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.892 I llama_new_context_with_model: n_ctx         = 128
0.00.053.892 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.892 I llama_new_context_with_model: n_batch       = 128
0.00.053.892 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.892 I llama_new_context_with_model: flash_attn    = 0
0.00.053.893 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.893 I llama_new_context_with_model: freq_scale    = 1
0.00.053.893 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.894 I ggml_metal_init: allocating
0.00.053.897 I ggml_metal_init: found device: Apple M4
0.00.053.899 I ggml_metal_init: picking default device: Apple M4
0.00.054.486 I ggml_metal_init: using embedded metal library
0.00.056.801 I ggml_metal_init: GPU name:   Apple M4
0.00.056.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.803 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.803 I ggml_metal_init: simdgroup reduction   = true
0.00.056.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.804 I ggml_metal_init: has bfloat            = true
0.00.056.804 I ggml_metal_init: use bfloat            = true
0.00.056.804 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.872 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.874 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.889 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.834 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.835 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.835 I llama_new_context_with_model: graph nodes  = 967
0.00.068.835 I llama_new_context_with_model: graph splits = 2
0.00.068.848 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.058 I 
0.00.387.092 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.105 I perplexity: tokenizing the input ..
0.00.394.937 I perplexity: tokenization took 7.83 ms
0.00.394.943 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.527.729 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.528.994 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.011 I llama_perf_context_print:        load time =     377.28 ms
0.00.529.013 I llama_perf_context_print: prompt eval time =     132.53 ms /   128 tokens (    1.04 ms per token,   965.84 tokens per second)
0.00.529.013 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.014 I llama_perf_context_print:       total time =     141.95 ms /   129 tokens
0.00.529.314 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.078s
sys	0m0.071s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.361 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.992 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.673 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.676 I llama_model_loader: - type  f32:  194 tensors
0.00.024.676 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.676 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.676 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.676 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.034 I llm_load_vocab: special tokens cache size = 25
0.00.050.990 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.992 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.993 I llm_load_print_meta: arch             = gptneox
0.00.050.993 I llm_load_print_meta: vocab type       = BPE
0.00.050.993 I llm_load_print_meta: n_vocab          = 50304
0.00.050.994 I llm_load_print_meta: n_merges         = 50009
0.00.050.994 I llm_load_print_meta: vocab_only       = 0
0.00.050.994 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.994 I llm_load_print_meta: n_embd           = 2048
0.00.050.994 I llm_load_print_meta: n_layer          = 24
0.00.051.009 I llm_load_print_meta: n_head           = 16
0.00.051.010 I llm_load_print_meta: n_head_kv        = 16
0.00.051.010 I llm_load_print_meta: n_rot            = 32
0.00.051.011 I llm_load_print_meta: n_swa            = 0
0.00.051.011 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.011 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.011 I llm_load_print_meta: n_gqa            = 1
0.00.051.012 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.013 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.019 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.019 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.019 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.019 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.019 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.020 I llm_load_print_meta: n_ff             = 8192
0.00.051.021 I llm_load_print_meta: n_expert         = 0
0.00.051.022 I llm_load_print_meta: n_expert_used    = 0
0.00.051.023 I llm_load_print_meta: causal attn      = 1
0.00.051.023 I llm_load_print_meta: pooling type     = 0
0.00.051.023 I llm_load_print_meta: rope type        = 2
0.00.051.023 I llm_load_print_meta: rope scaling     = linear
0.00.051.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.024 I llm_load_print_meta: freq_scale_train = 1
0.00.051.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.024 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.024 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.024 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.024 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.025 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.025 I llm_load_print_meta: model type       = 1.4B
0.00.051.026 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.026 I llm_load_print_meta: model params     = 1.41 B
0.00.051.027 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.027 I llm_load_print_meta: general.name     = 1.4B
0.00.051.028 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.028 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.028 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.028 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.028 I llm_load_print_meta: LF token         = 128 ''
0.00.051.029 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.029 I llm_load_print_meta: max token length = 1024
0.00.053.029 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.029 I llm_load_tensors: offloading output layer to GPU
0.00.053.029 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.040 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.041 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.979 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.979 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.979 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.980 I llama_new_context_with_model: n_batch       = 2048
0.00.053.980 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.980 I llama_new_context_with_model: flash_attn    = 0
0.00.053.980 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.980 I llama_new_context_with_model: freq_scale    = 1
0.00.053.981 I ggml_metal_init: allocating
0.00.053.984 I ggml_metal_init: found device: Apple M4
0.00.053.986 I ggml_metal_init: picking default device: Apple M4
0.00.054.596 I ggml_metal_init: using embedded metal library
0.00.056.904 I ggml_metal_init: GPU name:   Apple M4
0.00.056.906 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.907 I ggml_metal_init: simdgroup reduction   = true
0.00.056.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.908 I ggml_metal_init: has bfloat            = true
0.00.056.908 I ggml_metal_init: use bfloat            = true
0.00.056.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.866 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.871 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.889 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.877 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.879 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.879 I llama_new_context_with_model: graph nodes  = 967
0.00.086.879 I llama_new_context_with_model: graph splits = 2
0.00.086.904 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.293 I main: llama threadpool init, n_threads = 4
0.00.538.330 I 
0.00.538.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.381 I 
0.00.538.612 I sampler seed: 1234
0.00.538.616 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.538.650 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.538.651 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.538.651 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.284.291 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.284.291 I llama_perf_context_print:        load time =     528.93 ms
0.01.284.292 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.89 tokens per second)
0.01.284.292 I llama_perf_context_print:        eval time =     702.18 ms /    63 runs   (   11.15 ms per token,    89.72 tokens per second)
0.01.284.293 I llama_perf_context_print:       total time =     746.00 ms /    70 tokens
0.01.284.471 I ggml_metal_free: deallocating

real	0m1.300s
user	0m0.109s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.114 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.949 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.950 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.950 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.952 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.952 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.953 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.700 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.701 I llama_model_loader: - type  f32:  194 tensors
0.00.023.701 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.701 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.702 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.702 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.609 I llm_load_vocab: special tokens cache size = 25
0.00.050.541 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.544 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.545 I llm_load_print_meta: arch             = gptneox
0.00.050.545 I llm_load_print_meta: vocab type       = BPE
0.00.050.545 I llm_load_print_meta: n_vocab          = 50304
0.00.050.545 I llm_load_print_meta: n_merges         = 50009
0.00.050.546 I llm_load_print_meta: vocab_only       = 0
0.00.050.546 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.546 I llm_load_print_meta: n_embd           = 2048
0.00.050.546 I llm_load_print_meta: n_layer          = 24
0.00.050.555 I llm_load_print_meta: n_head           = 16
0.00.050.556 I llm_load_print_meta: n_head_kv        = 16
0.00.050.556 I llm_load_print_meta: n_rot            = 32
0.00.050.556 I llm_load_print_meta: n_swa            = 0
0.00.050.559 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.559 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.559 I llm_load_print_meta: n_gqa            = 1
0.00.050.560 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.561 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.562 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.562 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.563 I llm_load_print_meta: n_ff             = 8192
0.00.050.564 I llm_load_print_meta: n_expert         = 0
0.00.050.564 I llm_load_print_meta: n_expert_used    = 0
0.00.050.566 I llm_load_print_meta: causal attn      = 1
0.00.050.566 I llm_load_print_meta: pooling type     = 0
0.00.050.566 I llm_load_print_meta: rope type        = 2
0.00.050.566 I llm_load_print_meta: rope scaling     = linear
0.00.050.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.567 I llm_load_print_meta: freq_scale_train = 1
0.00.050.567 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.567 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.567 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.567 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.567 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.567 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.568 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.568 I llm_load_print_meta: model type       = 1.4B
0.00.050.569 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.569 I llm_load_print_meta: model params     = 1.41 B
0.00.050.569 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.570 I llm_load_print_meta: general.name     = 1.4B
0.00.050.573 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.574 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.574 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.574 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.574 I llm_load_print_meta: LF token         = 128 ''
0.00.050.575 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.575 I llm_load_print_meta: max token length = 1024
0.00.052.313 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.313 I llm_load_tensors: offloading output layer to GPU
0.00.052.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.318 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.319 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.178 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.179 I llama_new_context_with_model: n_ctx         = 128
0.00.053.179 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.179 I llama_new_context_with_model: n_batch       = 128
0.00.053.179 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.180 I llama_new_context_with_model: flash_attn    = 0
0.00.053.180 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.180 I llama_new_context_with_model: freq_scale    = 1
0.00.053.181 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.181 I ggml_metal_init: allocating
0.00.053.184 I ggml_metal_init: found device: Apple M4
0.00.053.186 I ggml_metal_init: picking default device: Apple M4
0.00.053.722 I ggml_metal_init: using embedded metal library
0.00.056.067 I ggml_metal_init: GPU name:   Apple M4
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.070 I ggml_metal_init: simdgroup reduction   = true
0.00.056.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.070 I ggml_metal_init: has bfloat            = true
0.00.056.070 I ggml_metal_init: use bfloat            = true
0.00.056.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.788 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.804 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.680 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.681 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.681 I llama_new_context_with_model: graph nodes  = 967
0.00.067.682 I llama_new_context_with_model: graph splits = 2
0.00.067.689 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.598 I 
0.00.487.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.651 I perplexity: tokenizing the input ..
0.00.495.846 I perplexity: tokenization took 8.193 ms
0.00.495.850 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.628.042 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.629.207 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.629.219 I llama_perf_context_print:        load time =     478.48 ms
0.00.629.220 I llama_perf_context_print: prompt eval time =     131.96 ms /   128 tokens (    1.03 ms per token,   970.02 tokens per second)
0.00.629.220 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.629.221 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.629.532 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.078s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.477 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.771 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.771 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.772 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.772 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.773 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.773 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.773 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.774 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.774 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.776 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.776 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.680 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.617 I llama_model_loader: - type  f32:  194 tensors
0.00.024.617 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.617 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.618 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.930 I llm_load_vocab: special tokens cache size = 25
0.00.050.870 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.873 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.873 I llm_load_print_meta: arch             = gptneox
0.00.050.873 I llm_load_print_meta: vocab type       = BPE
0.00.050.874 I llm_load_print_meta: n_vocab          = 50304
0.00.050.874 I llm_load_print_meta: n_merges         = 50009
0.00.050.874 I llm_load_print_meta: vocab_only       = 0
0.00.050.874 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.874 I llm_load_print_meta: n_embd           = 2048
0.00.050.874 I llm_load_print_meta: n_layer          = 24
0.00.050.888 I llm_load_print_meta: n_head           = 16
0.00.050.889 I llm_load_print_meta: n_head_kv        = 16
0.00.050.889 I llm_load_print_meta: n_rot            = 32
0.00.050.889 I llm_load_print_meta: n_swa            = 0
0.00.050.890 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.890 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.891 I llm_load_print_meta: n_gqa            = 1
0.00.050.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.897 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.897 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.897 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.898 I llm_load_print_meta: n_ff             = 8192
0.00.050.898 I llm_load_print_meta: n_expert         = 0
0.00.050.898 I llm_load_print_meta: n_expert_used    = 0
0.00.050.898 I llm_load_print_meta: causal attn      = 1
0.00.050.898 I llm_load_print_meta: pooling type     = 0
0.00.050.898 I llm_load_print_meta: rope type        = 2
0.00.050.899 I llm_load_print_meta: rope scaling     = linear
0.00.050.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.899 I llm_load_print_meta: freq_scale_train = 1
0.00.050.899 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.900 I llm_load_print_meta: model type       = 1.4B
0.00.050.901 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.901 I llm_load_print_meta: model params     = 1.41 B
0.00.050.902 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.902 I llm_load_print_meta: general.name     = 1.4B
0.00.050.902 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: LF token         = 128 ''
0.00.050.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: max token length = 1024
0.00.052.903 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.904 I llm_load_tensors: offloading output layer to GPU
0.00.052.904 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.914 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.915 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.806 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.806 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.807 I llama_new_context_with_model: n_batch       = 2048
0.00.053.807 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.807 I llama_new_context_with_model: flash_attn    = 0
0.00.053.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.808 I llama_new_context_with_model: freq_scale    = 1
0.00.053.808 I ggml_metal_init: allocating
0.00.053.816 I ggml_metal_init: found device: Apple M4
0.00.053.819 I ggml_metal_init: picking default device: Apple M4
0.00.054.430 I ggml_metal_init: using embedded metal library
0.00.056.757 I ggml_metal_init: GPU name:   Apple M4
0.00.056.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.760 I ggml_metal_init: simdgroup reduction   = true
0.00.056.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.760 I ggml_metal_init: has bfloat            = true
0.00.056.760 I ggml_metal_init: use bfloat            = true
0.00.056.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.036 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.041 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.060 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.141 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.143 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.143 I llama_new_context_with_model: graph nodes  = 967
0.00.088.143 I llama_new_context_with_model: graph splits = 2
0.00.088.168 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.592 I main: llama threadpool init, n_threads = 4
0.00.629.637 I 
0.00.629.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.675 I 
0.00.629.878 I sampler seed: 1234
0.00.629.882 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.906 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.906 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.906 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.417.899 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.417.900 I llama_perf_context_print:        load time =     620.11 ms
0.01.417.901 I llama_perf_context_print: prompt eval time =      46.95 ms /     7 tokens (    6.71 ms per token,   149.11 tokens per second)
0.01.417.901 I llama_perf_context_print:        eval time =     737.97 ms /    63 runs   (   11.71 ms per token,    85.37 tokens per second)
0.01.417.903 I llama_perf_context_print:       total time =     788.31 ms /    70 tokens
0.01.418.120 I ggml_metal_free: deallocating

real	0m1.438s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.451 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.138 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.138 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.142 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.147 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.005 I llama_model_loader: - type  f32:  194 tensors
0.00.025.005 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.005 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.005 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.055 I llm_load_vocab: special tokens cache size = 25
0.00.051.985 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.987 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.988 I llm_load_print_meta: arch             = gptneox
0.00.051.988 I llm_load_print_meta: vocab type       = BPE
0.00.051.988 I llm_load_print_meta: n_vocab          = 50304
0.00.051.988 I llm_load_print_meta: n_merges         = 50009
0.00.051.989 I llm_load_print_meta: vocab_only       = 0
0.00.051.989 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.989 I llm_load_print_meta: n_embd           = 2048
0.00.051.989 I llm_load_print_meta: n_layer          = 24
0.00.052.003 I llm_load_print_meta: n_head           = 16
0.00.052.003 I llm_load_print_meta: n_head_kv        = 16
0.00.052.004 I llm_load_print_meta: n_rot            = 32
0.00.052.004 I llm_load_print_meta: n_swa            = 0
0.00.052.004 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.006 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.007 I llm_load_print_meta: n_gqa            = 1
0.00.052.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.008 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.009 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.009 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.009 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.010 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.010 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.010 I llm_load_print_meta: n_ff             = 8192
0.00.052.010 I llm_load_print_meta: n_expert         = 0
0.00.052.010 I llm_load_print_meta: n_expert_used    = 0
0.00.052.011 I llm_load_print_meta: causal attn      = 1
0.00.052.011 I llm_load_print_meta: pooling type     = 0
0.00.052.011 I llm_load_print_meta: rope type        = 2
0.00.052.011 I llm_load_print_meta: rope scaling     = linear
0.00.052.012 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.012 I llm_load_print_meta: freq_scale_train = 1
0.00.052.012 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.012 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.013 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.013 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.014 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.014 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.014 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.014 I llm_load_print_meta: model type       = 1.4B
0.00.052.015 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.015 I llm_load_print_meta: model params     = 1.41 B
0.00.052.016 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.016 I llm_load_print_meta: general.name     = 1.4B
0.00.052.016 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.016 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.016 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.016 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.017 I llm_load_print_meta: LF token         = 128 ''
0.00.052.017 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.017 I llm_load_print_meta: max token length = 1024
0.00.054.047 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.047 I llm_load_tensors: offloading output layer to GPU
0.00.054.047 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.058 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.059 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.942 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.943 I llama_new_context_with_model: n_ctx         = 128
0.00.054.943 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.943 I llama_new_context_with_model: n_batch       = 128
0.00.054.943 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.943 I llama_new_context_with_model: flash_attn    = 0
0.00.054.944 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.944 I llama_new_context_with_model: freq_scale    = 1
0.00.054.944 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.945 I ggml_metal_init: allocating
0.00.054.948 I ggml_metal_init: found device: Apple M4
0.00.054.950 I ggml_metal_init: picking default device: Apple M4
0.00.055.512 I ggml_metal_init: using embedded metal library
0.00.057.827 I ggml_metal_init: GPU name:   Apple M4
0.00.057.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.829 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.829 I ggml_metal_init: simdgroup reduction   = true
0.00.057.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.829 I ggml_metal_init: has bfloat            = true
0.00.057.829 I ggml_metal_init: use bfloat            = true
0.00.057.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.941 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.943 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.880 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.881 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.882 I llama_new_context_with_model: graph nodes  = 967
0.00.069.882 I llama_new_context_with_model: graph splits = 2
0.00.069.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.305 I 
0.00.573.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.573.381 I perplexity: tokenizing the input ..
0.00.581.581 I perplexity: tokenization took 8.199 ms
0.00.581.588 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.972 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.158 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.172 I llama_perf_context_print:        load time =     562.85 ms
0.00.717.173 I llama_perf_context_print: prompt eval time =     134.16 ms /   128 tokens (    1.05 ms per token,   954.07 tokens per second)
0.00.717.174 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.175 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.717.532 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.079s
sys	0m0.108s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.054 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.645 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.658 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.659 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.413 I llama_model_loader: - type  f32:  194 tensors
0.00.025.414 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.414 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.642 I llm_load_vocab: special tokens cache size = 25
0.00.051.622 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.625 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.625 I llm_load_print_meta: arch             = gptneox
0.00.051.626 I llm_load_print_meta: vocab type       = BPE
0.00.051.626 I llm_load_print_meta: n_vocab          = 50304
0.00.051.626 I llm_load_print_meta: n_merges         = 50009
0.00.051.626 I llm_load_print_meta: vocab_only       = 0
0.00.051.627 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.627 I llm_load_print_meta: n_embd           = 2048
0.00.051.627 I llm_load_print_meta: n_layer          = 24
0.00.051.641 I llm_load_print_meta: n_head           = 16
0.00.051.642 I llm_load_print_meta: n_head_kv        = 16
0.00.051.642 I llm_load_print_meta: n_rot            = 32
0.00.051.643 I llm_load_print_meta: n_swa            = 0
0.00.051.643 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.643 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.644 I llm_load_print_meta: n_gqa            = 1
0.00.051.644 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.649 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.650 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.650 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.650 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.650 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.651 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.652 I llm_load_print_meta: n_ff             = 8192
0.00.051.652 I llm_load_print_meta: n_expert         = 0
0.00.051.652 I llm_load_print_meta: n_expert_used    = 0
0.00.051.653 I llm_load_print_meta: causal attn      = 1
0.00.051.655 I llm_load_print_meta: pooling type     = 0
0.00.051.655 I llm_load_print_meta: rope type        = 2
0.00.051.655 I llm_load_print_meta: rope scaling     = linear
0.00.051.655 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.656 I llm_load_print_meta: freq_scale_train = 1
0.00.051.656 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.656 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.656 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.656 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.656 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.656 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.656 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.657 I llm_load_print_meta: model type       = 1.4B
0.00.051.657 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.657 I llm_load_print_meta: model params     = 1.41 B
0.00.051.660 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.660 I llm_load_print_meta: general.name     = 1.4B
0.00.051.661 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.661 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.661 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: LF token         = 128 ''
0.00.051.663 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.663 I llm_load_print_meta: max token length = 1024
0.00.053.310 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.310 I llm_load_tensors: offloading output layer to GPU
0.00.053.310 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.321 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.322 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.144 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.145 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.145 I llama_new_context_with_model: n_batch       = 2048
0.00.054.145 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.145 I llama_new_context_with_model: flash_attn    = 0
0.00.054.145 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.146 I llama_new_context_with_model: freq_scale    = 1
0.00.054.146 I ggml_metal_init: allocating
0.00.054.153 I ggml_metal_init: found device: Apple M4
0.00.054.155 I ggml_metal_init: picking default device: Apple M4
0.00.054.741 I ggml_metal_init: using embedded metal library
0.00.057.071 I ggml_metal_init: GPU name:   Apple M4
0.00.057.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.075 I ggml_metal_init: simdgroup reduction   = true
0.00.057.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.075 I ggml_metal_init: has bfloat            = true
0.00.057.075 I ggml_metal_init: use bfloat            = true
0.00.057.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.670 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.677 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.698 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.660 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.661 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.661 I llama_new_context_with_model: graph nodes  = 967
0.00.087.662 I llama_new_context_with_model: graph splits = 2
0.00.087.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.319 I main: llama threadpool init, n_threads = 4
0.00.709.363 I 
0.00.709.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.393 I 
0.00.709.564 I sampler seed: 1234
0.00.709.570 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.709.601 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.709.602 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.709.603 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.588.110 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.588.111 I llama_perf_context_print:        load time =     700.26 ms
0.01.588.111 I llama_perf_context_print: prompt eval time =      51.67 ms /     7 tokens (    7.38 ms per token,   135.46 tokens per second)
0.01.588.112 I llama_perf_context_print:        eval time =     823.73 ms /    63 runs   (   13.08 ms per token,    76.48 tokens per second)
0.01.588.112 I llama_perf_context_print:       total time =     878.79 ms /    70 tokens
0.01.588.310 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.554 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.320 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.321 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.321 I llama_model_loader: - type  f32:  194 tensors
0.00.023.322 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.322 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.378 I llm_load_vocab: special tokens cache size = 25
0.00.049.381 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.385 I llm_load_print_meta: arch             = gptneox
0.00.049.385 I llm_load_print_meta: vocab type       = BPE
0.00.049.385 I llm_load_print_meta: n_vocab          = 50304
0.00.049.385 I llm_load_print_meta: n_merges         = 50009
0.00.049.386 I llm_load_print_meta: vocab_only       = 0
0.00.049.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.386 I llm_load_print_meta: n_embd           = 2048
0.00.049.386 I llm_load_print_meta: n_layer          = 24
0.00.049.400 I llm_load_print_meta: n_head           = 16
0.00.049.401 I llm_load_print_meta: n_head_kv        = 16
0.00.049.401 I llm_load_print_meta: n_rot            = 32
0.00.049.402 I llm_load_print_meta: n_swa            = 0
0.00.049.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.403 I llm_load_print_meta: n_gqa            = 1
0.00.049.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.406 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.406 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.407 I llm_load_print_meta: n_ff             = 8192
0.00.049.407 I llm_load_print_meta: n_expert         = 0
0.00.049.408 I llm_load_print_meta: n_expert_used    = 0
0.00.049.408 I llm_load_print_meta: causal attn      = 1
0.00.049.408 I llm_load_print_meta: pooling type     = 0
0.00.049.408 I llm_load_print_meta: rope type        = 2
0.00.049.408 I llm_load_print_meta: rope scaling     = linear
0.00.049.408 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.409 I llm_load_print_meta: freq_scale_train = 1
0.00.049.409 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.409 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.409 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.409 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.409 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.410 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.410 I llm_load_print_meta: model type       = 1.4B
0.00.049.410 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.411 I llm_load_print_meta: model params     = 1.41 B
0.00.049.411 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.413 I llm_load_print_meta: general.name     = 1.4B
0.00.049.413 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.413 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.413 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.414 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.414 I llm_load_print_meta: LF token         = 128 ''
0.00.049.414 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.415 I llm_load_print_meta: max token length = 1024
0.00.051.395 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.395 I llm_load_tensors: offloading output layer to GPU
0.00.051.396 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.406 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.407 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.300 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.300 I llama_new_context_with_model: n_ctx         = 128
0.00.052.301 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.301 I llama_new_context_with_model: n_batch       = 128
0.00.052.301 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.301 I llama_new_context_with_model: flash_attn    = 0
0.00.052.301 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.302 I llama_new_context_with_model: freq_scale    = 1
0.00.052.302 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.303 I ggml_metal_init: allocating
0.00.052.309 I ggml_metal_init: found device: Apple M4
0.00.052.312 I ggml_metal_init: picking default device: Apple M4
0.00.052.876 I ggml_metal_init: using embedded metal library
0.00.055.207 I ggml_metal_init: GPU name:   Apple M4
0.00.055.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.209 I ggml_metal_init: simdgroup reduction   = true
0.00.055.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.209 I ggml_metal_init: has bfloat            = true
0.00.055.210 I ggml_metal_init: use bfloat            = true
0.00.055.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.006 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.010 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.026 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.932 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.933 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.933 I llama_new_context_with_model: graph nodes  = 967
0.00.066.933 I llama_new_context_with_model: graph splits = 2
0.00.066.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.859 I 
0.00.646.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.950 I perplexity: tokenizing the input ..
0.00.654.860 I perplexity: tokenization took 7.909 ms
0.00.654.868 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.886 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.063 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.075 I llama_perf_context_print:        load time =     638.04 ms
0.00.797.076 I llama_perf_context_print: prompt eval time =     140.78 ms /   128 tokens (    1.10 ms per token,   909.23 tokens per second)
0.00.797.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.079 I llama_perf_context_print:       total time =     150.22 ms /   129 tokens
0.00.797.441 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.115s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.938 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.905 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.907 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.909 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.909 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.909 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.910 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.913 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.815 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.699 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.700 I llama_model_loader: - type  f32:  194 tensors
0.00.025.700 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.894 I llm_load_vocab: special tokens cache size = 25
0.00.051.813 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.816 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.816 I llm_load_print_meta: arch             = gptneox
0.00.051.817 I llm_load_print_meta: vocab type       = BPE
0.00.051.817 I llm_load_print_meta: n_vocab          = 50304
0.00.051.817 I llm_load_print_meta: n_merges         = 50009
0.00.051.817 I llm_load_print_meta: vocab_only       = 0
0.00.051.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.818 I llm_load_print_meta: n_embd           = 2048
0.00.051.818 I llm_load_print_meta: n_layer          = 24
0.00.051.832 I llm_load_print_meta: n_head           = 16
0.00.051.833 I llm_load_print_meta: n_head_kv        = 16
0.00.051.833 I llm_load_print_meta: n_rot            = 32
0.00.051.833 I llm_load_print_meta: n_swa            = 0
0.00.051.834 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.834 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.835 I llm_load_print_meta: n_gqa            = 1
0.00.051.836 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.836 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.837 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.837 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.837 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.838 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.838 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.840 I llm_load_print_meta: n_ff             = 8192
0.00.051.840 I llm_load_print_meta: n_expert         = 0
0.00.051.840 I llm_load_print_meta: n_expert_used    = 0
0.00.051.840 I llm_load_print_meta: causal attn      = 1
0.00.051.841 I llm_load_print_meta: pooling type     = 0
0.00.051.842 I llm_load_print_meta: rope type        = 2
0.00.051.843 I llm_load_print_meta: rope scaling     = linear
0.00.051.843 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.843 I llm_load_print_meta: freq_scale_train = 1
0.00.051.843 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.844 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.844 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.844 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.844 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.844 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.844 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.845 I llm_load_print_meta: model type       = 1.4B
0.00.051.845 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.845 I llm_load_print_meta: model params     = 1.41 B
0.00.051.845 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.846 I llm_load_print_meta: general.name     = 1.4B
0.00.051.846 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.846 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.846 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.848 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.848 I llm_load_print_meta: LF token         = 128 ''
0.00.051.848 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.848 I llm_load_print_meta: max token length = 1024
0.00.053.500 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.500 I llm_load_tensors: offloading output layer to GPU
0.00.053.500 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.510 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.511 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.384 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.385 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.385 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.385 I llama_new_context_with_model: n_batch       = 2048
0.00.054.385 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.386 I llama_new_context_with_model: flash_attn    = 0
0.00.054.386 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.386 I llama_new_context_with_model: freq_scale    = 1
0.00.054.387 I ggml_metal_init: allocating
0.00.054.394 I ggml_metal_init: found device: Apple M4
0.00.054.396 I ggml_metal_init: picking default device: Apple M4
0.00.054.998 I ggml_metal_init: using embedded metal library
0.00.057.347 I ggml_metal_init: GPU name:   Apple M4
0.00.057.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.350 I ggml_metal_init: simdgroup reduction   = true
0.00.057.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.351 I ggml_metal_init: has bfloat            = true
0.00.057.351 I ggml_metal_init: use bfloat            = true
0.00.057.352 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.352 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.550 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.557 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.577 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.644 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.646 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.646 I llama_new_context_with_model: graph nodes  = 967
0.00.089.647 I llama_new_context_with_model: graph splits = 2
0.00.089.671 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.811 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.109 I main: llama threadpool init, n_threads = 4
0.00.766.148 I 
0.00.766.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.193 I 
0.00.766.365 I sampler seed: 1234
0.00.766.370 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.416 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.418 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.419 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.689.161 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.689.162 I llama_perf_context_print:        load time =     756.17 ms
0.01.689.163 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.689.163 I llama_perf_context_print:        eval time =     865.25 ms /    63 runs   (   13.73 ms per token,    72.81 tokens per second)
0.01.689.164 I llama_perf_context_print:       total time =     923.06 ms /    70 tokens
0.01.689.357 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4338 (d58f8a1b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.799 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.163 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.043 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.044 I llama_model_loader: - type  f32:  194 tensors
0.00.024.045 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.097 I llm_load_vocab: special tokens cache size = 25
0.00.049.895 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.898 I llm_load_print_meta: arch             = gptneox
0.00.049.898 I llm_load_print_meta: vocab type       = BPE
0.00.049.898 I llm_load_print_meta: n_vocab          = 50304
0.00.049.898 I llm_load_print_meta: n_merges         = 50009
0.00.049.899 I llm_load_print_meta: vocab_only       = 0
0.00.049.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.899 I llm_load_print_meta: n_embd           = 2048
0.00.049.899 I llm_load_print_meta: n_layer          = 24
0.00.049.913 I llm_load_print_meta: n_head           = 16
0.00.049.914 I llm_load_print_meta: n_head_kv        = 16
0.00.049.914 I llm_load_print_meta: n_rot            = 32
0.00.049.914 I llm_load_print_meta: n_swa            = 0
0.00.049.914 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.914 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.915 I llm_load_print_meta: n_gqa            = 1
0.00.049.916 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.916 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.917 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.917 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.918 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.918 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.918 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.920 I llm_load_print_meta: n_ff             = 8192
0.00.049.920 I llm_load_print_meta: n_expert         = 0
0.00.049.920 I llm_load_print_meta: n_expert_used    = 0
0.00.049.921 I llm_load_print_meta: causal attn      = 1
0.00.049.921 I llm_load_print_meta: pooling type     = 0
0.00.049.921 I llm_load_print_meta: rope type        = 2
0.00.049.921 I llm_load_print_meta: rope scaling     = linear
0.00.049.922 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.922 I llm_load_print_meta: freq_scale_train = 1
0.00.049.922 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.922 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.923 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.923 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.923 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.923 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.923 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.923 I llm_load_print_meta: model type       = 1.4B
0.00.049.923 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.924 I llm_load_print_meta: model params     = 1.41 B
0.00.049.924 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.924 I llm_load_print_meta: general.name     = 1.4B
0.00.049.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: LF token         = 128 ''
0.00.049.926 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.926 I llm_load_print_meta: max token length = 1024
0.00.051.958 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.958 I llm_load_tensors: offloading output layer to GPU
0.00.051.958 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.968 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.970 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.836 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.837 I llama_new_context_with_model: n_ctx         = 128
0.00.052.837 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.837 I llama_new_context_with_model: n_batch       = 128
0.00.052.837 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.837 I llama_new_context_with_model: flash_attn    = 0
0.00.052.838 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.838 I llama_new_context_with_model: freq_scale    = 1
0.00.052.838 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.839 I ggml_metal_init: allocating
0.00.052.842 I ggml_metal_init: found device: Apple M4
0.00.052.844 I ggml_metal_init: picking default device: Apple M4
0.00.053.386 I ggml_metal_init: using embedded metal library
0.00.055.675 I ggml_metal_init: GPU name:   Apple M4
0.00.055.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.677 I ggml_metal_init: simdgroup reduction   = true
0.00.055.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.677 I ggml_metal_init: has bfloat            = true
0.00.055.677 I ggml_metal_init: use bfloat            = true
0.00.055.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.421 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.434 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.359 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.360 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.360 I llama_new_context_with_model: graph nodes  = 967
0.00.067.360 I llama_new_context_with_model: graph splits = 2
0.00.067.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.117.709 I 
0.00.117.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.117.766 I perplexity: tokenizing the input ..
0.00.125.482 I perplexity: tokenization took 7.712 ms
0.00.125.486 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.263.912 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.265.079 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.265.094 I llama_perf_context_print:        load time =     107.91 ms
0.00.265.094 I llama_perf_context_print: prompt eval time =     138.19 ms /   128 tokens (    1.08 ms per token,   926.26 tokens per second)
0.00.265.095 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.265.096 I llama_perf_context_print:       total time =     147.39 ms /   129 tokens
0.00.265.636 I ggml_metal_free: deallocating

real	0m0.282s
user	0m0.078s
sys	0m0.038s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4338 (d58f8a1b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123607350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123607a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123608010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1236085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123608b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123609120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1236096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123609c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12360a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12360a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12360ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12360b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12360bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12360c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12360cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12360d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12360da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12360e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12360e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12360f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12360f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12360fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1236105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123610e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123612ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1236132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123613760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1236142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1236147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123614ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123614f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1236153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123615890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123615d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1236161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123616670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123616b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123616fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123617450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123617710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123617d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123618330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123619260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123619870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12361a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12361aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12361b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12361b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12361bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12361c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12361c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12361cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12361d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12361d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12361da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12361dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12361e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12361e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12361ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12361f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12361f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12361fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12361ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1236203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123620840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123620ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123621230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123621780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123621cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123622220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123622770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123622cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123623210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123623760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123623cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123624200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123624750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123624ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1236251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123625740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123625c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123626730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123626c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1236271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123627720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1236281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123628710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123628c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1236290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123629dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12362a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12362a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12362adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12362b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12362b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12362bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12362c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12362c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12362cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12362d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12362d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12362dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12362e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12362e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12362eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12362f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12362f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12362f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12362fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123630290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123630730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123631510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1236319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1236322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123632c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1236330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123633570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123633a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123633eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123634350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1236347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123635130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1236355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123635a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123635f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1236363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123637630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123637f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123638410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1236388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123638d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1236391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123639690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123639b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123639fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12363a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12363a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12363adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12363b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12363b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12363bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12363c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12363c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12363c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12363ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12363d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12363d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12363dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12363e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12363e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12363e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12363ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12363f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12363f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12363fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1236400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123640590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123640a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123640ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123641370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123641810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123641cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123642150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1236425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123642a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123642f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1236433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123643870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123643d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1236441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123644650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123644f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1236454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123645a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123645f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1236464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123646790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1236473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1236479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1236481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123648650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123648910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123648f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123649530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123649d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12364a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12364a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12364ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12364b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12364b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12364bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12364c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12364c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12364cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12364d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12364d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12364dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12364e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12364e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12364ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12364f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12364f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12364fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123650260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1236507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123650d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123651250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1236517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123651cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123652240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123652790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123652ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123653230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123653780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123653cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123654220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123654770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123654cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123655210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123655760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123655cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123656200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123656750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123656ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1236571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123657740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123657c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1236581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123658730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123658c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1236591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123659720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123659c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12365a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12365a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12365ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12365b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12365b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12365bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12365c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12365c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12365cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12365d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12365d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12365dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12365e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12365e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12365ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12365eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12365f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12365f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12365fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123660130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1236605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123660f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1236613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123661850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123661cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123662190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1236626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123662e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123663520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123663c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123664360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123664620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123664e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1236650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1236656e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.191 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1236298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123629d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12362a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12362a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12362aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12362af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12362b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12362b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12362bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12362c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12362c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12362cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12362d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12362db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12362e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12362ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12362f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12362f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12362ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1236308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123630fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123631690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123631d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123632470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123632b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123632fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123633440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1236338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123633d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123634190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123634600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123634a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1236351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123635610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123635a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123635ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123636360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1236367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123636c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1236370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123637520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123637e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123638270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1236386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123638fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123639430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1236398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123639d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12363a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12363a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12363aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12363aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12363b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12363b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12363bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12363c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12363c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12363c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12363cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12363d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12363d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12363db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12363dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12363e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12363e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12363ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12363f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12363f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12363fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12363feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123640320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123640790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123640c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1236414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123642230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1236426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1236433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123643860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123643cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123644140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123644a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123644e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123645300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123645770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123645be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123646050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1236464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123647680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123647af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1236483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123648840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123649120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123649590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123649a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123649e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12364a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12364a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12364abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12364b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12364b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12364b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12364bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12364c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12364c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12364cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12364cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12364d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12364d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12364dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12364e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12364e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12364e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12364ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12364f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12364f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12364fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1236508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123650d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1236511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123651640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1236530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123653550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1236539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1236542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123654710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1236558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1236561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123656620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123656a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123656f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1236577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1236580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123658530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1236589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123658e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123659280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1236596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123659fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12365a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12365a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12365ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12365b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12365b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12365ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12365bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12365c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12365c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12365cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12365d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12365d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12365d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12365ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12365e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12365e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12365eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12365efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12365f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12365f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12365fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1236605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123660a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123660ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123661330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1236617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123661c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123662080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1236624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123662960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123662dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123663240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1236636b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123663b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123663f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123664400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123664870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123664ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123665150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1236655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123608400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123607e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123607350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1236220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1236236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1236248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1236255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1236267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1236286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1236147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123614c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1236150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123615530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1236159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123615e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123616280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1236166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123616b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123616fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123617440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1236178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123617d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123618190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123618600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123618a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123618ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123619350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1236197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123619c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12361a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12361a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12361a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12361adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12361b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12361b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12361bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12361bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12361c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12361c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12361cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12361d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12361d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12361da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12361dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12361e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12361e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12361ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12361f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12361f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12361f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12361fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123620240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1236206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123620b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123620f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123621680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123613420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123613b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123614200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12360a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12360ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12360b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12360b5f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a07db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a08220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a08690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a08b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a08f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a09850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a09cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a0a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a0ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a0d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a0d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a0f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a0fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a10d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a12bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a15f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134a19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134a19580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134a19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134a19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134a1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134a1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134a1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134a1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134a1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134a1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134a1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134a1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134a1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134a1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134a1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134a1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134a1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134a1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134a1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134a1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134a1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134a1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134a1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134a1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134a1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134a20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134a20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134a20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134a20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134a21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134a216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134a21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134a21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134a22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134a22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134a22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134a23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134a235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134a23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134a23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134a24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134a24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134a24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134a25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134a254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134a25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134a25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134a26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134a266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134a26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134a26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134a273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134a27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134a27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134a28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134a285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134a28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134a28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134a29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134a29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134a29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134a2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134a2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134a2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134a2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134a2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134a2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134a2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134a2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134a2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134a2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134a2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134a2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134a2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134a2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134a2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134a2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134a2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134a2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134a2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134a2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134a2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134a2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134a301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134a30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134a30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134a30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134a313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134a31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134a31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134a32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134a32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134a329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134a32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134a332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134a33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134a33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134a34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134a34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134a348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134a34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134a351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134a35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134a35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134a35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134a36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134a36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134a36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134a370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134a37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134a379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134a37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134a382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134a38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134a38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134a38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134a39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134a398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134a39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134a3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134a3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134a3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134a3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134a3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134a3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134a3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134a3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134a3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134a3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134a3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134a3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134a3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134a3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134a3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134a3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134a3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134a3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134a3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134a3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134a3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134a3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134a40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134a407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134a40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134a410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134a41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134a41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134a41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134a42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134a426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134a42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134a42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134a43540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134a439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134a43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134a44970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134a44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134a44ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134a45360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134a457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134a45c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134a460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134a46520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134a46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134a46e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134a47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134a476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134a47b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134a47fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134a48430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134a488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134a48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134a49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134a495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134a49a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134a49ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134a4a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134a4a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134a4ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134a4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134a4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134a4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134a4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134a4c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134a4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134a4cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134a4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134a4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134a4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134a4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134a4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134a4e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134a4ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134a4f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134a4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134a4fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134a4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134a503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134a50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134a50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134a51110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134a51580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134a519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134a51e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134a522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134a52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134a52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134a53020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134a53490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134a53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134a53d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134a541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134a54650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134a54ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134a54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134a553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134a55810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134a55c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134a560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134a56560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134a569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134a56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134a572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134a57720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134a57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134a58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134a58470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134a588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134a59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134a59a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134a5a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134a5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134a5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134a5afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134a5b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134a5bbf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.815s
user	0m0.301s
sys	0m0.284s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4338 (d58f8a1b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ef0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ef0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ef0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ef0de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ef0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ef0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ef0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ef0f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ef0faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ef0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ef104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ef109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ef11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ef11cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ef124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ef12bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ef13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ef13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ef14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ef14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ef15040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ef15760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ef15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ef16720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ef16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ef17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ef17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ef18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ef188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ef18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ef19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ef192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ef19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ef1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ef1a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ef1a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ef1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ef1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ef1b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ef1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ef1bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ef1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ef1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ef1cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ef1cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ef1d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ef1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ef1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ef1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ef1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ef1f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ef1fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ef20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ef20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ef21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ef21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ef21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ef21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ef22370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ef22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ef22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ef232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ef23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ef23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ef240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ef24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ef249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ef24e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ef25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ef257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ef25c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ef26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ef265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ef26af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ef27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ef27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ef27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ef28030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ef28580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ef28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ef29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ef29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ef29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ef2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ef2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ef2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ef2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ef2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ef2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ef2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ef2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ef2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ef2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ef2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ef2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ef2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ef2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ef1e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ef2e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ef2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ef2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ef2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ef30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ef30680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ef30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ef31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ef31670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ef31bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ef32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ef32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ef32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ef33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ef33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ef33af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ef33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ef34430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ef348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ef34d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ef35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ef356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ef35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ef35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ef36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ef36930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ef36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ef37270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ef37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ef37bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ef38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ef384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ef38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ef38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ef392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ef39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ef39c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ef3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ef3a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ef3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ef3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ef3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ef3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ef3bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ef3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ef3c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ef3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ef3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ef3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ef3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ef3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ef3e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ef3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ef3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ef3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ef3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ef3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ef3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ef401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ef40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ef40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ef40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ef41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ef418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ef41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ef42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ef426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ef42b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ef43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ef434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ef43950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ef43df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ef44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ef44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ef44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ef45070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ef45510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ef459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ef45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ef462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ef46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ef46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ef470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ef47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ef47a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ef47eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ef48350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ef487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ef48c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ef49130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ef495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ef49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ef49f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ef4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ef4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ef4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ef4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ef4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ef4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ef4c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ef4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ef4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ef4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ef4da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ef4df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ef4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ef4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ef4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ef4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ef4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ef4ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ef503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ef50b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ef510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ef51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ef51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ef520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ef52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ef52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ef530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ef535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ef53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ef54090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ef545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ef54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ef55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ef555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ef55b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ef56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ef565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ef56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ef57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ef575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ef57b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ef58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ef585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ef58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ef59040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ef59590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ef59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ef5a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ef5a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ef5aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ef5b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ef5b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ef5bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ef5c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ef5c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ef5cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ef5d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ef5d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ef5daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ef5dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ef5e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ef5ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ef5efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ef5f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ef5fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ef5ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ef60520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ef60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ef60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ef61510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ef61a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ef61fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ef62500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ef62a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ef62fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ef634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ef63990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ef63e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ef642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ef64770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ef64c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ef650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ef65550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ef659f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ef65e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ef66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ef667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ef66c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ef67110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ef675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ef67a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ef67fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ef686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ef68de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ef69500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ef69c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ef69ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ef6a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ef6a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ef6afa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.553 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ee08fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ee09450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ee098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ee09d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ee0a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ee0a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ee0aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ee0aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ee0b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ee0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ee0bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ee0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ee0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ee0d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ee0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ee0e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ee0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ee0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ee0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ee101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ee108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ee11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ee11720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ee11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ee12560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ee12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ee12ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ee12f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ee133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ee13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ee13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ee14240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ee146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ee14970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ee14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ee15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ee157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ee15cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ee161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ee166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ee16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ee170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ee175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ee17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ee17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ee18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ee18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ee18d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ee19170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ee195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ee19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ee19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ee1a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ee1a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ee1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ee1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ee1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ee1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ee1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ee1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ee1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ee1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ee1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ee1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ee1e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ee1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ee1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ee1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ee1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ee1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ee1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ee200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ee20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ee20ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ee21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ee21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ee21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ee21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ee22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ee22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ee22fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ee23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ee23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ee23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ee24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ee24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ee24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ee25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ee25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ee25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ee26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ee26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ee26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ee274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ee27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ee27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ee284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ee28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ee28f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ee294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ee29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ee29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ee2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ee2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ee2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ee2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ee2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ee2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ee2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ee2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ee2cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ee2d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ee2d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ee2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ee2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ee2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ee2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ee2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ee2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ee2fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ee2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ee30380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ee30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ee30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ee31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ee31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ee31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ee31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ee323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ee32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ee32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ee331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ee33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ee33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ee33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ee34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ee348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ee34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ee35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ee356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ee35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ee36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ee364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ee36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ee36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ee37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ee37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ee37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ee38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ee38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ee389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ee38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ee392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ee39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ee39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ee3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ee3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ee3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ee3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ee3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ee3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ee3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ee3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ee3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ee3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ee3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ee3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ee3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ee3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ee3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ee3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ee3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ee3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ee3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ee3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ee3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ee401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ee40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ee40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ee40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ee41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ee41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ee41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ee42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ee426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ee42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ee43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ee434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ee43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ee43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ee442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ee44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ee44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ee45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ee45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ee45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ee46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ee463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ee469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ee47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ee47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ee47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ee482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ee48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ee48b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ee49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ee49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ee49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ee4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ee4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ee4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ee4b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ee4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ee4bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ee4c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ee4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ee4cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ee4d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ee4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ee4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ee4e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ee4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ee4eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ee4f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ee4f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ee4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ee50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ee50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ee50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ee513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ee51940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ee51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ee523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ee52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ee52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ee533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ee53920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ee53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ee543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ee54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ee54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ee553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ee55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ee55e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ee563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ee568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ee56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ee57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ee578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ee57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ee58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ee588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ee58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ee59370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ee598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ee59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ee5a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ee5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ee5ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ee5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ee5b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ee5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ee5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ee5c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ee5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ee5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ee5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ee5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ee5e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ee5e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ee5eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ee5efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ee5f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ee5f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ee5fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ee60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ee606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ee60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ee61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ee614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ee61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ee61de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ee62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ee62a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ee63170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ee63890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ee63fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ee64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ee64a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ee64d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ee65330 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ef046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ef04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ef04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ef05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ef058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ef05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ef06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ef065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ef06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ef06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ef07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ef079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ef08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ef08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ef094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ef09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ef0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ef0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ef0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ef0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ef0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ef0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ef0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ef0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ef0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ef0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ef0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ef0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ef0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ef0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ef0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ef0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ef0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ef10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ef104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ef10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ef10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ef11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ef11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ef11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ef11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ef123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ef12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ef12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ef13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ef13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ef13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ef13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ef142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ef14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ef14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ef15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ef154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ef15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ef15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ef161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ef16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ef16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ef170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ef17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ef179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ef17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ef18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ef18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ef18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ef18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ef19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ef198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ef19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ef1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ef1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ef1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ef1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ef1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ef1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ef1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ef1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ef1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ef1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ef1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ef1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ef1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ef1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ef1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ef1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ef1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ef1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ef1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ef1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ef1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ef1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ef20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ef207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ef20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ef21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ef21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ef21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ef21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ef22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ef226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ef22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ef22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ef23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ef23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ef23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ef24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ef245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ef24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ef24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ef25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ef25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ef25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ef26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ef264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ef26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ef26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ef27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ef276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ef27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ef27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ef283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ef28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ef28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ef29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ef295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ef29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ef29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ef2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ef2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ef2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ef2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ef2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ef2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ef2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ef2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ef2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ef2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ef2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ef2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ef2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ef2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ef2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ef2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ef2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ef2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ef2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ef2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ef2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ef30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ef304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ef30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ef30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ef311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ef31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ef31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ef31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ef323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ef32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ef32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ef33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ef33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ef339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ef33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ef342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ef34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ef34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ef35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ef35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ef358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ef35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ef361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ef36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ef36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ef36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ef37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ef37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ef37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ef380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ef38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ef389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ef38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ef392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ef39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ef39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ef39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ef3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ef3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ef3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ef3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ef3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ef3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ef3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ef3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ef3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ef3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ef3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ef3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ef3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ef3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ef3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ef3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ef3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ef3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ef3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ef3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ef3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ef40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ef40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ef40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ef41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ef41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ef41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ef420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ef42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ef429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ef42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ef43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ef43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ef43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ef43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ef44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ef448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ef44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ef451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ef45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ef45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ef45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ef46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ef467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ef46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ef470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ef47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ef47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ef47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ef48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ef486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ef48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ef48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ef49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ef498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ef49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ef4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ef4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ef4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ef4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ef4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ef4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ef4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ef4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ef4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ef4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ef4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ef4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ef4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ef4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ef4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ef4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ef4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ef4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ef4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ef4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ef4fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ef50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ef50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ef50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ef50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ef513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ef51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ef51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ef52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ef52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ef529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ef52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ef532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ef53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ef53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ef54020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ef54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ef54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ef54d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ef551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ef55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ef55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ef56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ef56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ef57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ef57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ef57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ef581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ef587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ef58dd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.941s
user	0m0.243s
sys	0m0.146s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.54 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.25 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
