### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.08 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.52 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.89 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.98 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.09 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.66 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.60 sec*proc (29 tests)

Total Test time (real) = 164.61 sec

real	2m44.678s
user	4m38.149s
sys	0m5.740s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.74 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.49 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.16 sec*proc (29 tests)

Total Test time (real) =  48.17 sec

real	0m48.182s
user	0m54.528s
sys	0m5.139s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.122 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.123 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.540 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.549 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.550 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.551 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.552 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.553 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.554 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.554 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.555 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.555 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.558 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.559 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.559 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.560 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.560 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.561 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.561 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.082 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.084 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.084 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.085 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.085 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.085 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.086 I llama_model_loader: - type  f32:  124 tensors
0.00.027.086 I llama_model_loader: - type  f16:   73 tensors
0.00.027.087 I print_info: file format = GGUF V3 (latest)
0.00.027.088 I print_info: file type   = F16
0.00.027.089 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.082 I load: special tokens cache size = 5
0.00.033.264 I load: token to piece cache size = 0.2032 MB
0.00.033.268 I print_info: arch             = bert
0.00.033.268 I print_info: vocab_only       = 0
0.00.033.269 I print_info: n_ctx_train      = 512
0.00.033.269 I print_info: n_embd           = 384
0.00.033.269 I print_info: n_layer          = 12
0.00.033.272 I print_info: n_head           = 12
0.00.033.273 I print_info: n_head_kv        = 12
0.00.033.274 I print_info: n_rot            = 32
0.00.033.274 I print_info: n_swa            = 0
0.00.033.274 I print_info: n_embd_head_k    = 32
0.00.033.274 I print_info: n_embd_head_v    = 32
0.00.033.275 I print_info: n_gqa            = 1
0.00.033.279 I print_info: n_embd_k_gqa     = 384
0.00.033.280 I print_info: n_embd_v_gqa     = 384
0.00.033.281 I print_info: f_norm_eps       = 1.0e-12
0.00.033.281 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.281 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.282 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.282 I print_info: f_logit_scale    = 0.0e+00
0.00.033.283 I print_info: n_ff             = 1536
0.00.033.283 I print_info: n_expert         = 0
0.00.033.283 I print_info: n_expert_used    = 0
0.00.033.283 I print_info: causal attn      = 0
0.00.033.284 I print_info: pooling type     = 2
0.00.033.284 I print_info: rope type        = 2
0.00.033.284 I print_info: rope scaling     = linear
0.00.033.285 I print_info: freq_base_train  = 10000.0
0.00.033.285 I print_info: freq_scale_train = 1
0.00.033.286 I print_info: n_ctx_orig_yarn  = 512
0.00.033.286 I print_info: rope_finetuned   = unknown
0.00.033.286 I print_info: ssm_d_conv       = 0
0.00.033.286 I print_info: ssm_d_inner      = 0
0.00.033.287 I print_info: ssm_d_state      = 0
0.00.033.287 I print_info: ssm_dt_rank      = 0
0.00.033.287 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.287 I print_info: model type       = 33M
0.00.033.288 I print_info: model params     = 33.21 M
0.00.033.288 I print_info: general.name     = Bge Small
0.00.033.289 I print_info: vocab type       = WPM
0.00.033.289 I print_info: n_vocab          = 30522
0.00.033.289 I print_info: n_merges         = 0
0.00.033.290 I print_info: BOS token        = 101 '[CLS]'
0.00.033.290 I print_info: UNK token        = 100 '[UNK]'
0.00.033.290 I print_info: SEP token        = 102 '[SEP]'
0.00.033.290 I print_info: PAD token        = 0 '[PAD]'
0.00.033.291 I print_info: MASK token       = 103 '[MASK]'
0.00.033.291 I print_info: LF token         = 0 '[PAD]'
0.00.033.293 I print_info: max token length = 21
0.00.033.294 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.036.577 I load_tensors: offloading 12 repeating layers to GPU
0.00.036.579 I load_tensors: offloading output layer to GPU
0.00.036.579 I load_tensors: offloaded 13/13 layers to GPU
0.00.036.604 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.605 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.036.857 I llama_init_from_model: n_seq_max     = 1
0.00.036.858 I llama_init_from_model: n_ctx         = 512
0.00.036.858 I llama_init_from_model: n_ctx_per_seq = 512
0.00.036.858 I llama_init_from_model: n_batch       = 2048
0.00.036.858 I llama_init_from_model: n_ubatch      = 2048
0.00.036.859 I llama_init_from_model: flash_attn    = 0
0.00.036.859 I llama_init_from_model: freq_base     = 10000.0
0.00.036.860 I llama_init_from_model: freq_scale    = 1
0.00.036.860 I ggml_metal_init: allocating
0.00.036.865 I ggml_metal_init: found device: Apple M4
0.00.036.869 I ggml_metal_init: picking default device: Apple M4
0.00.037.639 I ggml_metal_init: using embedded metal library
0.00.041.553 I ggml_metal_init: GPU name:   Apple M4
0.00.041.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.557 I ggml_metal_init: simdgroup reduction   = true
0.00.041.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.557 I ggml_metal_init: has residency sets    = true
0.00.041.557 I ggml_metal_init: has bfloat            = true
0.00.041.557 I ggml_metal_init: use bfloat            = true
0.00.041.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.911 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.053.597 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.599 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.618 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.054.748 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.054.750 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.054.750 I llama_init_from_model: graph nodes  = 429
0.00.054.750 I llama_init_from_model: graph splits = 2
0.00.054.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.302 I 
0.00.060.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.992 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.147 I llama_perf_context_print:        load time =      44.17 ms
0.00.066.148 I llama_perf_context_print: prompt eval time =       5.00 ms /     9 tokens (    0.56 ms per token,  1798.56 tokens per second)
0.00.066.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.149 I llama_perf_context_print:       total time =       5.85 ms /    10 tokens
0.00.066.283 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.048s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.456 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.118 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.123 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.129 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.129 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.129 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.130 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.131 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.131 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.131 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.132 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.133 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.134 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.134 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.134 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.135 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.135 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.542 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.174 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.176 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.176 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.176 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.176 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.177 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.177 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.178 I llama_model_loader: - type  f32:  124 tensors
0.00.015.178 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.179 I print_info: file format = GGUF V3 (latest)
0.00.015.179 I print_info: file type   = Q8_0
0.00.015.181 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.669 I load: special tokens cache size = 5
0.00.018.948 I load: token to piece cache size = 0.2032 MB
0.00.018.952 I print_info: arch             = bert
0.00.018.952 I print_info: vocab_only       = 0
0.00.018.952 I print_info: n_ctx_train      = 512
0.00.018.952 I print_info: n_embd           = 384
0.00.018.953 I print_info: n_layer          = 12
0.00.018.956 I print_info: n_head           = 12
0.00.018.956 I print_info: n_head_kv        = 12
0.00.018.957 I print_info: n_rot            = 32
0.00.018.957 I print_info: n_swa            = 0
0.00.018.957 I print_info: n_embd_head_k    = 32
0.00.018.957 I print_info: n_embd_head_v    = 32
0.00.018.958 I print_info: n_gqa            = 1
0.00.018.961 I print_info: n_embd_k_gqa     = 384
0.00.018.961 I print_info: n_embd_v_gqa     = 384
0.00.018.962 I print_info: f_norm_eps       = 1.0e-12
0.00.018.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.963 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.963 I print_info: f_logit_scale    = 0.0e+00
0.00.018.964 I print_info: n_ff             = 1536
0.00.018.964 I print_info: n_expert         = 0
0.00.018.964 I print_info: n_expert_used    = 0
0.00.018.965 I print_info: causal attn      = 0
0.00.018.965 I print_info: pooling type     = 2
0.00.018.965 I print_info: rope type        = 2
0.00.018.965 I print_info: rope scaling     = linear
0.00.018.965 I print_info: freq_base_train  = 10000.0
0.00.018.966 I print_info: freq_scale_train = 1
0.00.018.966 I print_info: n_ctx_orig_yarn  = 512
0.00.018.966 I print_info: rope_finetuned   = unknown
0.00.018.966 I print_info: ssm_d_conv       = 0
0.00.018.966 I print_info: ssm_d_inner      = 0
0.00.018.966 I print_info: ssm_d_state      = 0
0.00.018.967 I print_info: ssm_dt_rank      = 0
0.00.018.967 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.969 I print_info: model type       = 33M
0.00.018.969 I print_info: model params     = 33.21 M
0.00.018.970 I print_info: general.name     = Bge Small
0.00.018.970 I print_info: vocab type       = WPM
0.00.018.970 I print_info: n_vocab          = 30522
0.00.018.970 I print_info: n_merges         = 0
0.00.018.971 I print_info: BOS token        = 101 '[CLS]'
0.00.018.971 I print_info: UNK token        = 100 '[UNK]'
0.00.018.971 I print_info: SEP token        = 102 '[SEP]'
0.00.018.971 I print_info: PAD token        = 0 '[PAD]'
0.00.018.971 I print_info: MASK token       = 103 '[MASK]'
0.00.018.971 I print_info: LF token         = 0 '[PAD]'
0.00.018.972 I print_info: max token length = 21
0.00.018.972 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.635 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.636 I load_tensors: offloading output layer to GPU
0.00.020.636 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.642 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.643 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.809 I llama_init_from_model: n_seq_max     = 1
0.00.020.810 I llama_init_from_model: n_ctx         = 512
0.00.020.810 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.810 I llama_init_from_model: n_batch       = 2048
0.00.020.811 I llama_init_from_model: n_ubatch      = 2048
0.00.020.811 I llama_init_from_model: flash_attn    = 0
0.00.020.811 I llama_init_from_model: freq_base     = 10000.0
0.00.020.811 I llama_init_from_model: freq_scale    = 1
0.00.020.812 I ggml_metal_init: allocating
0.00.020.815 I ggml_metal_init: found device: Apple M4
0.00.020.819 I ggml_metal_init: picking default device: Apple M4
0.00.021.336 I ggml_metal_init: using embedded metal library
0.00.023.860 I ggml_metal_init: GPU name:   Apple M4
0.00.023.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.864 I ggml_metal_init: simdgroup reduction   = true
0.00.023.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.864 I ggml_metal_init: has residency sets    = true
0.00.023.864 I ggml_metal_init: has bfloat            = true
0.00.023.864 I ggml_metal_init: use bfloat            = true
0.00.023.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.865 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.055 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.645 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.647 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.656 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.617 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.618 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.618 I llama_init_from_model: graph nodes  = 429
0.00.035.618 I llama_init_from_model: graph splits = 2
0.00.035.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.720 I 
0.00.039.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.282 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.728 I llama_perf_context_print:        load time =      30.26 ms
0.00.044.730 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2088.65 tokens per second)
0.00.044.731 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.732 I llama_perf_context_print:       total time =       5.01 ms /    10 tokens
0.00.044.936 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.162 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.798 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.805 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.024.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.806 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.024.807 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.024.813 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.024.814 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.024.814 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.024.815 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.024.815 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.024.816 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.024.818 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.819 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.819 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.024.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.028.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.029.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.032.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.365 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.032.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.032.365 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.032.366 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.032.366 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.032.366 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.032.367 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.032.367 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.032.367 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.032.368 I llama_model_loader: - type  f32:   40 tensors
0.00.032.368 I llama_model_loader: - type  f16:   30 tensors
0.00.032.368 I print_info: file format = GGUF V3 (latest)
0.00.032.369 I print_info: file type   = F16
0.00.032.370 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.035.116 W load: empty token at index 5
0.00.038.696 W load: model vocab missing newline token, using special_pad_id instead
0.00.039.813 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.039.844 I load: special tokens cache size = 5
0.00.304.563 I load: token to piece cache size = 1.5060 MB
0.00.304.576 I print_info: arch             = jina-bert-v2
0.00.304.577 I print_info: vocab_only       = 0
0.00.304.577 I print_info: n_ctx_train      = 8192
0.00.304.577 I print_info: n_embd           = 384
0.00.304.578 I print_info: n_layer          = 4
0.00.304.582 I print_info: n_head           = 12
0.00.304.583 I print_info: n_head_kv        = 12
0.00.304.583 I print_info: n_rot            = 32
0.00.304.583 I print_info: n_swa            = 0
0.00.304.583 I print_info: n_embd_head_k    = 32
0.00.304.584 I print_info: n_embd_head_v    = 32
0.00.304.584 I print_info: n_gqa            = 1
0.00.304.585 I print_info: n_embd_k_gqa     = 384
0.00.304.585 I print_info: n_embd_v_gqa     = 384
0.00.304.586 I print_info: f_norm_eps       = 1.0e-12
0.00.304.586 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.304.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.304.586 I print_info: f_max_alibi_bias = 8.0e+00
0.00.304.586 I print_info: f_logit_scale    = 0.0e+00
0.00.304.587 I print_info: n_ff             = 1536
0.00.304.587 I print_info: n_expert         = 0
0.00.304.587 I print_info: n_expert_used    = 0
0.00.304.587 I print_info: causal attn      = 0
0.00.304.587 I print_info: pooling type     = -1
0.00.304.588 I print_info: rope type        = -1
0.00.304.588 I print_info: rope scaling     = linear
0.00.304.588 I print_info: freq_base_train  = 10000.0
0.00.304.588 I print_info: freq_scale_train = 1
0.00.304.589 I print_info: n_ctx_orig_yarn  = 8192
0.00.304.589 I print_info: rope_finetuned   = unknown
0.00.304.589 I print_info: ssm_d_conv       = 0
0.00.304.589 I print_info: ssm_d_inner      = 0
0.00.304.589 I print_info: ssm_d_state      = 0
0.00.304.590 I print_info: ssm_dt_rank      = 0
0.00.304.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.304.590 I print_info: model type       = 33M
0.00.304.590 I print_info: model params     = 32.90 M
0.00.304.591 I print_info: general.name     = Jina Bert Implementation
0.00.304.591 I print_info: vocab type       = BPE
0.00.304.591 I print_info: n_vocab          = 61056
0.00.304.591 I print_info: n_merges         = 39382
0.00.304.592 I print_info: BOS token        = 0 '<s>'
0.00.304.592 I print_info: EOS token        = 2 '</s>'
0.00.304.592 I print_info: UNK token        = 3 '<unk>'
0.00.304.592 I print_info: SEP token        = 2 '</s>'
0.00.304.595 I print_info: PAD token        = 1 '<pad>'
0.00.304.595 I print_info: MASK token       = 4 '<mask>'
0.00.304.595 I print_info: EOG token        = 2 '</s>'
0.00.304.595 I print_info: max token length = 45
0.00.304.596 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.305.895 I load_tensors: offloading 4 repeating layers to GPU
0.00.305.896 I load_tensors: offloading output layer to GPU
0.00.305.896 I load_tensors: offloaded 5/5 layers to GPU
0.00.305.917 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.305.918 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.306.085 I llama_init_from_model: n_seq_max     = 1
0.00.306.086 I llama_init_from_model: n_ctx         = 8192
0.00.306.086 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.306.086 I llama_init_from_model: n_batch       = 2048
0.00.306.086 I llama_init_from_model: n_ubatch      = 2048
0.00.306.086 I llama_init_from_model: flash_attn    = 0
0.00.306.087 I llama_init_from_model: freq_base     = 10000.0
0.00.306.087 I llama_init_from_model: freq_scale    = 1
0.00.306.088 I ggml_metal_init: allocating
0.00.306.092 I ggml_metal_init: found device: Apple M4
0.00.306.095 I ggml_metal_init: picking default device: Apple M4
0.00.306.686 I ggml_metal_init: using embedded metal library
0.00.309.282 I ggml_metal_init: GPU name:   Apple M4
0.00.309.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.309.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.309.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.309.285 I ggml_metal_init: simdgroup reduction   = true
0.00.309.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.309.285 I ggml_metal_init: has residency sets    = true
0.00.309.286 I ggml_metal_init: has bfloat            = true
0.00.309.286 I ggml_metal_init: use bfloat            = true
0.00.309.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.309.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.319.689 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.323.130 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.323.132 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.323.157 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.330.197 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.330.200 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.330.200 I llama_init_from_model: graph nodes  = 154
0.00.330.201 I llama_init_from_model: graph splits = 2
0.00.330.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.330.202 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.338.403 I 
0.00.338.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.338.574 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.338.575 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.338.578 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.338.579 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.338.584 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.338.584 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.339.140 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.342.707 I llama_perf_context_print:        load time =     321.60 ms
0.00.342.708 I llama_perf_context_print: prompt eval time =       3.56 ms /    62 tokens (    0.06 ms per token, 17425.52 tokens per second)
0.00.342.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.342.709 I llama_perf_context_print:       total time =       4.31 ms /    63 tokens
0.00.342.969 I ggml_metal_free: deallocating

real	0m1.064s
user	0m0.317s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.185 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.354 I main: llama backend init
0.00.000.360 I main: load the model and apply lora adapter, if any
0.00.048.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.061.417 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.457 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.548 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.079.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.561 I llama_model_loader: - type  f32:  194 tensors
0.00.079.562 I llama_model_loader: - type  f16:   98 tensors
0.00.079.563 I print_info: file format = GGUF V3 (latest)
0.00.079.568 I print_info: file type   = all F32 (guessed)
0.00.079.570 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.095.110 I load: special tokens cache size = 25
0.00.105.399 I load: token to piece cache size = 0.2984 MB
0.00.105.403 I print_info: arch             = gptneox
0.00.105.403 I print_info: vocab_only       = 0
0.00.105.403 I print_info: n_ctx_train      = 2048
0.00.105.403 I print_info: n_embd           = 2048
0.00.105.404 I print_info: n_layer          = 24
0.00.105.408 I print_info: n_head           = 16
0.00.105.409 I print_info: n_head_kv        = 16
0.00.105.409 I print_info: n_rot            = 32
0.00.105.409 I print_info: n_swa            = 0
0.00.105.409 I print_info: n_embd_head_k    = 128
0.00.105.410 I print_info: n_embd_head_v    = 128
0.00.105.411 I print_info: n_gqa            = 1
0.00.105.412 I print_info: n_embd_k_gqa     = 2048
0.00.105.412 I print_info: n_embd_v_gqa     = 2048
0.00.105.413 I print_info: f_norm_eps       = 1.0e-05
0.00.105.413 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.415 I print_info: f_logit_scale    = 0.0e+00
0.00.105.416 I print_info: n_ff             = 8192
0.00.105.416 I print_info: n_expert         = 0
0.00.105.417 I print_info: n_expert_used    = 0
0.00.105.417 I print_info: causal attn      = 1
0.00.105.417 I print_info: pooling type     = 0
0.00.105.417 I print_info: rope type        = 2
0.00.105.418 I print_info: rope scaling     = linear
0.00.105.418 I print_info: freq_base_train  = 10000.0
0.00.105.418 I print_info: freq_scale_train = 1
0.00.105.419 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.419 I print_info: rope_finetuned   = unknown
0.00.105.419 I print_info: ssm_d_conv       = 0
0.00.105.419 I print_info: ssm_d_inner      = 0
0.00.105.419 I print_info: ssm_d_state      = 0
0.00.105.420 I print_info: ssm_dt_rank      = 0
0.00.105.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.420 I print_info: model type       = 1.4B
0.00.105.421 I print_info: model params     = 1.41 B
0.00.105.421 I print_info: general.name     = 1.4B
0.00.105.422 I print_info: vocab type       = BPE
0.00.105.422 I print_info: n_vocab          = 50304
0.00.105.424 I print_info: n_merges         = 50009
0.00.105.424 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.425 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.425 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.425 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.425 I print_info: LF token         = 187 ''
0.00.105.426 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.426 I print_info: max token length = 1024
0.00.105.426 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.166.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.166.269 I load_tensors: offloading output layer to GPU
0.00.166.269 I load_tensors: offloaded 25/25 layers to GPU
0.00.166.296 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.166.298 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.166.830 I llama_init_from_model: n_seq_max     = 1
0.00.166.831 I llama_init_from_model: n_ctx         = 2048
0.00.166.831 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.166.831 I llama_init_from_model: n_batch       = 2048
0.00.166.831 I llama_init_from_model: n_ubatch      = 512
0.00.166.832 I llama_init_from_model: flash_attn    = 0
0.00.166.832 I llama_init_from_model: freq_base     = 10000.0
0.00.166.832 I llama_init_from_model: freq_scale    = 1
0.00.166.835 I ggml_metal_init: allocating
0.00.166.873 I ggml_metal_init: found device: Apple M4
0.00.166.878 I ggml_metal_init: picking default device: Apple M4
0.00.167.552 I ggml_metal_init: using embedded metal library
0.00.272.248 I ggml_metal_init: GPU name:   Apple M4
0.00.272.254 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.272.255 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.272.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.272.256 I ggml_metal_init: simdgroup reduction   = true
0.00.272.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.272.256 I ggml_metal_init: has residency sets    = true
0.00.272.256 I ggml_metal_init: has bfloat            = true
0.00.272.256 I ggml_metal_init: use bfloat            = true
0.00.272.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.272.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.300.728 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.328.815 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.328.822 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.328.870 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.332.702 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.332.705 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.332.705 I llama_init_from_model: graph nodes  = 967
0.00.332.705 I llama_init_from_model: graph splits = 2
0.00.332.710 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.332.841 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.332.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.399.759 I main: llama threadpool init, n_threads = 4
0.00.399.806 I 
0.00.399.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.399.837 I 
0.00.400.010 I sampler seed: 1234
0.00.400.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.400.040 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.400.041 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.400.041 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.229.216 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.02.229.216 I llama_perf_context_print:        load time =     350.17 ms
0.02.229.217 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.14 tokens per second)
0.02.229.218 I llama_perf_context_print:        eval time =    1782.57 ms /    63 runs   (   28.29 ms per token,    35.34 tokens per second)
0.02.229.218 I llama_perf_context_print:       total time =    1830.33 ms /    70 tokens
0.02.229.433 I ggml_metal_free: deallocating

real	0m2.541s
user	0m0.136s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.677 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.564 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.415 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.426 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.427 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.427 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.428 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.430 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.430 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.431 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.907 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.450 I llama_model_loader: - type  f32:  194 tensors
0.00.053.451 I llama_model_loader: - type  f16:   98 tensors
0.00.053.451 I print_info: file format = GGUF V3 (latest)
0.00.053.452 I print_info: file type   = all F32 (guessed)
0.00.053.453 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.251 I load: special tokens cache size = 25
0.00.072.671 I load: token to piece cache size = 0.2984 MB
0.00.072.675 I print_info: arch             = gptneox
0.00.072.675 I print_info: vocab_only       = 0
0.00.072.675 I print_info: n_ctx_train      = 2048
0.00.072.675 I print_info: n_embd           = 2048
0.00.072.676 I print_info: n_layer          = 24
0.00.072.680 I print_info: n_head           = 16
0.00.072.681 I print_info: n_head_kv        = 16
0.00.072.681 I print_info: n_rot            = 32
0.00.072.681 I print_info: n_swa            = 0
0.00.072.681 I print_info: n_embd_head_k    = 128
0.00.072.681 I print_info: n_embd_head_v    = 128
0.00.072.682 I print_info: n_gqa            = 1
0.00.072.683 I print_info: n_embd_k_gqa     = 2048
0.00.072.683 I print_info: n_embd_v_gqa     = 2048
0.00.072.684 I print_info: f_norm_eps       = 1.0e-05
0.00.072.685 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.685 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.685 I print_info: f_logit_scale    = 0.0e+00
0.00.072.686 I print_info: n_ff             = 8192
0.00.072.686 I print_info: n_expert         = 0
0.00.072.686 I print_info: n_expert_used    = 0
0.00.072.686 I print_info: causal attn      = 1
0.00.072.686 I print_info: pooling type     = 0
0.00.072.687 I print_info: rope type        = 2
0.00.072.687 I print_info: rope scaling     = linear
0.00.072.687 I print_info: freq_base_train  = 10000.0
0.00.072.688 I print_info: freq_scale_train = 1
0.00.072.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.688 I print_info: rope_finetuned   = unknown
0.00.072.688 I print_info: ssm_d_conv       = 0
0.00.072.688 I print_info: ssm_d_inner      = 0
0.00.072.688 I print_info: ssm_d_state      = 0
0.00.072.688 I print_info: ssm_dt_rank      = 0
0.00.072.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.689 I print_info: model type       = 1.4B
0.00.072.689 I print_info: model params     = 1.41 B
0.00.072.689 I print_info: general.name     = 1.4B
0.00.072.690 I print_info: vocab type       = BPE
0.00.072.690 I print_info: n_vocab          = 50304
0.00.072.690 I print_info: n_merges         = 50009
0.00.072.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.691 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.691 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.691 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.691 I print_info: LF token         = 187 ''
0.00.072.692 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.698 I print_info: max token length = 1024
0.00.072.699 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.466.980 I load_tensors: offloading 24 repeating layers to GPU
0.01.466.985 I load_tensors: offloading output layer to GPU
0.01.466.986 I load_tensors: offloaded 25/25 layers to GPU
0.01.467.011 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.467.014 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.467.932 I llama_init_from_model: n_seq_max     = 1
0.01.467.933 I llama_init_from_model: n_ctx         = 128
0.01.467.933 I llama_init_from_model: n_ctx_per_seq = 128
0.01.467.934 I llama_init_from_model: n_batch       = 128
0.01.467.934 I llama_init_from_model: n_ubatch      = 128
0.01.467.934 I llama_init_from_model: flash_attn    = 0
0.01.467.935 I llama_init_from_model: freq_base     = 10000.0
0.01.467.935 I llama_init_from_model: freq_scale    = 1
0.01.467.935 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.467.942 I ggml_metal_init: allocating
0.01.468.000 I ggml_metal_init: found device: Apple M4
0.01.468.008 I ggml_metal_init: picking default device: Apple M4
0.01.469.178 I ggml_metal_init: using embedded metal library
0.01.473.279 I ggml_metal_init: GPU name:   Apple M4
0.01.473.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.473.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.473.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.473.283 I ggml_metal_init: simdgroup reduction   = true
0.01.473.283 I ggml_metal_init: simdgroup matrix mul. = true
0.01.473.283 I ggml_metal_init: has residency sets    = true
0.01.473.284 I ggml_metal_init: has bfloat            = true
0.01.473.284 I ggml_metal_init: use bfloat            = true
0.01.473.284 I ggml_metal_init: hasUnifiedMemory      = true
0.01.473.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.484.595 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.486.416 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.486.418 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.486.444 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.488.169 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.488.170 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.488.171 I llama_init_from_model: graph nodes  = 967
0.01.488.171 I llama_init_from_model: graph splits = 2
0.01.488.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.488.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.523.569 I 
0.01.523.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.523.618 I perplexity: tokenizing the input ..
0.01.528.733 I perplexity: tokenization took 5.113 ms
0.01.528.738 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.647.690 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.649.023 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.649.055 I llama_perf_context_print:        load time =    1502.00 ms
0.01.649.056 I llama_perf_context_print: prompt eval time =     118.64 ms /   128 tokens (    0.93 ms per token,  1078.86 tokens per second)
0.01.649.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.649.057 I llama_perf_context_print:       total time =     125.49 ms /   129 tokens
0.01.649.435 I ggml_metal_free: deallocating

real	0m1.837s
user	0m0.098s
sys	0m0.284s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.918 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.071 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.073 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.061 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.934 I llama_model_loader: - type  f32:  194 tensors
0.00.039.934 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.935 I print_info: file format = GGUF V3 (latest)
0.00.039.936 I print_info: file type   = Q8_0
0.00.039.937 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.569 I load: special tokens cache size = 25
0.00.056.680 I load: token to piece cache size = 0.2984 MB
0.00.056.684 I print_info: arch             = gptneox
0.00.056.684 I print_info: vocab_only       = 0
0.00.056.684 I print_info: n_ctx_train      = 2048
0.00.056.685 I print_info: n_embd           = 2048
0.00.056.685 I print_info: n_layer          = 24
0.00.056.690 I print_info: n_head           = 16
0.00.056.691 I print_info: n_head_kv        = 16
0.00.056.691 I print_info: n_rot            = 32
0.00.056.691 I print_info: n_swa            = 0
0.00.056.691 I print_info: n_embd_head_k    = 128
0.00.056.692 I print_info: n_embd_head_v    = 128
0.00.056.692 I print_info: n_gqa            = 1
0.00.056.693 I print_info: n_embd_k_gqa     = 2048
0.00.056.693 I print_info: n_embd_v_gqa     = 2048
0.00.056.694 I print_info: f_norm_eps       = 1.0e-05
0.00.056.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.695 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.695 I print_info: f_logit_scale    = 0.0e+00
0.00.056.696 I print_info: n_ff             = 8192
0.00.056.696 I print_info: n_expert         = 0
0.00.056.696 I print_info: n_expert_used    = 0
0.00.056.696 I print_info: causal attn      = 1
0.00.056.696 I print_info: pooling type     = 0
0.00.056.696 I print_info: rope type        = 2
0.00.056.696 I print_info: rope scaling     = linear
0.00.056.697 I print_info: freq_base_train  = 10000.0
0.00.056.697 I print_info: freq_scale_train = 1
0.00.056.697 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.697 I print_info: rope_finetuned   = unknown
0.00.056.698 I print_info: ssm_d_conv       = 0
0.00.056.698 I print_info: ssm_d_inner      = 0
0.00.056.698 I print_info: ssm_d_state      = 0
0.00.056.698 I print_info: ssm_dt_rank      = 0
0.00.056.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.701 I print_info: model type       = 1.4B
0.00.056.702 I print_info: model params     = 1.41 B
0.00.056.702 I print_info: general.name     = 1.4B
0.00.056.703 I print_info: vocab type       = BPE
0.00.056.703 I print_info: n_vocab          = 50304
0.00.056.703 I print_info: n_merges         = 50009
0.00.056.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.704 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.704 I print_info: LF token         = 187 ''
0.00.056.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.705 I print_info: max token length = 1024
0.00.056.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.247.125 I load_tensors: offloading 24 repeating layers to GPU
0.01.247.131 I load_tensors: offloading output layer to GPU
0.01.247.132 I load_tensors: offloaded 25/25 layers to GPU
0.01.247.157 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.247.158 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.248.381 I llama_init_from_model: n_seq_max     = 1
0.01.248.383 I llama_init_from_model: n_ctx         = 2048
0.01.248.384 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.248.384 I llama_init_from_model: n_batch       = 2048
0.01.248.384 I llama_init_from_model: n_ubatch      = 512
0.01.248.385 I llama_init_from_model: flash_attn    = 0
0.01.248.386 I llama_init_from_model: freq_base     = 10000.0
0.01.248.386 I llama_init_from_model: freq_scale    = 1
0.01.248.387 I ggml_metal_init: allocating
0.01.248.397 I ggml_metal_init: found device: Apple M4
0.01.248.404 I ggml_metal_init: picking default device: Apple M4
0.01.249.705 I ggml_metal_init: using embedded metal library
0.01.255.082 I ggml_metal_init: GPU name:   Apple M4
0.01.255.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.255.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.255.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.255.087 I ggml_metal_init: simdgroup reduction   = true
0.01.255.088 I ggml_metal_init: simdgroup matrix mul. = true
0.01.255.088 I ggml_metal_init: has residency sets    = true
0.01.255.088 I ggml_metal_init: has bfloat            = true
0.01.255.088 I ggml_metal_init: use bfloat            = true
0.01.255.089 I ggml_metal_init: hasUnifiedMemory      = true
0.01.255.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.271.408 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.327.582 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.327.588 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.327.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.332.096 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.332.098 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.332.098 I llama_init_from_model: graph nodes  = 967
0.01.332.099 I llama_init_from_model: graph splits = 2
0.01.332.104 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.332.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.332.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.389.488 I main: llama threadpool init, n_threads = 4
0.01.389.526 I 
0.01.389.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.389.547 I 
0.01.389.722 I sampler seed: 1234
0.01.389.726 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.389.776 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.389.789 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.389.790 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.482.827 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48663.47 tokens per second)
0.02.482.828 I llama_perf_context_print:        load time =    1378.87 ms
0.02.482.830 I llama_perf_context_print: prompt eval time =      45.27 ms /     7 tokens (    6.47 ms per token,   154.64 tokens per second)
0.02.482.830 I llama_perf_context_print:        eval time =    1045.18 ms /    63 runs   (   16.59 ms per token,    60.28 tokens per second)
0.02.482.832 I llama_perf_context_print:       total time =    1094.04 ms /    70 tokens
0.02.483.120 I ggml_metal_free: deallocating

real	0m2.503s
user	0m0.110s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.256 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.519 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.195 I llama_model_loader: - type  f32:  194 tensors
0.00.026.196 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.197 I print_info: file format = GGUF V3 (latest)
0.00.026.197 I print_info: file type   = Q8_0
0.00.026.198 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.188 I load: special tokens cache size = 25
0.00.040.471 I load: token to piece cache size = 0.2984 MB
0.00.040.477 I print_info: arch             = gptneox
0.00.040.477 I print_info: vocab_only       = 0
0.00.040.477 I print_info: n_ctx_train      = 2048
0.00.040.477 I print_info: n_embd           = 2048
0.00.040.477 I print_info: n_layer          = 24
0.00.040.482 I print_info: n_head           = 16
0.00.040.483 I print_info: n_head_kv        = 16
0.00.040.483 I print_info: n_rot            = 32
0.00.040.483 I print_info: n_swa            = 0
0.00.040.483 I print_info: n_embd_head_k    = 128
0.00.040.483 I print_info: n_embd_head_v    = 128
0.00.040.484 I print_info: n_gqa            = 1
0.00.040.485 I print_info: n_embd_k_gqa     = 2048
0.00.040.485 I print_info: n_embd_v_gqa     = 2048
0.00.040.486 I print_info: f_norm_eps       = 1.0e-05
0.00.040.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.490 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.490 I print_info: f_logit_scale    = 0.0e+00
0.00.040.490 I print_info: n_ff             = 8192
0.00.040.491 I print_info: n_expert         = 0
0.00.040.491 I print_info: n_expert_used    = 0
0.00.040.491 I print_info: causal attn      = 1
0.00.040.491 I print_info: pooling type     = 0
0.00.040.491 I print_info: rope type        = 2
0.00.040.491 I print_info: rope scaling     = linear
0.00.040.492 I print_info: freq_base_train  = 10000.0
0.00.040.492 I print_info: freq_scale_train = 1
0.00.040.492 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.492 I print_info: rope_finetuned   = unknown
0.00.040.493 I print_info: ssm_d_conv       = 0
0.00.040.493 I print_info: ssm_d_inner      = 0
0.00.040.494 I print_info: ssm_d_state      = 0
0.00.040.494 I print_info: ssm_dt_rank      = 0
0.00.040.494 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.494 I print_info: model type       = 1.4B
0.00.040.495 I print_info: model params     = 1.41 B
0.00.040.496 I print_info: general.name     = 1.4B
0.00.040.496 I print_info: vocab type       = BPE
0.00.040.497 I print_info: n_vocab          = 50304
0.00.040.497 I print_info: n_merges         = 50009
0.00.040.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.498 I print_info: LF token         = 187 ''
0.00.040.498 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.498 I print_info: max token length = 1024
0.00.040.499 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.942.467 I load_tensors: offloading 24 repeating layers to GPU
0.00.942.474 I load_tensors: offloading output layer to GPU
0.00.942.475 I load_tensors: offloaded 25/25 layers to GPU
0.00.942.504 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.942.506 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.943.782 I llama_init_from_model: n_seq_max     = 1
0.00.943.784 I llama_init_from_model: n_ctx         = 128
0.00.943.784 I llama_init_from_model: n_ctx_per_seq = 128
0.00.943.784 I llama_init_from_model: n_batch       = 128
0.00.943.785 I llama_init_from_model: n_ubatch      = 128
0.00.943.785 I llama_init_from_model: flash_attn    = 0
0.00.943.786 I llama_init_from_model: freq_base     = 10000.0
0.00.943.786 I llama_init_from_model: freq_scale    = 1
0.00.943.787 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.943.789 I ggml_metal_init: allocating
0.00.943.871 I ggml_metal_init: found device: Apple M4
0.00.943.883 I ggml_metal_init: picking default device: Apple M4
0.00.945.359 I ggml_metal_init: using embedded metal library
0.00.951.054 I ggml_metal_init: GPU name:   Apple M4
0.00.951.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.951.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.951.060 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.951.060 I ggml_metal_init: simdgroup reduction   = true
0.00.951.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.951.061 I ggml_metal_init: has residency sets    = true
0.00.951.061 I ggml_metal_init: has bfloat            = true
0.00.951.061 I ggml_metal_init: use bfloat            = true
0.00.951.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.951.064 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.967.470 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.970.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.970.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.970.934 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.973.627 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.973.628 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.973.629 I llama_init_from_model: graph nodes  = 967
0.00.973.629 I llama_init_from_model: graph splits = 2
0.00.973.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.973.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.002.017 I 
0.01.002.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.002.095 I perplexity: tokenizing the input ..
0.01.009.153 I perplexity: tokenization took 7.054 ms
0.01.009.159 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.148.521 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.150.004 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.150.036 I llama_perf_context_print:        load time =     991.49 ms
0.01.150.037 I llama_perf_context_print: prompt eval time =     138.40 ms /   128 tokens (    1.08 ms per token,   924.88 tokens per second)
0.01.150.038 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.150.038 I llama_perf_context_print:       total time =     148.02 ms /   129 tokens
0.01.150.454 I ggml_metal_free: deallocating

real	0m1.166s
user	0m0.079s
sys	0m0.186s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.015.352 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.186 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.198 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.198 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.199 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.199 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.199 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.200 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.200 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.205 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.205 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.205 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.273 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.510 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.511 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.512 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.512 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.512 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.513 I llama_model_loader: - type  f32:  194 tensors
0.00.045.513 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.513 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.514 I print_info: file format = GGUF V3 (latest)
0.00.045.515 I print_info: file type   = Q4_0
0.00.045.516 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.818 I load: special tokens cache size = 25
0.00.062.701 I load: token to piece cache size = 0.2984 MB
0.00.062.705 I print_info: arch             = gptneox
0.00.062.705 I print_info: vocab_only       = 0
0.00.062.705 I print_info: n_ctx_train      = 2048
0.00.062.705 I print_info: n_embd           = 2048
0.00.062.706 I print_info: n_layer          = 24
0.00.062.710 I print_info: n_head           = 16
0.00.062.711 I print_info: n_head_kv        = 16
0.00.062.711 I print_info: n_rot            = 32
0.00.062.711 I print_info: n_swa            = 0
0.00.062.711 I print_info: n_embd_head_k    = 128
0.00.062.712 I print_info: n_embd_head_v    = 128
0.00.062.713 I print_info: n_gqa            = 1
0.00.062.713 I print_info: n_embd_k_gqa     = 2048
0.00.062.714 I print_info: n_embd_v_gqa     = 2048
0.00.062.715 I print_info: f_norm_eps       = 1.0e-05
0.00.062.715 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.715 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.716 I print_info: f_logit_scale    = 0.0e+00
0.00.062.716 I print_info: n_ff             = 8192
0.00.062.717 I print_info: n_expert         = 0
0.00.062.717 I print_info: n_expert_used    = 0
0.00.062.717 I print_info: causal attn      = 1
0.00.062.717 I print_info: pooling type     = 0
0.00.062.719 I print_info: rope type        = 2
0.00.062.721 I print_info: rope scaling     = linear
0.00.062.722 I print_info: freq_base_train  = 10000.0
0.00.062.722 I print_info: freq_scale_train = 1
0.00.062.722 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.723 I print_info: rope_finetuned   = unknown
0.00.062.723 I print_info: ssm_d_conv       = 0
0.00.062.723 I print_info: ssm_d_inner      = 0
0.00.062.723 I print_info: ssm_d_state      = 0
0.00.062.723 I print_info: ssm_dt_rank      = 0
0.00.062.723 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.724 I print_info: model type       = 1.4B
0.00.062.724 I print_info: model params     = 1.41 B
0.00.062.729 I print_info: general.name     = 1.4B
0.00.062.730 I print_info: vocab type       = BPE
0.00.062.730 I print_info: n_vocab          = 50304
0.00.062.730 I print_info: n_merges         = 50009
0.00.062.730 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.731 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.733 I print_info: LF token         = 187 ''
0.00.062.733 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.734 I print_info: max token length = 1024
0.00.062.734 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.674.308 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.326 I load_tensors: offloading output layer to GPU
0.00.674.327 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.357 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.674.358 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.675.817 I llama_init_from_model: n_seq_max     = 1
0.00.675.826 I llama_init_from_model: n_ctx         = 2048
0.00.675.826 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.675.827 I llama_init_from_model: n_batch       = 2048
0.00.675.828 I llama_init_from_model: n_ubatch      = 512
0.00.675.828 I llama_init_from_model: flash_attn    = 0
0.00.675.829 I llama_init_from_model: freq_base     = 10000.0
0.00.675.830 I llama_init_from_model: freq_scale    = 1
0.00.675.835 I ggml_metal_init: allocating
0.00.675.883 I ggml_metal_init: found device: Apple M4
0.00.675.897 I ggml_metal_init: picking default device: Apple M4
0.00.677.988 I ggml_metal_init: using embedded metal library
0.00.684.154 I ggml_metal_init: GPU name:   Apple M4
0.00.684.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.168 I ggml_metal_init: simdgroup reduction   = true
0.00.684.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.168 I ggml_metal_init: has residency sets    = true
0.00.684.169 I ggml_metal_init: has bfloat            = true
0.00.684.169 I ggml_metal_init: use bfloat            = true
0.00.684.171 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.579 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.765.151 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.765.156 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.765.190 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.397 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.769.399 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.769.399 I llama_init_from_model: graph nodes  = 967
0.00.769.399 I llama_init_from_model: graph splits = 2
0.00.769.405 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.769.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.769.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.824.120 I main: llama threadpool init, n_threads = 4
0.00.824.167 I 
0.00.824.188 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.824.190 I 
0.00.824.332 I sampler seed: 1234
0.00.824.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.824.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.824.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.824.348 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.509.900 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.509.901 I llama_perf_context_print:        load time =     808.06 ms
0.01.509.902 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.78 tokens per second)
0.01.509.902 I llama_perf_context_print:        eval time =     633.65 ms /    63 runs   (   10.06 ms per token,    99.42 tokens per second)
0.01.509.904 I llama_perf_context_print:       total time =     686.49 ms /    70 tokens
0.01.510.136 I ggml_metal_free: deallocating

real	0m1.530s
user	0m0.115s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.816 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.258 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.980 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.980 I llama_model_loader: - type  f32:  194 tensors
0.00.026.981 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.981 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.981 I print_info: file format = GGUF V3 (latest)
0.00.026.982 I print_info: file type   = Q4_0
0.00.026.983 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.427 I load: special tokens cache size = 25
0.00.041.792 I load: token to piece cache size = 0.2984 MB
0.00.041.797 I print_info: arch             = gptneox
0.00.041.797 I print_info: vocab_only       = 0
0.00.041.797 I print_info: n_ctx_train      = 2048
0.00.041.798 I print_info: n_embd           = 2048
0.00.041.798 I print_info: n_layer          = 24
0.00.041.803 I print_info: n_head           = 16
0.00.041.803 I print_info: n_head_kv        = 16
0.00.041.805 I print_info: n_rot            = 32
0.00.041.805 I print_info: n_swa            = 0
0.00.041.806 I print_info: n_embd_head_k    = 128
0.00.041.806 I print_info: n_embd_head_v    = 128
0.00.041.806 I print_info: n_gqa            = 1
0.00.041.807 I print_info: n_embd_k_gqa     = 2048
0.00.041.808 I print_info: n_embd_v_gqa     = 2048
0.00.041.808 I print_info: f_norm_eps       = 1.0e-05
0.00.041.809 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.809 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.809 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.809 I print_info: f_logit_scale    = 0.0e+00
0.00.041.810 I print_info: n_ff             = 8192
0.00.041.810 I print_info: n_expert         = 0
0.00.041.810 I print_info: n_expert_used    = 0
0.00.041.810 I print_info: causal attn      = 1
0.00.041.810 I print_info: pooling type     = 0
0.00.041.812 I print_info: rope type        = 2
0.00.041.813 I print_info: rope scaling     = linear
0.00.041.813 I print_info: freq_base_train  = 10000.0
0.00.041.813 I print_info: freq_scale_train = 1
0.00.041.813 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.813 I print_info: rope_finetuned   = unknown
0.00.041.814 I print_info: ssm_d_conv       = 0
0.00.041.814 I print_info: ssm_d_inner      = 0
0.00.041.814 I print_info: ssm_d_state      = 0
0.00.041.814 I print_info: ssm_dt_rank      = 0
0.00.041.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.814 I print_info: model type       = 1.4B
0.00.041.815 I print_info: model params     = 1.41 B
0.00.041.816 I print_info: general.name     = 1.4B
0.00.041.817 I print_info: vocab type       = BPE
0.00.041.817 I print_info: n_vocab          = 50304
0.00.041.817 I print_info: n_merges         = 50009
0.00.041.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.818 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.818 I print_info: LF token         = 187 ''
0.00.041.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.818 I print_info: max token length = 1024
0.00.041.819 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.055 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.073 I load_tensors: offloading output layer to GPU
0.00.636.073 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.102 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.636.103 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.637.450 I llama_init_from_model: n_seq_max     = 1
0.00.637.453 I llama_init_from_model: n_ctx         = 128
0.00.637.454 I llama_init_from_model: n_ctx_per_seq = 128
0.00.637.455 I llama_init_from_model: n_batch       = 128
0.00.637.455 I llama_init_from_model: n_ubatch      = 128
0.00.637.455 I llama_init_from_model: flash_attn    = 0
0.00.637.457 I llama_init_from_model: freq_base     = 10000.0
0.00.637.458 I llama_init_from_model: freq_scale    = 1
0.00.637.458 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.461 I ggml_metal_init: allocating
0.00.637.537 I ggml_metal_init: found device: Apple M4
0.00.637.555 I ggml_metal_init: picking default device: Apple M4
0.00.639.336 I ggml_metal_init: using embedded metal library
0.00.644.701 I ggml_metal_init: GPU name:   Apple M4
0.00.644.715 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.716 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.716 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.717 I ggml_metal_init: simdgroup reduction   = true
0.00.644.717 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.718 I ggml_metal_init: has residency sets    = true
0.00.644.718 I ggml_metal_init: has bfloat            = true
0.00.644.718 I ggml_metal_init: use bfloat            = true
0.00.644.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.175 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.668.640 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.668.647 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.668.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.671.723 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.671.725 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.671.726 I llama_init_from_model: graph nodes  = 967
0.00.671.726 I llama_init_from_model: graph splits = 2
0.00.671.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.671.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.854 I 
0.00.700.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.945 I perplexity: tokenizing the input ..
0.00.707.784 I perplexity: tokenization took 6.838 ms
0.00.707.792 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.675 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.846.094 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.846.117 I llama_perf_context_print:        load time =     690.03 ms
0.00.846.118 I llama_perf_context_print: prompt eval time =     135.91 ms /   128 tokens (    1.06 ms per token,   941.82 tokens per second)
0.00.846.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.119 I llama_perf_context_print:       total time =     145.27 ms /   129 tokens
0.00.846.469 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.081s
sys	0m0.143s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.322 I llama_model_loader: - type  f32:  194 tensors
0.00.029.323 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.323 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.323 I print_info: file format = GGUF V3 (latest)
0.00.029.324 I print_info: file type   = Q4_1
0.00.029.325 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.037.021 I load: special tokens cache size = 25
0.00.043.157 I load: token to piece cache size = 0.2984 MB
0.00.043.160 I print_info: arch             = gptneox
0.00.043.160 I print_info: vocab_only       = 0
0.00.043.161 I print_info: n_ctx_train      = 2048
0.00.043.161 I print_info: n_embd           = 2048
0.00.043.161 I print_info: n_layer          = 24
0.00.043.164 I print_info: n_head           = 16
0.00.043.165 I print_info: n_head_kv        = 16
0.00.043.165 I print_info: n_rot            = 32
0.00.043.165 I print_info: n_swa            = 0
0.00.043.165 I print_info: n_embd_head_k    = 128
0.00.043.166 I print_info: n_embd_head_v    = 128
0.00.043.167 I print_info: n_gqa            = 1
0.00.043.168 I print_info: n_embd_k_gqa     = 2048
0.00.043.169 I print_info: n_embd_v_gqa     = 2048
0.00.043.169 I print_info: f_norm_eps       = 1.0e-05
0.00.043.169 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.170 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.170 I print_info: f_logit_scale    = 0.0e+00
0.00.043.170 I print_info: n_ff             = 8192
0.00.043.171 I print_info: n_expert         = 0
0.00.043.171 I print_info: n_expert_used    = 0
0.00.043.171 I print_info: causal attn      = 1
0.00.043.171 I print_info: pooling type     = 0
0.00.043.172 I print_info: rope type        = 2
0.00.043.174 I print_info: rope scaling     = linear
0.00.043.175 I print_info: freq_base_train  = 10000.0
0.00.043.175 I print_info: freq_scale_train = 1
0.00.043.175 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.175 I print_info: rope_finetuned   = unknown
0.00.043.176 I print_info: ssm_d_conv       = 0
0.00.043.177 I print_info: ssm_d_inner      = 0
0.00.043.177 I print_info: ssm_d_state      = 0
0.00.043.177 I print_info: ssm_dt_rank      = 0
0.00.043.177 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.177 I print_info: model type       = 1.4B
0.00.043.178 I print_info: model params     = 1.41 B
0.00.043.178 I print_info: general.name     = 1.4B
0.00.043.178 I print_info: vocab type       = BPE
0.00.043.178 I print_info: n_vocab          = 50304
0.00.043.179 I print_info: n_merges         = 50009
0.00.043.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.183 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.183 I print_info: LF token         = 187 ''
0.00.043.184 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.184 I print_info: max token length = 1024
0.00.043.184 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.796.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.796.170 I load_tensors: offloading output layer to GPU
0.00.796.171 I load_tensors: offloaded 25/25 layers to GPU
0.00.796.204 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.796.205 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.797.734 I llama_init_from_model: n_seq_max     = 1
0.00.797.736 I llama_init_from_model: n_ctx         = 2048
0.00.797.737 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.797.737 I llama_init_from_model: n_batch       = 2048
0.00.797.738 I llama_init_from_model: n_ubatch      = 512
0.00.797.738 I llama_init_from_model: flash_attn    = 0
0.00.797.741 I llama_init_from_model: freq_base     = 10000.0
0.00.797.741 I llama_init_from_model: freq_scale    = 1
0.00.797.744 I ggml_metal_init: allocating
0.00.797.811 I ggml_metal_init: found device: Apple M4
0.00.797.825 I ggml_metal_init: picking default device: Apple M4
0.00.799.724 I ggml_metal_init: using embedded metal library
0.00.806.231 I ggml_metal_init: GPU name:   Apple M4
0.00.806.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.806.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.806.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.806.237 I ggml_metal_init: simdgroup reduction   = true
0.00.806.238 I ggml_metal_init: simdgroup matrix mul. = true
0.00.806.238 I ggml_metal_init: has residency sets    = true
0.00.806.238 I ggml_metal_init: has bfloat            = true
0.00.806.238 I ggml_metal_init: use bfloat            = true
0.00.806.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.806.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.823.521 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.878.043 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.878.052 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.878.091 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.883.230 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.883.233 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.883.233 I llama_init_from_model: graph nodes  = 967
0.00.883.234 I llama_init_from_model: graph splits = 2
0.00.883.240 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.883.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.883.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.940.235 I main: llama threadpool init, n_threads = 4
0.00.940.278 I 
0.00.940.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.940.301 I 
0.00.940.443 I sampler seed: 1234
0.00.940.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.940.458 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.940.458 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.940.458 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.681.446 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.01.681.446 I llama_perf_context_print:        load time =     930.65 ms
0.01.681.447 I llama_perf_context_print: prompt eval time =      49.04 ms /     7 tokens (    7.01 ms per token,   142.74 tokens per second)
0.01.681.448 I llama_perf_context_print:        eval time =     689.15 ms /    63 runs   (   10.94 ms per token,    91.42 tokens per second)
0.01.681.448 I llama_perf_context_print:       total time =     741.95 ms /    70 tokens
0.01.681.675 I ggml_metal_free: deallocating

real	0m1.702s
user	0m0.107s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.128 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.128 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.128 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.129 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.131 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.131 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.687 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.688 I llama_model_loader: - type  f32:  194 tensors
0.00.024.688 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.689 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.689 I print_info: file format = GGUF V3 (latest)
0.00.024.690 I print_info: file type   = Q4_1
0.00.024.691 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.060 I load: special tokens cache size = 25
0.00.039.271 I load: token to piece cache size = 0.2984 MB
0.00.039.276 I print_info: arch             = gptneox
0.00.039.276 I print_info: vocab_only       = 0
0.00.039.276 I print_info: n_ctx_train      = 2048
0.00.039.277 I print_info: n_embd           = 2048
0.00.039.277 I print_info: n_layer          = 24
0.00.039.280 I print_info: n_head           = 16
0.00.039.281 I print_info: n_head_kv        = 16
0.00.039.281 I print_info: n_rot            = 32
0.00.039.281 I print_info: n_swa            = 0
0.00.039.285 I print_info: n_embd_head_k    = 128
0.00.039.285 I print_info: n_embd_head_v    = 128
0.00.039.286 I print_info: n_gqa            = 1
0.00.039.286 I print_info: n_embd_k_gqa     = 2048
0.00.039.287 I print_info: n_embd_v_gqa     = 2048
0.00.039.287 I print_info: f_norm_eps       = 1.0e-05
0.00.039.288 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.288 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.288 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.288 I print_info: f_logit_scale    = 0.0e+00
0.00.039.289 I print_info: n_ff             = 8192
0.00.039.289 I print_info: n_expert         = 0
0.00.039.289 I print_info: n_expert_used    = 0
0.00.039.290 I print_info: causal attn      = 1
0.00.039.290 I print_info: pooling type     = 0
0.00.039.290 I print_info: rope type        = 2
0.00.039.290 I print_info: rope scaling     = linear
0.00.039.291 I print_info: freq_base_train  = 10000.0
0.00.039.291 I print_info: freq_scale_train = 1
0.00.039.291 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.291 I print_info: rope_finetuned   = unknown
0.00.039.317 I print_info: ssm_d_conv       = 0
0.00.039.318 I print_info: ssm_d_inner      = 0
0.00.039.319 I print_info: ssm_d_state      = 0
0.00.039.319 I print_info: ssm_dt_rank      = 0
0.00.039.319 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.319 I print_info: model type       = 1.4B
0.00.039.320 I print_info: model params     = 1.41 B
0.00.039.320 I print_info: general.name     = 1.4B
0.00.039.320 I print_info: vocab type       = BPE
0.00.039.321 I print_info: n_vocab          = 50304
0.00.039.321 I print_info: n_merges         = 50009
0.00.039.322 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: LF token         = 187 ''
0.00.039.323 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.323 I print_info: max token length = 1024
0.00.039.323 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.837 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.852 I load_tensors: offloading output layer to GPU
0.00.643.853 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.885 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.643.887 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.645.474 I llama_init_from_model: n_seq_max     = 1
0.00.645.477 I llama_init_from_model: n_ctx         = 128
0.00.645.478 I llama_init_from_model: n_ctx_per_seq = 128
0.00.645.478 I llama_init_from_model: n_batch       = 128
0.00.645.478 I llama_init_from_model: n_ubatch      = 128
0.00.645.479 I llama_init_from_model: flash_attn    = 0
0.00.645.483 I llama_init_from_model: freq_base     = 10000.0
0.00.645.489 I llama_init_from_model: freq_scale    = 1
0.00.645.489 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.492 I ggml_metal_init: allocating
0.00.645.594 I ggml_metal_init: found device: Apple M4
0.00.645.610 I ggml_metal_init: picking default device: Apple M4
0.00.647.395 I ggml_metal_init: using embedded metal library
0.00.653.878 I ggml_metal_init: GPU name:   Apple M4
0.00.653.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.890 I ggml_metal_init: simdgroup reduction   = true
0.00.653.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.891 I ggml_metal_init: has residency sets    = true
0.00.653.891 I ggml_metal_init: has bfloat            = true
0.00.653.891 I ggml_metal_init: use bfloat            = true
0.00.653.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.075 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.635 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.675.642 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.731 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.090 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.091 I llama_init_from_model: graph nodes  = 967
0.00.679.091 I llama_init_from_model: graph splits = 2
0.00.679.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.061 I 
0.00.708.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.160 I perplexity: tokenizing the input ..
0.00.715.319 I perplexity: tokenization took 7.154 ms
0.00.715.330 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.756 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.851.120 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.851.144 I llama_perf_context_print:        load time =     699.00 ms
0.00.851.145 I llama_perf_context_print: prompt eval time =     133.77 ms /   128 tokens (    1.05 ms per token,   956.89 tokens per second)
0.00.851.145 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.146 I llama_perf_context_print:       total time =     143.09 ms /   129 tokens
0.00.851.528 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.079s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.367 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.601 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.602 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.602 I llama_model_loader: - type  f32:  194 tensors
0.00.027.603 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.603 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.603 I print_info: file format = GGUF V3 (latest)
0.00.027.604 I print_info: file type   = Q5_0
0.00.027.605 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.458 I load: special tokens cache size = 25
0.00.041.363 I load: token to piece cache size = 0.2984 MB
0.00.041.365 I print_info: arch             = gptneox
0.00.041.366 I print_info: vocab_only       = 0
0.00.041.366 I print_info: n_ctx_train      = 2048
0.00.041.366 I print_info: n_embd           = 2048
0.00.041.366 I print_info: n_layer          = 24
0.00.041.369 I print_info: n_head           = 16
0.00.041.370 I print_info: n_head_kv        = 16
0.00.041.370 I print_info: n_rot            = 32
0.00.041.370 I print_info: n_swa            = 0
0.00.041.370 I print_info: n_embd_head_k    = 128
0.00.041.371 I print_info: n_embd_head_v    = 128
0.00.041.371 I print_info: n_gqa            = 1
0.00.041.372 I print_info: n_embd_k_gqa     = 2048
0.00.041.373 I print_info: n_embd_v_gqa     = 2048
0.00.041.373 I print_info: f_norm_eps       = 1.0e-05
0.00.041.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.374 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.374 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.374 I print_info: f_logit_scale    = 0.0e+00
0.00.041.377 I print_info: n_ff             = 8192
0.00.041.377 I print_info: n_expert         = 0
0.00.041.377 I print_info: n_expert_used    = 0
0.00.041.377 I print_info: causal attn      = 1
0.00.041.377 I print_info: pooling type     = 0
0.00.041.377 I print_info: rope type        = 2
0.00.041.378 I print_info: rope scaling     = linear
0.00.041.379 I print_info: freq_base_train  = 10000.0
0.00.041.380 I print_info: freq_scale_train = 1
0.00.041.380 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.380 I print_info: rope_finetuned   = unknown
0.00.041.380 I print_info: ssm_d_conv       = 0
0.00.041.380 I print_info: ssm_d_inner      = 0
0.00.041.380 I print_info: ssm_d_state      = 0
0.00.041.380 I print_info: ssm_dt_rank      = 0
0.00.041.381 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.381 I print_info: model type       = 1.4B
0.00.041.381 I print_info: model params     = 1.41 B
0.00.041.381 I print_info: general.name     = 1.4B
0.00.041.382 I print_info: vocab type       = BPE
0.00.041.382 I print_info: n_vocab          = 50304
0.00.041.382 I print_info: n_merges         = 50009
0.00.041.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.383 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.383 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.383 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.384 I print_info: LF token         = 187 ''
0.00.041.384 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.384 I print_info: max token length = 1024
0.00.041.385 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.736.094 I load_tensors: offloading 24 repeating layers to GPU
0.00.736.106 I load_tensors: offloading output layer to GPU
0.00.736.106 I load_tensors: offloaded 25/25 layers to GPU
0.00.736.141 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.736.143 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.737.563 I llama_init_from_model: n_seq_max     = 1
0.00.737.566 I llama_init_from_model: n_ctx         = 2048
0.00.737.567 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.737.568 I llama_init_from_model: n_batch       = 2048
0.00.737.568 I llama_init_from_model: n_ubatch      = 512
0.00.737.569 I llama_init_from_model: flash_attn    = 0
0.00.737.571 I llama_init_from_model: freq_base     = 10000.0
0.00.737.572 I llama_init_from_model: freq_scale    = 1
0.00.737.575 I ggml_metal_init: allocating
0.00.737.698 I ggml_metal_init: found device: Apple M4
0.00.737.715 I ggml_metal_init: picking default device: Apple M4
0.00.739.811 I ggml_metal_init: using embedded metal library
0.00.746.297 I ggml_metal_init: GPU name:   Apple M4
0.00.746.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.303 I ggml_metal_init: simdgroup reduction   = true
0.00.746.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.304 I ggml_metal_init: has residency sets    = true
0.00.746.304 I ggml_metal_init: has bfloat            = true
0.00.746.304 I ggml_metal_init: use bfloat            = true
0.00.746.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.763.813 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.818.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.818.745 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.818.779 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.822.916 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.822.919 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.822.919 I llama_init_from_model: graph nodes  = 967
0.00.822.919 I llama_init_from_model: graph splits = 2
0.00.822.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.823.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.823.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.878.593 I main: llama threadpool init, n_threads = 4
0.00.878.638 I 
0.00.878.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.878.662 I 
0.00.878.813 I sampler seed: 1234
0.00.878.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.878.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.878.828 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.878.828 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.669.485 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.669.486 I llama_perf_context_print:        load time =     866.50 ms
0.01.669.486 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.18 tokens per second)
0.01.669.487 I llama_perf_context_print:        eval time =     744.60 ms /    63 runs   (   11.82 ms per token,    84.61 tokens per second)
0.01.669.487 I llama_perf_context_print:       total time =     791.62 ms /    70 tokens
0.01.669.721 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.109s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.284 I llama_model_loader: - type  f32:  194 tensors
0.00.028.284 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.284 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.285 I print_info: file format = GGUF V3 (latest)
0.00.028.285 I print_info: file type   = Q5_0
0.00.028.287 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.535 I load: special tokens cache size = 25
0.00.042.593 I load: token to piece cache size = 0.2984 MB
0.00.042.598 I print_info: arch             = gptneox
0.00.042.598 I print_info: vocab_only       = 0
0.00.042.598 I print_info: n_ctx_train      = 2048
0.00.042.598 I print_info: n_embd           = 2048
0.00.042.599 I print_info: n_layer          = 24
0.00.042.603 I print_info: n_head           = 16
0.00.042.604 I print_info: n_head_kv        = 16
0.00.042.604 I print_info: n_rot            = 32
0.00.042.604 I print_info: n_swa            = 0
0.00.042.604 I print_info: n_embd_head_k    = 128
0.00.042.604 I print_info: n_embd_head_v    = 128
0.00.042.605 I print_info: n_gqa            = 1
0.00.042.606 I print_info: n_embd_k_gqa     = 2048
0.00.042.607 I print_info: n_embd_v_gqa     = 2048
0.00.042.607 I print_info: f_norm_eps       = 1.0e-05
0.00.042.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.608 I print_info: f_logit_scale    = 0.0e+00
0.00.042.609 I print_info: n_ff             = 8192
0.00.042.609 I print_info: n_expert         = 0
0.00.042.609 I print_info: n_expert_used    = 0
0.00.042.610 I print_info: causal attn      = 1
0.00.042.610 I print_info: pooling type     = 0
0.00.042.610 I print_info: rope type        = 2
0.00.042.610 I print_info: rope scaling     = linear
0.00.042.610 I print_info: freq_base_train  = 10000.0
0.00.042.611 I print_info: freq_scale_train = 1
0.00.042.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.611 I print_info: rope_finetuned   = unknown
0.00.042.611 I print_info: ssm_d_conv       = 0
0.00.042.612 I print_info: ssm_d_inner      = 0
0.00.042.612 I print_info: ssm_d_state      = 0
0.00.042.615 I print_info: ssm_dt_rank      = 0
0.00.042.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.615 I print_info: model type       = 1.4B
0.00.042.616 I print_info: model params     = 1.41 B
0.00.042.617 I print_info: general.name     = 1.4B
0.00.042.618 I print_info: vocab type       = BPE
0.00.042.618 I print_info: n_vocab          = 50304
0.00.042.618 I print_info: n_merges         = 50009
0.00.042.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.619 I print_info: LF token         = 187 ''
0.00.042.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.620 I print_info: max token length = 1024
0.00.042.620 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.733.350 I load_tensors: offloading 24 repeating layers to GPU
0.00.733.365 I load_tensors: offloading output layer to GPU
0.00.733.366 I load_tensors: offloaded 25/25 layers to GPU
0.00.733.399 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.733.401 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.734.988 I llama_init_from_model: n_seq_max     = 1
0.00.734.990 I llama_init_from_model: n_ctx         = 128
0.00.734.991 I llama_init_from_model: n_ctx_per_seq = 128
0.00.734.991 I llama_init_from_model: n_batch       = 128
0.00.734.992 I llama_init_from_model: n_ubatch      = 128
0.00.734.992 I llama_init_from_model: flash_attn    = 0
0.00.734.995 I llama_init_from_model: freq_base     = 10000.0
0.00.734.996 I llama_init_from_model: freq_scale    = 1
0.00.734.996 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.735.003 I ggml_metal_init: allocating
0.00.735.086 I ggml_metal_init: found device: Apple M4
0.00.735.100 I ggml_metal_init: picking default device: Apple M4
0.00.736.903 I ggml_metal_init: using embedded metal library
0.00.743.550 I ggml_metal_init: GPU name:   Apple M4
0.00.743.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.743.557 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.743.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.743.559 I ggml_metal_init: simdgroup reduction   = true
0.00.743.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.743.559 I ggml_metal_init: has residency sets    = true
0.00.743.560 I ggml_metal_init: has bfloat            = true
0.00.743.560 I ggml_metal_init: use bfloat            = true
0.00.743.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.743.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.761.404 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.968 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.764.972 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.765.012 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.768.381 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.768.383 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.768.384 I llama_init_from_model: graph nodes  = 967
0.00.768.384 I llama_init_from_model: graph splits = 2
0.00.768.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.768.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.897 I 
0.00.797.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.980 I perplexity: tokenizing the input ..
0.00.805.424 I perplexity: tokenization took 7.442 ms
0.00.805.432 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.953.792 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.955.132 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.955.155 I llama_perf_context_print:        load time =     788.06 ms
0.00.955.156 I llama_perf_context_print: prompt eval time =     147.48 ms /   128 tokens (    1.15 ms per token,   867.92 tokens per second)
0.00.955.157 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.955.157 I llama_perf_context_print:       total time =     157.26 ms /   129 tokens
0.00.955.510 I ggml_metal_free: deallocating

real	0m0.971s
user	0m0.083s
sys	0m0.133s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.089 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.091 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.092 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.092 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.101 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.618 I llama_model_loader: - type  f32:  194 tensors
0.00.025.618 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.618 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.619 I print_info: file format = GGUF V3 (latest)
0.00.025.619 I print_info: file type   = Q5_1
0.00.025.620 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.403 I load: special tokens cache size = 25
0.00.039.337 I load: token to piece cache size = 0.2984 MB
0.00.039.340 I print_info: arch             = gptneox
0.00.039.341 I print_info: vocab_only       = 0
0.00.039.341 I print_info: n_ctx_train      = 2048
0.00.039.341 I print_info: n_embd           = 2048
0.00.039.341 I print_info: n_layer          = 24
0.00.039.344 I print_info: n_head           = 16
0.00.039.345 I print_info: n_head_kv        = 16
0.00.039.345 I print_info: n_rot            = 32
0.00.039.345 I print_info: n_swa            = 0
0.00.039.347 I print_info: n_embd_head_k    = 128
0.00.039.347 I print_info: n_embd_head_v    = 128
0.00.039.348 I print_info: n_gqa            = 1
0.00.039.349 I print_info: n_embd_k_gqa     = 2048
0.00.039.354 I print_info: n_embd_v_gqa     = 2048
0.00.039.354 I print_info: f_norm_eps       = 1.0e-05
0.00.039.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.356 I print_info: f_logit_scale    = 0.0e+00
0.00.039.359 I print_info: n_ff             = 8192
0.00.039.360 I print_info: n_expert         = 0
0.00.039.361 I print_info: n_expert_used    = 0
0.00.039.361 I print_info: causal attn      = 1
0.00.039.361 I print_info: pooling type     = 0
0.00.039.362 I print_info: rope type        = 2
0.00.039.363 I print_info: rope scaling     = linear
0.00.039.364 I print_info: freq_base_train  = 10000.0
0.00.039.364 I print_info: freq_scale_train = 1
0.00.039.364 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.364 I print_info: rope_finetuned   = unknown
0.00.039.364 I print_info: ssm_d_conv       = 0
0.00.039.365 I print_info: ssm_d_inner      = 0
0.00.039.365 I print_info: ssm_d_state      = 0
0.00.039.365 I print_info: ssm_dt_rank      = 0
0.00.039.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.365 I print_info: model type       = 1.4B
0.00.039.365 I print_info: model params     = 1.41 B
0.00.039.365 I print_info: general.name     = 1.4B
0.00.039.366 I print_info: vocab type       = BPE
0.00.039.366 I print_info: n_vocab          = 50304
0.00.039.366 I print_info: n_merges         = 50009
0.00.039.367 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: LF token         = 187 ''
0.00.039.370 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: max token length = 1024
0.00.039.370 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.624.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.624.660 I load_tensors: offloading output layer to GPU
0.00.624.661 I load_tensors: offloaded 25/25 layers to GPU
0.00.624.694 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.624.696 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.626.199 I llama_init_from_model: n_seq_max     = 1
0.00.626.202 I llama_init_from_model: n_ctx         = 2048
0.00.626.202 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.203 I llama_init_from_model: n_batch       = 2048
0.00.626.203 I llama_init_from_model: n_ubatch      = 512
0.00.626.203 I llama_init_from_model: flash_attn    = 0
0.00.626.205 I llama_init_from_model: freq_base     = 10000.0
0.00.626.205 I llama_init_from_model: freq_scale    = 1
0.00.626.206 I ggml_metal_init: allocating
0.00.626.224 I ggml_metal_init: found device: Apple M4
0.00.626.234 I ggml_metal_init: picking default device: Apple M4
0.00.627.736 I ggml_metal_init: using embedded metal library
0.00.633.901 I ggml_metal_init: GPU name:   Apple M4
0.00.633.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.906 I ggml_metal_init: simdgroup reduction   = true
0.00.633.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.906 I ggml_metal_init: has residency sets    = true
0.00.633.907 I ggml_metal_init: has bfloat            = true
0.00.633.907 I ggml_metal_init: use bfloat            = true
0.00.633.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.669 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.165 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.174 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.258 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.432 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.711.434 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.711.434 I llama_init_from_model: graph nodes  = 967
0.00.711.434 I llama_init_from_model: graph splits = 2
0.00.711.439 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.711.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.597 I main: llama threadpool init, n_threads = 4
0.00.770.643 I 
0.00.770.665 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.665 I 
0.00.770.810 I sampler seed: 1234
0.00.770.815 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.834 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.834 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.622.840 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.622.841 I llama_perf_context_print:        load time =     761.15 ms
0.01.622.842 I llama_perf_context_print: prompt eval time =      51.90 ms /     7 tokens (    7.41 ms per token,   134.88 tokens per second)
0.01.622.842 I llama_perf_context_print:        eval time =     797.28 ms /    63 runs   (   12.66 ms per token,    79.02 tokens per second)
0.01.622.843 I llama_perf_context_print:       total time =     852.96 ms /    70 tokens
0.01.623.103 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.107s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.280 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.511 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.513 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.514 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.514 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.516 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.039 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.040 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.042 I llama_model_loader: - type  f32:  194 tensors
0.00.025.042 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.043 I print_info: file format = GGUF V3 (latest)
0.00.025.044 I print_info: file type   = Q5_1
0.00.025.045 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.336 I load: special tokens cache size = 25
0.00.039.364 I load: token to piece cache size = 0.2984 MB
0.00.039.368 I print_info: arch             = gptneox
0.00.039.368 I print_info: vocab_only       = 0
0.00.039.368 I print_info: n_ctx_train      = 2048
0.00.039.369 I print_info: n_embd           = 2048
0.00.039.369 I print_info: n_layer          = 24
0.00.039.373 I print_info: n_head           = 16
0.00.039.374 I print_info: n_head_kv        = 16
0.00.039.374 I print_info: n_rot            = 32
0.00.039.375 I print_info: n_swa            = 0
0.00.039.375 I print_info: n_embd_head_k    = 128
0.00.039.375 I print_info: n_embd_head_v    = 128
0.00.039.376 I print_info: n_gqa            = 1
0.00.039.377 I print_info: n_embd_k_gqa     = 2048
0.00.039.377 I print_info: n_embd_v_gqa     = 2048
0.00.039.378 I print_info: f_norm_eps       = 1.0e-05
0.00.039.378 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.378 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.378 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.379 I print_info: f_logit_scale    = 0.0e+00
0.00.039.379 I print_info: n_ff             = 8192
0.00.039.379 I print_info: n_expert         = 0
0.00.039.379 I print_info: n_expert_used    = 0
0.00.039.380 I print_info: causal attn      = 1
0.00.039.383 I print_info: pooling type     = 0
0.00.039.383 I print_info: rope type        = 2
0.00.039.383 I print_info: rope scaling     = linear
0.00.039.384 I print_info: freq_base_train  = 10000.0
0.00.039.384 I print_info: freq_scale_train = 1
0.00.039.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.384 I print_info: rope_finetuned   = unknown
0.00.039.384 I print_info: ssm_d_conv       = 0
0.00.039.385 I print_info: ssm_d_inner      = 0
0.00.039.385 I print_info: ssm_d_state      = 0
0.00.039.385 I print_info: ssm_dt_rank      = 0
0.00.039.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.385 I print_info: model type       = 1.4B
0.00.039.387 I print_info: model params     = 1.41 B
0.00.039.387 I print_info: general.name     = 1.4B
0.00.039.387 I print_info: vocab type       = BPE
0.00.039.387 I print_info: n_vocab          = 50304
0.00.039.388 I print_info: n_merges         = 50009
0.00.039.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.388 I print_info: LF token         = 187 ''
0.00.039.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.389 I print_info: max token length = 1024
0.00.039.389 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.196 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.207 I load_tensors: offloading output layer to GPU
0.00.638.208 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.236 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.638.238 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.639.728 I llama_init_from_model: n_seq_max     = 1
0.00.639.732 I llama_init_from_model: n_ctx         = 128
0.00.639.733 I llama_init_from_model: n_ctx_per_seq = 128
0.00.639.734 I llama_init_from_model: n_batch       = 128
0.00.639.734 I llama_init_from_model: n_ubatch      = 128
0.00.639.734 I llama_init_from_model: flash_attn    = 0
0.00.639.735 I llama_init_from_model: freq_base     = 10000.0
0.00.639.736 I llama_init_from_model: freq_scale    = 1
0.00.639.736 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.639.739 I ggml_metal_init: allocating
0.00.639.793 I ggml_metal_init: found device: Apple M4
0.00.639.806 I ggml_metal_init: picking default device: Apple M4
0.00.641.603 I ggml_metal_init: using embedded metal library
0.00.648.067 I ggml_metal_init: GPU name:   Apple M4
0.00.648.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.073 I ggml_metal_init: simdgroup reduction   = true
0.00.648.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.074 I ggml_metal_init: has residency sets    = true
0.00.648.074 I ggml_metal_init: has bfloat            = true
0.00.648.074 I ggml_metal_init: use bfloat            = true
0.00.648.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.661 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.279 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.669.282 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.669.342 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.574 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.575 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.576 I llama_init_from_model: graph nodes  = 967
0.00.672.576 I llama_init_from_model: graph splits = 2
0.00.672.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.887 I 
0.00.705.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.970 I perplexity: tokenizing the input ..
0.00.713.246 I perplexity: tokenization took 7.274 ms
0.00.713.260 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.860.315 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.861.656 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.861.680 I llama_perf_context_print:        load time =     696.60 ms
0.00.861.681 I llama_perf_context_print: prompt eval time =     146.06 ms /   128 tokens (    1.14 ms per token,   876.33 tokens per second)
0.00.861.681 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.682 I llama_perf_context_print:       total time =     155.80 ms /   129 tokens
0.00.862.058 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.080s
sys	0m0.143s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.581 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.740 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.243 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.244 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.244 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.244 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.245 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.245 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.245 I llama_model_loader: - type  f32:  194 tensors
0.00.026.246 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.246 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.246 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.247 I print_info: file format = GGUF V3 (latest)
0.00.026.247 I print_info: file type   = Q2_K - Medium
0.00.026.248 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.957 I load: special tokens cache size = 25
0.00.039.889 I load: token to piece cache size = 0.2984 MB
0.00.039.892 I print_info: arch             = gptneox
0.00.039.892 I print_info: vocab_only       = 0
0.00.039.892 I print_info: n_ctx_train      = 2048
0.00.039.893 I print_info: n_embd           = 2048
0.00.039.893 I print_info: n_layer          = 24
0.00.039.896 I print_info: n_head           = 16
0.00.039.896 I print_info: n_head_kv        = 16
0.00.039.897 I print_info: n_rot            = 32
0.00.039.897 I print_info: n_swa            = 0
0.00.039.897 I print_info: n_embd_head_k    = 128
0.00.039.897 I print_info: n_embd_head_v    = 128
0.00.039.898 I print_info: n_gqa            = 1
0.00.039.899 I print_info: n_embd_k_gqa     = 2048
0.00.039.899 I print_info: n_embd_v_gqa     = 2048
0.00.039.901 I print_info: f_norm_eps       = 1.0e-05
0.00.039.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.902 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.902 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.902 I print_info: f_logit_scale    = 0.0e+00
0.00.039.903 I print_info: n_ff             = 8192
0.00.039.903 I print_info: n_expert         = 0
0.00.039.903 I print_info: n_expert_used    = 0
0.00.039.903 I print_info: causal attn      = 1
0.00.039.903 I print_info: pooling type     = 0
0.00.039.904 I print_info: rope type        = 2
0.00.039.904 I print_info: rope scaling     = linear
0.00.039.904 I print_info: freq_base_train  = 10000.0
0.00.039.905 I print_info: freq_scale_train = 1
0.00.039.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.905 I print_info: rope_finetuned   = unknown
0.00.039.905 I print_info: ssm_d_conv       = 0
0.00.039.905 I print_info: ssm_d_inner      = 0
0.00.039.905 I print_info: ssm_d_state      = 0
0.00.039.906 I print_info: ssm_dt_rank      = 0
0.00.039.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.906 I print_info: model type       = 1.4B
0.00.039.907 I print_info: model params     = 1.41 B
0.00.039.907 I print_info: general.name     = 1.4B
0.00.039.907 I print_info: vocab type       = BPE
0.00.039.907 I print_info: n_vocab          = 50304
0.00.039.908 I print_info: n_merges         = 50009
0.00.039.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.909 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.909 I print_info: LF token         = 187 ''
0.00.039.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.909 I print_info: max token length = 1024
0.00.039.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.358.535 I load_tensors: offloading 24 repeating layers to GPU
0.00.358.553 I load_tensors: offloading output layer to GPU
0.00.358.554 I load_tensors: offloaded 25/25 layers to GPU
0.00.358.590 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.358.592 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.359.975 I llama_init_from_model: n_seq_max     = 1
0.00.359.979 I llama_init_from_model: n_ctx         = 2048
0.00.359.980 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.359.980 I llama_init_from_model: n_batch       = 2048
0.00.359.981 I llama_init_from_model: n_ubatch      = 512
0.00.359.981 I llama_init_from_model: flash_attn    = 0
0.00.359.983 I llama_init_from_model: freq_base     = 10000.0
0.00.359.984 I llama_init_from_model: freq_scale    = 1
0.00.359.986 I ggml_metal_init: allocating
0.00.360.090 I ggml_metal_init: found device: Apple M4
0.00.360.105 I ggml_metal_init: picking default device: Apple M4
0.00.362.129 I ggml_metal_init: using embedded metal library
0.00.368.159 I ggml_metal_init: GPU name:   Apple M4
0.00.368.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.368.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.368.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.368.181 I ggml_metal_init: simdgroup reduction   = true
0.00.368.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.368.182 I ggml_metal_init: has residency sets    = true
0.00.368.182 I ggml_metal_init: has bfloat            = true
0.00.368.182 I ggml_metal_init: use bfloat            = true
0.00.368.185 I ggml_metal_init: hasUnifiedMemory      = true
0.00.368.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.596 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.460.008 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.460.033 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.460.129 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.468.154 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.468.160 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.468.160 I llama_init_from_model: graph nodes  = 967
0.00.468.161 I llama_init_from_model: graph splits = 2
0.00.468.170 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.468.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.468.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.322 I main: llama threadpool init, n_threads = 4
0.00.524.363 I 
0.00.524.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.386 I 
0.00.524.523 I sampler seed: 1234
0.00.524.528 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.524.547 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.524.547 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.524.548 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.211.227 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.211.228 I llama_perf_context_print:        load time =     513.00 ms
0.01.211.229 I llama_perf_context_print: prompt eval time =      42.09 ms /     7 tokens (    6.01 ms per token,   166.29 tokens per second)
0.01.211.230 I llama_perf_context_print:        eval time =     641.99 ms /    63 runs   (   10.19 ms per token,    98.13 tokens per second)
0.01.211.230 I llama_perf_context_print:       total time =     687.64 ms /    70 tokens
0.01.211.475 I ggml_metal_free: deallocating

real	0m1.228s
user	0m0.117s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.691 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.271 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.272 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.273 I llama_model_loader: - type  f32:  194 tensors
0.00.025.273 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.273 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.274 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.274 I print_info: file format = GGUF V3 (latest)
0.00.025.275 I print_info: file type   = Q2_K - Medium
0.00.025.276 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.561 I load: special tokens cache size = 25
0.00.039.666 I load: token to piece cache size = 0.2984 MB
0.00.039.670 I print_info: arch             = gptneox
0.00.039.670 I print_info: vocab_only       = 0
0.00.039.670 I print_info: n_ctx_train      = 2048
0.00.039.670 I print_info: n_embd           = 2048
0.00.039.671 I print_info: n_layer          = 24
0.00.039.675 I print_info: n_head           = 16
0.00.039.676 I print_info: n_head_kv        = 16
0.00.039.676 I print_info: n_rot            = 32
0.00.039.676 I print_info: n_swa            = 0
0.00.039.676 I print_info: n_embd_head_k    = 128
0.00.039.677 I print_info: n_embd_head_v    = 128
0.00.039.677 I print_info: n_gqa            = 1
0.00.039.678 I print_info: n_embd_k_gqa     = 2048
0.00.039.681 I print_info: n_embd_v_gqa     = 2048
0.00.039.681 I print_info: f_norm_eps       = 1.0e-05
0.00.039.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.682 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.682 I print_info: f_logit_scale    = 0.0e+00
0.00.039.683 I print_info: n_ff             = 8192
0.00.039.684 I print_info: n_expert         = 0
0.00.039.684 I print_info: n_expert_used    = 0
0.00.039.684 I print_info: causal attn      = 1
0.00.039.684 I print_info: pooling type     = 0
0.00.039.685 I print_info: rope type        = 2
0.00.039.685 I print_info: rope scaling     = linear
0.00.039.685 I print_info: freq_base_train  = 10000.0
0.00.039.685 I print_info: freq_scale_train = 1
0.00.039.685 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.686 I print_info: rope_finetuned   = unknown
0.00.039.686 I print_info: ssm_d_conv       = 0
0.00.039.686 I print_info: ssm_d_inner      = 0
0.00.039.686 I print_info: ssm_d_state      = 0
0.00.039.686 I print_info: ssm_dt_rank      = 0
0.00.039.686 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.687 I print_info: model type       = 1.4B
0.00.039.687 I print_info: model params     = 1.41 B
0.00.039.687 I print_info: general.name     = 1.4B
0.00.039.688 I print_info: vocab type       = BPE
0.00.039.688 I print_info: n_vocab          = 50304
0.00.039.688 I print_info: n_merges         = 50009
0.00.039.688 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.688 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.689 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.689 I print_info: LF token         = 187 ''
0.00.039.689 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.689 I print_info: max token length = 1024
0.00.039.690 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.344.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.675 I load_tensors: offloading output layer to GPU
0.00.344.676 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.708 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.709 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.462 I llama_init_from_model: n_seq_max     = 1
0.00.346.469 I llama_init_from_model: n_ctx         = 128
0.00.346.470 I llama_init_from_model: n_ctx_per_seq = 128
0.00.346.470 I llama_init_from_model: n_batch       = 128
0.00.346.471 I llama_init_from_model: n_ubatch      = 128
0.00.346.471 I llama_init_from_model: flash_attn    = 0
0.00.346.473 I llama_init_from_model: freq_base     = 10000.0
0.00.346.474 I llama_init_from_model: freq_scale    = 1
0.00.346.474 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.346.476 I ggml_metal_init: allocating
0.00.346.537 I ggml_metal_init: found device: Apple M4
0.00.346.550 I ggml_metal_init: picking default device: Apple M4
0.00.348.318 I ggml_metal_init: using embedded metal library
0.00.353.563 I ggml_metal_init: GPU name:   Apple M4
0.00.353.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.585 I ggml_metal_init: simdgroup reduction   = true
0.00.353.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.586 I ggml_metal_init: has residency sets    = true
0.00.353.586 I ggml_metal_init: has bfloat            = true
0.00.353.586 I ggml_metal_init: use bfloat            = true
0.00.353.589 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.593 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.379.132 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.379.142 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.379.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.382.550 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.382.552 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.382.553 I llama_init_from_model: graph nodes  = 967
0.00.382.553 I llama_init_from_model: graph splits = 2
0.00.382.556 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.382.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.689 I 
0.00.413.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.778 I perplexity: tokenizing the input ..
0.00.418.867 I perplexity: tokenization took 5.088 ms
0.00.418.871 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.550.936 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.268 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.292 I llama_perf_context_print:        load time =     403.78 ms
0.00.552.294 I llama_perf_context_print: prompt eval time =     131.84 ms /   128 tokens (    1.03 ms per token,   970.91 tokens per second)
0.00.552.294 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.295 I llama_perf_context_print:       total time =     138.61 ms /   129 tokens
0.00.552.669 I ggml_metal_free: deallocating

real	0m0.568s
user	0m0.079s
sys	0m0.093s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.278 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.952 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.713 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.440 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.443 I llama_model_loader: - type  f32:  194 tensors
0.00.025.443 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.444 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.444 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.444 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.444 I print_info: file format = GGUF V3 (latest)
0.00.025.445 I print_info: file type   = Q3_K - Medium
0.00.025.446 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.485 I load: special tokens cache size = 25
0.00.039.788 I load: token to piece cache size = 0.2984 MB
0.00.039.791 I print_info: arch             = gptneox
0.00.039.791 I print_info: vocab_only       = 0
0.00.039.791 I print_info: n_ctx_train      = 2048
0.00.039.791 I print_info: n_embd           = 2048
0.00.039.792 I print_info: n_layer          = 24
0.00.039.795 I print_info: n_head           = 16
0.00.039.796 I print_info: n_head_kv        = 16
0.00.039.796 I print_info: n_rot            = 32
0.00.039.796 I print_info: n_swa            = 0
0.00.039.796 I print_info: n_embd_head_k    = 128
0.00.039.796 I print_info: n_embd_head_v    = 128
0.00.039.797 I print_info: n_gqa            = 1
0.00.039.798 I print_info: n_embd_k_gqa     = 2048
0.00.039.799 I print_info: n_embd_v_gqa     = 2048
0.00.039.799 I print_info: f_norm_eps       = 1.0e-05
0.00.039.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.800 I print_info: f_logit_scale    = 0.0e+00
0.00.039.801 I print_info: n_ff             = 8192
0.00.039.802 I print_info: n_expert         = 0
0.00.039.802 I print_info: n_expert_used    = 0
0.00.039.804 I print_info: causal attn      = 1
0.00.039.804 I print_info: pooling type     = 0
0.00.039.804 I print_info: rope type        = 2
0.00.039.804 I print_info: rope scaling     = linear
0.00.039.805 I print_info: freq_base_train  = 10000.0
0.00.039.805 I print_info: freq_scale_train = 1
0.00.039.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.805 I print_info: rope_finetuned   = unknown
0.00.039.806 I print_info: ssm_d_conv       = 0
0.00.039.806 I print_info: ssm_d_inner      = 0
0.00.039.806 I print_info: ssm_d_state      = 0
0.00.039.806 I print_info: ssm_dt_rank      = 0
0.00.039.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.807 I print_info: model type       = 1.4B
0.00.039.807 I print_info: model params     = 1.41 B
0.00.039.807 I print_info: general.name     = 1.4B
0.00.039.808 I print_info: vocab type       = BPE
0.00.039.809 I print_info: n_vocab          = 50304
0.00.039.809 I print_info: n_merges         = 50009
0.00.039.810 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: LF token         = 187 ''
0.00.039.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: max token length = 1024
0.00.039.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.463.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.463.600 I load_tensors: offloading output layer to GPU
0.00.463.601 I load_tensors: offloaded 25/25 layers to GPU
0.00.463.635 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.463.636 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.465.005 I llama_init_from_model: n_seq_max     = 1
0.00.465.013 I llama_init_from_model: n_ctx         = 2048
0.00.465.014 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.465.015 I llama_init_from_model: n_batch       = 2048
0.00.465.015 I llama_init_from_model: n_ubatch      = 512
0.00.465.015 I llama_init_from_model: flash_attn    = 0
0.00.465.018 I llama_init_from_model: freq_base     = 10000.0
0.00.465.019 I llama_init_from_model: freq_scale    = 1
0.00.465.022 I ggml_metal_init: allocating
0.00.465.106 I ggml_metal_init: found device: Apple M4
0.00.465.122 I ggml_metal_init: picking default device: Apple M4
0.00.467.161 I ggml_metal_init: using embedded metal library
0.00.473.178 I ggml_metal_init: GPU name:   Apple M4
0.00.473.194 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.473.195 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.473.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.473.196 I ggml_metal_init: simdgroup reduction   = true
0.00.473.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.473.197 I ggml_metal_init: has residency sets    = true
0.00.473.197 I ggml_metal_init: has bfloat            = true
0.00.473.197 I ggml_metal_init: use bfloat            = true
0.00.473.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.473.207 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.494.824 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.557.511 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.557.519 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.557.556 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.347 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.563.349 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.563.349 I llama_init_from_model: graph nodes  = 967
0.00.563.350 I llama_init_from_model: graph splits = 2
0.00.563.354 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.563.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.563.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.028 I main: llama threadpool init, n_threads = 4
0.00.612.077 I 
0.00.612.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.099 I 
0.00.612.216 I sampler seed: 1234
0.00.612.221 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.242 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.242 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.243 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.379.202 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.379.203 I llama_perf_context_print:        load time =     602.04 ms
0.01.379.204 I llama_perf_context_print: prompt eval time =      46.36 ms /     7 tokens (    6.62 ms per token,   151.00 tokens per second)
0.01.379.204 I llama_perf_context_print:        eval time =     717.64 ms /    63 runs   (   11.39 ms per token,    87.79 tokens per second)
0.01.379.205 I llama_perf_context_print:       total time =     767.88 ms /    70 tokens
0.01.379.459 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.113s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.697 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.028 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.669 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.672 I llama_model_loader: - type  f32:  194 tensors
0.00.024.673 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.673 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.673 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.674 I print_info: file format = GGUF V3 (latest)
0.00.024.675 I print_info: file type   = Q3_K - Medium
0.00.024.676 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.311 I load: special tokens cache size = 25
0.00.039.424 I load: token to piece cache size = 0.2984 MB
0.00.039.428 I print_info: arch             = gptneox
0.00.039.428 I print_info: vocab_only       = 0
0.00.039.429 I print_info: n_ctx_train      = 2048
0.00.039.429 I print_info: n_embd           = 2048
0.00.039.429 I print_info: n_layer          = 24
0.00.039.434 I print_info: n_head           = 16
0.00.039.434 I print_info: n_head_kv        = 16
0.00.039.435 I print_info: n_rot            = 32
0.00.039.435 I print_info: n_swa            = 0
0.00.039.435 I print_info: n_embd_head_k    = 128
0.00.039.435 I print_info: n_embd_head_v    = 128
0.00.039.436 I print_info: n_gqa            = 1
0.00.039.437 I print_info: n_embd_k_gqa     = 2048
0.00.039.437 I print_info: n_embd_v_gqa     = 2048
0.00.039.438 I print_info: f_norm_eps       = 1.0e-05
0.00.039.438 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.438 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.439 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.439 I print_info: f_logit_scale    = 0.0e+00
0.00.039.439 I print_info: n_ff             = 8192
0.00.039.440 I print_info: n_expert         = 0
0.00.039.440 I print_info: n_expert_used    = 0
0.00.039.443 I print_info: causal attn      = 1
0.00.039.443 I print_info: pooling type     = 0
0.00.039.443 I print_info: rope type        = 2
0.00.039.443 I print_info: rope scaling     = linear
0.00.039.444 I print_info: freq_base_train  = 10000.0
0.00.039.444 I print_info: freq_scale_train = 1
0.00.039.444 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.444 I print_info: rope_finetuned   = unknown
0.00.039.444 I print_info: ssm_d_conv       = 0
0.00.039.444 I print_info: ssm_d_inner      = 0
0.00.039.445 I print_info: ssm_d_state      = 0
0.00.039.445 I print_info: ssm_dt_rank      = 0
0.00.039.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.445 I print_info: model type       = 1.4B
0.00.039.445 I print_info: model params     = 1.41 B
0.00.039.446 I print_info: general.name     = 1.4B
0.00.039.446 I print_info: vocab type       = BPE
0.00.039.446 I print_info: n_vocab          = 50304
0.00.039.446 I print_info: n_merges         = 50009
0.00.039.447 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: LF token         = 187 ''
0.00.039.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: max token length = 1024
0.00.039.448 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.447.002 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.017 I load_tensors: offloading output layer to GPU
0.00.447.018 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.065 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.069 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.782 I llama_init_from_model: n_seq_max     = 1
0.00.448.787 I llama_init_from_model: n_ctx         = 128
0.00.448.788 I llama_init_from_model: n_ctx_per_seq = 128
0.00.448.788 I llama_init_from_model: n_batch       = 128
0.00.448.789 I llama_init_from_model: n_ubatch      = 128
0.00.448.789 I llama_init_from_model: flash_attn    = 0
0.00.448.791 I llama_init_from_model: freq_base     = 10000.0
0.00.448.791 I llama_init_from_model: freq_scale    = 1
0.00.448.792 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.448.794 I ggml_metal_init: allocating
0.00.448.878 I ggml_metal_init: found device: Apple M4
0.00.448.892 I ggml_metal_init: picking default device: Apple M4
0.00.450.706 I ggml_metal_init: using embedded metal library
0.00.456.263 I ggml_metal_init: GPU name:   Apple M4
0.00.456.271 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.272 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.273 I ggml_metal_init: simdgroup reduction   = true
0.00.456.273 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.274 I ggml_metal_init: has residency sets    = true
0.00.456.274 I ggml_metal_init: has bfloat            = true
0.00.456.274 I ggml_metal_init: use bfloat            = true
0.00.456.275 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.476.261 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.479.884 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.479.891 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.479.950 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.483.257 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.483.259 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.483.259 I llama_init_from_model: graph nodes  = 967
0.00.483.260 I llama_init_from_model: graph splits = 2
0.00.483.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.483.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.082 I 
0.00.512.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.166 I perplexity: tokenizing the input ..
0.00.517.510 I perplexity: tokenization took 5.343 ms
0.00.517.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.651.202 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.652.550 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.652.575 I llama_perf_context_print:        load time =     503.37 ms
0.00.652.576 I llama_perf_context_print: prompt eval time =     133.46 ms /   128 tokens (    1.04 ms per token,   959.11 tokens per second)
0.00.652.576 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.652.577 I llama_perf_context_print:       total time =     140.50 ms /   129 tokens
0.00.652.953 I ggml_metal_free: deallocating

real	0m0.666s
user	0m0.078s
sys	0m0.110s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.231 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.980 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.985 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.391 I llama_model_loader: - type  f32:  194 tensors
0.00.026.391 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.391 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.392 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.392 I print_info: file format = GGUF V3 (latest)
0.00.026.392 I print_info: file type   = Q4_K - Medium
0.00.026.397 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.085 I load: special tokens cache size = 25
0.00.040.072 I load: token to piece cache size = 0.2984 MB
0.00.040.075 I print_info: arch             = gptneox
0.00.040.075 I print_info: vocab_only       = 0
0.00.040.076 I print_info: n_ctx_train      = 2048
0.00.040.076 I print_info: n_embd           = 2048
0.00.040.076 I print_info: n_layer          = 24
0.00.040.079 I print_info: n_head           = 16
0.00.040.079 I print_info: n_head_kv        = 16
0.00.040.080 I print_info: n_rot            = 32
0.00.040.080 I print_info: n_swa            = 0
0.00.040.080 I print_info: n_embd_head_k    = 128
0.00.040.080 I print_info: n_embd_head_v    = 128
0.00.040.083 I print_info: n_gqa            = 1
0.00.040.083 I print_info: n_embd_k_gqa     = 2048
0.00.040.084 I print_info: n_embd_v_gqa     = 2048
0.00.040.089 I print_info: f_norm_eps       = 1.0e-05
0.00.040.089 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.089 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.090 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.090 I print_info: f_logit_scale    = 0.0e+00
0.00.040.092 I print_info: n_ff             = 8192
0.00.040.092 I print_info: n_expert         = 0
0.00.040.092 I print_info: n_expert_used    = 0
0.00.040.092 I print_info: causal attn      = 1
0.00.040.094 I print_info: pooling type     = 0
0.00.040.094 I print_info: rope type        = 2
0.00.040.094 I print_info: rope scaling     = linear
0.00.040.094 I print_info: freq_base_train  = 10000.0
0.00.040.095 I print_info: freq_scale_train = 1
0.00.040.095 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.095 I print_info: rope_finetuned   = unknown
0.00.040.095 I print_info: ssm_d_conv       = 0
0.00.040.095 I print_info: ssm_d_inner      = 0
0.00.040.095 I print_info: ssm_d_state      = 0
0.00.040.095 I print_info: ssm_dt_rank      = 0
0.00.040.096 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.096 I print_info: model type       = 1.4B
0.00.040.096 I print_info: model params     = 1.41 B
0.00.040.096 I print_info: general.name     = 1.4B
0.00.040.097 I print_info: vocab type       = BPE
0.00.040.097 I print_info: n_vocab          = 50304
0.00.040.097 I print_info: n_merges         = 50009
0.00.040.097 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.098 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.098 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.098 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.098 I print_info: LF token         = 187 ''
0.00.040.099 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.099 I print_info: max token length = 1024
0.00.040.099 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.550.070 I load_tensors: offloading 24 repeating layers to GPU
0.00.550.096 I load_tensors: offloading output layer to GPU
0.00.550.097 I load_tensors: offloaded 25/25 layers to GPU
0.00.550.133 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.550.135 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.551.419 I llama_init_from_model: n_seq_max     = 1
0.00.551.422 I llama_init_from_model: n_ctx         = 2048
0.00.551.423 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.551.423 I llama_init_from_model: n_batch       = 2048
0.00.551.423 I llama_init_from_model: n_ubatch      = 512
0.00.551.424 I llama_init_from_model: flash_attn    = 0
0.00.551.426 I llama_init_from_model: freq_base     = 10000.0
0.00.551.426 I llama_init_from_model: freq_scale    = 1
0.00.551.429 I ggml_metal_init: allocating
0.00.551.505 I ggml_metal_init: found device: Apple M4
0.00.551.520 I ggml_metal_init: picking default device: Apple M4
0.00.553.484 I ggml_metal_init: using embedded metal library
0.00.559.524 I ggml_metal_init: GPU name:   Apple M4
0.00.559.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.532 I ggml_metal_init: simdgroup reduction   = true
0.00.559.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.533 I ggml_metal_init: has residency sets    = true
0.00.559.534 I ggml_metal_init: has bfloat            = true
0.00.559.534 I ggml_metal_init: use bfloat            = true
0.00.559.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.579.066 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.323 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.635.331 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.635.375 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.292 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.640.294 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.640.294 I llama_init_from_model: graph nodes  = 967
0.00.640.294 I llama_init_from_model: graph splits = 2
0.00.640.299 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.640.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.640.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.140 I main: llama threadpool init, n_threads = 4
0.00.688.189 I 
0.00.688.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.213 I 
0.00.688.321 I sampler seed: 1234
0.00.688.326 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.688.346 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.688.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.688.346 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.474.549 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.474.550 I llama_perf_context_print:        load time =     678.15 ms
0.01.474.551 I llama_perf_context_print: prompt eval time =      47.33 ms /     7 tokens (    6.76 ms per token,   147.91 tokens per second)
0.01.474.552 I llama_perf_context_print:        eval time =     736.05 ms /    63 runs   (   11.68 ms per token,    85.59 tokens per second)
0.01.474.552 I llama_perf_context_print:       total time =     787.16 ms /    70 tokens
0.01.474.835 I ggml_metal_free: deallocating

real	0m1.490s
user	0m0.112s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.187 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.191 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.192 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.192 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.197 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.017 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.865 I llama_model_loader: - type  f32:  194 tensors
0.00.024.866 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.866 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.866 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.867 I print_info: file format = GGUF V3 (latest)
0.00.024.870 I print_info: file type   = Q4_K - Medium
0.00.024.871 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.155 I load: special tokens cache size = 25
0.00.039.322 I load: token to piece cache size = 0.2984 MB
0.00.039.326 I print_info: arch             = gptneox
0.00.039.327 I print_info: vocab_only       = 0
0.00.039.327 I print_info: n_ctx_train      = 2048
0.00.039.327 I print_info: n_embd           = 2048
0.00.039.327 I print_info: n_layer          = 24
0.00.039.332 I print_info: n_head           = 16
0.00.039.333 I print_info: n_head_kv        = 16
0.00.039.333 I print_info: n_rot            = 32
0.00.039.333 I print_info: n_swa            = 0
0.00.039.333 I print_info: n_embd_head_k    = 128
0.00.039.333 I print_info: n_embd_head_v    = 128
0.00.039.334 I print_info: n_gqa            = 1
0.00.039.335 I print_info: n_embd_k_gqa     = 2048
0.00.039.335 I print_info: n_embd_v_gqa     = 2048
0.00.039.336 I print_info: f_norm_eps       = 1.0e-05
0.00.039.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.337 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.337 I print_info: f_logit_scale    = 0.0e+00
0.00.039.337 I print_info: n_ff             = 8192
0.00.039.338 I print_info: n_expert         = 0
0.00.039.338 I print_info: n_expert_used    = 0
0.00.039.338 I print_info: causal attn      = 1
0.00.039.338 I print_info: pooling type     = 0
0.00.039.338 I print_info: rope type        = 2
0.00.039.338 I print_info: rope scaling     = linear
0.00.039.339 I print_info: freq_base_train  = 10000.0
0.00.039.339 I print_info: freq_scale_train = 1
0.00.039.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.339 I print_info: rope_finetuned   = unknown
0.00.039.339 I print_info: ssm_d_conv       = 0
0.00.039.339 I print_info: ssm_d_inner      = 0
0.00.039.340 I print_info: ssm_d_state      = 0
0.00.039.340 I print_info: ssm_dt_rank      = 0
0.00.039.341 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.342 I print_info: model type       = 1.4B
0.00.039.342 I print_info: model params     = 1.41 B
0.00.039.342 I print_info: general.name     = 1.4B
0.00.039.343 I print_info: vocab type       = BPE
0.00.039.343 I print_info: n_vocab          = 50304
0.00.039.343 I print_info: n_merges         = 50009
0.00.039.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: LF token         = 187 ''
0.00.039.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: max token length = 1024
0.00.039.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.081 I load_tensors: offloading output layer to GPU
0.00.538.081 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.111 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.112 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.539.820 I llama_init_from_model: n_seq_max     = 1
0.00.539.824 I llama_init_from_model: n_ctx         = 128
0.00.539.824 I llama_init_from_model: n_ctx_per_seq = 128
0.00.539.825 I llama_init_from_model: n_batch       = 128
0.00.539.825 I llama_init_from_model: n_ubatch      = 128
0.00.539.826 I llama_init_from_model: flash_attn    = 0
0.00.539.827 I llama_init_from_model: freq_base     = 10000.0
0.00.539.828 I llama_init_from_model: freq_scale    = 1
0.00.539.828 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.539.830 I ggml_metal_init: allocating
0.00.539.933 I ggml_metal_init: found device: Apple M4
0.00.539.947 I ggml_metal_init: picking default device: Apple M4
0.00.542.114 I ggml_metal_init: using embedded metal library
0.00.548.996 I ggml_metal_init: GPU name:   Apple M4
0.00.549.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.006 I ggml_metal_init: simdgroup reduction   = true
0.00.549.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.006 I ggml_metal_init: has residency sets    = true
0.00.549.007 I ggml_metal_init: has bfloat            = true
0.00.549.007 I ggml_metal_init: use bfloat            = true
0.00.549.008 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.567.065 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.570.776 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.570.784 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.570.828 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.574.118 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.574.120 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.574.120 I llama_init_from_model: graph nodes  = 967
0.00.574.121 I llama_init_from_model: graph splits = 2
0.00.574.123 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.574.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.456 I 
0.00.600.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.545 I perplexity: tokenizing the input ..
0.00.607.712 I perplexity: tokenization took 7.166 ms
0.00.607.716 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.640 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.742.159 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.742.187 I llama_perf_context_print:        load time =     591.53 ms
0.00.742.188 I llama_perf_context_print: prompt eval time =     132.50 ms /   128 tokens (    1.04 ms per token,   966.04 tokens per second)
0.00.742.189 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.742.189 I llama_perf_context_print:       total time =     141.74 ms /   129 tokens
0.00.742.601 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.079s
sys	0m0.134s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.392 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.329 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.343 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.889 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.891 I llama_model_loader: - type  f32:  194 tensors
0.00.026.891 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.892 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.892 I print_info: file format = GGUF V3 (latest)
0.00.026.893 I print_info: file type   = Q5_K - Medium
0.00.026.894 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.967 I load: special tokens cache size = 25
0.00.041.142 I load: token to piece cache size = 0.2984 MB
0.00.041.144 I print_info: arch             = gptneox
0.00.041.145 I print_info: vocab_only       = 0
0.00.041.145 I print_info: n_ctx_train      = 2048
0.00.041.145 I print_info: n_embd           = 2048
0.00.041.145 I print_info: n_layer          = 24
0.00.041.148 I print_info: n_head           = 16
0.00.041.149 I print_info: n_head_kv        = 16
0.00.041.149 I print_info: n_rot            = 32
0.00.041.150 I print_info: n_swa            = 0
0.00.041.150 I print_info: n_embd_head_k    = 128
0.00.041.150 I print_info: n_embd_head_v    = 128
0.00.041.151 I print_info: n_gqa            = 1
0.00.041.152 I print_info: n_embd_k_gqa     = 2048
0.00.041.152 I print_info: n_embd_v_gqa     = 2048
0.00.041.153 I print_info: f_norm_eps       = 1.0e-05
0.00.041.153 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.154 I print_info: f_logit_scale    = 0.0e+00
0.00.041.155 I print_info: n_ff             = 8192
0.00.041.155 I print_info: n_expert         = 0
0.00.041.155 I print_info: n_expert_used    = 0
0.00.041.155 I print_info: causal attn      = 1
0.00.041.155 I print_info: pooling type     = 0
0.00.041.156 I print_info: rope type        = 2
0.00.041.156 I print_info: rope scaling     = linear
0.00.041.156 I print_info: freq_base_train  = 10000.0
0.00.041.156 I print_info: freq_scale_train = 1
0.00.041.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.158 I print_info: rope_finetuned   = unknown
0.00.041.158 I print_info: ssm_d_conv       = 0
0.00.041.159 I print_info: ssm_d_inner      = 0
0.00.041.160 I print_info: ssm_d_state      = 0
0.00.041.160 I print_info: ssm_dt_rank      = 0
0.00.041.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.160 I print_info: model type       = 1.4B
0.00.041.161 I print_info: model params     = 1.41 B
0.00.041.161 I print_info: general.name     = 1.4B
0.00.041.161 I print_info: vocab type       = BPE
0.00.041.161 I print_info: n_vocab          = 50304
0.00.041.161 I print_info: n_merges         = 50009
0.00.041.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.162 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.162 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.166 I print_info: LF token         = 187 ''
0.00.041.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.167 I print_info: max token length = 1024
0.00.041.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.097 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.113 I load_tensors: offloading output layer to GPU
0.00.610.113 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.148 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.610.149 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.611.630 I llama_init_from_model: n_seq_max     = 1
0.00.611.633 I llama_init_from_model: n_ctx         = 2048
0.00.611.634 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.611.634 I llama_init_from_model: n_batch       = 2048
0.00.611.635 I llama_init_from_model: n_ubatch      = 512
0.00.611.635 I llama_init_from_model: flash_attn    = 0
0.00.611.638 I llama_init_from_model: freq_base     = 10000.0
0.00.611.638 I llama_init_from_model: freq_scale    = 1
0.00.611.641 I ggml_metal_init: allocating
0.00.611.720 I ggml_metal_init: found device: Apple M4
0.00.611.736 I ggml_metal_init: picking default device: Apple M4
0.00.613.736 I ggml_metal_init: using embedded metal library
0.00.620.519 I ggml_metal_init: GPU name:   Apple M4
0.00.620.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.528 I ggml_metal_init: simdgroup reduction   = true
0.00.620.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.529 I ggml_metal_init: has residency sets    = true
0.00.620.529 I ggml_metal_init: has bfloat            = true
0.00.620.529 I ggml_metal_init: use bfloat            = true
0.00.620.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.768 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.067 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.074 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.109 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.621 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.624 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.624 I llama_init_from_model: graph nodes  = 967
0.00.704.624 I llama_init_from_model: graph splits = 2
0.00.704.630 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.249 I main: llama threadpool init, n_threads = 4
0.00.761.300 I 
0.00.761.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.323 I 
0.00.761.448 I sampler seed: 1234
0.00.761.454 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.497 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.499 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.499 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.638.843 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.638.844 I llama_perf_context_print:        load time =     750.13 ms
0.01.638.845 I llama_perf_context_print: prompt eval time =      53.11 ms /     7 tokens (    7.59 ms per token,   131.81 tokens per second)
0.01.638.845 I llama_perf_context_print:        eval time =     821.34 ms /    63 runs   (   13.04 ms per token,    76.70 tokens per second)
0.01.638.846 I llama_perf_context_print:       total time =     878.32 ms /    70 tokens
0.01.639.109 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.113s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.721 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.257 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.260 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.261 I llama_model_loader: - type  f32:  194 tensors
0.00.025.261 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.261 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.262 I print_info: file format = GGUF V3 (latest)
0.00.025.262 I print_info: file type   = Q5_K - Medium
0.00.025.264 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.099 I load: special tokens cache size = 25
0.00.038.873 I load: token to piece cache size = 0.2984 MB
0.00.038.877 I print_info: arch             = gptneox
0.00.038.878 I print_info: vocab_only       = 0
0.00.038.878 I print_info: n_ctx_train      = 2048
0.00.038.878 I print_info: n_embd           = 2048
0.00.038.878 I print_info: n_layer          = 24
0.00.038.882 I print_info: n_head           = 16
0.00.038.883 I print_info: n_head_kv        = 16
0.00.038.883 I print_info: n_rot            = 32
0.00.038.884 I print_info: n_swa            = 0
0.00.038.884 I print_info: n_embd_head_k    = 128
0.00.038.884 I print_info: n_embd_head_v    = 128
0.00.038.885 I print_info: n_gqa            = 1
0.00.038.887 I print_info: n_embd_k_gqa     = 2048
0.00.038.888 I print_info: n_embd_v_gqa     = 2048
0.00.038.888 I print_info: f_norm_eps       = 1.0e-05
0.00.038.889 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.889 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.889 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.889 I print_info: f_logit_scale    = 0.0e+00
0.00.038.890 I print_info: n_ff             = 8192
0.00.038.890 I print_info: n_expert         = 0
0.00.038.890 I print_info: n_expert_used    = 0
0.00.038.890 I print_info: causal attn      = 1
0.00.038.890 I print_info: pooling type     = 0
0.00.038.891 I print_info: rope type        = 2
0.00.038.891 I print_info: rope scaling     = linear
0.00.038.891 I print_info: freq_base_train  = 10000.0
0.00.038.891 I print_info: freq_scale_train = 1
0.00.038.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.892 I print_info: rope_finetuned   = unknown
0.00.038.892 I print_info: ssm_d_conv       = 0
0.00.038.892 I print_info: ssm_d_inner      = 0
0.00.038.892 I print_info: ssm_d_state      = 0
0.00.038.892 I print_info: ssm_dt_rank      = 0
0.00.038.892 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.893 I print_info: model type       = 1.4B
0.00.038.893 I print_info: model params     = 1.41 B
0.00.038.893 I print_info: general.name     = 1.4B
0.00.038.894 I print_info: vocab type       = BPE
0.00.038.896 I print_info: n_vocab          = 50304
0.00.038.896 I print_info: n_merges         = 50009
0.00.038.896 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: LF token         = 187 ''
0.00.038.897 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: max token length = 1024
0.00.038.897 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.662 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.677 I load_tensors: offloading output layer to GPU
0.00.587.678 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.710 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.711 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.438 I llama_init_from_model: n_seq_max     = 1
0.00.589.441 I llama_init_from_model: n_ctx         = 128
0.00.589.442 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.442 I llama_init_from_model: n_batch       = 128
0.00.589.443 I llama_init_from_model: n_ubatch      = 128
0.00.589.443 I llama_init_from_model: flash_attn    = 0
0.00.589.445 I llama_init_from_model: freq_base     = 10000.0
0.00.589.445 I llama_init_from_model: freq_scale    = 1
0.00.589.446 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.448 I ggml_metal_init: allocating
0.00.589.504 I ggml_metal_init: found device: Apple M4
0.00.589.523 I ggml_metal_init: picking default device: Apple M4
0.00.591.078 I ggml_metal_init: using embedded metal library
0.00.597.301 I ggml_metal_init: GPU name:   Apple M4
0.00.597.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.307 I ggml_metal_init: simdgroup reduction   = true
0.00.597.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.308 I ggml_metal_init: has residency sets    = true
0.00.597.308 I ggml_metal_init: has bfloat            = true
0.00.597.308 I ggml_metal_init: use bfloat            = true
0.00.597.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.948 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.519 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.522 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.563 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.009 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.011 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.011 I llama_init_from_model: graph nodes  = 967
0.00.621.012 I llama_init_from_model: graph splits = 2
0.00.621.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.946 I 
0.00.656.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.036 I perplexity: tokenizing the input ..
0.00.663.015 I perplexity: tokenization took 6.975 ms
0.00.663.021 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.527 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.876 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.904 I llama_perf_context_print:        load time =     645.92 ms
0.00.801.905 I llama_perf_context_print: prompt eval time =     136.60 ms /   128 tokens (    1.07 ms per token,   937.03 tokens per second)
0.00.801.905 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.906 I llama_perf_context_print:       total time =     145.96 ms /   129 tokens
0.00.802.305 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.077s
sys	0m0.132s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.287 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.262 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.266 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.268 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.269 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.269 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.270 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.272 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.272 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.757 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.758 I llama_model_loader: - type  f32:  194 tensors
0.00.025.758 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.758 I print_info: file format = GGUF V3 (latest)
0.00.025.759 I print_info: file type   = Q6_K
0.00.025.760 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.825 I load: special tokens cache size = 25
0.00.039.681 I load: token to piece cache size = 0.2984 MB
0.00.039.684 I print_info: arch             = gptneox
0.00.039.685 I print_info: vocab_only       = 0
0.00.039.685 I print_info: n_ctx_train      = 2048
0.00.039.685 I print_info: n_embd           = 2048
0.00.039.685 I print_info: n_layer          = 24
0.00.039.688 I print_info: n_head           = 16
0.00.039.689 I print_info: n_head_kv        = 16
0.00.039.689 I print_info: n_rot            = 32
0.00.039.690 I print_info: n_swa            = 0
0.00.039.690 I print_info: n_embd_head_k    = 128
0.00.039.690 I print_info: n_embd_head_v    = 128
0.00.039.691 I print_info: n_gqa            = 1
0.00.039.692 I print_info: n_embd_k_gqa     = 2048
0.00.039.692 I print_info: n_embd_v_gqa     = 2048
0.00.039.693 I print_info: f_norm_eps       = 1.0e-05
0.00.039.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.695 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.695 I print_info: f_logit_scale    = 0.0e+00
0.00.039.696 I print_info: n_ff             = 8192
0.00.039.698 I print_info: n_expert         = 0
0.00.039.698 I print_info: n_expert_used    = 0
0.00.039.698 I print_info: causal attn      = 1
0.00.039.699 I print_info: pooling type     = 0
0.00.039.699 I print_info: rope type        = 2
0.00.039.699 I print_info: rope scaling     = linear
0.00.039.699 I print_info: freq_base_train  = 10000.0
0.00.039.700 I print_info: freq_scale_train = 1
0.00.039.700 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.700 I print_info: rope_finetuned   = unknown
0.00.039.700 I print_info: ssm_d_conv       = 0
0.00.039.700 I print_info: ssm_d_inner      = 0
0.00.039.700 I print_info: ssm_d_state      = 0
0.00.039.701 I print_info: ssm_dt_rank      = 0
0.00.039.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.701 I print_info: model type       = 1.4B
0.00.039.701 I print_info: model params     = 1.41 B
0.00.039.701 I print_info: general.name     = 1.4B
0.00.039.702 I print_info: vocab type       = BPE
0.00.039.702 I print_info: n_vocab          = 50304
0.00.039.702 I print_info: n_merges         = 50009
0.00.039.702 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.708 I print_info: LF token         = 187 ''
0.00.039.708 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.708 I print_info: max token length = 1024
0.00.039.708 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.658.971 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.993 I load_tensors: offloading output layer to GPU
0.00.658.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.031 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.659.032 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.660.398 I llama_init_from_model: n_seq_max     = 1
0.00.660.401 I llama_init_from_model: n_ctx         = 2048
0.00.660.401 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.402 I llama_init_from_model: n_batch       = 2048
0.00.660.402 I llama_init_from_model: n_ubatch      = 512
0.00.660.402 I llama_init_from_model: flash_attn    = 0
0.00.660.404 I llama_init_from_model: freq_base     = 10000.0
0.00.660.405 I llama_init_from_model: freq_scale    = 1
0.00.660.408 I ggml_metal_init: allocating
0.00.660.503 I ggml_metal_init: found device: Apple M4
0.00.660.518 I ggml_metal_init: picking default device: Apple M4
0.00.662.185 I ggml_metal_init: using embedded metal library
0.00.668.650 I ggml_metal_init: GPU name:   Apple M4
0.00.668.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.656 I ggml_metal_init: simdgroup reduction   = true
0.00.668.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.657 I ggml_metal_init: has residency sets    = true
0.00.668.657 I ggml_metal_init: has bfloat            = true
0.00.668.657 I ggml_metal_init: use bfloat            = true
0.00.668.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.825 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.033 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.040 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.075 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.622 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.624 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.624 I llama_init_from_model: graph nodes  = 967
0.00.748.625 I llama_init_from_model: graph splits = 2
0.00.748.634 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.759 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.343 I main: llama threadpool init, n_threads = 4
0.00.807.383 I 
0.00.807.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.405 I 
0.00.807.523 I sampler seed: 1234
0.00.807.528 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.560 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.574 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.575 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.730.705 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.730.705 I llama_perf_context_print:        load time =     797.26 ms
0.01.730.706 I llama_perf_context_print: prompt eval time =      57.90 ms /     7 tokens (    8.27 ms per token,   120.91 tokens per second)
0.01.730.707 I llama_perf_context_print:        eval time =     862.33 ms /    63 runs   (   13.69 ms per token,    73.06 tokens per second)
0.01.730.707 I llama_perf_context_print:       total time =     924.15 ms /    70 tokens
0.01.730.930 I ggml_metal_free: deallocating

real	0m1.746s
user	0m0.111s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4766 (651adf4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.836 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.842 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.843 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.845 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.846 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.846 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.849 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.849 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.412 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.414 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.414 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.415 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.416 I llama_model_loader: - type  f32:  194 tensors
0.00.024.416 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.417 I print_info: file format = GGUF V3 (latest)
0.00.024.417 I print_info: file type   = Q6_K
0.00.024.418 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.457 I load: special tokens cache size = 25
0.00.038.635 I load: token to piece cache size = 0.2984 MB
0.00.038.641 I print_info: arch             = gptneox
0.00.038.641 I print_info: vocab_only       = 0
0.00.038.641 I print_info: n_ctx_train      = 2048
0.00.038.642 I print_info: n_embd           = 2048
0.00.038.642 I print_info: n_layer          = 24
0.00.038.646 I print_info: n_head           = 16
0.00.038.646 I print_info: n_head_kv        = 16
0.00.038.648 I print_info: n_rot            = 32
0.00.038.648 I print_info: n_swa            = 0
0.00.038.648 I print_info: n_embd_head_k    = 128
0.00.038.648 I print_info: n_embd_head_v    = 128
0.00.038.649 I print_info: n_gqa            = 1
0.00.038.649 I print_info: n_embd_k_gqa     = 2048
0.00.038.650 I print_info: n_embd_v_gqa     = 2048
0.00.038.650 I print_info: f_norm_eps       = 1.0e-05
0.00.038.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.651 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.651 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.652 I print_info: f_logit_scale    = 0.0e+00
0.00.038.652 I print_info: n_ff             = 8192
0.00.038.652 I print_info: n_expert         = 0
0.00.038.652 I print_info: n_expert_used    = 0
0.00.038.653 I print_info: causal attn      = 1
0.00.038.653 I print_info: pooling type     = 0
0.00.038.655 I print_info: rope type        = 2
0.00.038.655 I print_info: rope scaling     = linear
0.00.038.655 I print_info: freq_base_train  = 10000.0
0.00.038.655 I print_info: freq_scale_train = 1
0.00.038.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.656 I print_info: rope_finetuned   = unknown
0.00.038.656 I print_info: ssm_d_conv       = 0
0.00.038.656 I print_info: ssm_d_inner      = 0
0.00.038.656 I print_info: ssm_d_state      = 0
0.00.038.656 I print_info: ssm_dt_rank      = 0
0.00.038.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.657 I print_info: model type       = 1.4B
0.00.038.657 I print_info: model params     = 1.41 B
0.00.038.657 I print_info: general.name     = 1.4B
0.00.038.658 I print_info: vocab type       = BPE
0.00.038.658 I print_info: n_vocab          = 50304
0.00.038.658 I print_info: n_merges         = 50009
0.00.038.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.658 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.659 I print_info: LF token         = 187 ''
0.00.038.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.659 I print_info: max token length = 1024
0.00.038.660 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.154 I load_tensors: offloading output layer to GPU
0.00.591.155 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.184 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.591.187 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.592.571 I llama_init_from_model: n_seq_max     = 1
0.00.592.573 I llama_init_from_model: n_ctx         = 128
0.00.592.574 I llama_init_from_model: n_ctx_per_seq = 128
0.00.592.574 I llama_init_from_model: n_batch       = 128
0.00.592.574 I llama_init_from_model: n_ubatch      = 128
0.00.592.575 I llama_init_from_model: flash_attn    = 0
0.00.592.576 I llama_init_from_model: freq_base     = 10000.0
0.00.592.576 I llama_init_from_model: freq_scale    = 1
0.00.592.577 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.578 I ggml_metal_init: allocating
0.00.592.628 I ggml_metal_init: found device: Apple M4
0.00.592.641 I ggml_metal_init: picking default device: Apple M4
0.00.594.072 I ggml_metal_init: using embedded metal library
0.00.599.835 I ggml_metal_init: GPU name:   Apple M4
0.00.599.838 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.840 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.840 I ggml_metal_init: simdgroup reduction   = true
0.00.599.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.841 I ggml_metal_init: has residency sets    = true
0.00.599.841 I ggml_metal_init: has bfloat            = true
0.00.599.841 I ggml_metal_init: use bfloat            = true
0.00.599.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.895 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.379 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.424 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.512 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.513 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.514 I llama_init_from_model: graph nodes  = 967
0.00.622.514 I llama_init_from_model: graph splits = 2
0.00.622.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.824 I 
0.00.657.910 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.919 I perplexity: tokenizing the input ..
0.00.665.201 I perplexity: tokenization took 7.279 ms
0.00.665.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.825 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.799.197 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.799.219 I llama_perf_context_print:        load time =     648.90 ms
0.00.799.223 I llama_perf_context_print: prompt eval time =     131.69 ms /   128 tokens (    1.03 ms per token,   971.96 tokens per second)
0.00.799.223 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.224 I llama_perf_context_print:       total time =     141.40 ms /   129 tokens
0.00.799.576 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.132s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4766 (651adf4b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132e09830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132e09f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132e0a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132e0aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132e0b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132e0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132e0bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132e0c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132e0c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132e0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132e0d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132e0d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132e0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132e0e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132e0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132e0f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132e0ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132e10680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132e10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132e11570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132e11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132e123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132e12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132e13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132e13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132e13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132e14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133a05010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133a054d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133a05940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133a05db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133a06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133a067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133a06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133a07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133a07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133a07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133a080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133a08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133a08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133a08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133a09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133a096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133a09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133a09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133a0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133a0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133a0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133a0b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133a0b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133a0be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133a0c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133a0c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133a0cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133a0d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133a0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133a0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133a0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133a0e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133a0e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133a0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133a0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133a0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133a0f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133a0fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133a100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133a10560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133a109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133a10e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133a112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133a11720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133a11b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133a12000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133a12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133a128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133a12d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133a131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133a13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133a13aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133a13f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133a14380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133a147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133a15050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133a15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133a15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133a160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133a16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133a16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133a171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133a17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133a17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133a182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133a188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133a18e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133a19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133a199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133a0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133a1a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133a1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133a1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133a1b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133a1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133a1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133a1c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133a1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133a1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133a1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133a1dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133a1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133a1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133a1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133a1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133a1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133a1fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133a20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133a20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133a20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133a21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133a21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133a21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133a22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133a22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133a22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133a23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133a23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133a23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133a23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133a24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133a24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133a24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133a25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133a25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133a25d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133a26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133a26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133a26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133a27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133a27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133a27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133a28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133a28590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133a28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133a29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133a29990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133a29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133a2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133a2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133a2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133a2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133a2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133a2bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133a2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133a2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133a2cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133a2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133a2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133a2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133a2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133a2e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133a2e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133a2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133a2f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133a2f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133a2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133a30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133a30790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133a30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133a31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133a31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133a31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133a32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133a32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133a32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133a32f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133a33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133a33990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133a33e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133a34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133a34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133a34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133a35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133a35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133a35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133a36190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133a36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133a36b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133a37090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133a37590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133a37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133a37f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133a38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133a38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133a38f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133a394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133a39aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133a3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133a3a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133a3ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133a3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133a3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133a3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133a3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133a3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133a3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133a3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133a3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133a3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133a3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133a3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133a3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133a3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133a3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133a400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133a40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133a40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133a410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133a415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133a41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133a42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133a425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133a42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133a43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133a435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133a43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133a44070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133a445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133a44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133a45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133a455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133a45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133a46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133a465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133a46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133a47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133a47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133a47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133a48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133a48580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133a48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133a49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133a49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133a49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133a4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133a4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133a4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133a4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133a4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133a4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133a4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133a4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133a4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133a4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133a4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133a4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133a4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133a4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133a4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133a4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133a4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133a4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133a50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133a50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133a50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133a514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133a51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133a51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133a522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133a52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133a52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133a530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133a53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133a539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133a53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133a54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133a547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133a54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133a55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133a555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133a55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133a55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133a566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133a56de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133a57500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133a57c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133a57ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133a586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133a58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133a58fa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.704.848 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126704bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126705040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1267054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126705920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126705d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126706200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126706670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126706ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126706f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1267073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126707830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126707f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126708a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1267091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126709a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12670a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12670a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12670af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12670b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12670bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12670c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12670cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12670d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12670da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12670e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12670e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12670e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12670eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12670efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12670f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12670f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12670fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126710230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1267104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126710960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126710dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1267116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126711b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126711f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126712400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126712870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126712ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126713150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1267135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126713a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126713ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126714310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126714780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126714bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126715060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1267154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126715940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126715db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126716220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126716690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126716c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126717100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126717570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1267179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126717e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1267182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126718730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126718ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126719010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126719480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1267198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126719d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12671a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12671a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12671aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12671af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12671b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12671b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12671bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12671c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12671c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12671c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12671ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12671d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12671d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12671db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12671dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12671e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12671e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12671ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12671f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12671f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12671fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12671ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126720370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1267207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126720c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1267210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126721530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1267219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126721e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126722280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1267226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126722b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126722fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126723440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1267238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126723d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126724190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126724600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126724a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1267257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126725c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1267260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126726510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126726df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126727260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1267276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126727b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126728890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126728d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126729170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1267295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126729ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12672a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12672a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12672ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12672b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12672b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12672b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12672bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12672c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12672c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12672cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12672cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12672d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12672d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12672dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12672e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12672e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12672ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12672eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12672f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12672f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12672fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1267304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126730940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126730db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126731220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126731b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126731f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1267323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126732850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1267335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126733a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126733e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1267342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126734bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126735c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126735f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1267361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126736660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126736ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126736f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1267373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126737820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126737c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126738100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126738570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1267389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126738e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1267392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126739730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126739ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12673a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12673a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12673a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12673ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12673b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12673b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12673bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12673bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12673c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12673c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12673cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12673d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12673d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12673d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12673de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12673e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12673e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12673eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12673eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12673f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12673f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12673fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126740340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1267407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126740c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126741090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1267415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126741ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126742630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1267428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126742eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126743470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126743a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126743ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1267445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126745130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1267456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126745cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126746270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126746830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126746df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1267473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126747970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126747f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1267484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126748ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126749070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126749630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126749bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12674a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12674a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12674ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12674b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12674b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12674be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12674c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12674c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12674cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12674d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12674db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12674e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12674e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12674ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12674f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12674f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12674fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126750370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126750930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126750ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1267514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126751a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126752030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1267525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126752bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126753170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126753730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126753cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1267542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126754870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126754e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1267553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1267559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126755f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126756530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126756af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126756ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1267574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1267579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126757ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1267583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1267588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126758df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1267592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1267597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126759cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12675a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12675a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12675abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12675b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12675b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12675c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12675c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12675ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12675d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12675d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12675e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12675e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12675e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133a0d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133a17a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133a19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133a174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133a1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133a1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133a1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133a196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133a1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133a39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133a19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133a16ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133a15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133a1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133a38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133a1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133a18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133a16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133a1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133a1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133a185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133a16390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133a1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133a1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133a18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133a15de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133a1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133a3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133a58c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133a3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133a3af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133a3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133a0ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133a3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133a3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133a59400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133a596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133a59980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133a59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133a59f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133a5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133a5a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133a5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133a5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133a5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133a5af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133a5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133a5b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133a5b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133a5ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133a5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133a5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133a5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133a5c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133a5c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133a5cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133a5cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133a5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133a5d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133a5d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133a5d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133a5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133a5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133a5e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133a5e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133a5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133a5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133a5ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133a5eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133a5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133a5f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133a5f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133a5f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133a5fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133a5ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133a60200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133a604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133a60780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133a60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133a60d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133a60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133a61280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133a61540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133a61800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133a61ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133a61d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133a62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133a62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133a625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133a62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133a62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133a62e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133a630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133a63380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133a63640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133a63900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133a63bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133a63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133a64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133a64400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133a646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133a64980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133a64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133a64f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133a651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133a65480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133a65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133a65a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133a65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133a65f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133a66240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133a66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133a667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133a66a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133a66d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133a67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133a672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133a67580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133a67840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133a67b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133a67dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133a68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133a68340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133a68600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133a688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133a68b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133a68e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133a69100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133a693c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133a69680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133a69940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133a69c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133a69ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133a6a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133a6a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133a6a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133a6a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133a6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133a6af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133a6b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133a6b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133a6b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133a6ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133a6bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133a6bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133a6c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133a6c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133a6c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133a6cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133a6cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133a6d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133a6d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133a6d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133a6d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133a6db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133a6de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133a6e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133a6e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133a6e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133a6e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133a6ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133a6ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133a6f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133a6f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133a6f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133a6f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133a6fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133a6ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133a701c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133a705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133a70880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133a70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133a70fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133a71420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133a71890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133a71d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133a72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133a725e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133a72a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133a72ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133a73330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133a737a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133a73c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133a74080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133a744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133a74960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133a74dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133a75240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133a756b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133a75b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133a75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133a76400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133a76870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133a76ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133a77150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133a775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133a77a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133a77ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133a78310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133a78780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133a78bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133a79060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133a79620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133a79b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133a79fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133a7a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133a7a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133a7acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133a7b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133a7b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133a7c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133a7c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133a7cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133a7d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133a7d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133a7dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133a7e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133a7e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133a7ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133a7f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133a7f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133a7fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133a80490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133a80a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133a81010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133a815d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133a81b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133a82150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133a82710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133a82cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133a83290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133a83850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133a83e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133a843d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133a84990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133a84f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133a85510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133a85ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133a86090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133a86650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133a86c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133a871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133a87790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133a87d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133a88310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133a888d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133a88e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133a89450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133a89a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133a89fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133a8a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133a8ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133a8b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133a8b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133a8bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133a8c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133a8c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133a8cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133a8d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133a8d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133a8df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133a8e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133a8ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133a8f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133a8f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133a8fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133a90190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133a90750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133a90c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133a91150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133a91650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133a91b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133a92050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133a92550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133a92a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133a92f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133a93450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133a93950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133a93e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133a94350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133a94850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133a94d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133a95250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133a95c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133a96380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133a96aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133a971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133a97480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133a97c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133a97f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133a98540 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.761s
user	0m0.278s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4766 (651adf4b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15560f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15560faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155610050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155610600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155610bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155611160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155611710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155611cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155612270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155612770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155612c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155613170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155613c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155614c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155615370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1556161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1556168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1556170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1556177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155617ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155618600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155618ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1556195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155619880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15561ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15561b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15561b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15561b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15561ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15561c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15561c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15561caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15561cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15561d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15561d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15561dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15561e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15561e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15561eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15561eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15561f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15561f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15561fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155620370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155620c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1556212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1556218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1556224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155622ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1556230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1556238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155623d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155624220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1556244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155624af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1556252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1556255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155625a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155625ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155626cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155627160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155627600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155627aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155627f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1556283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155628880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1556297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15562a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15562a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15562ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15562b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15562b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15562bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15562c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15562c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15562cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15562d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15562d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15562dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15562e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15562e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15562ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15562f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15562f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15562fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155630200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155630750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155630ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155620980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155631110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1556318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155631e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155632360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1556328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155633350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1556338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155633df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155634340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155635330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155635880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155635dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155636bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1556374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1556382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1556390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155639550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1556399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15563a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15563a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15563ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15563b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15563b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15563ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15563bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15563c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15563c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15563ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15563d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15563d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15563dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15563df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15563e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15563e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15563ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15563f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15563f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15563fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15563ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155640450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1556408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155641230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1556416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155641b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155642010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1556424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155642950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155644070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155644510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1556449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155644e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1556452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155645790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155645c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1556460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155646570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155646a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155646eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155647350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1556477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155647c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155648130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1556485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155648a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1556493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155649850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155649cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15564a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15564a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15564aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15564af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15564b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15564b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15564bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15564c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15564c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15564cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15564cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15564d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15564da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15564dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15564e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15564e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15564ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15564f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15564fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1556501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155650690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155650f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155651570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155651d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155652200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1556526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155652b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1556532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155653d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1556542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155654830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155654d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1556552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155655820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155655d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1556562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155656810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155656d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1556572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155657800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155657d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1556582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1556587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155658d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155659290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1556597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155659d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15565a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15565a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15565ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15565b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15565b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15565bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15565c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15565c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15565cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15565d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15565d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15565dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15565e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15565e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15565ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15565f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15565f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15565fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155660220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155660770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155660cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155661210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155661760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155661cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155662200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155662750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155662ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1556631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155663740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155663c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1556641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155664c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1556651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155665720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155665c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155666110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1556665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155666a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155666ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155667390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155667830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155668170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155668610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155668ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155668f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1556693f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155669890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155669d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15566a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15566a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15566ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15566b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15566bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15566c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15566c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15566ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15566d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15566d720 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.095.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1570061b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157006620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157006a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157006f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157007370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1570077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157007c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1570080c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157008530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1570089a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157008e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157009540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15700a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15700a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15700b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15700b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15700be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15700c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15700cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15700d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15700daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15700e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15700e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15700f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15700f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15700fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15700fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157010160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1570105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157010a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157010eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1570113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157011b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157011f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1570123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157012860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157012cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157013140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1570135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157013a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157013e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157014300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157014770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157014be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157015050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1570154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157015930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157015da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157016210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157016680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157016af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157016f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1570173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157017840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157017cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157018220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157018720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157018b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157019000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157019470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1570198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157019d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15701a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15701a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15701aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15701af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15701b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15701b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15701bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15701c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15701c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15701c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15701ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15701d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15701d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15701db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15701dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15701e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15701e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15701ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15701f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15701f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15701fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15701fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157020360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1570207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157020c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1570210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157021520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157021990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157021e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157022270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1570226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157022b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157022fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157023430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1570238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157023d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157024180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1570245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157024a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157024ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157025340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1570257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157025c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157026090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157026500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157026970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157026de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157027250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1570276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157027b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157027fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157028410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157028880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157028cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157029160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1570295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157029a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157029eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15702a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15702a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15702ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15702b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15702b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15702b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15702bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15702c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15702c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15702cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15702cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15702d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15702d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15702dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15702e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15702e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15702ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15702ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15702f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15702f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15702fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157030050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1570304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157030930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157030da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157031210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157031680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157031af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157031f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1570323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157032840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157032cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157033120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157033590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157033a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157033e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1570342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157034750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157034bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157035030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1570354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157035910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157035d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1570361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157036660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157037290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157037550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157037810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157037c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1570380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157038560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1570389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157038e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1570392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157039720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157039b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15703a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15703a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15703a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15703ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15703b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15703b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15703baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15703bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15703c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15703c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15703cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15703d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15703d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15703d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15703de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15703e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15703e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15703eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15703efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15703f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15703f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15703fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1570401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157040610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157040a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157040fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1570414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157041960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157041dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157042240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1570426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157042bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1570430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157043c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157043f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1570444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157044a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157045050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157045610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157045bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157046190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157046750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157046d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1570472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157047890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157047e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157048410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1570489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157048f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157049550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157049b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15704a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15704a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15704ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15704b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15704b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15704bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15704c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15704c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15704ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15704d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15704da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15704e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15704e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15704eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15704f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15704f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15704fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157050290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157050850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157050e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1570513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157051990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157051f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157052510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157052ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157053090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157053650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157053c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1570541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157054790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157054d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157055310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1570558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157055e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157056450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157056a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157057590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157057b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157058110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157058610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157058b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157059010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157059510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157059a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157059f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15705a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15705a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15705ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15705b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15705b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15705bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15705c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15705c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15705cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15705d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15705dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15705e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15705eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15705ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15705f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15705f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15705ff00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1572044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157204950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157204dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157205230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1572056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157205b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157205f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1572063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157206860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157206cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157207140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157207870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157208390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157208b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157209350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157209a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15720a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15720a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15720afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15720b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15720be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15720c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15720cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15720d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15720daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15720dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15720e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15720e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15720e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15720ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15720f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15720f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15720fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15720fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1572102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157210720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157210b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157211000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157211470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1572118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157211d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1572121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157212630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157212aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157212f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157213380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1572137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157213c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1572140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157214540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1572149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157214e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157215290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157215700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157215b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157215fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157216550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157216a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157216ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157217330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1572177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157217c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157218080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1572184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157218960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157218dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157219240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1572196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157219b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157219f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15721a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15721a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15721ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15721b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15721b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15721ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15721bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15721c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15721c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15721cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15721d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15721d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15721d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15721ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15721e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15721e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15721eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15721ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15721f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15721f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15721fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157220130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1572205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157220a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157220e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1572212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157221bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157222040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1572224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157222920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157222d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157223200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157223670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157223fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1572242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157224710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157224b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157224ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157225460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1572258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157225d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1572261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157226620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157226a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157226f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157227370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1572277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157227c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1572280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157228530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1572289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157228e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157229280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1572296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157229b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157229fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15722a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15722a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15722ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15722b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15722b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15722ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15722bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15722c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15722c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15722cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15722d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15722d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15722d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15722ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15722e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15722e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15722eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15722efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15722f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15722f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15722fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157230170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1572305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157230a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157230ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157231330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1572317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157231c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157232080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1572324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157232960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157232dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157233240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1572336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157233b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157233f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157234400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157234870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157234ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157235150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1572355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157235a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157235ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157236310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157236780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157236bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157237060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1572374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157237940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157237db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157238220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157238690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157238b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157238f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1572393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157239850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157239cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15723a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15723a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15723aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15723ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15723b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15723b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15723bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15723c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15723c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15723c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15723cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15723d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15723d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15723dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15723df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15723e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15723e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15723eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15723f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15723f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15723f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15723fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1572402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157240860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157240cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157241140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157241c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157241f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157242210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157242680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157242af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157242f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1572433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157243840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157243cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157244120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157244590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157244a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157244e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1572452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157245750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157245bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157246030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1572464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157246910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157246d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1572471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157247660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157247ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157247f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1572483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157248820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157248c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157249100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157249570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1572499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157249e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15724a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15724a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15724aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15724b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15724b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15724b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15724bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15724c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15724c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15724cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15724cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15724d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15724d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15724dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15724e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15724e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15724e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15724ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15724f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15724f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15724fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15724fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157250460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1572508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157250d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1572511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157251620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157251a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157251f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157252370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1572527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157252c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1572530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157253530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1572539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157253e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157254280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1572546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157254b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157254fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157255440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1572558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157256320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157256a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157257160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157257880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157257b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157257fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1572585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157258bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.953s
user	0m0.230s
sys	0m0.184s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.52 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.94 sec*proc (2 tests)

Total Test time (real) =   1.96 sec
        1.98 real         0.51 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.25 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.60 sec
        0.60 real         0.14 user         0.09 sys
```
