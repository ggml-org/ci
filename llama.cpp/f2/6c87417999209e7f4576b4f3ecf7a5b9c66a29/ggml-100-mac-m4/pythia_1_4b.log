Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.561s
user	0m0.867s
sys	0m1.237s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking C executable ../bin/test-c
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Built target test-backend-ops
[ 61%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target llama-batched-bench
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-gguf-split
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-imatrix
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-bench
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Built target llama-infill
[ 79%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-cli
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-parallel
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-quantize
[ 86%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-perplexity
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.921s
user	0m6.205s
sys	0m9.806s

main: quantize time =  5818.13 ms
main:    total time =  5818.13 ms

main: quantize time =  2513.22 ms
main:    total time =  2513.22 ms

main: quantize time =  1618.26 ms
main:    total time =  1618.26 ms

main: quantize time =  1926.82 ms
main:    total time =  1926.82 ms

main: quantize time =  2594.16 ms
main:    total time =  2594.16 ms

main: quantize time =  5132.85 ms
main:    total time =  5132.85 ms

main: quantize time =  5804.80 ms
main:    total time =  5804.80 ms

main: quantize time =  6784.41 ms
main:    total time =  6784.41 ms

main: quantize time =  6126.95 ms
main:    total time =  6126.95 ms

main: quantize time =  4624.48 ms
main:    total time =  4624.48 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.174 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.282 I main: llama backend init
0.00.000.288 I main: load the model and apply lora adapter, if any
0.00.026.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.976 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.993 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.994 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.004 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.005 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.006 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.006 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.007 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.907 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.308 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.311 I llama_model_loader: - type  f32:  194 tensors
0.00.057.311 I llama_model_loader: - type  f16:   98 tensors
0.00.057.315 I print_info: file format = GGUF V3 (latest)
0.00.057.317 I print_info: file type   = all F32 (guessed)
0.00.057.319 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.087.069 I load: special tokens cache size = 25
0.00.094.208 I load: token to piece cache size = 0.2984 MB
0.00.094.211 I print_info: arch             = gptneox
0.00.094.211 I print_info: vocab_only       = 0
0.00.094.212 I print_info: n_ctx_train      = 2048
0.00.094.212 I print_info: n_embd           = 2048
0.00.094.212 I print_info: n_layer          = 24
0.00.094.215 I print_info: n_head           = 16
0.00.094.215 I print_info: n_head_kv        = 16
0.00.094.216 I print_info: n_rot            = 32
0.00.094.217 I print_info: n_swa            = 0
0.00.094.217 I print_info: n_embd_head_k    = 128
0.00.094.217 I print_info: n_embd_head_v    = 128
0.00.094.218 I print_info: n_gqa            = 1
0.00.094.219 I print_info: n_embd_k_gqa     = 2048
0.00.094.219 I print_info: n_embd_v_gqa     = 2048
0.00.094.220 I print_info: f_norm_eps       = 1.0e-05
0.00.094.220 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.094.220 I print_info: f_clamp_kqv      = 0.0e+00
0.00.094.221 I print_info: f_max_alibi_bias = 0.0e+00
0.00.094.221 I print_info: f_logit_scale    = 0.0e+00
0.00.094.221 I print_info: n_ff             = 8192
0.00.094.222 I print_info: n_expert         = 0
0.00.094.222 I print_info: n_expert_used    = 0
0.00.094.222 I print_info: causal attn      = 1
0.00.094.222 I print_info: pooling type     = 0
0.00.094.222 I print_info: rope type        = 2
0.00.094.222 I print_info: rope scaling     = linear
0.00.094.224 I print_info: freq_base_train  = 10000.0
0.00.094.225 I print_info: freq_scale_train = 1
0.00.094.225 I print_info: n_ctx_orig_yarn  = 2048
0.00.094.225 I print_info: rope_finetuned   = unknown
0.00.094.225 I print_info: ssm_d_conv       = 0
0.00.094.225 I print_info: ssm_d_inner      = 0
0.00.094.226 I print_info: ssm_d_state      = 0
0.00.094.227 I print_info: ssm_dt_rank      = 0
0.00.094.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.094.228 I print_info: model type       = 1.4B
0.00.094.228 I print_info: model params     = 1.41 B
0.00.094.228 I print_info: general.name     = 1.4B
0.00.094.229 I print_info: vocab type       = BPE
0.00.094.229 I print_info: n_vocab          = 50304
0.00.094.229 I print_info: n_merges         = 50009
0.00.094.229 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.094.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.094.230 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.094.230 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.094.230 I print_info: LF token         = 128 'Ä'
0.00.094.234 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.094.234 I print_info: max token length = 1024
0.00.096.858 I load_tensors: offloading 24 repeating layers to GPU
0.00.096.858 I load_tensors: offloading output layer to GPU
0.00.096.858 I load_tensors: offloaded 25/25 layers to GPU
0.00.096.877 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.878 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.097.176 I llama_init_from_model: n_seq_max     = 1
0.00.097.176 I llama_init_from_model: n_ctx         = 2048
0.00.097.177 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.097.177 I llama_init_from_model: n_batch       = 2048
0.00.097.177 I llama_init_from_model: n_ubatch      = 512
0.00.097.177 I llama_init_from_model: flash_attn    = 0
0.00.097.177 I llama_init_from_model: freq_base     = 10000.0
0.00.097.178 I llama_init_from_model: freq_scale    = 1
0.00.097.178 I ggml_metal_init: allocating
0.00.097.181 I ggml_metal_init: found device: Apple M4
0.00.097.183 I ggml_metal_init: picking default device: Apple M4
0.00.097.874 I ggml_metal_init: using embedded metal library
0.00.100.742 I ggml_metal_init: GPU name:   Apple M4
0.00.100.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.744 I ggml_metal_init: simdgroup reduction   = true
0.00.100.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.745 I ggml_metal_init: has bfloat            = true
0.00.100.745 I ggml_metal_init: use bfloat            = true
0.00.100.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.466 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.129.032 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.059 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.079 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.130.008 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.130.009 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.130.010 I llama_init_from_model: graph nodes  = 967
0.00.130.010 I llama_init_from_model: graph splits = 2
0.00.130.013 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.143 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.209.711 I main: llama threadpool init, n_threads = 4
0.00.209.773 I 
0.00.209.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.209.817 I 
0.00.209.880 I sampler seed: 1234
0.00.209.884 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.209.909 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.209.911 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.209.911 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.045.936 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.045.937 I llama_perf_context_print:        load time =     182.94 ms
0.02.045.938 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.78 tokens per second)
0.02.045.939 I llama_perf_context_print:        eval time =    1789.58 ms /    63 runs   (   28.41 ms per token,    35.20 tokens per second)
0.02.045.940 I llama_perf_context_print:       total time =    1836.23 ms /    70 tokens
0.02.046.154 I ggml_metal_free: deallocating

real	0m2.346s
user	0m0.142s
sys	0m0.098s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.804 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.487 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.489 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.495 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.317 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.207 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.209 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.209 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.210 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.210 I llama_model_loader: - type  f32:  194 tensors
0.00.026.211 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.211 I print_info: file format = GGUF V3 (latest)
0.00.026.212 I print_info: file type   = Q8_0
0.00.026.212 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.485 I load: special tokens cache size = 25
0.00.052.371 I load: token to piece cache size = 0.2984 MB
0.00.052.375 I print_info: arch             = gptneox
0.00.052.375 I print_info: vocab_only       = 0
0.00.052.376 I print_info: n_ctx_train      = 2048
0.00.052.376 I print_info: n_embd           = 2048
0.00.052.376 I print_info: n_layer          = 24
0.00.052.383 I print_info: n_head           = 16
0.00.052.383 I print_info: n_head_kv        = 16
0.00.052.383 I print_info: n_rot            = 32
0.00.052.384 I print_info: n_swa            = 0
0.00.052.384 I print_info: n_embd_head_k    = 128
0.00.052.384 I print_info: n_embd_head_v    = 128
0.00.052.385 I print_info: n_gqa            = 1
0.00.052.385 I print_info: n_embd_k_gqa     = 2048
0.00.052.386 I print_info: n_embd_v_gqa     = 2048
0.00.052.387 I print_info: f_norm_eps       = 1.0e-05
0.00.052.387 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.387 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.388 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.388 I print_info: f_logit_scale    = 0.0e+00
0.00.052.389 I print_info: n_ff             = 8192
0.00.052.389 I print_info: n_expert         = 0
0.00.052.389 I print_info: n_expert_used    = 0
0.00.052.390 I print_info: causal attn      = 1
0.00.052.390 I print_info: pooling type     = 0
0.00.052.390 I print_info: rope type        = 2
0.00.052.390 I print_info: rope scaling     = linear
0.00.052.390 I print_info: freq_base_train  = 10000.0
0.00.052.393 I print_info: freq_scale_train = 1
0.00.052.394 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.394 I print_info: rope_finetuned   = unknown
0.00.052.394 I print_info: ssm_d_conv       = 0
0.00.052.394 I print_info: ssm_d_inner      = 0
0.00.052.394 I print_info: ssm_d_state      = 0
0.00.052.394 I print_info: ssm_dt_rank      = 0
0.00.052.395 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.395 I print_info: model type       = 1.4B
0.00.052.395 I print_info: model params     = 1.41 B
0.00.052.396 I print_info: general.name     = 1.4B
0.00.052.396 I print_info: vocab type       = BPE
0.00.052.396 I print_info: n_vocab          = 50304
0.00.052.397 I print_info: n_merges         = 50009
0.00.052.397 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.397 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.397 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.397 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.398 I print_info: LF token         = 128 'Ä'
0.00.052.398 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.398 I print_info: max token length = 1024
0.00.054.896 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.896 I load_tensors: offloading output layer to GPU
0.00.054.896 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.908 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.909 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.055.245 I llama_init_from_model: n_seq_max     = 1
0.00.055.246 I llama_init_from_model: n_ctx         = 2048
0.00.055.246 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.246 I llama_init_from_model: n_batch       = 2048
0.00.055.246 I llama_init_from_model: n_ubatch      = 512
0.00.055.246 I llama_init_from_model: flash_attn    = 0
0.00.055.247 I llama_init_from_model: freq_base     = 10000.0
0.00.055.247 I llama_init_from_model: freq_scale    = 1
0.00.055.248 I ggml_metal_init: allocating
0.00.055.252 I ggml_metal_init: found device: Apple M4
0.00.055.254 I ggml_metal_init: picking default device: Apple M4
0.00.056.048 I ggml_metal_init: using embedded metal library
0.00.058.618 I ggml_metal_init: GPU name:   Apple M4
0.00.058.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.620 I ggml_metal_init: simdgroup reduction   = true
0.00.058.620 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.620 I ggml_metal_init: has bfloat            = true
0.00.058.621 I ggml_metal_init: use bfloat            = true
0.00.058.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.111 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.401 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.429 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.565 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.568 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.568 I llama_init_from_model: graph nodes  = 967
0.00.096.569 I llama_init_from_model: graph splits = 2
0.00.096.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.013.050 I main: llama threadpool init, n_threads = 4
0.01.013.088 I 
0.01.013.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.013.124 I 
0.01.013.269 I sampler seed: 1234
0.01.013.274 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.013.319 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.013.322 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.013.322 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.097.024 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.02.097.024 I llama_perf_context_print:        load time =    1003.24 ms
0.02.097.025 I llama_perf_context_print: prompt eval time =      39.97 ms /     7 tokens (    5.71 ms per token,   175.15 tokens per second)
0.02.097.026 I llama_perf_context_print:        eval time =    1040.75 ms /    63 runs   (   16.52 ms per token,    60.53 tokens per second)
0.02.097.026 I llama_perf_context_print:       total time =    1083.98 ms /    70 tokens
0.02.097.238 I ggml_metal_free: deallocating

real	0m2.116s
user	0m0.111s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.285 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.049 I llama_model_loader: - type  f32:  194 tensors
0.00.028.049 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.049 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.050 I print_info: file format = GGUF V3 (latest)
0.00.028.052 I print_info: file type   = Q4_0
0.00.028.053 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.684 I load: special tokens cache size = 25
0.00.053.645 I load: token to piece cache size = 0.2984 MB
0.00.053.648 I print_info: arch             = gptneox
0.00.053.649 I print_info: vocab_only       = 0
0.00.053.649 I print_info: n_ctx_train      = 2048
0.00.053.649 I print_info: n_embd           = 2048
0.00.053.650 I print_info: n_layer          = 24
0.00.053.654 I print_info: n_head           = 16
0.00.053.655 I print_info: n_head_kv        = 16
0.00.053.655 I print_info: n_rot            = 32
0.00.053.655 I print_info: n_swa            = 0
0.00.053.655 I print_info: n_embd_head_k    = 128
0.00.053.656 I print_info: n_embd_head_v    = 128
0.00.053.656 I print_info: n_gqa            = 1
0.00.053.657 I print_info: n_embd_k_gqa     = 2048
0.00.053.658 I print_info: n_embd_v_gqa     = 2048
0.00.053.658 I print_info: f_norm_eps       = 1.0e-05
0.00.053.659 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.659 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.661 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.661 I print_info: f_logit_scale    = 0.0e+00
0.00.053.662 I print_info: n_ff             = 8192
0.00.053.663 I print_info: n_expert         = 0
0.00.053.663 I print_info: n_expert_used    = 0
0.00.053.663 I print_info: causal attn      = 1
0.00.053.663 I print_info: pooling type     = 0
0.00.053.663 I print_info: rope type        = 2
0.00.053.663 I print_info: rope scaling     = linear
0.00.053.664 I print_info: freq_base_train  = 10000.0
0.00.053.664 I print_info: freq_scale_train = 1
0.00.053.664 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.664 I print_info: rope_finetuned   = unknown
0.00.053.665 I print_info: ssm_d_conv       = 0
0.00.053.665 I print_info: ssm_d_inner      = 0
0.00.053.665 I print_info: ssm_d_state      = 0
0.00.053.665 I print_info: ssm_dt_rank      = 0
0.00.053.665 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.666 I print_info: model type       = 1.4B
0.00.053.666 I print_info: model params     = 1.41 B
0.00.053.666 I print_info: general.name     = 1.4B
0.00.053.667 I print_info: vocab type       = BPE
0.00.053.667 I print_info: n_vocab          = 50304
0.00.053.668 I print_info: n_merges         = 50009
0.00.053.668 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.668 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.668 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.668 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.669 I print_info: LF token         = 128 'Ä'
0.00.053.669 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.669 I print_info: max token length = 1024
0.00.055.966 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.966 I load_tensors: offloading output layer to GPU
0.00.055.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.977 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.979 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.302 I llama_init_from_model: n_seq_max     = 1
0.00.056.303 I llama_init_from_model: n_ctx         = 2048
0.00.056.303 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.303 I llama_init_from_model: n_batch       = 2048
0.00.056.303 I llama_init_from_model: n_ubatch      = 512
0.00.056.303 I llama_init_from_model: flash_attn    = 0
0.00.056.304 I llama_init_from_model: freq_base     = 10000.0
0.00.056.304 I llama_init_from_model: freq_scale    = 1
0.00.056.304 I ggml_metal_init: allocating
0.00.056.307 I ggml_metal_init: found device: Apple M4
0.00.056.309 I ggml_metal_init: picking default device: Apple M4
0.00.057.054 I ggml_metal_init: using embedded metal library
0.00.059.614 I ggml_metal_init: GPU name:   Apple M4
0.00.059.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.616 I ggml_metal_init: simdgroup reduction   = true
0.00.059.616 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.617 I ggml_metal_init: has bfloat            = true
0.00.059.617 I ggml_metal_init: use bfloat            = true
0.00.059.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.527 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.826 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.849 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.119 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.098.122 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.098.123 I llama_init_from_model: graph nodes  = 967
0.00.098.123 I llama_init_from_model: graph splits = 2
0.00.098.127 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.542 I main: llama threadpool init, n_threads = 4
0.00.665.585 I 
0.00.665.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.618 I 
0.00.665.856 I sampler seed: 1234
0.00.665.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.893 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.895 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.895 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.338.186 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.338.186 I llama_perf_context_print:        load time =     653.68 ms
0.01.338.187 I llama_perf_context_print: prompt eval time =      42.56 ms /     7 tokens (    6.08 ms per token,   164.47 tokens per second)
0.01.338.188 I llama_perf_context_print:        eval time =     626.64 ms /    63 runs   (    9.95 ms per token,   100.54 tokens per second)
0.01.338.188 I llama_perf_context_print:       total time =     672.65 ms /    70 tokens
0.01.338.413 I ggml_metal_free: deallocating

real	0m1.356s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.883 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.654 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.464 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.244 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.246 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.246 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.247 I llama_model_loader: - type  f32:  194 tensors
0.00.027.247 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.247 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.248 I print_info: file format = GGUF V3 (latest)
0.00.027.248 I print_info: file type   = Q4_1
0.00.027.249 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.793 I load: special tokens cache size = 25
0.00.051.799 I load: token to piece cache size = 0.2984 MB
0.00.051.802 I print_info: arch             = gptneox
0.00.051.802 I print_info: vocab_only       = 0
0.00.051.802 I print_info: n_ctx_train      = 2048
0.00.051.802 I print_info: n_embd           = 2048
0.00.051.802 I print_info: n_layer          = 24
0.00.051.805 I print_info: n_head           = 16
0.00.051.806 I print_info: n_head_kv        = 16
0.00.051.806 I print_info: n_rot            = 32
0.00.051.807 I print_info: n_swa            = 0
0.00.051.807 I print_info: n_embd_head_k    = 128
0.00.051.807 I print_info: n_embd_head_v    = 128
0.00.051.808 I print_info: n_gqa            = 1
0.00.051.808 I print_info: n_embd_k_gqa     = 2048
0.00.051.809 I print_info: n_embd_v_gqa     = 2048
0.00.051.810 I print_info: f_norm_eps       = 1.0e-05
0.00.051.810 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.810 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.810 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.811 I print_info: f_logit_scale    = 0.0e+00
0.00.051.811 I print_info: n_ff             = 8192
0.00.051.812 I print_info: n_expert         = 0
0.00.051.812 I print_info: n_expert_used    = 0
0.00.051.812 I print_info: causal attn      = 1
0.00.051.812 I print_info: pooling type     = 0
0.00.051.812 I print_info: rope type        = 2
0.00.051.813 I print_info: rope scaling     = linear
0.00.051.813 I print_info: freq_base_train  = 10000.0
0.00.051.813 I print_info: freq_scale_train = 1
0.00.051.814 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.814 I print_info: rope_finetuned   = unknown
0.00.051.814 I print_info: ssm_d_conv       = 0
0.00.051.814 I print_info: ssm_d_inner      = 0
0.00.051.814 I print_info: ssm_d_state      = 0
0.00.051.814 I print_info: ssm_dt_rank      = 0
0.00.051.815 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.815 I print_info: model type       = 1.4B
0.00.051.815 I print_info: model params     = 1.41 B
0.00.051.815 I print_info: general.name     = 1.4B
0.00.051.816 I print_info: vocab type       = BPE
0.00.051.816 I print_info: n_vocab          = 50304
0.00.051.817 I print_info: n_merges         = 50009
0.00.051.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.819 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.819 I print_info: LF token         = 128 'Ä'
0.00.051.820 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.820 I print_info: max token length = 1024
0.00.053.669 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.669 I load_tensors: offloading output layer to GPU
0.00.053.669 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.680 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.681 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.982 I llama_init_from_model: n_seq_max     = 1
0.00.053.983 I llama_init_from_model: n_ctx         = 2048
0.00.053.983 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.983 I llama_init_from_model: n_batch       = 2048
0.00.053.983 I llama_init_from_model: n_ubatch      = 512
0.00.053.983 I llama_init_from_model: flash_attn    = 0
0.00.053.984 I llama_init_from_model: freq_base     = 10000.0
0.00.053.984 I llama_init_from_model: freq_scale    = 1
0.00.053.984 I ggml_metal_init: allocating
0.00.053.987 I ggml_metal_init: found device: Apple M4
0.00.053.989 I ggml_metal_init: picking default device: Apple M4
0.00.054.583 I ggml_metal_init: using embedded metal library
0.00.056.907 I ggml_metal_init: GPU name:   Apple M4
0.00.056.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.910 I ggml_metal_init: simdgroup reduction   = true
0.00.056.910 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.910 I ggml_metal_init: has bfloat            = true
0.00.056.910 I ggml_metal_init: use bfloat            = true
0.00.056.911 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.911 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.581 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.288 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.309 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.331 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.480 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.481 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.482 I llama_init_from_model: graph nodes  = 967
0.00.087.482 I llama_init_from_model: graph splits = 2
0.00.087.486 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.808 I main: llama threadpool init, n_threads = 4
0.00.706.849 I 
0.00.706.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.894 I 
0.00.707.109 I sampler seed: 1234
0.00.707.117 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.174 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.176 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.425.769 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64311.59 tokens per second)
0.01.425.770 I llama_perf_context_print:        load time =     695.92 ms
0.01.425.770 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.52 tokens per second)
0.01.425.771 I llama_perf_context_print:        eval time =     672.66 ms /    63 runs   (   10.68 ms per token,    93.66 tokens per second)
0.01.425.771 I llama_perf_context_print:       total time =     718.96 ms /    70 tokens
0.01.425.976 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.108s
sys	0m0.160s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.975 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.849 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.854 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.860 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.861 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.709 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.689 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.480 I llama_model_loader: - type  f32:  194 tensors
0.00.026.480 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.480 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.481 I print_info: file format = GGUF V3 (latest)
0.00.026.481 I print_info: file type   = Q5_0
0.00.026.483 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.989 I load: special tokens cache size = 25
0.00.051.958 I load: token to piece cache size = 0.2984 MB
0.00.051.961 I print_info: arch             = gptneox
0.00.051.961 I print_info: vocab_only       = 0
0.00.051.962 I print_info: n_ctx_train      = 2048
0.00.051.962 I print_info: n_embd           = 2048
0.00.051.962 I print_info: n_layer          = 24
0.00.051.965 I print_info: n_head           = 16
0.00.051.965 I print_info: n_head_kv        = 16
0.00.051.966 I print_info: n_rot            = 32
0.00.051.966 I print_info: n_swa            = 0
0.00.051.966 I print_info: n_embd_head_k    = 128
0.00.051.966 I print_info: n_embd_head_v    = 128
0.00.051.967 I print_info: n_gqa            = 1
0.00.051.968 I print_info: n_embd_k_gqa     = 2048
0.00.051.968 I print_info: n_embd_v_gqa     = 2048
0.00.051.969 I print_info: f_norm_eps       = 1.0e-05
0.00.051.969 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.970 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.970 I print_info: f_logit_scale    = 0.0e+00
0.00.051.971 I print_info: n_ff             = 8192
0.00.051.971 I print_info: n_expert         = 0
0.00.051.971 I print_info: n_expert_used    = 0
0.00.051.971 I print_info: causal attn      = 1
0.00.051.971 I print_info: pooling type     = 0
0.00.051.971 I print_info: rope type        = 2
0.00.051.972 I print_info: rope scaling     = linear
0.00.051.972 I print_info: freq_base_train  = 10000.0
0.00.051.972 I print_info: freq_scale_train = 1
0.00.051.973 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.973 I print_info: rope_finetuned   = unknown
0.00.051.973 I print_info: ssm_d_conv       = 0
0.00.051.973 I print_info: ssm_d_inner      = 0
0.00.051.973 I print_info: ssm_d_state      = 0
0.00.051.974 I print_info: ssm_dt_rank      = 0
0.00.051.974 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.974 I print_info: model type       = 1.4B
0.00.051.974 I print_info: model params     = 1.41 B
0.00.051.974 I print_info: general.name     = 1.4B
0.00.051.975 I print_info: vocab type       = BPE
0.00.051.975 I print_info: n_vocab          = 50304
0.00.051.976 I print_info: n_merges         = 50009
0.00.051.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.976 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.976 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.976 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.978 I print_info: LF token         = 128 'Ä'
0.00.051.979 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.979 I print_info: max token length = 1024
0.00.053.523 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.523 I load_tensors: offloading output layer to GPU
0.00.053.523 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.533 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.534 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.824 I llama_init_from_model: n_seq_max     = 1
0.00.053.825 I llama_init_from_model: n_ctx         = 2048
0.00.053.825 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.825 I llama_init_from_model: n_batch       = 2048
0.00.053.825 I llama_init_from_model: n_ubatch      = 512
0.00.053.825 I llama_init_from_model: flash_attn    = 0
0.00.053.826 I llama_init_from_model: freq_base     = 10000.0
0.00.053.826 I llama_init_from_model: freq_scale    = 1
0.00.053.826 I ggml_metal_init: allocating
0.00.053.829 I ggml_metal_init: found device: Apple M4
0.00.053.831 I ggml_metal_init: picking default device: Apple M4
0.00.054.399 I ggml_metal_init: using embedded metal library
0.00.056.729 I ggml_metal_init: GPU name:   Apple M4
0.00.056.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.732 I ggml_metal_init: simdgroup reduction   = true
0.00.056.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.732 I ggml_metal_init: has bfloat            = true
0.00.056.732 I ggml_metal_init: use bfloat            = true
0.00.056.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.516 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.538 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.561 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.590 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.591 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.592 I llama_init_from_model: graph nodes  = 967
0.00.087.592 I llama_init_from_model: graph splits = 2
0.00.087.595 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.458 I main: llama threadpool init, n_threads = 4
0.00.744.521 I 
0.00.744.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.555 I 
0.00.744.773 I sampler seed: 1234
0.00.744.780 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.792 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.792 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.793 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.526.892 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.526.893 I llama_perf_context_print:        load time =     734.48 ms
0.01.526.894 I llama_perf_context_print: prompt eval time =      46.32 ms /     7 tokens (    6.62 ms per token,   151.12 tokens per second)
0.01.526.894 I llama_perf_context_print:        eval time =     732.69 ms /    63 runs   (   11.63 ms per token,    85.98 tokens per second)
0.01.526.895 I llama_perf_context_print:       total time =     782.44 ms /    70 tokens
0.01.527.094 I ggml_metal_free: deallocating

real	0m1.546s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.015.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.843 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.022.848 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.854 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.855 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.855 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.856 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.857 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.857 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.858 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.858 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.858 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.859 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.859 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.861 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.673 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.514 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.514 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.515 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.031.516 I llama_model_loader: - type  f32:  194 tensors
0.00.031.516 I llama_model_loader: - type q5_1:   97 tensors
0.00.031.516 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.517 I print_info: file format = GGUF V3 (latest)
0.00.031.517 I print_info: file type   = Q5_1
0.00.031.518 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.050.591 I load: special tokens cache size = 25
0.00.056.582 I load: token to piece cache size = 0.2984 MB
0.00.056.586 I print_info: arch             = gptneox
0.00.056.587 I print_info: vocab_only       = 0
0.00.056.587 I print_info: n_ctx_train      = 2048
0.00.056.587 I print_info: n_embd           = 2048
0.00.056.587 I print_info: n_layer          = 24
0.00.056.592 I print_info: n_head           = 16
0.00.056.593 I print_info: n_head_kv        = 16
0.00.056.593 I print_info: n_rot            = 32
0.00.056.594 I print_info: n_swa            = 0
0.00.056.594 I print_info: n_embd_head_k    = 128
0.00.056.594 I print_info: n_embd_head_v    = 128
0.00.056.595 I print_info: n_gqa            = 1
0.00.056.596 I print_info: n_embd_k_gqa     = 2048
0.00.056.596 I print_info: n_embd_v_gqa     = 2048
0.00.056.597 I print_info: f_norm_eps       = 1.0e-05
0.00.056.597 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.597 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.597 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.598 I print_info: f_logit_scale    = 0.0e+00
0.00.056.598 I print_info: n_ff             = 8192
0.00.056.598 I print_info: n_expert         = 0
0.00.056.599 I print_info: n_expert_used    = 0
0.00.056.599 I print_info: causal attn      = 1
0.00.056.599 I print_info: pooling type     = 0
0.00.056.599 I print_info: rope type        = 2
0.00.056.599 I print_info: rope scaling     = linear
0.00.056.600 I print_info: freq_base_train  = 10000.0
0.00.056.600 I print_info: freq_scale_train = 1
0.00.056.600 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.600 I print_info: rope_finetuned   = unknown
0.00.056.601 I print_info: ssm_d_conv       = 0
0.00.056.603 I print_info: ssm_d_inner      = 0
0.00.056.603 I print_info: ssm_d_state      = 0
0.00.056.603 I print_info: ssm_dt_rank      = 0
0.00.056.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.604 I print_info: model type       = 1.4B
0.00.056.604 I print_info: model params     = 1.41 B
0.00.056.604 I print_info: general.name     = 1.4B
0.00.056.605 I print_info: vocab type       = BPE
0.00.056.605 I print_info: n_vocab          = 50304
0.00.056.605 I print_info: n_merges         = 50009
0.00.056.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.605 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.606 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.606 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.606 I print_info: LF token         = 128 'Ä'
0.00.056.606 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.606 I print_info: max token length = 1024
0.00.058.694 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.694 I load_tensors: offloading output layer to GPU
0.00.058.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.705 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.058.706 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.059.068 I llama_init_from_model: n_seq_max     = 1
0.00.059.069 I llama_init_from_model: n_ctx         = 2048
0.00.059.069 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.059.069 I llama_init_from_model: n_batch       = 2048
0.00.059.069 I llama_init_from_model: n_ubatch      = 512
0.00.059.070 I llama_init_from_model: flash_attn    = 0
0.00.059.070 I llama_init_from_model: freq_base     = 10000.0
0.00.059.070 I llama_init_from_model: freq_scale    = 1
0.00.059.071 I ggml_metal_init: allocating
0.00.059.074 I ggml_metal_init: found device: Apple M4
0.00.059.076 I ggml_metal_init: picking default device: Apple M4
0.00.059.693 I ggml_metal_init: using embedded metal library
0.00.062.010 I ggml_metal_init: GPU name:   Apple M4
0.00.062.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.013 I ggml_metal_init: simdgroup reduction   = true
0.00.062.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.013 I ggml_metal_init: has bfloat            = true
0.00.062.013 I ggml_metal_init: use bfloat            = true
0.00.062.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.627 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.732 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.750 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.768 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.967 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.969 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.969 I llama_init_from_model: graph nodes  = 967
0.00.093.969 I llama_init_from_model: graph splits = 2
0.00.093.973 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.034 I main: llama threadpool init, n_threads = 4
0.00.782.078 I 
0.00.782.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.118 I 
0.00.782.329 I sampler seed: 1234
0.00.782.335 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.385 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.387 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.387 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.624.962 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.624.963 I llama_perf_context_print:        load time =     766.84 ms
0.01.624.964 I llama_perf_context_print: prompt eval time =      46.19 ms /     7 tokens (    6.60 ms per token,   151.55 tokens per second)
0.01.624.964 I llama_perf_context_print:        eval time =     793.32 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.624.965 I llama_perf_context_print:       total time =     842.93 ms /    70 tokens
0.01.625.149 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.201 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.815 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.815 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.820 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.821 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.821 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.821 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.822 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.823 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.824 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.824 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.485 I llama_model_loader: - type  f32:  194 tensors
0.00.025.486 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.486 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.487 I print_info: file format = GGUF V3 (latest)
0.00.025.487 I print_info: file type   = Q2_K - Medium
0.00.025.488 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.020 I load: special tokens cache size = 25
0.00.050.098 I load: token to piece cache size = 0.2984 MB
0.00.050.102 I print_info: arch             = gptneox
0.00.050.102 I print_info: vocab_only       = 0
0.00.050.102 I print_info: n_ctx_train      = 2048
0.00.050.102 I print_info: n_embd           = 2048
0.00.050.102 I print_info: n_layer          = 24
0.00.050.105 I print_info: n_head           = 16
0.00.050.106 I print_info: n_head_kv        = 16
0.00.050.106 I print_info: n_rot            = 32
0.00.050.107 I print_info: n_swa            = 0
0.00.050.107 I print_info: n_embd_head_k    = 128
0.00.050.107 I print_info: n_embd_head_v    = 128
0.00.050.108 I print_info: n_gqa            = 1
0.00.050.109 I print_info: n_embd_k_gqa     = 2048
0.00.050.109 I print_info: n_embd_v_gqa     = 2048
0.00.050.111 I print_info: f_norm_eps       = 1.0e-05
0.00.050.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.112 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.114 I print_info: f_logit_scale    = 0.0e+00
0.00.050.115 I print_info: n_ff             = 8192
0.00.050.116 I print_info: n_expert         = 0
0.00.050.116 I print_info: n_expert_used    = 0
0.00.050.116 I print_info: causal attn      = 1
0.00.050.116 I print_info: pooling type     = 0
0.00.050.116 I print_info: rope type        = 2
0.00.050.117 I print_info: rope scaling     = linear
0.00.050.117 I print_info: freq_base_train  = 10000.0
0.00.050.117 I print_info: freq_scale_train = 1
0.00.050.118 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.118 I print_info: rope_finetuned   = unknown
0.00.050.118 I print_info: ssm_d_conv       = 0
0.00.050.118 I print_info: ssm_d_inner      = 0
0.00.050.118 I print_info: ssm_d_state      = 0
0.00.050.120 I print_info: ssm_dt_rank      = 0
0.00.050.120 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.120 I print_info: model type       = 1.4B
0.00.050.120 I print_info: model params     = 1.41 B
0.00.050.120 I print_info: general.name     = 1.4B
0.00.050.121 I print_info: vocab type       = BPE
0.00.050.121 I print_info: n_vocab          = 50304
0.00.050.121 I print_info: n_merges         = 50009
0.00.050.122 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.123 I print_info: LF token         = 128 'Ä'
0.00.050.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.123 I print_info: max token length = 1024
0.00.051.957 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.957 I load_tensors: offloading output layer to GPU
0.00.051.957 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.968 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.969 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.237 I llama_init_from_model: n_seq_max     = 1
0.00.052.238 I llama_init_from_model: n_ctx         = 2048
0.00.052.238 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.238 I llama_init_from_model: n_batch       = 2048
0.00.052.238 I llama_init_from_model: n_ubatch      = 512
0.00.052.238 I llama_init_from_model: flash_attn    = 0
0.00.052.239 I llama_init_from_model: freq_base     = 10000.0
0.00.052.239 I llama_init_from_model: freq_scale    = 1
0.00.052.239 I ggml_metal_init: allocating
0.00.052.242 I ggml_metal_init: found device: Apple M4
0.00.052.244 I ggml_metal_init: picking default device: Apple M4
0.00.052.834 I ggml_metal_init: using embedded metal library
0.00.055.219 I ggml_metal_init: GPU name:   Apple M4
0.00.055.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.222 I ggml_metal_init: simdgroup reduction   = true
0.00.055.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.222 I ggml_metal_init: has bfloat            = true
0.00.055.222 I ggml_metal_init: use bfloat            = true
0.00.055.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.856 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.027 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.050 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.118 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.120 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.120 I llama_init_from_model: graph nodes  = 967
0.00.086.120 I llama_init_from_model: graph splits = 2
0.00.086.123 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.252 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.466.837 I main: llama threadpool init, n_threads = 4
0.00.466.885 I 
0.00.466.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.466.920 I 
0.00.467.164 I sampler seed: 1234
0.00.467.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.467.228 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.467.232 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.467.233 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.146.529 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.146.529 I llama_perf_context_print:        load time =     456.63 ms
0.01.146.530 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.41 tokens per second)
0.01.146.534 I llama_perf_context_print:        eval time =     640.42 ms /    63 runs   (   10.17 ms per token,    98.37 tokens per second)
0.01.146.535 I llama_perf_context_print:       total time =     679.70 ms /    70 tokens
0.01.146.797 I ggml_metal_free: deallocating

real	0m1.166s
user	0m0.108s
sys	0m0.112s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.241 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.857 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.860 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.440 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.441 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.443 I llama_model_loader: - type  f32:  194 tensors
0.00.027.443 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.443 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.444 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.444 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.444 I print_info: file format = GGUF V3 (latest)
0.00.027.445 I print_info: file type   = Q3_K - Medium
0.00.027.445 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.046.003 I load: special tokens cache size = 25
0.00.051.937 I load: token to piece cache size = 0.2984 MB
0.00.051.940 I print_info: arch             = gptneox
0.00.051.941 I print_info: vocab_only       = 0
0.00.051.941 I print_info: n_ctx_train      = 2048
0.00.051.941 I print_info: n_embd           = 2048
0.00.051.941 I print_info: n_layer          = 24
0.00.051.944 I print_info: n_head           = 16
0.00.051.945 I print_info: n_head_kv        = 16
0.00.051.945 I print_info: n_rot            = 32
0.00.051.945 I print_info: n_swa            = 0
0.00.051.946 I print_info: n_embd_head_k    = 128
0.00.051.946 I print_info: n_embd_head_v    = 128
0.00.051.947 I print_info: n_gqa            = 1
0.00.051.947 I print_info: n_embd_k_gqa     = 2048
0.00.051.948 I print_info: n_embd_v_gqa     = 2048
0.00.051.949 I print_info: f_norm_eps       = 1.0e-05
0.00.051.949 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.949 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.949 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.949 I print_info: f_logit_scale    = 0.0e+00
0.00.051.950 I print_info: n_ff             = 8192
0.00.051.950 I print_info: n_expert         = 0
0.00.051.951 I print_info: n_expert_used    = 0
0.00.051.952 I print_info: causal attn      = 1
0.00.051.954 I print_info: pooling type     = 0
0.00.051.954 I print_info: rope type        = 2
0.00.051.954 I print_info: rope scaling     = linear
0.00.051.955 I print_info: freq_base_train  = 10000.0
0.00.051.955 I print_info: freq_scale_train = 1
0.00.051.955 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.955 I print_info: rope_finetuned   = unknown
0.00.051.955 I print_info: ssm_d_conv       = 0
0.00.051.956 I print_info: ssm_d_inner      = 0
0.00.051.956 I print_info: ssm_d_state      = 0
0.00.051.956 I print_info: ssm_dt_rank      = 0
0.00.051.956 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.957 I print_info: model type       = 1.4B
0.00.051.958 I print_info: model params     = 1.41 B
0.00.051.958 I print_info: general.name     = 1.4B
0.00.051.958 I print_info: vocab type       = BPE
0.00.051.960 I print_info: n_vocab          = 50304
0.00.051.960 I print_info: n_merges         = 50009
0.00.051.960 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.960 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.961 I print_info: LF token         = 128 'Ä'
0.00.051.961 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.961 I print_info: max token length = 1024
0.00.053.876 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.877 I load_tensors: offloading output layer to GPU
0.00.053.877 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.888 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.889 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.162 I llama_init_from_model: n_seq_max     = 1
0.00.054.163 I llama_init_from_model: n_ctx         = 2048
0.00.054.163 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.163 I llama_init_from_model: n_batch       = 2048
0.00.054.163 I llama_init_from_model: n_ubatch      = 512
0.00.054.163 I llama_init_from_model: flash_attn    = 0
0.00.054.164 I llama_init_from_model: freq_base     = 10000.0
0.00.054.164 I llama_init_from_model: freq_scale    = 1
0.00.054.164 I ggml_metal_init: allocating
0.00.054.167 I ggml_metal_init: found device: Apple M4
0.00.054.169 I ggml_metal_init: picking default device: Apple M4
0.00.054.761 I ggml_metal_init: using embedded metal library
0.00.057.110 I ggml_metal_init: GPU name:   Apple M4
0.00.057.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.111 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.112 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.112 I ggml_metal_init: simdgroup reduction   = true
0.00.057.112 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.112 I ggml_metal_init: has bfloat            = true
0.00.057.112 I ggml_metal_init: use bfloat            = true
0.00.057.113 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.700 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.967 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.990 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.014 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.006 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.007 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.008 I llama_init_from_model: graph nodes  = 967
0.00.087.008 I llama_init_from_model: graph splits = 2
0.00.087.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.236 I main: llama threadpool init, n_threads = 4
0.00.527.280 I 
0.00.527.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.331 I 
0.00.527.554 I sampler seed: 1234
0.00.527.558 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.527.599 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.527.600 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.527.600 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.273.117 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.273.118 I llama_perf_context_print:        load time =     515.99 ms
0.01.273.118 I llama_perf_context_print: prompt eval time =      40.43 ms /     7 tokens (    5.78 ms per token,   173.14 tokens per second)
0.01.273.120 I llama_perf_context_print:        eval time =     702.12 ms /    63 runs   (   11.14 ms per token,    89.73 tokens per second)
0.01.273.120 I llama_perf_context_print:       total time =     745.88 ms /    70 tokens
0.01.273.329 I ggml_metal_free: deallocating

real	0m1.290s
user	0m0.107s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.125 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.125 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.126 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.126 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.032 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.838 I llama_model_loader: - type  f32:  194 tensors
0.00.025.838 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.838 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.839 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.839 I print_info: file format = GGUF V3 (latest)
0.00.025.840 I print_info: file type   = Q4_K - Medium
0.00.025.840 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.610 I load: special tokens cache size = 25
0.00.050.605 I load: token to piece cache size = 0.2984 MB
0.00.050.608 I print_info: arch             = gptneox
0.00.050.608 I print_info: vocab_only       = 0
0.00.050.609 I print_info: n_ctx_train      = 2048
0.00.050.609 I print_info: n_embd           = 2048
0.00.050.609 I print_info: n_layer          = 24
0.00.050.612 I print_info: n_head           = 16
0.00.050.613 I print_info: n_head_kv        = 16
0.00.050.613 I print_info: n_rot            = 32
0.00.050.613 I print_info: n_swa            = 0
0.00.050.613 I print_info: n_embd_head_k    = 128
0.00.050.614 I print_info: n_embd_head_v    = 128
0.00.050.616 I print_info: n_gqa            = 1
0.00.050.617 I print_info: n_embd_k_gqa     = 2048
0.00.050.617 I print_info: n_embd_v_gqa     = 2048
0.00.050.618 I print_info: f_norm_eps       = 1.0e-05
0.00.050.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.619 I print_info: f_logit_scale    = 0.0e+00
0.00.050.620 I print_info: n_ff             = 8192
0.00.050.620 I print_info: n_expert         = 0
0.00.050.620 I print_info: n_expert_used    = 0
0.00.050.620 I print_info: causal attn      = 1
0.00.050.621 I print_info: pooling type     = 0
0.00.050.622 I print_info: rope type        = 2
0.00.050.623 I print_info: rope scaling     = linear
0.00.050.623 I print_info: freq_base_train  = 10000.0
0.00.050.623 I print_info: freq_scale_train = 1
0.00.050.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.624 I print_info: rope_finetuned   = unknown
0.00.050.627 I print_info: ssm_d_conv       = 0
0.00.050.628 I print_info: ssm_d_inner      = 0
0.00.050.628 I print_info: ssm_d_state      = 0
0.00.050.629 I print_info: ssm_dt_rank      = 0
0.00.050.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.630 I print_info: model type       = 1.4B
0.00.050.630 I print_info: model params     = 1.41 B
0.00.050.630 I print_info: general.name     = 1.4B
0.00.050.631 I print_info: vocab type       = BPE
0.00.050.631 I print_info: n_vocab          = 50304
0.00.050.631 I print_info: n_merges         = 50009
0.00.050.631 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.631 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.631 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.631 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.632 I print_info: LF token         = 128 'Ä'
0.00.050.635 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: max token length = 1024
0.00.052.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.546 I load_tensors: offloading output layer to GPU
0.00.052.546 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.556 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.558 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.826 I llama_init_from_model: n_seq_max     = 1
0.00.052.826 I llama_init_from_model: n_ctx         = 2048
0.00.052.827 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.827 I llama_init_from_model: n_batch       = 2048
0.00.052.827 I llama_init_from_model: n_ubatch      = 512
0.00.052.827 I llama_init_from_model: flash_attn    = 0
0.00.052.828 I llama_init_from_model: freq_base     = 10000.0
0.00.052.828 I llama_init_from_model: freq_scale    = 1
0.00.052.828 I ggml_metal_init: allocating
0.00.052.831 I ggml_metal_init: found device: Apple M4
0.00.052.833 I ggml_metal_init: picking default device: Apple M4
0.00.053.433 I ggml_metal_init: using embedded metal library
0.00.055.749 I ggml_metal_init: GPU name:   Apple M4
0.00.055.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.751 I ggml_metal_init: simdgroup reduction   = true
0.00.055.751 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.751 I ggml_metal_init: has bfloat            = true
0.00.055.752 I ggml_metal_init: use bfloat            = true
0.00.055.752 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.338 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.112 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.131 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.219 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.221 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.221 I llama_init_from_model: graph nodes  = 967
0.00.086.221 I llama_init_from_model: graph splits = 2
0.00.086.224 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.770 I main: llama threadpool init, n_threads = 4
0.00.600.815 I 
0.00.600.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.856 I 
0.00.601.103 I sampler seed: 1234
0.00.601.108 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.150 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.151 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.363.876 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.363.876 I llama_perf_context_print:        load time =     591.08 ms
0.01.363.877 I llama_perf_context_print: prompt eval time =      47.14 ms /     7 tokens (    6.73 ms per token,   148.48 tokens per second)
0.01.363.883 I llama_perf_context_print:        eval time =     712.76 ms /    63 runs   (   11.31 ms per token,    88.39 tokens per second)
0.01.363.885 I llama_perf_context_print:       total time =     763.11 ms /    70 tokens
0.01.364.177 I ggml_metal_free: deallocating

real	0m1.383s
user	0m0.109s
sys	0m0.137s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.658 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.665 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.718 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.636 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.637 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.638 I llama_model_loader: - type  f32:  194 tensors
0.00.025.638 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.638 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.639 I print_info: file format = GGUF V3 (latest)
0.00.025.640 I print_info: file type   = Q5_K - Medium
0.00.025.641 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.948 I load: special tokens cache size = 25
0.00.051.116 I load: token to piece cache size = 0.2984 MB
0.00.051.120 I print_info: arch             = gptneox
0.00.051.120 I print_info: vocab_only       = 0
0.00.051.120 I print_info: n_ctx_train      = 2048
0.00.051.120 I print_info: n_embd           = 2048
0.00.051.121 I print_info: n_layer          = 24
0.00.051.125 I print_info: n_head           = 16
0.00.051.128 I print_info: n_head_kv        = 16
0.00.051.128 I print_info: n_rot            = 32
0.00.051.128 I print_info: n_swa            = 0
0.00.051.128 I print_info: n_embd_head_k    = 128
0.00.051.128 I print_info: n_embd_head_v    = 128
0.00.051.129 I print_info: n_gqa            = 1
0.00.051.130 I print_info: n_embd_k_gqa     = 2048
0.00.051.130 I print_info: n_embd_v_gqa     = 2048
0.00.051.131 I print_info: f_norm_eps       = 1.0e-05
0.00.051.131 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.131 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.131 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.132 I print_info: f_logit_scale    = 0.0e+00
0.00.051.132 I print_info: n_ff             = 8192
0.00.051.132 I print_info: n_expert         = 0
0.00.051.132 I print_info: n_expert_used    = 0
0.00.051.133 I print_info: causal attn      = 1
0.00.051.133 I print_info: pooling type     = 0
0.00.051.133 I print_info: rope type        = 2
0.00.051.133 I print_info: rope scaling     = linear
0.00.051.133 I print_info: freq_base_train  = 10000.0
0.00.051.134 I print_info: freq_scale_train = 1
0.00.051.134 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.134 I print_info: rope_finetuned   = unknown
0.00.051.134 I print_info: ssm_d_conv       = 0
0.00.051.134 I print_info: ssm_d_inner      = 0
0.00.051.134 I print_info: ssm_d_state      = 0
0.00.051.135 I print_info: ssm_dt_rank      = 0
0.00.051.135 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.135 I print_info: model type       = 1.4B
0.00.051.135 I print_info: model params     = 1.41 B
0.00.051.135 I print_info: general.name     = 1.4B
0.00.051.136 I print_info: vocab type       = BPE
0.00.051.136 I print_info: n_vocab          = 50304
0.00.051.136 I print_info: n_merges         = 50009
0.00.051.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.137 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.137 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.137 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.137 I print_info: LF token         = 128 'Ä'
0.00.051.138 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.138 I print_info: max token length = 1024
0.00.053.114 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.114 I load_tensors: offloading output layer to GPU
0.00.053.114 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.126 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.127 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.469 I llama_init_from_model: n_seq_max     = 1
0.00.053.469 I llama_init_from_model: n_ctx         = 2048
0.00.053.470 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.470 I llama_init_from_model: n_batch       = 2048
0.00.053.470 I llama_init_from_model: n_ubatch      = 512
0.00.053.470 I llama_init_from_model: flash_attn    = 0
0.00.053.471 I llama_init_from_model: freq_base     = 10000.0
0.00.053.471 I llama_init_from_model: freq_scale    = 1
0.00.053.472 I ggml_metal_init: allocating
0.00.053.475 I ggml_metal_init: found device: Apple M4
0.00.053.477 I ggml_metal_init: picking default device: Apple M4
0.00.054.099 I ggml_metal_init: using embedded metal library
0.00.056.564 I ggml_metal_init: GPU name:   Apple M4
0.00.056.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.567 I ggml_metal_init: simdgroup reduction   = true
0.00.056.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.567 I ggml_metal_init: has bfloat            = true
0.00.056.567 I ggml_metal_init: use bfloat            = true
0.00.056.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.691 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.185 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.204 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.222 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.266 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.267 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.267 I llama_init_from_model: graph nodes  = 967
0.00.087.267 I llama_init_from_model: graph splits = 2
0.00.087.270 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.148.476 I main: llama threadpool init, n_threads = 4
0.01.148.525 I 
0.01.148.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.148.554 I 
0.01.148.773 I sampler seed: 1234
0.01.148.777 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.148.789 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.148.789 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.148.789 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.993.154 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.01.993.155 I llama_perf_context_print:        load time =    1139.66 ms
0.01.993.156 I llama_perf_context_print: prompt eval time =      51.34 ms /     7 tokens (    7.33 ms per token,   136.33 tokens per second)
0.01.993.157 I llama_perf_context_print:        eval time =     790.37 ms /    63 runs   (   12.55 ms per token,    79.71 tokens per second)
0.01.993.157 I llama_perf_context_print:       total time =     844.68 ms /    70 tokens
0.01.993.422 I ggml_metal_free: deallocating

real	0m2.012s
user	0m0.110s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.012.735 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.020.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.111 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.113 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.113 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.113 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.113 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.118 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.118 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.120 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.783 I llama_model_loader: - type  f32:  194 tensors
0.00.028.783 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.784 I print_info: file format = GGUF V3 (latest)
0.00.028.784 I print_info: file type   = Q6_K
0.00.028.785 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.048.145 I load: special tokens cache size = 25
0.00.054.107 I load: token to piece cache size = 0.2984 MB
0.00.054.110 I print_info: arch             = gptneox
0.00.054.110 I print_info: vocab_only       = 0
0.00.054.110 I print_info: n_ctx_train      = 2048
0.00.054.110 I print_info: n_embd           = 2048
0.00.054.111 I print_info: n_layer          = 24
0.00.054.113 I print_info: n_head           = 16
0.00.054.114 I print_info: n_head_kv        = 16
0.00.054.116 I print_info: n_rot            = 32
0.00.054.117 I print_info: n_swa            = 0
0.00.054.117 I print_info: n_embd_head_k    = 128
0.00.054.117 I print_info: n_embd_head_v    = 128
0.00.054.118 I print_info: n_gqa            = 1
0.00.054.119 I print_info: n_embd_k_gqa     = 2048
0.00.054.124 I print_info: n_embd_v_gqa     = 2048
0.00.054.124 I print_info: f_norm_eps       = 1.0e-05
0.00.054.125 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.125 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.125 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.127 I print_info: f_logit_scale    = 0.0e+00
0.00.054.129 I print_info: n_ff             = 8192
0.00.054.129 I print_info: n_expert         = 0
0.00.054.130 I print_info: n_expert_used    = 0
0.00.054.130 I print_info: causal attn      = 1
0.00.054.133 I print_info: pooling type     = 0
0.00.054.133 I print_info: rope type        = 2
0.00.054.133 I print_info: rope scaling     = linear
0.00.054.133 I print_info: freq_base_train  = 10000.0
0.00.054.135 I print_info: freq_scale_train = 1
0.00.054.135 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.135 I print_info: rope_finetuned   = unknown
0.00.054.135 I print_info: ssm_d_conv       = 0
0.00.054.135 I print_info: ssm_d_inner      = 0
0.00.054.136 I print_info: ssm_d_state      = 0
0.00.054.136 I print_info: ssm_dt_rank      = 0
0.00.054.136 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.136 I print_info: model type       = 1.4B
0.00.054.136 I print_info: model params     = 1.41 B
0.00.054.137 I print_info: general.name     = 1.4B
0.00.054.137 I print_info: vocab type       = BPE
0.00.054.137 I print_info: n_vocab          = 50304
0.00.054.138 I print_info: n_merges         = 50009
0.00.054.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.142 I print_info: LF token         = 128 'Ä'
0.00.054.142 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.142 I print_info: max token length = 1024
0.00.056.195 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.195 I load_tensors: offloading output layer to GPU
0.00.056.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.206 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.207 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.056.489 I llama_init_from_model: n_seq_max     = 1
0.00.056.490 I llama_init_from_model: n_ctx         = 2048
0.00.056.490 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.490 I llama_init_from_model: n_batch       = 2048
0.00.056.491 I llama_init_from_model: n_ubatch      = 512
0.00.056.491 I llama_init_from_model: flash_attn    = 0
0.00.056.491 I llama_init_from_model: freq_base     = 10000.0
0.00.056.491 I llama_init_from_model: freq_scale    = 1
0.00.056.492 I ggml_metal_init: allocating
0.00.056.495 I ggml_metal_init: found device: Apple M4
0.00.056.497 I ggml_metal_init: picking default device: Apple M4
0.00.057.126 I ggml_metal_init: using embedded metal library
0.00.059.466 I ggml_metal_init: GPU name:   Apple M4
0.00.059.467 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.467 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.468 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.468 I ggml_metal_init: simdgroup reduction   = true
0.00.059.468 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.468 I ggml_metal_init: has bfloat            = true
0.00.059.469 I ggml_metal_init: use bfloat            = true
0.00.059.469 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.470 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.764 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.894 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.908 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.928 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.082 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.092.083 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.092.083 I llama_init_from_model: graph nodes  = 967
0.00.092.084 I llama_init_from_model: graph splits = 2
0.00.092.087 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.861 I main: llama threadpool init, n_threads = 4
0.00.747.898 I 
0.00.747.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.931 I 
0.00.748.156 I sampler seed: 1234
0.00.748.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.201 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.201 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.201 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.629.633 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.629.634 I llama_perf_context_print:        load time =     735.12 ms
0.01.629.635 I llama_perf_context_print: prompt eval time =      58.33 ms /     7 tokens (    8.33 ms per token,   120.00 tokens per second)
0.01.629.635 I llama_perf_context_print:        eval time =     820.09 ms /    63 runs   (   13.02 ms per token,    76.82 tokens per second)
0.01.629.636 I llama_perf_context_print:       total time =     881.77 ms /    70 tokens
0.01.629.860 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.111s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.066 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.025 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.031 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.034 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.039 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.040 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.044 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.050 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.050 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.589 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.978 I llama_model_loader: - type  f32:  194 tensors
0.00.054.978 I llama_model_loader: - type  f16:   98 tensors
0.00.054.979 I print_info: file format = GGUF V3 (latest)
0.00.054.980 I print_info: file type   = all F32 (guessed)
0.00.054.981 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.715 I load: special tokens cache size = 25
0.00.089.509 I load: token to piece cache size = 0.2984 MB
0.00.089.512 I print_info: arch             = gptneox
0.00.089.512 I print_info: vocab_only       = 0
0.00.089.512 I print_info: n_ctx_train      = 2048
0.00.089.512 I print_info: n_embd           = 2048
0.00.089.512 I print_info: n_layer          = 24
0.00.089.515 I print_info: n_head           = 16
0.00.089.516 I print_info: n_head_kv        = 16
0.00.089.517 I print_info: n_rot            = 32
0.00.089.517 I print_info: n_swa            = 0
0.00.089.517 I print_info: n_embd_head_k    = 128
0.00.089.517 I print_info: n_embd_head_v    = 128
0.00.089.518 I print_info: n_gqa            = 1
0.00.089.520 I print_info: n_embd_k_gqa     = 2048
0.00.089.521 I print_info: n_embd_v_gqa     = 2048
0.00.089.522 I print_info: f_norm_eps       = 1.0e-05
0.00.089.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.522 I print_info: f_logit_scale    = 0.0e+00
0.00.089.523 I print_info: n_ff             = 8192
0.00.089.523 I print_info: n_expert         = 0
0.00.089.523 I print_info: n_expert_used    = 0
0.00.089.523 I print_info: causal attn      = 1
0.00.089.523 I print_info: pooling type     = 0
0.00.089.524 I print_info: rope type        = 2
0.00.089.524 I print_info: rope scaling     = linear
0.00.089.525 I print_info: freq_base_train  = 10000.0
0.00.089.525 I print_info: freq_scale_train = 1
0.00.089.526 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.526 I print_info: rope_finetuned   = unknown
0.00.089.526 I print_info: ssm_d_conv       = 0
0.00.089.526 I print_info: ssm_d_inner      = 0
0.00.089.526 I print_info: ssm_d_state      = 0
0.00.089.526 I print_info: ssm_dt_rank      = 0
0.00.089.527 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.527 I print_info: model type       = 1.4B
0.00.089.527 I print_info: model params     = 1.41 B
0.00.089.527 I print_info: general.name     = 1.4B
0.00.089.528 I print_info: vocab type       = BPE
0.00.089.528 I print_info: n_vocab          = 50304
0.00.089.528 I print_info: n_merges         = 50009
0.00.089.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.529 I print_info: LF token         = 128 'Ä'
0.00.089.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.529 I print_info: max token length = 1024
0.00.091.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.970 I load_tensors: offloading output layer to GPU
0.00.091.971 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.981 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.982 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.238 I llama_init_from_model: n_seq_max     = 1
0.00.092.238 I llama_init_from_model: n_ctx         = 128
0.00.092.239 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.239 I llama_init_from_model: n_batch       = 128
0.00.092.239 I llama_init_from_model: n_ubatch      = 128
0.00.092.239 I llama_init_from_model: flash_attn    = 0
0.00.092.239 I llama_init_from_model: freq_base     = 10000.0
0.00.092.240 I llama_init_from_model: freq_scale    = 1
0.00.092.240 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.240 I ggml_metal_init: allocating
0.00.092.243 I ggml_metal_init: found device: Apple M4
0.00.092.245 I ggml_metal_init: picking default device: Apple M4
0.00.092.844 I ggml_metal_init: using embedded metal library
0.00.095.490 I ggml_metal_init: GPU name:   Apple M4
0.00.095.492 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.493 I ggml_metal_init: simdgroup reduction   = true
0.00.095.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.493 I ggml_metal_init: has bfloat            = true
0.00.095.493 I ggml_metal_init: use bfloat            = true
0.00.095.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.139 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.461 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.475 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.490 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.392 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.393 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.393 I llama_init_from_model: graph nodes  = 967
0.00.107.393 I llama_init_from_model: graph splits = 2
0.00.107.394 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.394 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.139 I 
0.00.928.277 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.292 I perplexity: tokenizing the input ..
0.00.943.929 I perplexity: tokenization took 15.632 ms
0.00.943.939 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.063.844 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.065.454 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.065.497 I llama_perf_context_print:        load time =     905.05 ms
0.01.065.498 I llama_perf_context_print: prompt eval time =     119.66 ms /   128 tokens (    0.93 ms per token,  1069.67 tokens per second)
0.01.065.499 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.065.500 I llama_perf_context_print:       total time =     137.37 ms /   129 tokens
0.01.066.084 I ggml_metal_free: deallocating

real	0m1.257s
user	0m0.125s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.554 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.871 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.877 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.880 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.881 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.881 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.881 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.882 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.883 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.883 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.884 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.884 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.885 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.887 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.888 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.929 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.929 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.930 I llama_model_loader: - type  f32:  194 tensors
0.00.029.931 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.931 I print_info: file format = GGUF V3 (latest)
0.00.029.932 I print_info: file type   = Q8_0
0.00.029.932 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.052.425 I load: special tokens cache size = 25
0.00.058.600 I load: token to piece cache size = 0.2984 MB
0.00.058.603 I print_info: arch             = gptneox
0.00.058.604 I print_info: vocab_only       = 0
0.00.058.604 I print_info: n_ctx_train      = 2048
0.00.058.604 I print_info: n_embd           = 2048
0.00.058.604 I print_info: n_layer          = 24
0.00.058.607 I print_info: n_head           = 16
0.00.058.608 I print_info: n_head_kv        = 16
0.00.058.608 I print_info: n_rot            = 32
0.00.058.608 I print_info: n_swa            = 0
0.00.058.609 I print_info: n_embd_head_k    = 128
0.00.058.609 I print_info: n_embd_head_v    = 128
0.00.058.609 I print_info: n_gqa            = 1
0.00.058.610 I print_info: n_embd_k_gqa     = 2048
0.00.058.611 I print_info: n_embd_v_gqa     = 2048
0.00.058.611 I print_info: f_norm_eps       = 1.0e-05
0.00.058.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.614 I print_info: f_logit_scale    = 0.0e+00
0.00.058.615 I print_info: n_ff             = 8192
0.00.058.616 I print_info: n_expert         = 0
0.00.058.617 I print_info: n_expert_used    = 0
0.00.058.617 I print_info: causal attn      = 1
0.00.058.617 I print_info: pooling type     = 0
0.00.058.617 I print_info: rope type        = 2
0.00.058.617 I print_info: rope scaling     = linear
0.00.058.618 I print_info: freq_base_train  = 10000.0
0.00.058.618 I print_info: freq_scale_train = 1
0.00.058.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.618 I print_info: rope_finetuned   = unknown
0.00.058.618 I print_info: ssm_d_conv       = 0
0.00.058.619 I print_info: ssm_d_inner      = 0
0.00.058.619 I print_info: ssm_d_state      = 0
0.00.058.619 I print_info: ssm_dt_rank      = 0
0.00.058.619 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.619 I print_info: model type       = 1.4B
0.00.058.620 I print_info: model params     = 1.41 B
0.00.058.623 I print_info: general.name     = 1.4B
0.00.058.624 I print_info: vocab type       = BPE
0.00.058.624 I print_info: n_vocab          = 50304
0.00.058.624 I print_info: n_merges         = 50009
0.00.058.625 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.625 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.625 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.626 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.626 I print_info: LF token         = 128 'Ä'
0.00.058.626 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.626 I print_info: max token length = 1024
0.00.060.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.796 I load_tensors: offloading output layer to GPU
0.00.060.796 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.808 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.809 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.061.105 I llama_init_from_model: n_seq_max     = 1
0.00.061.105 I llama_init_from_model: n_ctx         = 128
0.00.061.105 I llama_init_from_model: n_ctx_per_seq = 128
0.00.061.106 I llama_init_from_model: n_batch       = 128
0.00.061.106 I llama_init_from_model: n_ubatch      = 128
0.00.061.106 I llama_init_from_model: flash_attn    = 0
0.00.061.106 I llama_init_from_model: freq_base     = 10000.0
0.00.061.107 I llama_init_from_model: freq_scale    = 1
0.00.061.107 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.107 I ggml_metal_init: allocating
0.00.061.111 I ggml_metal_init: found device: Apple M4
0.00.061.113 I ggml_metal_init: picking default device: Apple M4
0.00.061.708 I ggml_metal_init: using embedded metal library
0.00.064.260 I ggml_metal_init: GPU name:   Apple M4
0.00.064.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.262 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.262 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.263 I ggml_metal_init: simdgroup reduction   = true
0.00.064.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.263 I ggml_metal_init: has bfloat            = true
0.00.064.263 I ggml_metal_init: use bfloat            = true
0.00.064.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.588 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.075.843 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.859 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.874 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.076.836 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.076.837 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.076.837 I llama_init_from_model: graph nodes  = 967
0.00.076.837 I llama_init_from_model: graph splits = 2
0.00.076.839 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.076.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.920 I 
0.00.860.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.971 I perplexity: tokenizing the input ..
0.00.868.829 I perplexity: tokenization took 7.857 ms
0.00.868.839 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.993.417 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.994.573 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.994.597 I llama_perf_context_print:        load time =     851.36 ms
0.00.994.599 I llama_perf_context_print: prompt eval time =     124.35 ms /   128 tokens (    0.97 ms per token,  1029.33 tokens per second)
0.00.994.599 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.994.600 I llama_perf_context_print:       total time =     133.68 ms /   129 tokens
0.00.995.029 I ggml_metal_free: deallocating

real	0m1.011s
user	0m0.089s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.159 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.280 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.285 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.851 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.852 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.853 I llama_model_loader: - type  f32:  194 tensors
0.00.025.853 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.853 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.854 I print_info: file format = GGUF V3 (latest)
0.00.025.854 I print_info: file type   = Q4_0
0.00.025.855 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.245 I load: special tokens cache size = 25
0.00.051.235 I load: token to piece cache size = 0.2984 MB
0.00.051.238 I print_info: arch             = gptneox
0.00.051.238 I print_info: vocab_only       = 0
0.00.051.238 I print_info: n_ctx_train      = 2048
0.00.051.238 I print_info: n_embd           = 2048
0.00.051.239 I print_info: n_layer          = 24
0.00.051.242 I print_info: n_head           = 16
0.00.051.243 I print_info: n_head_kv        = 16
0.00.051.243 I print_info: n_rot            = 32
0.00.051.243 I print_info: n_swa            = 0
0.00.051.243 I print_info: n_embd_head_k    = 128
0.00.051.243 I print_info: n_embd_head_v    = 128
0.00.051.244 I print_info: n_gqa            = 1
0.00.051.245 I print_info: n_embd_k_gqa     = 2048
0.00.051.248 I print_info: n_embd_v_gqa     = 2048
0.00.051.249 I print_info: f_norm_eps       = 1.0e-05
0.00.051.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.256 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.258 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.258 I print_info: f_logit_scale    = 0.0e+00
0.00.051.260 I print_info: n_ff             = 8192
0.00.051.260 I print_info: n_expert         = 0
0.00.051.260 I print_info: n_expert_used    = 0
0.00.051.261 I print_info: causal attn      = 1
0.00.051.261 I print_info: pooling type     = 0
0.00.051.261 I print_info: rope type        = 2
0.00.051.261 I print_info: rope scaling     = linear
0.00.051.261 I print_info: freq_base_train  = 10000.0
0.00.051.262 I print_info: freq_scale_train = 1
0.00.051.262 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.262 I print_info: rope_finetuned   = unknown
0.00.051.262 I print_info: ssm_d_conv       = 0
0.00.051.262 I print_info: ssm_d_inner      = 0
0.00.051.262 I print_info: ssm_d_state      = 0
0.00.051.262 I print_info: ssm_dt_rank      = 0
0.00.051.263 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.263 I print_info: model type       = 1.4B
0.00.051.264 I print_info: model params     = 1.41 B
0.00.051.264 I print_info: general.name     = 1.4B
0.00.051.265 I print_info: vocab type       = BPE
0.00.051.265 I print_info: n_vocab          = 50304
0.00.051.265 I print_info: n_merges         = 50009
0.00.051.266 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.266 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.266 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.266 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.267 I print_info: LF token         = 128 'Ä'
0.00.051.267 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.268 I print_info: max token length = 1024
0.00.053.143 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.143 I load_tensors: offloading output layer to GPU
0.00.053.143 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.154 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.155 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.422 I llama_init_from_model: n_seq_max     = 1
0.00.053.422 I llama_init_from_model: n_ctx         = 128
0.00.053.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.423 I llama_init_from_model: n_batch       = 128
0.00.053.423 I llama_init_from_model: n_ubatch      = 128
0.00.053.423 I llama_init_from_model: flash_attn    = 0
0.00.053.423 I llama_init_from_model: freq_base     = 10000.0
0.00.053.424 I llama_init_from_model: freq_scale    = 1
0.00.053.424 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.424 I ggml_metal_init: allocating
0.00.053.427 I ggml_metal_init: found device: Apple M4
0.00.053.429 I ggml_metal_init: picking default device: Apple M4
0.00.053.983 I ggml_metal_init: using embedded metal library
0.00.056.365 I ggml_metal_init: GPU name:   Apple M4
0.00.056.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.368 I ggml_metal_init: simdgroup reduction   = true
0.00.056.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.368 I ggml_metal_init: has bfloat            = true
0.00.056.368 I ggml_metal_init: use bfloat            = true
0.00.056.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.084 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.410 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.426 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.259 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.261 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.261 I llama_init_from_model: graph nodes  = 967
0.00.068.261 I llama_init_from_model: graph splits = 2
0.00.068.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.211 I 
0.00.568.250 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.258 I perplexity: tokenizing the input ..
0.00.576.598 I perplexity: tokenization took 8.338 ms
0.00.576.603 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.266 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.700.577 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.700.609 I llama_perf_context_print:        load time =     558.05 ms
0.00.700.611 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.44 tokens per second)
0.00.700.612 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.612 I llama_perf_context_print:       total time =     132.40 ms /   129 tokens
0.00.701.130 I ggml_metal_free: deallocating

real	0m0.718s
user	0m0.077s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.512 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.758 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.759 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.760 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.344 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.345 I llama_model_loader: - type  f32:  194 tensors
0.00.025.346 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.346 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.346 I print_info: file format = GGUF V3 (latest)
0.00.025.347 I print_info: file type   = Q4_1
0.00.025.352 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.945 I load: special tokens cache size = 25
0.00.049.950 I load: token to piece cache size = 0.2984 MB
0.00.049.953 I print_info: arch             = gptneox
0.00.049.953 I print_info: vocab_only       = 0
0.00.049.953 I print_info: n_ctx_train      = 2048
0.00.049.953 I print_info: n_embd           = 2048
0.00.049.954 I print_info: n_layer          = 24
0.00.049.957 I print_info: n_head           = 16
0.00.049.958 I print_info: n_head_kv        = 16
0.00.049.958 I print_info: n_rot            = 32
0.00.049.958 I print_info: n_swa            = 0
0.00.049.958 I print_info: n_embd_head_k    = 128
0.00.049.958 I print_info: n_embd_head_v    = 128
0.00.049.959 I print_info: n_gqa            = 1
0.00.049.960 I print_info: n_embd_k_gqa     = 2048
0.00.049.961 I print_info: n_embd_v_gqa     = 2048
0.00.049.961 I print_info: f_norm_eps       = 1.0e-05
0.00.049.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.962 I print_info: f_logit_scale    = 0.0e+00
0.00.049.963 I print_info: n_ff             = 8192
0.00.049.963 I print_info: n_expert         = 0
0.00.049.963 I print_info: n_expert_used    = 0
0.00.049.963 I print_info: causal attn      = 1
0.00.049.963 I print_info: pooling type     = 0
0.00.049.964 I print_info: rope type        = 2
0.00.049.966 I print_info: rope scaling     = linear
0.00.049.967 I print_info: freq_base_train  = 10000.0
0.00.049.968 I print_info: freq_scale_train = 1
0.00.049.968 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.968 I print_info: rope_finetuned   = unknown
0.00.049.968 I print_info: ssm_d_conv       = 0
0.00.049.968 I print_info: ssm_d_inner      = 0
0.00.049.969 I print_info: ssm_d_state      = 0
0.00.049.969 I print_info: ssm_dt_rank      = 0
0.00.049.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.969 I print_info: model type       = 1.4B
0.00.049.969 I print_info: model params     = 1.41 B
0.00.049.969 I print_info: general.name     = 1.4B
0.00.049.970 I print_info: vocab type       = BPE
0.00.049.970 I print_info: n_vocab          = 50304
0.00.049.971 I print_info: n_merges         = 50009
0.00.049.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.971 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.971 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.972 I print_info: LF token         = 128 'Ä'
0.00.049.972 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.972 I print_info: max token length = 1024
0.00.051.939 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.939 I load_tensors: offloading output layer to GPU
0.00.051.939 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.950 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.951 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.265 I llama_init_from_model: n_seq_max     = 1
0.00.052.266 I llama_init_from_model: n_ctx         = 128
0.00.052.266 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.266 I llama_init_from_model: n_batch       = 128
0.00.052.266 I llama_init_from_model: n_ubatch      = 128
0.00.052.266 I llama_init_from_model: flash_attn    = 0
0.00.052.267 I llama_init_from_model: freq_base     = 10000.0
0.00.052.267 I llama_init_from_model: freq_scale    = 1
0.00.052.267 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.268 I ggml_metal_init: allocating
0.00.052.270 I ggml_metal_init: found device: Apple M4
0.00.052.273 I ggml_metal_init: picking default device: Apple M4
0.00.052.871 I ggml_metal_init: using embedded metal library
0.00.055.206 I ggml_metal_init: GPU name:   Apple M4
0.00.055.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.208 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.209 I ggml_metal_init: simdgroup reduction   = true
0.00.055.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.209 I ggml_metal_init: has bfloat            = true
0.00.055.209 I ggml_metal_init: use bfloat            = true
0.00.055.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.210 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.756 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.993 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.008 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.023 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.949 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.950 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.950 I llama_init_from_model: graph nodes  = 967
0.00.066.950 I llama_init_from_model: graph splits = 2
0.00.066.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.540 I 
0.00.621.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.576 I perplexity: tokenizing the input ..
0.00.629.646 I perplexity: tokenization took 8.068 ms
0.00.629.649 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.316 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.753.467 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.753.497 I llama_perf_context_print:        load time =     612.02 ms
0.00.753.498 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.42 tokens per second)
0.00.753.499 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.499 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.753.967 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.077s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.498 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.498 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.499 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.502 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.336 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.085 I llama_model_loader: - type  f32:  194 tensors
0.00.026.085 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.086 I print_info: file format = GGUF V3 (latest)
0.00.026.086 I print_info: file type   = Q5_0
0.00.026.087 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.348 I load: special tokens cache size = 25
0.00.051.299 I load: token to piece cache size = 0.2984 MB
0.00.051.302 I print_info: arch             = gptneox
0.00.051.302 I print_info: vocab_only       = 0
0.00.051.303 I print_info: n_ctx_train      = 2048
0.00.051.303 I print_info: n_embd           = 2048
0.00.051.303 I print_info: n_layer          = 24
0.00.051.306 I print_info: n_head           = 16
0.00.051.307 I print_info: n_head_kv        = 16
0.00.051.307 I print_info: n_rot            = 32
0.00.051.307 I print_info: n_swa            = 0
0.00.051.307 I print_info: n_embd_head_k    = 128
0.00.051.308 I print_info: n_embd_head_v    = 128
0.00.051.308 I print_info: n_gqa            = 1
0.00.051.309 I print_info: n_embd_k_gqa     = 2048
0.00.051.310 I print_info: n_embd_v_gqa     = 2048
0.00.051.310 I print_info: f_norm_eps       = 1.0e-05
0.00.051.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.311 I print_info: f_logit_scale    = 0.0e+00
0.00.051.312 I print_info: n_ff             = 8192
0.00.051.312 I print_info: n_expert         = 0
0.00.051.312 I print_info: n_expert_used    = 0
0.00.051.312 I print_info: causal attn      = 1
0.00.051.312 I print_info: pooling type     = 0
0.00.051.315 I print_info: rope type        = 2
0.00.051.315 I print_info: rope scaling     = linear
0.00.051.315 I print_info: freq_base_train  = 10000.0
0.00.051.316 I print_info: freq_scale_train = 1
0.00.051.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.316 I print_info: rope_finetuned   = unknown
0.00.051.316 I print_info: ssm_d_conv       = 0
0.00.051.316 I print_info: ssm_d_inner      = 0
0.00.051.316 I print_info: ssm_d_state      = 0
0.00.051.317 I print_info: ssm_dt_rank      = 0
0.00.051.317 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.317 I print_info: model type       = 1.4B
0.00.051.319 I print_info: model params     = 1.41 B
0.00.051.319 I print_info: general.name     = 1.4B
0.00.051.319 I print_info: vocab type       = BPE
0.00.051.319 I print_info: n_vocab          = 50304
0.00.051.320 I print_info: n_merges         = 50009
0.00.051.320 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.321 I print_info: LF token         = 128 'Ä'
0.00.051.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: max token length = 1024
0.00.053.362 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.363 I load_tensors: offloading output layer to GPU
0.00.053.363 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.374 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.375 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.638 I llama_init_from_model: n_seq_max     = 1
0.00.053.639 I llama_init_from_model: n_ctx         = 128
0.00.053.639 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.640 I llama_init_from_model: n_batch       = 128
0.00.053.640 I llama_init_from_model: n_ubatch      = 128
0.00.053.640 I llama_init_from_model: flash_attn    = 0
0.00.053.640 I llama_init_from_model: freq_base     = 10000.0
0.00.053.640 I llama_init_from_model: freq_scale    = 1
0.00.053.641 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.641 I ggml_metal_init: allocating
0.00.053.644 I ggml_metal_init: found device: Apple M4
0.00.053.646 I ggml_metal_init: picking default device: Apple M4
0.00.054.241 I ggml_metal_init: using embedded metal library
0.00.056.616 I ggml_metal_init: GPU name:   Apple M4
0.00.056.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.618 I ggml_metal_init: simdgroup reduction   = true
0.00.056.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.619 I ggml_metal_init: has bfloat            = true
0.00.056.619 I ggml_metal_init: use bfloat            = true
0.00.056.619 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.620 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.453 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.694 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.707 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.667 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.668 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.668 I llama_init_from_model: graph nodes  = 967
0.00.068.668 I llama_init_from_model: graph splits = 2
0.00.068.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.655 I 
0.00.680.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.719 I perplexity: tokenizing the input ..
0.00.688.658 I perplexity: tokenization took 7.937 ms
0.00.688.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.883 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.825.165 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.825.195 I llama_perf_context_print:        load time =     670.40 ms
0.00.825.197 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.21 tokens per second)
0.00.825.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.198 I llama_perf_context_print:       total time =     144.54 ms /   129 tokens
0.00.825.645 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.210 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.004 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.010 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.011 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.011 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.015 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.018 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.004 I llama_model_loader: - type  f32:  194 tensors
0.00.025.004 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.004 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.005 I print_info: file format = GGUF V3 (latest)
0.00.025.005 I print_info: file type   = Q5_1
0.00.025.006 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.504 I load: special tokens cache size = 25
0.00.049.446 I load: token to piece cache size = 0.2984 MB
0.00.049.449 I print_info: arch             = gptneox
0.00.049.449 I print_info: vocab_only       = 0
0.00.049.449 I print_info: n_ctx_train      = 2048
0.00.049.449 I print_info: n_embd           = 2048
0.00.049.450 I print_info: n_layer          = 24
0.00.049.453 I print_info: n_head           = 16
0.00.049.453 I print_info: n_head_kv        = 16
0.00.049.454 I print_info: n_rot            = 32
0.00.049.454 I print_info: n_swa            = 0
0.00.049.454 I print_info: n_embd_head_k    = 128
0.00.049.454 I print_info: n_embd_head_v    = 128
0.00.049.455 I print_info: n_gqa            = 1
0.00.049.456 I print_info: n_embd_k_gqa     = 2048
0.00.049.456 I print_info: n_embd_v_gqa     = 2048
0.00.049.457 I print_info: f_norm_eps       = 1.0e-05
0.00.049.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.458 I print_info: f_logit_scale    = 0.0e+00
0.00.049.458 I print_info: n_ff             = 8192
0.00.049.459 I print_info: n_expert         = 0
0.00.049.459 I print_info: n_expert_used    = 0
0.00.049.459 I print_info: causal attn      = 1
0.00.049.459 I print_info: pooling type     = 0
0.00.049.460 I print_info: rope type        = 2
0.00.049.460 I print_info: rope scaling     = linear
0.00.049.460 I print_info: freq_base_train  = 10000.0
0.00.049.461 I print_info: freq_scale_train = 1
0.00.049.461 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.463 I print_info: rope_finetuned   = unknown
0.00.049.463 I print_info: ssm_d_conv       = 0
0.00.049.463 I print_info: ssm_d_inner      = 0
0.00.049.463 I print_info: ssm_d_state      = 0
0.00.049.463 I print_info: ssm_dt_rank      = 0
0.00.049.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.464 I print_info: model type       = 1.4B
0.00.049.464 I print_info: model params     = 1.41 B
0.00.049.464 I print_info: general.name     = 1.4B
0.00.049.465 I print_info: vocab type       = BPE
0.00.049.465 I print_info: n_vocab          = 50304
0.00.049.465 I print_info: n_merges         = 50009
0.00.049.465 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.473 I print_info: LF token         = 128 'Ä'
0.00.049.473 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.473 I print_info: max token length = 1024
0.00.051.453 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.453 I load_tensors: offloading output layer to GPU
0.00.051.453 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.464 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.465 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.781 I llama_init_from_model: n_seq_max     = 1
0.00.051.781 I llama_init_from_model: n_ctx         = 128
0.00.051.781 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.782 I llama_init_from_model: n_batch       = 128
0.00.051.782 I llama_init_from_model: n_ubatch      = 128
0.00.051.782 I llama_init_from_model: flash_attn    = 0
0.00.051.782 I llama_init_from_model: freq_base     = 10000.0
0.00.051.782 I llama_init_from_model: freq_scale    = 1
0.00.051.783 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.783 I ggml_metal_init: allocating
0.00.051.786 I ggml_metal_init: found device: Apple M4
0.00.051.788 I ggml_metal_init: picking default device: Apple M4
0.00.052.384 I ggml_metal_init: using embedded metal library
0.00.054.699 I ggml_metal_init: GPU name:   Apple M4
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.702 I ggml_metal_init: simdgroup reduction   = true
0.00.054.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.702 I ggml_metal_init: has bfloat            = true
0.00.054.702 I ggml_metal_init: use bfloat            = true
0.00.054.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.703 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.287 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.592 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.605 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.582 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.583 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.583 I llama_init_from_model: graph nodes  = 967
0.00.066.583 I llama_init_from_model: graph splits = 2
0.00.066.584 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.585 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.146 I 
0.00.707.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.185 I perplexity: tokenizing the input ..
0.00.714.754 I perplexity: tokenization took 7.567 ms
0.00.714.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.705 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.850.854 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.850.882 I llama_perf_context_print:        load time =     697.93 ms
0.00.850.883 I llama_perf_context_print: prompt eval time =     134.71 ms /   128 tokens (    1.05 ms per token,   950.17 tokens per second)
0.00.850.884 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.884 I llama_perf_context_print:       total time =     143.74 ms /   129 tokens
0.00.851.330 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.077s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.158 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.842 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.842 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.846 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.379 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.380 I llama_model_loader: - type  f32:  194 tensors
0.00.025.380 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.380 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.381 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.381 I print_info: file format = GGUF V3 (latest)
0.00.025.381 I print_info: file type   = Q2_K - Medium
0.00.025.382 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.865 I load: special tokens cache size = 25
0.00.049.810 I load: token to piece cache size = 0.2984 MB
0.00.049.813 I print_info: arch             = gptneox
0.00.049.814 I print_info: vocab_only       = 0
0.00.049.814 I print_info: n_ctx_train      = 2048
0.00.049.814 I print_info: n_embd           = 2048
0.00.049.814 I print_info: n_layer          = 24
0.00.049.817 I print_info: n_head           = 16
0.00.049.818 I print_info: n_head_kv        = 16
0.00.049.818 I print_info: n_rot            = 32
0.00.049.818 I print_info: n_swa            = 0
0.00.049.819 I print_info: n_embd_head_k    = 128
0.00.049.819 I print_info: n_embd_head_v    = 128
0.00.049.819 I print_info: n_gqa            = 1
0.00.049.820 I print_info: n_embd_k_gqa     = 2048
0.00.049.821 I print_info: n_embd_v_gqa     = 2048
0.00.049.822 I print_info: f_norm_eps       = 1.0e-05
0.00.049.822 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.822 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.822 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.822 I print_info: f_logit_scale    = 0.0e+00
0.00.049.823 I print_info: n_ff             = 8192
0.00.049.823 I print_info: n_expert         = 0
0.00.049.824 I print_info: n_expert_used    = 0
0.00.049.824 I print_info: causal attn      = 1
0.00.049.824 I print_info: pooling type     = 0
0.00.049.826 I print_info: rope type        = 2
0.00.049.826 I print_info: rope scaling     = linear
0.00.049.826 I print_info: freq_base_train  = 10000.0
0.00.049.827 I print_info: freq_scale_train = 1
0.00.049.827 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.827 I print_info: rope_finetuned   = unknown
0.00.049.827 I print_info: ssm_d_conv       = 0
0.00.049.827 I print_info: ssm_d_inner      = 0
0.00.049.828 I print_info: ssm_d_state      = 0
0.00.049.828 I print_info: ssm_dt_rank      = 0
0.00.049.828 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.828 I print_info: model type       = 1.4B
0.00.049.828 I print_info: model params     = 1.41 B
0.00.049.829 I print_info: general.name     = 1.4B
0.00.049.829 I print_info: vocab type       = BPE
0.00.049.829 I print_info: n_vocab          = 50304
0.00.049.829 I print_info: n_merges         = 50009
0.00.049.830 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.830 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.830 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.832 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.832 I print_info: LF token         = 128 'Ä'
0.00.049.832 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.833 I print_info: max token length = 1024
0.00.051.655 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.655 I load_tensors: offloading output layer to GPU
0.00.051.655 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.666 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.667 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.963 I llama_init_from_model: n_seq_max     = 1
0.00.051.964 I llama_init_from_model: n_ctx         = 128
0.00.051.964 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.964 I llama_init_from_model: n_batch       = 128
0.00.051.964 I llama_init_from_model: n_ubatch      = 128
0.00.051.964 I llama_init_from_model: flash_attn    = 0
0.00.051.965 I llama_init_from_model: freq_base     = 10000.0
0.00.051.965 I llama_init_from_model: freq_scale    = 1
0.00.051.965 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.966 I ggml_metal_init: allocating
0.00.051.969 I ggml_metal_init: found device: Apple M4
0.00.051.971 I ggml_metal_init: picking default device: Apple M4
0.00.052.524 I ggml_metal_init: using embedded metal library
0.00.054.868 I ggml_metal_init: GPU name:   Apple M4
0.00.054.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.870 I ggml_metal_init: simdgroup reduction   = true
0.00.054.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.870 I ggml_metal_init: has bfloat            = true
0.00.054.870 I ggml_metal_init: use bfloat            = true
0.00.054.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.309 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.704 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.719 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.737 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.651 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.652 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.652 I llama_init_from_model: graph nodes  = 967
0.00.066.652 I llama_init_from_model: graph splits = 2
0.00.066.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.406.888 I 
0.00.406.934 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.406.939 I perplexity: tokenizing the input ..
0.00.414.702 I perplexity: tokenization took 7.761 ms
0.00.414.706 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.547.172 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.548.481 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.548.507 I llama_perf_context_print:        load time =     396.72 ms
0.00.548.508 I llama_perf_context_print: prompt eval time =     132.24 ms /   128 tokens (    1.03 ms per token,   967.93 tokens per second)
0.00.548.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.548.509 I llama_perf_context_print:       total time =     141.63 ms /   129 tokens
0.00.548.857 I ggml_metal_free: deallocating

real	0m0.565s
user	0m0.076s
sys	0m0.067s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.080 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.888 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.713 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.713 I llama_model_loader: - type  f32:  194 tensors
0.00.024.713 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.713 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.714 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.714 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.714 I print_info: file format = GGUF V3 (latest)
0.00.024.715 I print_info: file type   = Q3_K - Medium
0.00.024.715 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.215 I load: special tokens cache size = 25
0.00.049.222 I load: token to piece cache size = 0.2984 MB
0.00.049.225 I print_info: arch             = gptneox
0.00.049.226 I print_info: vocab_only       = 0
0.00.049.226 I print_info: n_ctx_train      = 2048
0.00.049.226 I print_info: n_embd           = 2048
0.00.049.226 I print_info: n_layer          = 24
0.00.049.229 I print_info: n_head           = 16
0.00.049.230 I print_info: n_head_kv        = 16
0.00.049.230 I print_info: n_rot            = 32
0.00.049.230 I print_info: n_swa            = 0
0.00.049.231 I print_info: n_embd_head_k    = 128
0.00.049.231 I print_info: n_embd_head_v    = 128
0.00.049.232 I print_info: n_gqa            = 1
0.00.049.232 I print_info: n_embd_k_gqa     = 2048
0.00.049.233 I print_info: n_embd_v_gqa     = 2048
0.00.049.234 I print_info: f_norm_eps       = 1.0e-05
0.00.049.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.234 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.234 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.235 I print_info: f_logit_scale    = 0.0e+00
0.00.049.235 I print_info: n_ff             = 8192
0.00.049.235 I print_info: n_expert         = 0
0.00.049.235 I print_info: n_expert_used    = 0
0.00.049.236 I print_info: causal attn      = 1
0.00.049.236 I print_info: pooling type     = 0
0.00.049.236 I print_info: rope type        = 2
0.00.049.238 I print_info: rope scaling     = linear
0.00.049.238 I print_info: freq_base_train  = 10000.0
0.00.049.239 I print_info: freq_scale_train = 1
0.00.049.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.239 I print_info: rope_finetuned   = unknown
0.00.049.239 I print_info: ssm_d_conv       = 0
0.00.049.239 I print_info: ssm_d_inner      = 0
0.00.049.239 I print_info: ssm_d_state      = 0
0.00.049.239 I print_info: ssm_dt_rank      = 0
0.00.049.239 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.240 I print_info: model type       = 1.4B
0.00.049.240 I print_info: model params     = 1.41 B
0.00.049.240 I print_info: general.name     = 1.4B
0.00.049.241 I print_info: vocab type       = BPE
0.00.049.241 I print_info: n_vocab          = 50304
0.00.049.241 I print_info: n_merges         = 50009
0.00.049.241 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.241 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.241 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.241 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.242 I print_info: LF token         = 128 'Ä'
0.00.049.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.245 I print_info: max token length = 1024
0.00.051.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.165 I load_tensors: offloading output layer to GPU
0.00.051.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.176 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.177 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.460 I llama_init_from_model: n_seq_max     = 1
0.00.051.460 I llama_init_from_model: n_ctx         = 128
0.00.051.461 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.461 I llama_init_from_model: n_batch       = 128
0.00.051.461 I llama_init_from_model: n_ubatch      = 128
0.00.051.461 I llama_init_from_model: flash_attn    = 0
0.00.051.461 I llama_init_from_model: freq_base     = 10000.0
0.00.051.462 I llama_init_from_model: freq_scale    = 1
0.00.051.462 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.463 I ggml_metal_init: allocating
0.00.051.465 I ggml_metal_init: found device: Apple M4
0.00.051.467 I ggml_metal_init: picking default device: Apple M4
0.00.052.048 I ggml_metal_init: using embedded metal library
0.00.054.384 I ggml_metal_init: GPU name:   Apple M4
0.00.054.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.387 I ggml_metal_init: simdgroup reduction   = true
0.00.054.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.387 I ggml_metal_init: has bfloat            = true
0.00.054.387 I ggml_metal_init: use bfloat            = true
0.00.054.387 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.388 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.429 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.442 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.469 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.397 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.398 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.399 I llama_init_from_model: graph nodes  = 967
0.00.066.399 I llama_init_from_model: graph splits = 2
0.00.066.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.449 I 
0.00.471.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.499 I perplexity: tokenizing the input ..
0.00.479.474 I perplexity: tokenization took 7.973 ms
0.00.479.477 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.610.762 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.612.149 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.612.178 I llama_perf_context_print:        load time =     462.36 ms
0.00.612.179 I llama_perf_context_print: prompt eval time =     131.05 ms /   128 tokens (    1.02 ms per token,   976.74 tokens per second)
0.00.612.179 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.612.180 I llama_perf_context_print:       total time =     140.73 ms /   129 tokens
0.00.612.547 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.078s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.987 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.003 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.004 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.005 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.005 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.006 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.006 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.007 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.009 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.703 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.551 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.552 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.554 I llama_model_loader: - type  f32:  194 tensors
0.00.025.554 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.554 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.554 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.555 I print_info: file format = GGUF V3 (latest)
0.00.025.556 I print_info: file type   = Q4_K - Medium
0.00.025.557 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.059 I load: special tokens cache size = 25
0.00.051.258 I load: token to piece cache size = 0.2984 MB
0.00.051.263 I print_info: arch             = gptneox
0.00.051.263 I print_info: vocab_only       = 0
0.00.051.264 I print_info: n_ctx_train      = 2048
0.00.051.264 I print_info: n_embd           = 2048
0.00.051.268 I print_info: n_layer          = 24
0.00.051.273 I print_info: n_head           = 16
0.00.051.273 I print_info: n_head_kv        = 16
0.00.051.274 I print_info: n_rot            = 32
0.00.051.274 I print_info: n_swa            = 0
0.00.051.274 I print_info: n_embd_head_k    = 128
0.00.051.274 I print_info: n_embd_head_v    = 128
0.00.051.275 I print_info: n_gqa            = 1
0.00.051.276 I print_info: n_embd_k_gqa     = 2048
0.00.051.276 I print_info: n_embd_v_gqa     = 2048
0.00.051.277 I print_info: f_norm_eps       = 1.0e-05
0.00.051.277 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.277 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.277 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.278 I print_info: f_logit_scale    = 0.0e+00
0.00.051.278 I print_info: n_ff             = 8192
0.00.051.279 I print_info: n_expert         = 0
0.00.051.279 I print_info: n_expert_used    = 0
0.00.051.279 I print_info: causal attn      = 1
0.00.051.279 I print_info: pooling type     = 0
0.00.051.279 I print_info: rope type        = 2
0.00.051.279 I print_info: rope scaling     = linear
0.00.051.280 I print_info: freq_base_train  = 10000.0
0.00.051.280 I print_info: freq_scale_train = 1
0.00.051.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.281 I print_info: rope_finetuned   = unknown
0.00.051.281 I print_info: ssm_d_conv       = 0
0.00.051.281 I print_info: ssm_d_inner      = 0
0.00.051.281 I print_info: ssm_d_state      = 0
0.00.051.281 I print_info: ssm_dt_rank      = 0
0.00.051.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.283 I print_info: model type       = 1.4B
0.00.051.283 I print_info: model params     = 1.41 B
0.00.051.284 I print_info: general.name     = 1.4B
0.00.051.284 I print_info: vocab type       = BPE
0.00.051.286 I print_info: n_vocab          = 50304
0.00.051.286 I print_info: n_merges         = 50009
0.00.051.286 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.286 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.287 I print_info: LF token         = 128 'Ä'
0.00.051.287 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.288 I print_info: max token length = 1024
0.00.053.376 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.376 I load_tensors: offloading output layer to GPU
0.00.053.376 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.388 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.389 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.680 I llama_init_from_model: n_seq_max     = 1
0.00.053.681 I llama_init_from_model: n_ctx         = 128
0.00.053.681 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.681 I llama_init_from_model: n_batch       = 128
0.00.053.681 I llama_init_from_model: n_ubatch      = 128
0.00.053.681 I llama_init_from_model: flash_attn    = 0
0.00.053.682 I llama_init_from_model: freq_base     = 10000.0
0.00.053.682 I llama_init_from_model: freq_scale    = 1
0.00.053.682 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.683 I ggml_metal_init: allocating
0.00.053.685 I ggml_metal_init: found device: Apple M4
0.00.053.687 I ggml_metal_init: picking default device: Apple M4
0.00.054.320 I ggml_metal_init: using embedded metal library
0.00.060.304 I ggml_metal_init: GPU name:   Apple M4
0.00.060.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.307 I ggml_metal_init: simdgroup reduction   = true
0.00.060.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.308 I ggml_metal_init: has bfloat            = true
0.00.060.308 I ggml_metal_init: use bfloat            = true
0.00.060.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.887 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.337 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.354 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.371 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.290 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.291 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.291 I llama_init_from_model: graph nodes  = 967
0.00.072.291 I llama_init_from_model: graph splits = 2
0.00.072.293 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.880 I 
0.00.537.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.919 I perplexity: tokenizing the input ..
0.00.545.844 I perplexity: tokenization took 7.924 ms
0.00.545.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.680.693 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.682.014 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.682.050 I llama_perf_context_print:        load time =     527.93 ms
0.00.682.050 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.82 tokens per second)
0.00.682.051 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.682.052 I llama_perf_context_print:       total time =     144.17 ms /   129 tokens
0.00.682.537 I ggml_metal_free: deallocating

real	0m0.701s
user	0m0.078s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.847 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.848 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.848 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.849 I llama_model_loader: - type  f32:  194 tensors
0.00.024.849 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.850 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.850 I print_info: file format = GGUF V3 (latest)
0.00.024.851 I print_info: file type   = Q5_K - Medium
0.00.024.851 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.354 I load: special tokens cache size = 25
0.00.049.279 I load: token to piece cache size = 0.2984 MB
0.00.049.281 I print_info: arch             = gptneox
0.00.049.282 I print_info: vocab_only       = 0
0.00.049.282 I print_info: n_ctx_train      = 2048
0.00.049.282 I print_info: n_embd           = 2048
0.00.049.282 I print_info: n_layer          = 24
0.00.049.286 I print_info: n_head           = 16
0.00.049.286 I print_info: n_head_kv        = 16
0.00.049.287 I print_info: n_rot            = 32
0.00.049.287 I print_info: n_swa            = 0
0.00.049.287 I print_info: n_embd_head_k    = 128
0.00.049.287 I print_info: n_embd_head_v    = 128
0.00.049.288 I print_info: n_gqa            = 1
0.00.049.289 I print_info: n_embd_k_gqa     = 2048
0.00.049.289 I print_info: n_embd_v_gqa     = 2048
0.00.049.290 I print_info: f_norm_eps       = 1.0e-05
0.00.049.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.290 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.290 I print_info: f_logit_scale    = 0.0e+00
0.00.049.291 I print_info: n_ff             = 8192
0.00.049.291 I print_info: n_expert         = 0
0.00.049.291 I print_info: n_expert_used    = 0
0.00.049.292 I print_info: causal attn      = 1
0.00.049.292 I print_info: pooling type     = 0
0.00.049.294 I print_info: rope type        = 2
0.00.049.295 I print_info: rope scaling     = linear
0.00.049.295 I print_info: freq_base_train  = 10000.0
0.00.049.295 I print_info: freq_scale_train = 1
0.00.049.295 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.296 I print_info: rope_finetuned   = unknown
0.00.049.296 I print_info: ssm_d_conv       = 0
0.00.049.296 I print_info: ssm_d_inner      = 0
0.00.049.296 I print_info: ssm_d_state      = 0
0.00.049.296 I print_info: ssm_dt_rank      = 0
0.00.049.296 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.296 I print_info: model type       = 1.4B
0.00.049.297 I print_info: model params     = 1.41 B
0.00.049.297 I print_info: general.name     = 1.4B
0.00.049.298 I print_info: vocab type       = BPE
0.00.049.298 I print_info: n_vocab          = 50304
0.00.049.298 I print_info: n_merges         = 50009
0.00.049.298 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.298 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.299 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.299 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.304 I print_info: LF token         = 128 'Ä'
0.00.049.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.305 I print_info: max token length = 1024
0.00.051.307 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.307 I load_tensors: offloading output layer to GPU
0.00.051.307 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.318 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.319 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.594 I llama_init_from_model: n_seq_max     = 1
0.00.051.595 I llama_init_from_model: n_ctx         = 128
0.00.051.595 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.595 I llama_init_from_model: n_batch       = 128
0.00.051.596 I llama_init_from_model: n_ubatch      = 128
0.00.051.596 I llama_init_from_model: flash_attn    = 0
0.00.051.596 I llama_init_from_model: freq_base     = 10000.0
0.00.051.596 I llama_init_from_model: freq_scale    = 1
0.00.051.597 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.597 I ggml_metal_init: allocating
0.00.051.600 I ggml_metal_init: found device: Apple M4
0.00.051.602 I ggml_metal_init: picking default device: Apple M4
0.00.052.178 I ggml_metal_init: using embedded metal library
0.00.054.533 I ggml_metal_init: GPU name:   Apple M4
0.00.054.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.536 I ggml_metal_init: simdgroup reduction   = true
0.00.054.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.536 I ggml_metal_init: has bfloat            = true
0.00.054.536 I ggml_metal_init: use bfloat            = true
0.00.054.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.049 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.374 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.387 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.411 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.309 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.310 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.310 I llama_init_from_model: graph nodes  = 967
0.00.066.310 I llama_init_from_model: graph splits = 2
0.00.066.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.377 I 
0.00.633.414 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.422 I perplexity: tokenizing the input ..
0.00.641.609 I perplexity: tokenization took 8.185 ms
0.00.641.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.464 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.783.642 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.783.668 I llama_perf_context_print:        load time =     624.29 ms
0.00.783.669 I llama_perf_context_print: prompt eval time =     140.62 ms /   128 tokens (    1.10 ms per token,   910.23 tokens per second)
0.00.783.670 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.674 I llama_perf_context_print:       total time =     150.29 ms /   129 tokens
0.00.784.241 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.077s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.168 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.169 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.170 I llama_model_loader: - type  f32:  194 tensors
0.00.025.170 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.170 I print_info: file format = GGUF V3 (latest)
0.00.025.171 I print_info: file type   = Q6_K
0.00.025.171 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.531 I load: special tokens cache size = 25
0.00.049.475 I load: token to piece cache size = 0.2984 MB
0.00.049.478 I print_info: arch             = gptneox
0.00.049.478 I print_info: vocab_only       = 0
0.00.049.478 I print_info: n_ctx_train      = 2048
0.00.049.479 I print_info: n_embd           = 2048
0.00.049.479 I print_info: n_layer          = 24
0.00.049.482 I print_info: n_head           = 16
0.00.049.483 I print_info: n_head_kv        = 16
0.00.049.483 I print_info: n_rot            = 32
0.00.049.483 I print_info: n_swa            = 0
0.00.049.483 I print_info: n_embd_head_k    = 128
0.00.049.484 I print_info: n_embd_head_v    = 128
0.00.049.484 I print_info: n_gqa            = 1
0.00.049.485 I print_info: n_embd_k_gqa     = 2048
0.00.049.486 I print_info: n_embd_v_gqa     = 2048
0.00.049.491 I print_info: f_norm_eps       = 1.0e-05
0.00.049.493 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.493 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.493 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.494 I print_info: f_logit_scale    = 0.0e+00
0.00.049.494 I print_info: n_ff             = 8192
0.00.049.495 I print_info: n_expert         = 0
0.00.049.495 I print_info: n_expert_used    = 0
0.00.049.495 I print_info: causal attn      = 1
0.00.049.498 I print_info: pooling type     = 0
0.00.049.498 I print_info: rope type        = 2
0.00.049.498 I print_info: rope scaling     = linear
0.00.049.499 I print_info: freq_base_train  = 10000.0
0.00.049.499 I print_info: freq_scale_train = 1
0.00.049.500 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.500 I print_info: rope_finetuned   = unknown
0.00.049.500 I print_info: ssm_d_conv       = 0
0.00.049.500 I print_info: ssm_d_inner      = 0
0.00.049.500 I print_info: ssm_d_state      = 0
0.00.049.501 I print_info: ssm_dt_rank      = 0
0.00.049.501 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.501 I print_info: model type       = 1.4B
0.00.049.501 I print_info: model params     = 1.41 B
0.00.049.501 I print_info: general.name     = 1.4B
0.00.049.502 I print_info: vocab type       = BPE
0.00.049.502 I print_info: n_vocab          = 50304
0.00.049.502 I print_info: n_merges         = 50009
0.00.049.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.503 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.503 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.504 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.504 I print_info: LF token         = 128 'Ä'
0.00.049.504 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.504 I print_info: max token length = 1024
0.00.051.486 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.487 I load_tensors: offloading output layer to GPU
0.00.051.487 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.497 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.498 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.768 I llama_init_from_model: n_seq_max     = 1
0.00.051.769 I llama_init_from_model: n_ctx         = 128
0.00.051.769 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.769 I llama_init_from_model: n_batch       = 128
0.00.051.769 I llama_init_from_model: n_ubatch      = 128
0.00.051.769 I llama_init_from_model: flash_attn    = 0
0.00.051.770 I llama_init_from_model: freq_base     = 10000.0
0.00.051.770 I llama_init_from_model: freq_scale    = 1
0.00.051.770 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.771 I ggml_metal_init: allocating
0.00.051.774 I ggml_metal_init: found device: Apple M4
0.00.051.776 I ggml_metal_init: picking default device: Apple M4
0.00.052.345 I ggml_metal_init: using embedded metal library
0.00.054.674 I ggml_metal_init: GPU name:   Apple M4
0.00.054.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.677 I ggml_metal_init: simdgroup reduction   = true
0.00.054.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.677 I ggml_metal_init: has bfloat            = true
0.00.054.677 I ggml_metal_init: use bfloat            = true
0.00.054.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.200 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.214 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.075 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.076 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.076 I llama_init_from_model: graph nodes  = 967
0.00.066.076 I llama_init_from_model: graph splits = 2
0.00.066.078 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.160.598 I 
0.00.160.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.160.689 I perplexity: tokenizing the input ..
0.00.168.711 I perplexity: tokenization took 8.02 ms
0.00.168.716 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.308.160 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.309.290 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.309.318 I llama_perf_context_print:        load time =     150.66 ms
0.00.309.319 I llama_perf_context_print: prompt eval time =     139.18 ms /   128 tokens (    1.09 ms per token,   919.70 tokens per second)
0.00.309.320 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.309.320 I llama_perf_context_print:       total time =     148.73 ms /   129 tokens
0.00.309.757 I ggml_metal_free: deallocating

real	0m0.326s
user	0m0.076s
sys	0m0.044s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.261 I build: 4505 (f26c8741) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.604 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.786 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.807 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.236 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.237 I llama_model_loader: - type  f32:  194 tensors
0.00.054.237 I llama_model_loader: - type  f16:   98 tensors
0.00.054.238 I print_info: file format = GGUF V3 (latest)
0.00.054.239 I print_info: file type   = all F32 (guessed)
0.00.054.240 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.822 I load: special tokens cache size = 25
0.00.085.171 I load: token to piece cache size = 0.2984 MB
0.00.085.173 I print_info: arch             = gptneox
0.00.085.173 I print_info: vocab_only       = 0
0.00.085.174 I print_info: n_ctx_train      = 2048
0.00.085.174 I print_info: n_embd           = 2048
0.00.085.174 I print_info: n_layer          = 24
0.00.085.177 I print_info: n_head           = 16
0.00.085.178 I print_info: n_head_kv        = 16
0.00.085.178 I print_info: n_rot            = 32
0.00.085.178 I print_info: n_swa            = 0
0.00.085.178 I print_info: n_embd_head_k    = 128
0.00.085.178 I print_info: n_embd_head_v    = 128
0.00.085.179 I print_info: n_gqa            = 1
0.00.085.180 I print_info: n_embd_k_gqa     = 2048
0.00.085.181 I print_info: n_embd_v_gqa     = 2048
0.00.085.181 I print_info: f_norm_eps       = 1.0e-05
0.00.085.184 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.184 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.184 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.184 I print_info: f_logit_scale    = 0.0e+00
0.00.085.185 I print_info: n_ff             = 8192
0.00.085.185 I print_info: n_expert         = 0
0.00.085.185 I print_info: n_expert_used    = 0
0.00.085.186 I print_info: causal attn      = 1
0.00.085.186 I print_info: pooling type     = 0
0.00.085.186 I print_info: rope type        = 2
0.00.085.186 I print_info: rope scaling     = linear
0.00.085.187 I print_info: freq_base_train  = 10000.0
0.00.085.187 I print_info: freq_scale_train = 1
0.00.085.187 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.187 I print_info: rope_finetuned   = unknown
0.00.085.187 I print_info: ssm_d_conv       = 0
0.00.085.188 I print_info: ssm_d_inner      = 0
0.00.085.188 I print_info: ssm_d_state      = 0
0.00.085.188 I print_info: ssm_dt_rank      = 0
0.00.085.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.188 I print_info: model type       = 1.4B
0.00.085.189 I print_info: model params     = 1.41 B
0.00.085.189 I print_info: general.name     = 1.4B
0.00.085.190 I print_info: vocab type       = BPE
0.00.085.190 I print_info: n_vocab          = 50304
0.00.085.190 I print_info: n_merges         = 50009
0.00.085.190 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.191 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.191 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.191 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.192 I print_info: LF token         = 128 'Ä'
0.00.085.192 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.193 I print_info: max token length = 1024
0.00.087.760 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.760 I load_tensors: offloading output layer to GPU
0.00.087.761 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.771 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.772 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.118 I llama_init_from_model: n_seq_max     = 1
0.00.088.119 I llama_init_from_model: n_ctx         = 128
0.00.088.119 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.120 I llama_init_from_model: n_batch       = 128
0.00.088.120 I llama_init_from_model: n_ubatch      = 128
0.00.088.120 I llama_init_from_model: flash_attn    = 0
0.00.088.120 I llama_init_from_model: freq_base     = 10000.0
0.00.088.121 I llama_init_from_model: freq_scale    = 1
0.00.088.121 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.121 I ggml_metal_init: allocating
0.00.088.124 I ggml_metal_init: found device: Apple M4
0.00.088.126 I ggml_metal_init: picking default device: Apple M4
0.00.088.739 I ggml_metal_init: using embedded metal library
0.00.091.238 I ggml_metal_init: GPU name:   Apple M4
0.00.091.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.240 I ggml_metal_init: simdgroup reduction   = true
0.00.091.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.241 I ggml_metal_init: has bfloat            = true
0.00.091.241 I ggml_metal_init: use bfloat            = true
0.00.091.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.511 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.526 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.372 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.373 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.373 I llama_init_from_model: graph nodes  = 967
0.00.102.373 I llama_init_from_model: graph splits = 2
0.00.102.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.375 I 
0.00.102.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.410 I compute_imatrix: tokenizing the input ..
0.00.109.073 I compute_imatrix: tokenization took 6.662 ms
0.00.109.074 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.594.675 I compute_imatrix: 1.49 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.597.728 I llama_perf_context_print:        load time =    1571.07 ms
0.01.597.729 I llama_perf_context_print: prompt eval time =    1484.96 ms /   128 tokens (   11.60 ms per token,    86.20 tokens per second)
0.01.597.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.597.732 I llama_perf_context_print:       total time =    1574.12 ms /   129 tokens
0.01.598.754 I ggml_metal_free: deallocating

real	0m1.783s
user	0m0.166s
sys	0m0.250s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4505 (f26c8741)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11de0a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11de0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11de0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11de0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11de0bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11de0c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11de0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11de0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11de0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11de0d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11de0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11de0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11de0ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11de0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11de0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11de10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11de10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11de11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11de11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11de12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11de12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11de12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11de135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11de13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11de14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11de14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11de14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11de15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11de15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11de162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11de16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11de16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11de17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11de177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11de17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11de17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11de183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11de18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11de18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11de191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11de19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11de19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11de19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11de1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11de1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11de1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11de1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11de1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11de1c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11de1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11de1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11de1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11de1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11de1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11de1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11de1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11de1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11de1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11de1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11de20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11de20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11de209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11de20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11de21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11de217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11de21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11de22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11de225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11de22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11de22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11de23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11de23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11de23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11de24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11de24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11de24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11de25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11de25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11de25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11de261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11de26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11de26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11de271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11de27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11de27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11de281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11de28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11de28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11de291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11de29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11de29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11de2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11de2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11de2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11de2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11de2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11de2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11de1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11de2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11de2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11de2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11de2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11de2d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11de2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11de2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11de2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11de2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11de2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11de2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11de2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11de302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11de30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11de30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11de31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11de316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11de31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11de31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11de32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11de32930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11de32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11de33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11de33710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11de33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11de34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11de344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11de34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11de34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11de352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11de35770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11de35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11de360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11de36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11de369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11de36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11de37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11de377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11de37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11de38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11de385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11de38a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11de38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11de39390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11de39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11de39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11de3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11de3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11de3aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11de3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11de3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11de3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11de3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11de3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11de3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11de3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11de3cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11de3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11de3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11de3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11de3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11de3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11de3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11de3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11de3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11de3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11de3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11de40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11de40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11de40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11de41070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11de41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11de419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11de41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11de422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11de42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11de42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11de430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11de43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11de43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11de43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11de44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11de447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11de44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11de45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11de455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11de45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11de45f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11de463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11de46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11de46cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11de47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11de47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11de47ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11de47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11de484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11de48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11de48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11de494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11de49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11de49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11de4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11de4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11de4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11de4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11de4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11de4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11de4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11de4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11de4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11de4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11de4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11de4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11de4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11de4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11de4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11de4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11de4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11de50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11de507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11de50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11de51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11de517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11de51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11de52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11de527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11de52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11de53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11de53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11de53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11de54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11de54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11de54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11de55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11de55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11de55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11de56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11de56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11de56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11de57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11de57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11de57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11de581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11de58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11de58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11de591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11de59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11de59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11de5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11de5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11de5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11de5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11de5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11de5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11de5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11de5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11de5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11de5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11de5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11de5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11de5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11de5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11de5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11de5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11de5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11de5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11de60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11de606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11de60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11de610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11de61550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11de619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11de61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11de62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11de627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11de62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11de63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11de635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11de63a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11de63ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11de64390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11de64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11de64cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11de65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11de656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11de65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11de66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11de66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11de67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11de67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11de67df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11de680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11de686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.145.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.145.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ec04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ec05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ec055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ec05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ec05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ec06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ec06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ec06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ec07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ec074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ec07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ec08070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ec08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ec09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ec09b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ec0a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ec0a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ec0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ec0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ec0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ec0c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ec0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ec0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ec0db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ec0e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ec0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ec0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ec0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ec0f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ec0f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ec0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ec0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ec10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ec10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ec10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ec10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ec11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ec11800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ec11c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ec120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ec12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ec129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ec12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ec132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ec13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ec13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ec13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ec14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ec148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ec14d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ec151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ec15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ec15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ec15f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ec16370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ec167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ec16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ec17250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ec176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ec17b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ec17fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ec18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ec18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ec18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ec19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ec195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ec19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ec19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ec1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ec1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ec1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ec1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ec1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ec1b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ec1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ec1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ec1c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ec1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ec1cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ec1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ec1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ec1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ec1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ec1e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ec1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ec1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ec1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ec1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ec1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ec20050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ec204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ec20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ec20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ec21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ec21680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ec21af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ec21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ec223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ec22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ec22cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ec23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ec23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ec23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ec23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ec242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ec24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ec24bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ec25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ec254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ec25910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ec25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ec261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ec26660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ec26ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ec26f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ec273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ec27820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ec27c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ec28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ec28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ec289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ec28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ec292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ec29730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ec29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ec2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ec2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ec2a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ec2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ec2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ec2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ec2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ec2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ec2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ec2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ec2cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ec2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ec2d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ec2d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ec2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ec2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ec2e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ec2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ec2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ec2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ec2f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ec2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ec301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ec30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ec30a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ec30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ec31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ec317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ec31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ec320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ec32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ec329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ec32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ec33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ec336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ec33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ec33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ec34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ec348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ec34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ec35190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ec35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ec36080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ec36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ec367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ec36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ec37090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ec37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ec37970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ec37de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ec38250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ec386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ec38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ec38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ec39410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ec39880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ec39cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ec3a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ec3a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ec3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ec3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ec3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ec3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ec3bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ec3c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ec3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ec3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ec3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ec3d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ec3d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ec3db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ec3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ec3e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ec3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ec3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ec3f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ec3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ec3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ec40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ec40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ec40900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ec40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ec411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ec41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ec41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ec42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ec42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ec43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ec435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ec43b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ec44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ec44700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ec44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ec45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ec45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ec45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ec463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ec46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ec46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ec47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ec47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ec48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ec48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ec48c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ec491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ec49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ec49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ec4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ec4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ec4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ec4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ec4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ec4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ec4c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ec4cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ec4d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ec4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ec4dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ec4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ec4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ec4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ec4f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ec4f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ec4ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ec504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ec50a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ec51040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ec51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ec51bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ec52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ec52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ec52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ec532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ec53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ec53e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ec54400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ec549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ec54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ec55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ec55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ec560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ec56680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ec56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ec57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ec57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ec57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ec58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ec58540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ec58a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ec58f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ec59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ec59940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ec59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ec5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ec5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ec5ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ec5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ec5b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ec5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ec5c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ec5cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ec5d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ec5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ec5e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ec5e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ec5ea30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11de68370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11de4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11de49a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11de4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11de1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11de1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11de1f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11de4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11de14ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11de1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11de1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11de1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11de1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11de1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11de13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11de1fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11de2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11de678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11de16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11de16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11de4c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11de4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11de150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11de153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11de15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11de68b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11de68de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11de690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11de69360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11de69620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11de698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11de69ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11de69e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11de6a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11de6a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11de6a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11de6a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11de6ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11de6aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11de6b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11de6b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11de6b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11de6b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11de6bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11de6bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11de6c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11de6c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11de6c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11de6ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11de6cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11de6cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11de6d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11de6d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11de6d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11de6dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11de6dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11de6e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11de6e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11de6e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11de6e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11de6eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11de6ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11de6f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11de6f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11de6f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11de6f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11de6fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11de6fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11de70160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11de70420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11de706e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11de709a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11de70c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11de70f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11de711e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11de714a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11de71760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11de71a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11de71ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11de71fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11de72260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11de72520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11de727e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11de72aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11de72d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11de73020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11de732e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11de735a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11de73860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11de73b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11de73de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11de740a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11de74360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11de74620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11de748e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11de74ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11de74e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11de75120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11de753e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11de756a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11de75960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11de75c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11de75ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11de761a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11de76460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11de76720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11de769e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11de76ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11de76f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11de77220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11de774e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11de777a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11de77a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11de77d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11de77fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11de782a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11de78560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11de78820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11de78ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11de78da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11de79060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11de79320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11de795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11de798a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11de79b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11de79e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11de7a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11de7a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11de7a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11de7a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11de7abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11de7aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11de7b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11de7b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11de7b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11de7b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11de7bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11de7bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11de7c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11de7c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11de7c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11de7ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11de7cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11de7cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11de7d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11de7d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11de7d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11de7daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11de7dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11de7e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11de7e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11de7e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11de7e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11de7eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11de7ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11de7f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11de7f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11de7f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11de7f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11de7fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11de7fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11de80120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11de803e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11de806a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11de80960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11de80c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11de80ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11de811a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11de81460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11de81720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11de819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11de81ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11de81f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11de82220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11de824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11de827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11de82a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11de82d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11de82fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11de832a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11de83560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11de83820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11de83ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11de83da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11de84060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11de84320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11de845e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11de848a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11de84b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11de84e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11de850e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11de853a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11de85660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11de85920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11de85be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11de85ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11de86160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11de86420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11de866e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11de869a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11de86c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11de86f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11de871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11de874a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11de87760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11de87a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11de87ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11de87fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11de88260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11de88520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11de88af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11de89040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11de89590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11de89ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11de8a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11de8a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11de8aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11de8b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11de8b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11de8bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11de8c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11de8c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11de8cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11de8d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11de8d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11de8daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11de8dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11de8e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11de8ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11de8efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11de8f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11de8fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11de8ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11de90520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11de90a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11de90fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11de91510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11de91a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11de91fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11de92500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11de92a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11de92fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11de934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11de93a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11de93f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11de944e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11de94a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11de94f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11de954d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11de95a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11de95f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11de964c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11de96a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11de96f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11de974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11de97a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11de97f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11de984a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11de989f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11de98f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11de99490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11de999e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11de99f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11de9a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11de9a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11de9af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11de9b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11de9b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11de9b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11de9bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11de9c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11de9c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11de9ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11de9ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11de9d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11de9d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11de9dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11de9e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11de9e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11de9e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11de9ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11de9f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11de9f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11de9fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11dea07c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11dea0ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11dea1600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11dea18c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11dea1d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11dea2330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11dea2940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.787s
user	0m0.298s
sys	0m0.311s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4505 (f26c8741)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b70afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b70b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b70bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b70c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b70c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b70cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b70d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b70d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b70dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b70e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b70e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b70eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b70f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b710070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b710880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b710fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b7116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b711de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b712500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b712cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b7133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b713b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b714230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b714ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b7151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b7154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b716730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b716c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b716f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b7173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b717690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b717f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b718460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b718720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b718bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b719500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b7199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b719e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b71a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b71a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b71ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b71b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b71b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b71b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b71bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b71c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b71ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b71daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b71e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b71e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b71f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b71f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b71fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b720110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b720720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b7211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b721670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b721b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b721fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b722450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b7228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b722d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b7236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b723b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b724010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b7244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b724950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b724ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b7253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b725940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b725e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b7263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b726e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b7273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b727920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b727e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b7283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b728910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b728e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b7293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b729900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b729e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b72a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b72a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b72ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b72b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b72b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b72be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b72c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b72c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b71c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b72cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b72d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b72da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b72df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b72e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b72ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b72ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b72f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b72fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b72ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b7304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b730a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b730f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b7314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b731a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b731ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b732340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b7327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b732c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b733120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b7335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b733a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b733f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b7343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b734840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b734ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b735180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b735620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b735ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b735f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b736400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b7368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b736d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b7371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b737680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b737b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b737fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b738460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b738900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b738da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b739240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b7396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b739b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b73a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b73a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b73a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b73ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b73b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b73b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b73bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b73c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b73c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b73c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b73ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b73d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b73d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b73dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b73e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b73e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b73ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b73eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b73f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b73f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b73fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b740140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b7405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b740a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b740f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b7413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b741860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b741d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b7421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b742640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b742ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b742f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b743420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b7438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b743d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b744200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b7446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b744b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b744fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b745480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b745920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b745dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b746260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b746700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b747040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b7474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b747980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b747e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b7482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b748760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b748c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b749150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b7496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b749bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b74a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b74a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b74aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b74b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b74b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b74be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b74c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b74c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b74cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b74d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b74d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b74de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b74e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b74e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b74ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b74f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b74f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b74ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b750460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b7509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b750f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b751450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b7519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b751ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b752990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b753980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b753ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b754420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b754970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b754ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b755410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b755eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b756400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b756950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b756ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b7573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b757940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b757e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b7583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b758930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b758e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b7593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b759920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b75a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b75a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b75ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b75b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b75b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b75be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b75c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b75c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b75ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b75d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b75d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b75de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b75e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b75e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b75ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b75f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b75f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b75fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b760360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b7608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b760e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b761350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b7618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b761d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b7621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b762680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b762b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b762fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b763460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b763900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b763da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b764240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b7646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b764b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b765020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b7654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b765960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b765e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b766350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b766a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b767190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b7678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b767fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b768290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b768a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b768d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b769350 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.697 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fe50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fe51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fe51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fe51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fe521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fe52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fe52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fe532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fe538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fe53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fe54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fe549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fe54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fe55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fe55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fe560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fe566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fe56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fe57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fe57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fe57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fe58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fe58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fe58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fe58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fe59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fe59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fe59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fe5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fe5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fe5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fe5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fe5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fe5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fe5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fe5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fe5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fe5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fe5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fe5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fe5ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c8044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c8056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c8063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c80a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c80a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c80af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c80b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c80be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c80c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c80cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c80d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c80dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c80dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c80e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c80e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c80e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c80edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c80f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c80f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c80fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c80fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c8102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c8114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c8133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c8149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c8152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c8177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c8180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c8189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c8196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c81a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c81a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c81ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c81b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c81b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c81ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c81bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c81c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c81c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c81cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c81d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c81d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c81d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c81ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c81e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c81e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c81eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c81efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c81f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c81f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c81fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c8205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c8217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c8224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c8253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c8269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c8272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c8291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c82a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c82a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c82ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c82b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c82b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c82b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c82be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c82c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c82c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c82cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c82d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c82d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c82d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c82dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c82e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c82e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c82eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c82ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c82f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c82f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c82fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c8300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c8309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c8328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c8331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c8347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c8350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c8366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c8378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c8385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c8397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c83a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c83a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c83a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c83ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c83b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c83b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c83bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c83bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c83c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c83c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c83ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c83d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c83d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c83da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c83deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c83e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c83e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c83ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c83f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c83f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c83f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c83fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c8406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c8424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c8436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c8455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c8474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c8493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c84a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c84a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c84aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c84ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c84b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c84b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c84bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c84c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c84c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c84c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c84cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c84d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c84d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c84dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c84df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c84e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c84e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c84eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c84f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c84f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c84f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c84fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c8502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c8521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c8533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c8540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c8549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c8552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c8568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c8576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c8579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.914s
user	0m0.243s
sys	0m0.135s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
