Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.6s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.833s
user	0m0.849s
sys	0m1.298s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Built target xxhash
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-chat
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-log
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-arg-parser
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-chat-template
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-backend-ops
[ 63%] Built target test-chat-template
[ 63%] Built target test-rope
[ 63%] Built target test-arg-parser
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-batched
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-lookup
[ 81%] Built target llama-parallel
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-passkey
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-quantize
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-run
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tts
[ 91%] Built target llama-gen-docs
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.303s
user	0m6.498s
sys	0m10.602s

main: quantize time =  5488.43 ms
main:    total time =  5488.43 ms

main: quantize time =  3507.82 ms
main:    total time =  3507.82 ms

main: quantize time =  3587.10 ms
main:    total time =  3587.10 ms

main: quantize time =  3877.40 ms
main:    total time =  3877.40 ms

main: quantize time =  2440.47 ms
main:    total time =  2440.47 ms

main: quantize time =  5458.37 ms
main:    total time =  5458.37 ms

main: quantize time =  6088.23 ms
main:    total time =  6088.23 ms

main: quantize time =  6956.69 ms
main:    total time =  6956.69 ms

main: quantize time =  5809.36 ms
main:    total time =  5809.36 ms

main: quantize time =  4354.66 ms
main:    total time =  4354.66 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.333 I main: llama backend init
0.00.000.339 I main: load the model and apply lora adapter, if any
0.00.042.162 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.054.991 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.055.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.055.014 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.055.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.055.016 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.055.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.055.027 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.055.036 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.055.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.055.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.055.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.055.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.055.039 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.055.040 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.055.045 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.055.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.055.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.066.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.978 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.073.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.982 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.983 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.984 I llama_model_loader: - type  f32:  194 tensors
0.00.073.984 I llama_model_loader: - type  f16:   98 tensors
0.00.073.986 I print_info: file format = GGUF V3 (latest)
0.00.073.987 I print_info: file type   = all F32 (guessed)
0.00.073.990 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.087.620 I load: special tokens cache size = 25
0.00.095.999 I load: token to piece cache size = 0.2984 MB
0.00.096.023 I print_info: arch             = gptneox
0.00.096.024 I print_info: vocab_only       = 0
0.00.096.024 I print_info: n_ctx_train      = 2048
0.00.096.024 I print_info: n_embd           = 2048
0.00.096.025 I print_info: n_layer          = 24
0.00.096.027 I print_info: n_head           = 16
0.00.096.028 I print_info: n_head_kv        = 16
0.00.096.028 I print_info: n_rot            = 32
0.00.096.028 I print_info: n_swa            = 0
0.00.096.033 I print_info: n_embd_head_k    = 128
0.00.096.033 I print_info: n_embd_head_v    = 128
0.00.096.034 I print_info: n_gqa            = 1
0.00.096.035 I print_info: n_embd_k_gqa     = 2048
0.00.096.036 I print_info: n_embd_v_gqa     = 2048
0.00.096.037 I print_info: f_norm_eps       = 1.0e-05
0.00.096.037 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.037 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.040 I print_info: f_logit_scale    = 0.0e+00
0.00.096.048 I print_info: n_ff             = 8192
0.00.096.050 I print_info: n_expert         = 0
0.00.096.050 I print_info: n_expert_used    = 0
0.00.096.050 I print_info: causal attn      = 1
0.00.096.051 I print_info: pooling type     = 0
0.00.096.051 I print_info: rope type        = 2
0.00.096.051 I print_info: rope scaling     = linear
0.00.096.052 I print_info: freq_base_train  = 10000.0
0.00.096.052 I print_info: freq_scale_train = 1
0.00.096.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.052 I print_info: rope_finetuned   = unknown
0.00.096.054 I print_info: ssm_d_conv       = 0
0.00.096.054 I print_info: ssm_d_inner      = 0
0.00.096.054 I print_info: ssm_d_state      = 0
0.00.096.054 I print_info: ssm_dt_rank      = 0
0.00.096.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.055 I print_info: model type       = 1.4B
0.00.096.055 I print_info: model params     = 1.41 B
0.00.096.056 I print_info: general.name     = 1.4B
0.00.096.056 I print_info: vocab type       = BPE
0.00.096.056 I print_info: n_vocab          = 50304
0.00.096.057 I print_info: n_merges         = 50009
0.00.096.057 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.059 I print_info: LF token         = 187 'Ċ'
0.00.096.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.060 I print_info: max token length = 1024
0.00.096.060 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.139.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.139.493 I load_tensors: offloading output layer to GPU
0.00.139.493 I load_tensors: offloaded 25/25 layers to GPU
0.00.139.518 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.139.519 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.140.003 I llama_context: n_seq_max     = 1
0.00.140.003 I llama_context: n_ctx         = 2048
0.00.140.004 I llama_context: n_ctx_per_seq = 2048
0.00.140.004 I llama_context: n_batch       = 2048
0.00.140.004 I llama_context: n_ubatch      = 512
0.00.140.004 I llama_context: flash_attn    = 0
0.00.140.005 I llama_context: freq_base     = 10000.0
0.00.140.005 I llama_context: freq_scale    = 1
0.00.140.007 I ggml_metal_init: allocating
0.00.140.059 I ggml_metal_init: found device: Apple M4
0.00.140.066 I ggml_metal_init: picking default device: Apple M4
0.00.140.745 I ggml_metal_init: using embedded metal library
0.00.150.744 I ggml_metal_init: GPU name:   Apple M4
0.00.150.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.150.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.150.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.150.746 I ggml_metal_init: simdgroup reduction   = true
0.00.150.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.150.747 I ggml_metal_init: has residency sets    = true
0.00.150.747 I ggml_metal_init: has bfloat            = true
0.00.150.747 I ggml_metal_init: use bfloat            = true
0.00.150.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.150.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.746 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.174.748 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.203.778 I init:      Metal KV buffer size =   384.00 MiB
0.00.203.785 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.207.969 I init:      Metal compute buffer size =   102.25 MiB
0.00.207.972 I init:        CPU compute buffer size =     8.01 MiB
0.00.207.972 I init: graph nodes  = 967
0.00.207.972 I init: graph splits = 2
0.00.207.980 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.208.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.208.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.274.206 I main: llama threadpool init, n_threads = 4
0.00.274.249 I 
0.00.274.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.274.266 I 
0.00.274.441 I sampler seed: 1234
0.00.274.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.274.471 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.274.472 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.274.472 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.559 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.103.559 I llama_perf_context_print:        load time =     231.21 ms
0.02.103.560 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.52 tokens per second)
0.02.103.562 I llama_perf_context_print:        eval time =    1782.61 ms /    63 runs   (   28.30 ms per token,    35.34 tokens per second)
0.02.103.562 I llama_perf_context_print:       total time =    1830.18 ms /    70 tokens
0.02.107.402 I ggml_metal_free: deallocating

real	0m2.419s
user	0m0.129s
sys	0m0.142s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.825 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.934 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.940 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.941 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.942 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.944 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.829 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.830 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.830 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.831 I llama_model_loader: - type  f32:  194 tensors
0.00.034.831 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.832 I print_info: file format = GGUF V3 (latest)
0.00.034.833 I print_info: file type   = Q8_0
0.00.034.834 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.775 I load: special tokens cache size = 25
0.00.050.921 I load: token to piece cache size = 0.2984 MB
0.00.050.937 I print_info: arch             = gptneox
0.00.050.938 I print_info: vocab_only       = 0
0.00.050.938 I print_info: n_ctx_train      = 2048
0.00.050.939 I print_info: n_embd           = 2048
0.00.050.939 I print_info: n_layer          = 24
0.00.050.945 I print_info: n_head           = 16
0.00.050.946 I print_info: n_head_kv        = 16
0.00.050.946 I print_info: n_rot            = 32
0.00.050.946 I print_info: n_swa            = 0
0.00.050.946 I print_info: n_embd_head_k    = 128
0.00.050.946 I print_info: n_embd_head_v    = 128
0.00.050.947 I print_info: n_gqa            = 1
0.00.050.948 I print_info: n_embd_k_gqa     = 2048
0.00.050.949 I print_info: n_embd_v_gqa     = 2048
0.00.050.949 I print_info: f_norm_eps       = 1.0e-05
0.00.050.950 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.950 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.950 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.951 I print_info: f_logit_scale    = 0.0e+00
0.00.050.951 I print_info: n_ff             = 8192
0.00.050.951 I print_info: n_expert         = 0
0.00.050.952 I print_info: n_expert_used    = 0
0.00.050.952 I print_info: causal attn      = 1
0.00.050.952 I print_info: pooling type     = 0
0.00.050.952 I print_info: rope type        = 2
0.00.050.952 I print_info: rope scaling     = linear
0.00.050.953 I print_info: freq_base_train  = 10000.0
0.00.050.953 I print_info: freq_scale_train = 1
0.00.050.953 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.953 I print_info: rope_finetuned   = unknown
0.00.050.954 I print_info: ssm_d_conv       = 0
0.00.050.954 I print_info: ssm_d_inner      = 0
0.00.050.954 I print_info: ssm_d_state      = 0
0.00.050.954 I print_info: ssm_dt_rank      = 0
0.00.050.954 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.955 I print_info: model type       = 1.4B
0.00.050.955 I print_info: model params     = 1.41 B
0.00.050.955 I print_info: general.name     = 1.4B
0.00.050.956 I print_info: vocab type       = BPE
0.00.050.956 I print_info: n_vocab          = 50304
0.00.050.957 I print_info: n_merges         = 50009
0.00.050.957 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.958 I print_info: LF token         = 187 'Ċ'
0.00.050.958 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.958 I print_info: max token length = 1024
0.00.050.958 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.268.776 I load_tensors: offloading 24 repeating layers to GPU
0.01.268.780 I load_tensors: offloading output layer to GPU
0.01.268.782 I load_tensors: offloaded 25/25 layers to GPU
0.01.268.803 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.268.803 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.269.792 I llama_context: n_seq_max     = 1
0.01.269.794 I llama_context: n_ctx         = 2048
0.01.269.794 I llama_context: n_ctx_per_seq = 2048
0.01.269.794 I llama_context: n_batch       = 2048
0.01.269.795 I llama_context: n_ubatch      = 512
0.01.269.795 I llama_context: flash_attn    = 0
0.01.269.796 I llama_context: freq_base     = 10000.0
0.01.269.796 I llama_context: freq_scale    = 1
0.01.269.797 I ggml_metal_init: allocating
0.01.269.806 I ggml_metal_init: found device: Apple M4
0.01.269.814 I ggml_metal_init: picking default device: Apple M4
0.01.271.127 I ggml_metal_init: using embedded metal library
0.01.276.374 I ggml_metal_init: GPU name:   Apple M4
0.01.276.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.276.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.276.378 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.276.379 I ggml_metal_init: simdgroup reduction   = true
0.01.276.379 I ggml_metal_init: simdgroup matrix mul. = true
0.01.276.379 I ggml_metal_init: has residency sets    = true
0.01.276.379 I ggml_metal_init: has bfloat            = true
0.01.276.379 I ggml_metal_init: use bfloat            = true
0.01.276.380 I ggml_metal_init: hasUnifiedMemory      = true
0.01.276.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.290.895 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.290.899 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.335.800 I init:      Metal KV buffer size =   384.00 MiB
0.01.335.807 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.340.520 I init:      Metal compute buffer size =   102.25 MiB
0.01.340.523 I init:        CPU compute buffer size =     8.01 MiB
0.01.340.524 I init: graph nodes  = 967
0.01.340.524 I init: graph splits = 2
0.01.340.536 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.340.659 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.340.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.396.160 I main: llama threadpool init, n_threads = 4
0.01.396.201 I 
0.01.396.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.396.217 I 
0.01.396.376 I sampler seed: 1234
0.01.396.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.396.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.396.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.396.390 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.500.869 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 45954.69 tokens per second)
0.02.500.870 I llama_perf_context_print:        load time =    1385.63 ms
0.02.500.871 I llama_perf_context_print: prompt eval time =      48.61 ms /     7 tokens (    6.94 ms per token,   143.99 tokens per second)
0.02.500.872 I llama_perf_context_print:        eval time =    1053.24 ms /    63 runs   (   16.72 ms per token,    59.82 tokens per second)
0.02.500.873 I llama_perf_context_print:       total time =    1105.41 ms /    70 tokens
0.02.504.114 I ggml_metal_free: deallocating

real	0m2.520s
user	0m0.109s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.102 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.020.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.887 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.887 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.888 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.888 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.889 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.892 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.892 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.037 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.041 I llama_model_loader: - type  f32:  194 tensors
0.00.048.041 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.042 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.042 I print_info: file format = GGUF V3 (latest)
0.00.048.043 I print_info: file type   = Q4_0
0.00.048.044 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.647 I load: special tokens cache size = 25
0.00.064.393 I load: token to piece cache size = 0.2984 MB
0.00.064.409 I print_info: arch             = gptneox
0.00.064.411 I print_info: vocab_only       = 0
0.00.064.411 I print_info: n_ctx_train      = 2048
0.00.064.411 I print_info: n_embd           = 2048
0.00.064.412 I print_info: n_layer          = 24
0.00.064.415 I print_info: n_head           = 16
0.00.064.417 I print_info: n_head_kv        = 16
0.00.064.417 I print_info: n_rot            = 32
0.00.064.417 I print_info: n_swa            = 0
0.00.064.417 I print_info: n_embd_head_k    = 128
0.00.064.418 I print_info: n_embd_head_v    = 128
0.00.064.418 I print_info: n_gqa            = 1
0.00.064.419 I print_info: n_embd_k_gqa     = 2048
0.00.064.420 I print_info: n_embd_v_gqa     = 2048
0.00.064.421 I print_info: f_norm_eps       = 1.0e-05
0.00.064.422 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.422 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.424 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.426 I print_info: f_logit_scale    = 0.0e+00
0.00.064.427 I print_info: n_ff             = 8192
0.00.064.427 I print_info: n_expert         = 0
0.00.064.427 I print_info: n_expert_used    = 0
0.00.064.427 I print_info: causal attn      = 1
0.00.064.427 I print_info: pooling type     = 0
0.00.064.429 I print_info: rope type        = 2
0.00.064.430 I print_info: rope scaling     = linear
0.00.064.430 I print_info: freq_base_train  = 10000.0
0.00.064.430 I print_info: freq_scale_train = 1
0.00.064.431 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.431 I print_info: rope_finetuned   = unknown
0.00.064.431 I print_info: ssm_d_conv       = 0
0.00.064.431 I print_info: ssm_d_inner      = 0
0.00.064.431 I print_info: ssm_d_state      = 0
0.00.064.432 I print_info: ssm_dt_rank      = 0
0.00.064.432 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.432 I print_info: model type       = 1.4B
0.00.064.435 I print_info: model params     = 1.41 B
0.00.064.435 I print_info: general.name     = 1.4B
0.00.064.435 I print_info: vocab type       = BPE
0.00.064.435 I print_info: n_vocab          = 50304
0.00.064.436 I print_info: n_merges         = 50009
0.00.064.438 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.438 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.438 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.438 I print_info: LF token         = 187 'Ċ'
0.00.064.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.439 I print_info: max token length = 1024
0.00.064.440 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.969.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.969.360 I load_tensors: offloading output layer to GPU
0.00.969.360 I load_tensors: offloaded 25/25 layers to GPU
0.00.969.401 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.969.403 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.970.424 I llama_context: n_seq_max     = 1
0.00.970.427 I llama_context: n_ctx         = 2048
0.00.970.427 I llama_context: n_ctx_per_seq = 2048
0.00.970.428 I llama_context: n_batch       = 2048
0.00.970.428 I llama_context: n_ubatch      = 512
0.00.970.428 I llama_context: flash_attn    = 0
0.00.970.430 I llama_context: freq_base     = 10000.0
0.00.970.430 I llama_context: freq_scale    = 1
0.00.970.432 I ggml_metal_init: allocating
0.00.970.503 I ggml_metal_init: found device: Apple M4
0.00.970.520 I ggml_metal_init: picking default device: Apple M4
0.00.971.920 I ggml_metal_init: using embedded metal library
0.00.976.778 I ggml_metal_init: GPU name:   Apple M4
0.00.976.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.976.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.976.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.976.787 I ggml_metal_init: simdgroup reduction   = true
0.00.976.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.976.787 I ggml_metal_init: has residency sets    = true
0.00.976.787 I ggml_metal_init: has bfloat            = true
0.00.976.788 I ggml_metal_init: use bfloat            = true
0.00.976.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.976.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.993.023 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.993.027 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.030.863 I init:      Metal KV buffer size =   384.00 MiB
0.01.030.870 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.035.277 I init:      Metal compute buffer size =   102.25 MiB
0.01.035.279 I init:        CPU compute buffer size =     8.01 MiB
0.01.035.279 I init: graph nodes  = 967
0.01.035.280 I init: graph splits = 2
0.01.035.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.035.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.035.420 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.092.217 I main: llama threadpool init, n_threads = 4
0.01.092.258 I 
0.01.092.276 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.092.279 I 
0.01.092.447 I sampler seed: 1234
0.01.092.451 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.092.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.092.497 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.092.498 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.775.226 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.775.226 I llama_perf_context_print:        load time =    1070.76 ms
0.01.775.231 I llama_perf_context_print: prompt eval time =      49.54 ms /     7 tokens (    7.08 ms per token,   141.29 tokens per second)
0.01.775.233 I llama_perf_context_print:        eval time =     630.40 ms /    63 runs   (   10.01 ms per token,    99.94 tokens per second)
0.01.775.233 I llama_perf_context_print:       total time =     683.69 ms /    70 tokens
0.01.779.322 I ggml_metal_free: deallocating

real	0m1.803s
user	0m0.110s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.161 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.254 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.255 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.258 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.259 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.909 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.910 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.911 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.911 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.911 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.912 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.912 I llama_model_loader: - type  f32:  194 tensors
0.00.033.912 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.913 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.913 I print_info: file format = GGUF V3 (latest)
0.00.033.914 I print_info: file type   = Q4_1
0.00.033.918 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.449 I load: special tokens cache size = 25
0.00.048.384 I load: token to piece cache size = 0.2984 MB
0.00.048.399 I print_info: arch             = gptneox
0.00.048.400 I print_info: vocab_only       = 0
0.00.048.400 I print_info: n_ctx_train      = 2048
0.00.048.400 I print_info: n_embd           = 2048
0.00.048.400 I print_info: n_layer          = 24
0.00.048.403 I print_info: n_head           = 16
0.00.048.404 I print_info: n_head_kv        = 16
0.00.048.404 I print_info: n_rot            = 32
0.00.048.405 I print_info: n_swa            = 0
0.00.048.405 I print_info: n_embd_head_k    = 128
0.00.048.407 I print_info: n_embd_head_v    = 128
0.00.048.408 I print_info: n_gqa            = 1
0.00.048.409 I print_info: n_embd_k_gqa     = 2048
0.00.048.409 I print_info: n_embd_v_gqa     = 2048
0.00.048.410 I print_info: f_norm_eps       = 1.0e-05
0.00.048.411 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.411 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.411 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.411 I print_info: f_logit_scale    = 0.0e+00
0.00.048.412 I print_info: n_ff             = 8192
0.00.048.412 I print_info: n_expert         = 0
0.00.048.414 I print_info: n_expert_used    = 0
0.00.048.414 I print_info: causal attn      = 1
0.00.048.414 I print_info: pooling type     = 0
0.00.048.415 I print_info: rope type        = 2
0.00.048.417 I print_info: rope scaling     = linear
0.00.048.417 I print_info: freq_base_train  = 10000.0
0.00.048.417 I print_info: freq_scale_train = 1
0.00.048.417 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.417 I print_info: rope_finetuned   = unknown
0.00.048.418 I print_info: ssm_d_conv       = 0
0.00.048.418 I print_info: ssm_d_inner      = 0
0.00.048.418 I print_info: ssm_d_state      = 0
0.00.048.418 I print_info: ssm_dt_rank      = 0
0.00.048.418 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.418 I print_info: model type       = 1.4B
0.00.048.419 I print_info: model params     = 1.41 B
0.00.048.420 I print_info: general.name     = 1.4B
0.00.048.420 I print_info: vocab type       = BPE
0.00.048.420 I print_info: n_vocab          = 50304
0.00.048.420 I print_info: n_merges         = 50009
0.00.048.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.421 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.421 I print_info: LF token         = 187 'Ċ'
0.00.048.422 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.422 I print_info: max token length = 1024
0.00.048.422 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.984 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.998 I load_tensors: offloading output layer to GPU
0.00.660.999 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.034 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.661.035 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.662.434 I llama_context: n_seq_max     = 1
0.00.662.437 I llama_context: n_ctx         = 2048
0.00.662.438 I llama_context: n_ctx_per_seq = 2048
0.00.662.438 I llama_context: n_batch       = 2048
0.00.662.439 I llama_context: n_ubatch      = 512
0.00.662.439 I llama_context: flash_attn    = 0
0.00.662.441 I llama_context: freq_base     = 10000.0
0.00.662.442 I llama_context: freq_scale    = 1
0.00.662.444 I ggml_metal_init: allocating
0.00.662.517 I ggml_metal_init: found device: Apple M4
0.00.662.530 I ggml_metal_init: picking default device: Apple M4
0.00.664.400 I ggml_metal_init: using embedded metal library
0.00.671.027 I ggml_metal_init: GPU name:   Apple M4
0.00.671.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.033 I ggml_metal_init: simdgroup reduction   = true
0.00.671.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.034 I ggml_metal_init: has residency sets    = true
0.00.671.034 I ggml_metal_init: has bfloat            = true
0.00.671.034 I ggml_metal_init: use bfloat            = true
0.00.671.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.481 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.689.486 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.748.228 I init:      Metal KV buffer size =   384.00 MiB
0.00.748.237 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.479 I init:      Metal compute buffer size =   102.25 MiB
0.00.752.481 I init:        CPU compute buffer size =     8.01 MiB
0.00.752.482 I init: graph nodes  = 967
0.00.752.482 I init: graph splits = 2
0.00.752.488 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.752.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.598 I main: llama threadpool init, n_threads = 4
0.00.805.646 I 
0.00.805.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.663 I 
0.00.805.809 I sampler seed: 1234
0.00.805.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.834 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.834 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.533.640 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.533.641 I llama_perf_context_print:        load time =     795.74 ms
0.01.533.642 I llama_perf_context_print: prompt eval time =      48.46 ms /     7 tokens (    6.92 ms per token,   144.44 tokens per second)
0.01.533.643 I llama_perf_context_print:        eval time =     676.64 ms /    63 runs   (   10.74 ms per token,    93.11 tokens per second)
0.01.533.643 I llama_perf_context_print:       total time =     728.74 ms /    70 tokens
0.01.537.465 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.629 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.064 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.860 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.615 I llama_model_loader: - type  f32:  194 tensors
0.00.024.616 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.616 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.616 I print_info: file format = GGUF V3 (latest)
0.00.024.617 I print_info: file type   = Q5_0
0.00.024.618 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.405 I load: special tokens cache size = 25
0.00.038.755 I load: token to piece cache size = 0.2984 MB
0.00.038.769 I print_info: arch             = gptneox
0.00.038.770 I print_info: vocab_only       = 0
0.00.038.770 I print_info: n_ctx_train      = 2048
0.00.038.771 I print_info: n_embd           = 2048
0.00.038.771 I print_info: n_layer          = 24
0.00.038.774 I print_info: n_head           = 16
0.00.038.775 I print_info: n_head_kv        = 16
0.00.038.775 I print_info: n_rot            = 32
0.00.038.775 I print_info: n_swa            = 0
0.00.038.775 I print_info: n_embd_head_k    = 128
0.00.038.776 I print_info: n_embd_head_v    = 128
0.00.038.777 I print_info: n_gqa            = 1
0.00.038.777 I print_info: n_embd_k_gqa     = 2048
0.00.038.778 I print_info: n_embd_v_gqa     = 2048
0.00.038.779 I print_info: f_norm_eps       = 1.0e-05
0.00.038.779 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.779 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.779 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.780 I print_info: f_logit_scale    = 0.0e+00
0.00.038.780 I print_info: n_ff             = 8192
0.00.038.780 I print_info: n_expert         = 0
0.00.038.781 I print_info: n_expert_used    = 0
0.00.038.781 I print_info: causal attn      = 1
0.00.038.781 I print_info: pooling type     = 0
0.00.038.781 I print_info: rope type        = 2
0.00.038.781 I print_info: rope scaling     = linear
0.00.038.782 I print_info: freq_base_train  = 10000.0
0.00.038.782 I print_info: freq_scale_train = 1
0.00.038.782 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.782 I print_info: rope_finetuned   = unknown
0.00.038.782 I print_info: ssm_d_conv       = 0
0.00.038.783 I print_info: ssm_d_inner      = 0
0.00.038.783 I print_info: ssm_d_state      = 0
0.00.038.783 I print_info: ssm_dt_rank      = 0
0.00.038.783 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.783 I print_info: model type       = 1.4B
0.00.038.783 I print_info: model params     = 1.41 B
0.00.038.784 I print_info: general.name     = 1.4B
0.00.038.784 I print_info: vocab type       = BPE
0.00.038.784 I print_info: n_vocab          = 50304
0.00.038.784 I print_info: n_merges         = 50009
0.00.038.786 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.786 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.786 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.786 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.787 I print_info: LF token         = 187 'Ċ'
0.00.038.787 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.787 I print_info: max token length = 1024
0.00.038.787 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.735.578 I load_tensors: offloading 24 repeating layers to GPU
0.00.735.597 I load_tensors: offloading output layer to GPU
0.00.735.598 I load_tensors: offloaded 25/25 layers to GPU
0.00.735.632 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.735.633 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.736.992 I llama_context: n_seq_max     = 1
0.00.736.996 I llama_context: n_ctx         = 2048
0.00.736.997 I llama_context: n_ctx_per_seq = 2048
0.00.736.998 I llama_context: n_batch       = 2048
0.00.736.998 I llama_context: n_ubatch      = 512
0.00.736.998 I llama_context: flash_attn    = 0
0.00.737.000 I llama_context: freq_base     = 10000.0
0.00.737.001 I llama_context: freq_scale    = 1
0.00.737.003 I ggml_metal_init: allocating
0.00.737.088 I ggml_metal_init: found device: Apple M4
0.00.737.102 I ggml_metal_init: picking default device: Apple M4
0.00.738.885 I ggml_metal_init: using embedded metal library
0.00.744.005 I ggml_metal_init: GPU name:   Apple M4
0.00.744.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.744.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.744.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.744.018 I ggml_metal_init: simdgroup reduction   = true
0.00.744.018 I ggml_metal_init: simdgroup matrix mul. = true
0.00.744.018 I ggml_metal_init: has residency sets    = true
0.00.744.018 I ggml_metal_init: has bfloat            = true
0.00.744.018 I ggml_metal_init: use bfloat            = true
0.00.744.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.744.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.756.020 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.756.023 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.787.658 I init:      Metal KV buffer size =   384.00 MiB
0.00.787.664 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.791.822 I init:      Metal compute buffer size =   102.25 MiB
0.00.791.824 I init:        CPU compute buffer size =     8.01 MiB
0.00.791.825 I init: graph nodes  = 967
0.00.791.825 I init: graph splits = 2
0.00.791.830 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.791.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.791.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.850.458 I main: llama threadpool init, n_threads = 4
0.00.850.504 I 
0.00.850.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.850.521 I 
0.00.850.677 I sampler seed: 1234
0.00.850.682 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.850.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.850.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.850.702 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.638.104 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.638.105 I llama_perf_context_print:        load time =     841.12 ms
0.01.638.106 I llama_perf_context_print: prompt eval time =      52.82 ms /     7 tokens (    7.55 ms per token,   132.54 tokens per second)
0.01.638.107 I llama_perf_context_print:        eval time =     731.64 ms /    63 runs   (   11.61 ms per token,    86.11 tokens per second)
0.01.638.107 I llama_perf_context_print:       total time =     788.35 ms /    70 tokens
0.01.641.989 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.101s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.619 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.920 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.692 I llama_model_loader: - type  f32:  194 tensors
0.00.025.693 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.693 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.694 I print_info: file format = GGUF V3 (latest)
0.00.025.694 I print_info: file type   = Q5_1
0.00.025.695 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.536 I load: special tokens cache size = 25
0.00.039.732 I load: token to piece cache size = 0.2984 MB
0.00.039.746 I print_info: arch             = gptneox
0.00.039.747 I print_info: vocab_only       = 0
0.00.039.747 I print_info: n_ctx_train      = 2048
0.00.039.747 I print_info: n_embd           = 2048
0.00.039.747 I print_info: n_layer          = 24
0.00.039.750 I print_info: n_head           = 16
0.00.039.751 I print_info: n_head_kv        = 16
0.00.039.751 I print_info: n_rot            = 32
0.00.039.751 I print_info: n_swa            = 0
0.00.039.751 I print_info: n_embd_head_k    = 128
0.00.039.752 I print_info: n_embd_head_v    = 128
0.00.039.752 I print_info: n_gqa            = 1
0.00.039.753 I print_info: n_embd_k_gqa     = 2048
0.00.039.754 I print_info: n_embd_v_gqa     = 2048
0.00.039.755 I print_info: f_norm_eps       = 1.0e-05
0.00.039.755 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.755 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.755 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.755 I print_info: f_logit_scale    = 0.0e+00
0.00.039.756 I print_info: n_ff             = 8192
0.00.039.756 I print_info: n_expert         = 0
0.00.039.756 I print_info: n_expert_used    = 0
0.00.039.756 I print_info: causal attn      = 1
0.00.039.757 I print_info: pooling type     = 0
0.00.039.758 I print_info: rope type        = 2
0.00.039.759 I print_info: rope scaling     = linear
0.00.039.759 I print_info: freq_base_train  = 10000.0
0.00.039.760 I print_info: freq_scale_train = 1
0.00.039.760 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.760 I print_info: rope_finetuned   = unknown
0.00.039.760 I print_info: ssm_d_conv       = 0
0.00.039.761 I print_info: ssm_d_inner      = 0
0.00.039.761 I print_info: ssm_d_state      = 0
0.00.039.761 I print_info: ssm_dt_rank      = 0
0.00.039.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.762 I print_info: model type       = 1.4B
0.00.039.762 I print_info: model params     = 1.41 B
0.00.039.762 I print_info: general.name     = 1.4B
0.00.039.763 I print_info: vocab type       = BPE
0.00.039.763 I print_info: n_vocab          = 50304
0.00.039.763 I print_info: n_merges         = 50009
0.00.039.763 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: LF token         = 187 'Ċ'
0.00.039.764 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: max token length = 1024
0.00.039.765 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.798 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.808 I load_tensors: offloading output layer to GPU
0.00.641.809 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.837 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.641.838 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.643.118 I llama_context: n_seq_max     = 1
0.00.643.121 I llama_context: n_ctx         = 2048
0.00.643.121 I llama_context: n_ctx_per_seq = 2048
0.00.643.122 I llama_context: n_batch       = 2048
0.00.643.122 I llama_context: n_ubatch      = 512
0.00.643.123 I llama_context: flash_attn    = 0
0.00.643.125 I llama_context: freq_base     = 10000.0
0.00.643.126 I llama_context: freq_scale    = 1
0.00.643.128 I ggml_metal_init: allocating
0.00.643.173 I ggml_metal_init: found device: Apple M4
0.00.643.184 I ggml_metal_init: picking default device: Apple M4
0.00.644.899 I ggml_metal_init: using embedded metal library
0.00.651.575 I ggml_metal_init: GPU name:   Apple M4
0.00.651.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.583 I ggml_metal_init: simdgroup reduction   = true
0.00.651.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.583 I ggml_metal_init: has residency sets    = true
0.00.651.583 I ggml_metal_init: has bfloat            = true
0.00.651.584 I ggml_metal_init: use bfloat            = true
0.00.651.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.432 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.669.436 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.579 I init:      Metal KV buffer size =   384.00 MiB
0.00.723.587 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.728.282 I init:      Metal compute buffer size =   102.25 MiB
0.00.728.283 I init:        CPU compute buffer size =     8.01 MiB
0.00.728.284 I init: graph nodes  = 967
0.00.728.284 I init: graph splits = 2
0.00.728.295 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.418 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.416 I main: llama threadpool init, n_threads = 4
0.00.789.460 I 
0.00.789.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.474 I 
0.00.789.619 I sampler seed: 1234
0.00.789.624 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.635 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.635 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.635 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.632.746 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.632.747 I llama_perf_context_print:        load time =     779.09 ms
0.01.632.748 I llama_perf_context_print: prompt eval time =      51.96 ms /     7 tokens (    7.42 ms per token,   134.71 tokens per second)
0.01.632.748 I llama_perf_context_print:        eval time =     788.29 ms /    63 runs   (   12.51 ms per token,    79.92 tokens per second)
0.01.632.749 I llama_perf_context_print:       total time =     844.04 ms /    70 tokens
0.01.636.494 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.559 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.060 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.069 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.836 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.573 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.573 I llama_model_loader: - type  f32:  194 tensors
0.00.023.573 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.573 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.573 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.574 I print_info: file format = GGUF V3 (latest)
0.00.023.574 I print_info: file type   = Q2_K - Medium
0.00.023.575 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.377 I load: special tokens cache size = 25
0.00.037.495 I load: token to piece cache size = 0.2984 MB
0.00.037.509 I print_info: arch             = gptneox
0.00.037.510 I print_info: vocab_only       = 0
0.00.037.510 I print_info: n_ctx_train      = 2048
0.00.037.510 I print_info: n_embd           = 2048
0.00.037.511 I print_info: n_layer          = 24
0.00.037.514 I print_info: n_head           = 16
0.00.037.515 I print_info: n_head_kv        = 16
0.00.037.515 I print_info: n_rot            = 32
0.00.037.515 I print_info: n_swa            = 0
0.00.037.515 I print_info: n_embd_head_k    = 128
0.00.037.517 I print_info: n_embd_head_v    = 128
0.00.037.518 I print_info: n_gqa            = 1
0.00.037.519 I print_info: n_embd_k_gqa     = 2048
0.00.037.520 I print_info: n_embd_v_gqa     = 2048
0.00.037.520 I print_info: f_norm_eps       = 1.0e-05
0.00.037.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.522 I print_info: f_logit_scale    = 0.0e+00
0.00.037.523 I print_info: n_ff             = 8192
0.00.037.523 I print_info: n_expert         = 0
0.00.037.523 I print_info: n_expert_used    = 0
0.00.037.523 I print_info: causal attn      = 1
0.00.037.523 I print_info: pooling type     = 0
0.00.037.524 I print_info: rope type        = 2
0.00.037.524 I print_info: rope scaling     = linear
0.00.037.524 I print_info: freq_base_train  = 10000.0
0.00.037.524 I print_info: freq_scale_train = 1
0.00.037.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.525 I print_info: rope_finetuned   = unknown
0.00.037.525 I print_info: ssm_d_conv       = 0
0.00.037.528 I print_info: ssm_d_inner      = 0
0.00.037.529 I print_info: ssm_d_state      = 0
0.00.037.529 I print_info: ssm_dt_rank      = 0
0.00.037.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.529 I print_info: model type       = 1.4B
0.00.037.529 I print_info: model params     = 1.41 B
0.00.037.530 I print_info: general.name     = 1.4B
0.00.037.530 I print_info: vocab type       = BPE
0.00.037.530 I print_info: n_vocab          = 50304
0.00.037.530 I print_info: n_merges         = 50009
0.00.037.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.531 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.531 I print_info: LF token         = 187 'Ċ'
0.00.037.532 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.532 I print_info: max token length = 1024
0.00.037.532 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.347.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.347.259 I load_tensors: offloading output layer to GPU
0.00.347.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.347.293 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.347.295 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.348.852 I llama_context: n_seq_max     = 1
0.00.348.858 I llama_context: n_ctx         = 2048
0.00.348.859 I llama_context: n_ctx_per_seq = 2048
0.00.348.859 I llama_context: n_batch       = 2048
0.00.348.860 I llama_context: n_ubatch      = 512
0.00.348.860 I llama_context: flash_attn    = 0
0.00.348.862 I llama_context: freq_base     = 10000.0
0.00.348.862 I llama_context: freq_scale    = 1
0.00.348.864 I ggml_metal_init: allocating
0.00.348.958 I ggml_metal_init: found device: Apple M4
0.00.348.973 I ggml_metal_init: picking default device: Apple M4
0.00.350.866 I ggml_metal_init: using embedded metal library
0.00.356.266 I ggml_metal_init: GPU name:   Apple M4
0.00.356.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.284 I ggml_metal_init: simdgroup reduction   = true
0.00.356.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.284 I ggml_metal_init: has residency sets    = true
0.00.356.284 I ggml_metal_init: has bfloat            = true
0.00.356.285 I ggml_metal_init: use bfloat            = true
0.00.356.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.376.967 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.376.972 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.681 I init:      Metal KV buffer size =   384.00 MiB
0.00.429.689 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.805 I init:      Metal compute buffer size =   102.25 MiB
0.00.433.807 I init:        CPU compute buffer size =     8.01 MiB
0.00.433.807 I init: graph nodes  = 967
0.00.433.808 I init: graph splits = 2
0.00.433.814 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.433.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.433.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.449 I main: llama threadpool init, n_threads = 4
0.00.491.493 I 
0.00.491.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.508 I 
0.00.491.687 I sampler seed: 1234
0.00.491.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.491.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.491.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.491.704 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.165.657 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.165.658 I llama_perf_context_print:        load time =     482.19 ms
0.01.165.658 I llama_perf_context_print: prompt eval time =      35.72 ms /     7 tokens (    5.10 ms per token,   195.97 tokens per second)
0.01.165.659 I llama_perf_context_print:        eval time =     635.39 ms /    63 runs   (   10.09 ms per token,    99.15 tokens per second)
0.01.165.660 I llama_perf_context_print:       total time =     674.90 ms /    70 tokens
0.01.168.982 I ggml_metal_free: deallocating

real	0m1.184s
user	0m0.109s
sys	0m0.168s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.500 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.504 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.262 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.029 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.029 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.030 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.030 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.030 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.031 I llama_model_loader: - type  f32:  194 tensors
0.00.026.031 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.031 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.032 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.032 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.032 I print_info: file format = GGUF V3 (latest)
0.00.026.033 I print_info: file type   = Q3_K - Medium
0.00.026.034 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.205 I load: special tokens cache size = 25
0.00.040.481 I load: token to piece cache size = 0.2984 MB
0.00.040.496 I print_info: arch             = gptneox
0.00.040.497 I print_info: vocab_only       = 0
0.00.040.497 I print_info: n_ctx_train      = 2048
0.00.040.498 I print_info: n_embd           = 2048
0.00.040.498 I print_info: n_layer          = 24
0.00.040.501 I print_info: n_head           = 16
0.00.040.502 I print_info: n_head_kv        = 16
0.00.040.502 I print_info: n_rot            = 32
0.00.040.502 I print_info: n_swa            = 0
0.00.040.502 I print_info: n_embd_head_k    = 128
0.00.040.502 I print_info: n_embd_head_v    = 128
0.00.040.503 I print_info: n_gqa            = 1
0.00.040.504 I print_info: n_embd_k_gqa     = 2048
0.00.040.504 I print_info: n_embd_v_gqa     = 2048
0.00.040.505 I print_info: f_norm_eps       = 1.0e-05
0.00.040.505 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.506 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.506 I print_info: f_logit_scale    = 0.0e+00
0.00.040.506 I print_info: n_ff             = 8192
0.00.040.507 I print_info: n_expert         = 0
0.00.040.507 I print_info: n_expert_used    = 0
0.00.040.507 I print_info: causal attn      = 1
0.00.040.507 I print_info: pooling type     = 0
0.00.040.507 I print_info: rope type        = 2
0.00.040.507 I print_info: rope scaling     = linear
0.00.040.508 I print_info: freq_base_train  = 10000.0
0.00.040.508 I print_info: freq_scale_train = 1
0.00.040.508 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.508 I print_info: rope_finetuned   = unknown
0.00.040.508 I print_info: ssm_d_conv       = 0
0.00.040.509 I print_info: ssm_d_inner      = 0
0.00.040.509 I print_info: ssm_d_state      = 0
0.00.040.509 I print_info: ssm_dt_rank      = 0
0.00.040.509 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.509 I print_info: model type       = 1.4B
0.00.040.509 I print_info: model params     = 1.41 B
0.00.040.509 I print_info: general.name     = 1.4B
0.00.040.510 I print_info: vocab type       = BPE
0.00.040.510 I print_info: n_vocab          = 50304
0.00.040.510 I print_info: n_merges         = 50009
0.00.040.511 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.511 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.511 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.511 I print_info: LF token         = 187 'Ċ'
0.00.040.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.512 I print_info: max token length = 1024
0.00.040.512 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.056 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.071 I load_tensors: offloading output layer to GPU
0.00.442.072 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.107 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.108 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.443.666 I llama_context: n_seq_max     = 1
0.00.443.669 I llama_context: n_ctx         = 2048
0.00.443.670 I llama_context: n_ctx_per_seq = 2048
0.00.443.670 I llama_context: n_batch       = 2048
0.00.443.671 I llama_context: n_ubatch      = 512
0.00.443.671 I llama_context: flash_attn    = 0
0.00.443.673 I llama_context: freq_base     = 10000.0
0.00.443.674 I llama_context: freq_scale    = 1
0.00.443.683 I ggml_metal_init: allocating
0.00.443.757 I ggml_metal_init: found device: Apple M4
0.00.443.771 I ggml_metal_init: picking default device: Apple M4
0.00.445.743 I ggml_metal_init: using embedded metal library
0.00.451.704 I ggml_metal_init: GPU name:   Apple M4
0.00.451.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.710 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.711 I ggml_metal_init: simdgroup reduction   = true
0.00.451.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.712 I ggml_metal_init: has residency sets    = true
0.00.451.712 I ggml_metal_init: has bfloat            = true
0.00.451.712 I ggml_metal_init: use bfloat            = true
0.00.451.713 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.056 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.471.060 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.447 I init:      Metal KV buffer size =   384.00 MiB
0.00.527.454 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.531.591 I init:      Metal compute buffer size =   102.25 MiB
0.00.531.594 I init:        CPU compute buffer size =     8.01 MiB
0.00.531.594 I init: graph nodes  = 967
0.00.531.594 I init: graph splits = 2
0.00.531.601 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.264 I main: llama threadpool init, n_threads = 4
0.00.586.309 I 
0.00.586.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.586.325 I 
0.00.586.478 I sampler seed: 1234
0.00.586.482 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.586.492 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.586.497 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.586.497 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.326.659 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51900.58 tokens per second)
0.01.326.660 I llama_perf_context_print:        load time =     575.72 ms
0.01.326.661 I llama_perf_context_print: prompt eval time =      40.16 ms /     7 tokens (    5.74 ms per token,   174.32 tokens per second)
0.01.326.662 I llama_perf_context_print:        eval time =     697.15 ms /    63 runs   (   11.07 ms per token,    90.37 tokens per second)
0.01.326.663 I llama_perf_context_print:       total time =     741.13 ms /    70 tokens
0.01.330.478 I ggml_metal_free: deallocating

real	0m1.349s
user	0m0.108s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.345 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.769 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.773 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.773 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.774 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.774 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.229 I llama_model_loader: - type  f32:  194 tensors
0.00.025.229 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.229 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.229 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.230 I print_info: file format = GGUF V3 (latest)
0.00.025.230 I print_info: file type   = Q4_K - Medium
0.00.025.231 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.984 I load: special tokens cache size = 25
0.00.038.852 I load: token to piece cache size = 0.2984 MB
0.00.038.862 I print_info: arch             = gptneox
0.00.038.863 I print_info: vocab_only       = 0
0.00.038.863 I print_info: n_ctx_train      = 2048
0.00.038.863 I print_info: n_embd           = 2048
0.00.038.863 I print_info: n_layer          = 24
0.00.038.866 I print_info: n_head           = 16
0.00.038.867 I print_info: n_head_kv        = 16
0.00.038.867 I print_info: n_rot            = 32
0.00.038.867 I print_info: n_swa            = 0
0.00.038.868 I print_info: n_embd_head_k    = 128
0.00.038.868 I print_info: n_embd_head_v    = 128
0.00.038.869 I print_info: n_gqa            = 1
0.00.038.870 I print_info: n_embd_k_gqa     = 2048
0.00.038.870 I print_info: n_embd_v_gqa     = 2048
0.00.038.877 I print_info: f_norm_eps       = 1.0e-05
0.00.038.878 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.878 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.879 I print_info: f_logit_scale    = 0.0e+00
0.00.038.880 I print_info: n_ff             = 8192
0.00.038.880 I print_info: n_expert         = 0
0.00.038.881 I print_info: n_expert_used    = 0
0.00.038.881 I print_info: causal attn      = 1
0.00.038.881 I print_info: pooling type     = 0
0.00.038.882 I print_info: rope type        = 2
0.00.038.882 I print_info: rope scaling     = linear
0.00.038.883 I print_info: freq_base_train  = 10000.0
0.00.038.883 I print_info: freq_scale_train = 1
0.00.038.883 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.883 I print_info: rope_finetuned   = unknown
0.00.038.883 I print_info: ssm_d_conv       = 0
0.00.038.884 I print_info: ssm_d_inner      = 0
0.00.038.884 I print_info: ssm_d_state      = 0
0.00.038.884 I print_info: ssm_dt_rank      = 0
0.00.038.884 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.884 I print_info: model type       = 1.4B
0.00.038.884 I print_info: model params     = 1.41 B
0.00.038.885 I print_info: general.name     = 1.4B
0.00.038.887 I print_info: vocab type       = BPE
0.00.038.888 I print_info: n_vocab          = 50304
0.00.038.888 I print_info: n_merges         = 50009
0.00.038.888 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.888 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.888 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.889 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.889 I print_info: LF token         = 187 'Ċ'
0.00.038.889 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.889 I print_info: max token length = 1024
0.00.038.890 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.513.453 I load_tensors: offloading 24 repeating layers to GPU
0.00.513.469 I load_tensors: offloading output layer to GPU
0.00.513.469 I load_tensors: offloaded 25/25 layers to GPU
0.00.513.503 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.513.504 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.187 I llama_context: n_seq_max     = 1
0.00.515.193 I llama_context: n_ctx         = 2048
0.00.515.193 I llama_context: n_ctx_per_seq = 2048
0.00.515.194 I llama_context: n_batch       = 2048
0.00.515.194 I llama_context: n_ubatch      = 512
0.00.515.194 I llama_context: flash_attn    = 0
0.00.515.197 I llama_context: freq_base     = 10000.0
0.00.515.198 I llama_context: freq_scale    = 1
0.00.515.199 I ggml_metal_init: allocating
0.00.515.285 I ggml_metal_init: found device: Apple M4
0.00.515.299 I ggml_metal_init: picking default device: Apple M4
0.00.517.257 I ggml_metal_init: using embedded metal library
0.00.523.842 I ggml_metal_init: GPU name:   Apple M4
0.00.523.846 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.848 I ggml_metal_init: simdgroup reduction   = true
0.00.523.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.849 I ggml_metal_init: has residency sets    = true
0.00.523.849 I ggml_metal_init: has bfloat            = true
0.00.523.850 I ggml_metal_init: use bfloat            = true
0.00.523.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.541.425 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.541.429 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.599.465 I init:      Metal KV buffer size =   384.00 MiB
0.00.599.472 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.603.534 I init:      Metal compute buffer size =   102.25 MiB
0.00.603.536 I init:        CPU compute buffer size =     8.01 MiB
0.00.603.537 I init: graph nodes  = 967
0.00.603.537 I init: graph splits = 2
0.00.603.543 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.603.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.603.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.794 I main: llama threadpool init, n_threads = 4
0.00.663.843 I 
0.00.663.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.858 I 
0.00.664.027 I sampler seed: 1234
0.00.664.032 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.052 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.053 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.425.941 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.425.942 I llama_perf_context_print:        load time =     652.74 ms
0.01.425.943 I llama_perf_context_print: prompt eval time =      57.75 ms /     7 tokens (    8.25 ms per token,   121.21 tokens per second)
0.01.425.943 I llama_perf_context_print:        eval time =     701.22 ms /    63 runs   (   11.13 ms per token,    89.84 tokens per second)
0.01.425.944 I llama_perf_context_print:       total time =     762.85 ms /    70 tokens
0.01.430.952 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.109s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.318 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.241 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.242 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.242 I llama_model_loader: - type  f32:  194 tensors
0.00.024.243 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.243 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.244 I print_info: file format = GGUF V3 (latest)
0.00.024.244 I print_info: file type   = Q5_K - Medium
0.00.024.245 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.361 I load: special tokens cache size = 25
0.00.038.621 I load: token to piece cache size = 0.2984 MB
0.00.038.636 I print_info: arch             = gptneox
0.00.038.637 I print_info: vocab_only       = 0
0.00.038.637 I print_info: n_ctx_train      = 2048
0.00.038.637 I print_info: n_embd           = 2048
0.00.038.637 I print_info: n_layer          = 24
0.00.038.640 I print_info: n_head           = 16
0.00.038.641 I print_info: n_head_kv        = 16
0.00.038.641 I print_info: n_rot            = 32
0.00.038.641 I print_info: n_swa            = 0
0.00.038.642 I print_info: n_embd_head_k    = 128
0.00.038.642 I print_info: n_embd_head_v    = 128
0.00.038.643 I print_info: n_gqa            = 1
0.00.038.643 I print_info: n_embd_k_gqa     = 2048
0.00.038.644 I print_info: n_embd_v_gqa     = 2048
0.00.038.645 I print_info: f_norm_eps       = 1.0e-05
0.00.038.654 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.654 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.655 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.655 I print_info: f_logit_scale    = 0.0e+00
0.00.038.660 I print_info: n_ff             = 8192
0.00.038.661 I print_info: n_expert         = 0
0.00.038.661 I print_info: n_expert_used    = 0
0.00.038.661 I print_info: causal attn      = 1
0.00.038.661 I print_info: pooling type     = 0
0.00.038.663 I print_info: rope type        = 2
0.00.038.664 I print_info: rope scaling     = linear
0.00.038.664 I print_info: freq_base_train  = 10000.0
0.00.038.664 I print_info: freq_scale_train = 1
0.00.038.665 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.665 I print_info: rope_finetuned   = unknown
0.00.038.665 I print_info: ssm_d_conv       = 0
0.00.038.665 I print_info: ssm_d_inner      = 0
0.00.038.666 I print_info: ssm_d_state      = 0
0.00.038.666 I print_info: ssm_dt_rank      = 0
0.00.038.666 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.667 I print_info: model type       = 1.4B
0.00.038.667 I print_info: model params     = 1.41 B
0.00.038.667 I print_info: general.name     = 1.4B
0.00.038.668 I print_info: vocab type       = BPE
0.00.038.668 I print_info: n_vocab          = 50304
0.00.038.668 I print_info: n_merges         = 50009
0.00.038.668 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.668 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.669 I print_info: LF token         = 187 'Ċ'
0.00.038.669 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.670 I print_info: max token length = 1024
0.00.038.670 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.664 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.674 I load_tensors: offloading output layer to GPU
0.00.593.675 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.707 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.709 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.594.778 I llama_context: n_seq_max     = 1
0.00.594.779 I llama_context: n_ctx         = 2048
0.00.594.779 I llama_context: n_ctx_per_seq = 2048
0.00.594.779 I llama_context: n_batch       = 2048
0.00.594.780 I llama_context: n_ubatch      = 512
0.00.594.780 I llama_context: flash_attn    = 0
0.00.594.781 I llama_context: freq_base     = 10000.0
0.00.594.781 I llama_context: freq_scale    = 1
0.00.594.782 I ggml_metal_init: allocating
0.00.594.805 I ggml_metal_init: found device: Apple M4
0.00.594.811 I ggml_metal_init: picking default device: Apple M4
0.00.595.604 I ggml_metal_init: using embedded metal library
0.00.599.489 I ggml_metal_init: GPU name:   Apple M4
0.00.599.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.492 I ggml_metal_init: simdgroup reduction   = true
0.00.599.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.493 I ggml_metal_init: has residency sets    = true
0.00.599.493 I ggml_metal_init: has bfloat            = true
0.00.599.493 I ggml_metal_init: use bfloat            = true
0.00.599.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.610.641 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.610.644 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.555 I init:      Metal KV buffer size =   384.00 MiB
0.00.644.560 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.649.057 I init:      Metal compute buffer size =   102.25 MiB
0.00.649.058 I init:        CPU compute buffer size =     8.01 MiB
0.00.649.059 I init: graph nodes  = 967
0.00.649.059 I init: graph splits = 2
0.00.649.064 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.649.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.649.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.567 I main: llama threadpool init, n_threads = 4
0.00.710.601 I 
0.00.710.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.614 I 
0.00.710.776 I sampler seed: 1234
0.00.710.781 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.814 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.817 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.817 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.553.409 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49134.95 tokens per second)
0.01.553.410 I llama_perf_context_print:        load time =     701.16 ms
0.01.553.411 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.35 tokens per second)
0.01.553.412 I llama_perf_context_print:        eval time =     787.02 ms /    63 runs   (   12.49 ms per token,    80.05 tokens per second)
0.01.553.412 I llama_perf_context_print:       total time =     843.53 ms /    70 tokens
0.01.557.478 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.101s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.690 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.694 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.371 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.372 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.373 I llama_model_loader: - type  f32:  194 tensors
0.00.024.373 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.374 I print_info: file format = GGUF V3 (latest)
0.00.024.374 I print_info: file type   = Q6_K
0.00.024.375 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.640 I load: special tokens cache size = 25
0.00.038.663 I load: token to piece cache size = 0.2984 MB
0.00.038.678 I print_info: arch             = gptneox
0.00.038.679 I print_info: vocab_only       = 0
0.00.038.679 I print_info: n_ctx_train      = 2048
0.00.038.679 I print_info: n_embd           = 2048
0.00.038.679 I print_info: n_layer          = 24
0.00.038.682 I print_info: n_head           = 16
0.00.038.683 I print_info: n_head_kv        = 16
0.00.038.684 I print_info: n_rot            = 32
0.00.038.684 I print_info: n_swa            = 0
0.00.038.684 I print_info: n_embd_head_k    = 128
0.00.038.684 I print_info: n_embd_head_v    = 128
0.00.038.685 I print_info: n_gqa            = 1
0.00.038.686 I print_info: n_embd_k_gqa     = 2048
0.00.038.686 I print_info: n_embd_v_gqa     = 2048
0.00.038.687 I print_info: f_norm_eps       = 1.0e-05
0.00.038.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.695 I print_info: f_logit_scale    = 0.0e+00
0.00.038.696 I print_info: n_ff             = 8192
0.00.038.696 I print_info: n_expert         = 0
0.00.038.697 I print_info: n_expert_used    = 0
0.00.038.697 I print_info: causal attn      = 1
0.00.038.697 I print_info: pooling type     = 0
0.00.038.697 I print_info: rope type        = 2
0.00.038.698 I print_info: rope scaling     = linear
0.00.038.698 I print_info: freq_base_train  = 10000.0
0.00.038.698 I print_info: freq_scale_train = 1
0.00.038.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.699 I print_info: rope_finetuned   = unknown
0.00.038.699 I print_info: ssm_d_conv       = 0
0.00.038.699 I print_info: ssm_d_inner      = 0
0.00.038.699 I print_info: ssm_d_state      = 0
0.00.038.699 I print_info: ssm_dt_rank      = 0
0.00.038.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.700 I print_info: model type       = 1.4B
0.00.038.700 I print_info: model params     = 1.41 B
0.00.038.700 I print_info: general.name     = 1.4B
0.00.038.701 I print_info: vocab type       = BPE
0.00.038.701 I print_info: n_vocab          = 50304
0.00.038.701 I print_info: n_merges         = 50009
0.00.038.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: LF token         = 187 'Ċ'
0.00.038.702 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: max token length = 1024
0.00.038.703 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.652 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.655 I load_tensors: offloading output layer to GPU
0.00.650.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.676 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.678 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.652.074 I llama_context: n_seq_max     = 1
0.00.652.076 I llama_context: n_ctx         = 2048
0.00.652.076 I llama_context: n_ctx_per_seq = 2048
0.00.652.077 I llama_context: n_batch       = 2048
0.00.652.077 I llama_context: n_ubatch      = 512
0.00.652.077 I llama_context: flash_attn    = 0
0.00.652.078 I llama_context: freq_base     = 10000.0
0.00.652.079 I llama_context: freq_scale    = 1
0.00.652.080 I ggml_metal_init: allocating
0.00.652.137 I ggml_metal_init: found device: Apple M4
0.00.652.147 I ggml_metal_init: picking default device: Apple M4
0.00.653.725 I ggml_metal_init: using embedded metal library
0.00.659.934 I ggml_metal_init: GPU name:   Apple M4
0.00.659.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.939 I ggml_metal_init: simdgroup reduction   = true
0.00.659.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.940 I ggml_metal_init: has residency sets    = true
0.00.659.940 I ggml_metal_init: has bfloat            = true
0.00.659.940 I ggml_metal_init: use bfloat            = true
0.00.659.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.442 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.676.446 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.540 I init:      Metal KV buffer size =   384.00 MiB
0.00.734.547 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.738.781 I init:      Metal compute buffer size =   102.25 MiB
0.00.738.783 I init:        CPU compute buffer size =     8.01 MiB
0.00.738.783 I init: graph nodes  = 967
0.00.738.784 I init: graph splits = 2
0.00.738.791 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.895 I main: llama threadpool init, n_threads = 4
0.00.804.942 I 
0.00.804.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.959 I 
0.00.805.136 I sampler seed: 1234
0.00.805.141 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.151 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.152 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.157 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.677.605 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.677.606 I llama_perf_context_print:        load time =     795.14 ms
0.01.677.607 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.11 tokens per second)
0.01.677.608 I llama_perf_context_print:        eval time =     811.61 ms /    63 runs   (   12.88 ms per token,    77.62 tokens per second)
0.01.677.608 I llama_perf_context_print:       total time =     873.42 ms /    70 tokens
0.01.680.865 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.590 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.552 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.107 I llama_model_loader: - type  f32:  194 tensors
0.00.058.107 I llama_model_loader: - type  f16:   98 tensors
0.00.058.108 I print_info: file format = GGUF V3 (latest)
0.00.058.109 I print_info: file type   = all F32 (guessed)
0.00.058.110 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.718 I load: special tokens cache size = 25
0.00.078.804 I load: token to piece cache size = 0.2984 MB
0.00.078.819 I print_info: arch             = gptneox
0.00.078.821 I print_info: vocab_only       = 0
0.00.078.821 I print_info: n_ctx_train      = 2048
0.00.078.821 I print_info: n_embd           = 2048
0.00.078.821 I print_info: n_layer          = 24
0.00.078.825 I print_info: n_head           = 16
0.00.078.826 I print_info: n_head_kv        = 16
0.00.078.826 I print_info: n_rot            = 32
0.00.078.827 I print_info: n_swa            = 0
0.00.078.827 I print_info: n_embd_head_k    = 128
0.00.078.827 I print_info: n_embd_head_v    = 128
0.00.078.828 I print_info: n_gqa            = 1
0.00.078.829 I print_info: n_embd_k_gqa     = 2048
0.00.078.829 I print_info: n_embd_v_gqa     = 2048
0.00.078.830 I print_info: f_norm_eps       = 1.0e-05
0.00.078.830 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.831 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.831 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.831 I print_info: f_logit_scale    = 0.0e+00
0.00.078.832 I print_info: n_ff             = 8192
0.00.078.832 I print_info: n_expert         = 0
0.00.078.832 I print_info: n_expert_used    = 0
0.00.078.832 I print_info: causal attn      = 1
0.00.078.832 I print_info: pooling type     = 0
0.00.078.832 I print_info: rope type        = 2
0.00.078.833 I print_info: rope scaling     = linear
0.00.078.833 I print_info: freq_base_train  = 10000.0
0.00.078.833 I print_info: freq_scale_train = 1
0.00.078.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.834 I print_info: rope_finetuned   = unknown
0.00.078.836 I print_info: ssm_d_conv       = 0
0.00.078.836 I print_info: ssm_d_inner      = 0
0.00.078.836 I print_info: ssm_d_state      = 0
0.00.078.837 I print_info: ssm_dt_rank      = 0
0.00.078.837 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.837 I print_info: model type       = 1.4B
0.00.078.837 I print_info: model params     = 1.41 B
0.00.078.838 I print_info: general.name     = 1.4B
0.00.078.838 I print_info: vocab type       = BPE
0.00.078.838 I print_info: n_vocab          = 50304
0.00.078.839 I print_info: n_merges         = 50009
0.00.078.839 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.839 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.839 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.839 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.840 I print_info: LF token         = 187 'Ċ'
0.00.078.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.840 I print_info: max token length = 1024
0.00.078.841 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.430.110 I load_tensors: offloading 24 repeating layers to GPU
0.01.430.114 I load_tensors: offloading output layer to GPU
0.01.430.115 I load_tensors: offloaded 25/25 layers to GPU
0.01.430.136 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.430.138 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.430.910 I llama_context: n_seq_max     = 1
0.01.430.911 I llama_context: n_ctx         = 128
0.01.430.911 I llama_context: n_ctx_per_seq = 128
0.01.430.912 I llama_context: n_batch       = 128
0.01.430.913 I llama_context: n_ubatch      = 128
0.01.430.913 I llama_context: flash_attn    = 0
0.01.430.913 I llama_context: freq_base     = 10000.0
0.01.430.914 I llama_context: freq_scale    = 1
0.01.430.914 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.430.915 I ggml_metal_init: allocating
0.01.430.957 I ggml_metal_init: found device: Apple M4
0.01.430.964 I ggml_metal_init: picking default device: Apple M4
0.01.432.083 I ggml_metal_init: using embedded metal library
0.01.435.783 I ggml_metal_init: GPU name:   Apple M4
0.01.435.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.435.786 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.435.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.435.786 I ggml_metal_init: simdgroup reduction   = true
0.01.435.787 I ggml_metal_init: simdgroup matrix mul. = true
0.01.435.787 I ggml_metal_init: has residency sets    = true
0.01.435.787 I ggml_metal_init: has bfloat            = true
0.01.435.787 I ggml_metal_init: use bfloat            = true
0.01.435.787 I ggml_metal_init: hasUnifiedMemory      = true
0.01.435.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.446.148 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.446.150 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.447.870 I init:      Metal KV buffer size =    24.00 MiB
0.01.447.872 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.449.540 I init:      Metal compute buffer size =    25.56 MiB
0.01.449.541 I init:        CPU compute buffer size =     1.06 MiB
0.01.449.542 I init: graph nodes  = 967
0.01.449.542 I init: graph splits = 2
0.01.449.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.449.544 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.484.746 I 
0.01.484.771 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.484.775 I perplexity: tokenizing the input ..
0.01.489.736 I perplexity: tokenization took 4.959 ms
0.01.489.740 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.608.060 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.609.435 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.471 I llama_perf_context_print:        load time =    1458.73 ms
0.01.609.472 I llama_perf_context_print: prompt eval time =     118.01 ms /   128 tokens (    0.92 ms per token,  1084.63 tokens per second)
0.01.609.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.473 I llama_perf_context_print:       total time =     124.72 ms /   129 tokens
0.01.610.019 I ggml_metal_free: deallocating

real	0m1.802s
user	0m0.098s
sys	0m0.254s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.262 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.409 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.416 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.418 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.420 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.421 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.423 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.424 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.278 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.143 I llama_model_loader: - type  f32:  194 tensors
0.00.025.144 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.145 I print_info: file format = GGUF V3 (latest)
0.00.025.145 I print_info: file type   = Q8_0
0.00.025.146 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.194 I load: special tokens cache size = 25
0.00.039.393 I load: token to piece cache size = 0.2984 MB
0.00.039.410 I print_info: arch             = gptneox
0.00.039.411 I print_info: vocab_only       = 0
0.00.039.411 I print_info: n_ctx_train      = 2048
0.00.039.411 I print_info: n_embd           = 2048
0.00.039.411 I print_info: n_layer          = 24
0.00.039.416 I print_info: n_head           = 16
0.00.039.417 I print_info: n_head_kv        = 16
0.00.039.417 I print_info: n_rot            = 32
0.00.039.417 I print_info: n_swa            = 0
0.00.039.417 I print_info: n_embd_head_k    = 128
0.00.039.417 I print_info: n_embd_head_v    = 128
0.00.039.418 I print_info: n_gqa            = 1
0.00.039.419 I print_info: n_embd_k_gqa     = 2048
0.00.039.419 I print_info: n_embd_v_gqa     = 2048
0.00.039.420 I print_info: f_norm_eps       = 1.0e-05
0.00.039.420 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.421 I print_info: f_logit_scale    = 0.0e+00
0.00.039.421 I print_info: n_ff             = 8192
0.00.039.421 I print_info: n_expert         = 0
0.00.039.421 I print_info: n_expert_used    = 0
0.00.039.421 I print_info: causal attn      = 1
0.00.039.422 I print_info: pooling type     = 0
0.00.039.422 I print_info: rope type        = 2
0.00.039.422 I print_info: rope scaling     = linear
0.00.039.422 I print_info: freq_base_train  = 10000.0
0.00.039.423 I print_info: freq_scale_train = 1
0.00.039.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.423 I print_info: rope_finetuned   = unknown
0.00.039.423 I print_info: ssm_d_conv       = 0
0.00.039.423 I print_info: ssm_d_inner      = 0
0.00.039.423 I print_info: ssm_d_state      = 0
0.00.039.424 I print_info: ssm_dt_rank      = 0
0.00.039.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.424 I print_info: model type       = 1.4B
0.00.039.424 I print_info: model params     = 1.41 B
0.00.039.425 I print_info: general.name     = 1.4B
0.00.039.428 I print_info: vocab type       = BPE
0.00.039.428 I print_info: n_vocab          = 50304
0.00.039.428 I print_info: n_merges         = 50009
0.00.039.429 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: LF token         = 187 'Ċ'
0.00.039.430 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: max token length = 1024
0.00.039.430 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.870.150 I load_tensors: offloading 24 repeating layers to GPU
0.00.870.157 I load_tensors: offloading output layer to GPU
0.00.870.158 I load_tensors: offloaded 25/25 layers to GPU
0.00.870.188 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.870.190 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.871.671 I llama_context: n_seq_max     = 1
0.00.871.674 I llama_context: n_ctx         = 128
0.00.871.674 I llama_context: n_ctx_per_seq = 128
0.00.871.675 I llama_context: n_batch       = 128
0.00.871.675 I llama_context: n_ubatch      = 128
0.00.871.675 I llama_context: flash_attn    = 0
0.00.871.676 I llama_context: freq_base     = 10000.0
0.00.871.677 I llama_context: freq_scale    = 1
0.00.871.677 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.871.679 I ggml_metal_init: allocating
0.00.871.773 I ggml_metal_init: found device: Apple M4
0.00.871.784 I ggml_metal_init: picking default device: Apple M4
0.00.873.210 I ggml_metal_init: using embedded metal library
0.00.878.430 I ggml_metal_init: GPU name:   Apple M4
0.00.878.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.878.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.878.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.878.435 I ggml_metal_init: simdgroup reduction   = true
0.00.878.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.878.436 I ggml_metal_init: has residency sets    = true
0.00.878.436 I ggml_metal_init: has bfloat            = true
0.00.878.436 I ggml_metal_init: use bfloat            = true
0.00.878.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.878.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.893.588 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.893.591 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.896.910 I init:      Metal KV buffer size =    24.00 MiB
0.00.896.913 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.899.864 I init:      Metal compute buffer size =    25.56 MiB
0.00.899.865 I init:        CPU compute buffer size =     1.06 MiB
0.00.899.866 I init: graph nodes  = 967
0.00.899.866 I init: graph splits = 2
0.00.899.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.899.872 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.926.974 I 
0.00.927.025 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.927.032 I perplexity: tokenizing the input ..
0.00.933.686 I perplexity: tokenization took 6.652 ms
0.00.933.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.072.786 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.074.285 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.074.308 I llama_perf_context_print:        load time =     917.70 ms
0.01.074.309 I llama_perf_context_print: prompt eval time =     138.16 ms /   128 tokens (    1.08 ms per token,   926.46 tokens per second)
0.01.074.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.074.310 I llama_perf_context_print:       total time =     147.34 ms /   129 tokens
0.01.074.831 I ggml_metal_free: deallocating

real	0m1.090s
user	0m0.075s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.209 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.525 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.525 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.531 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.531 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.510 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.510 I llama_model_loader: - type  f32:  194 tensors
0.00.026.511 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.511 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.517 I print_info: file format = GGUF V3 (latest)
0.00.026.517 I print_info: file type   = Q4_0
0.00.026.518 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.340 I load: special tokens cache size = 25
0.00.040.450 I load: token to piece cache size = 0.2984 MB
0.00.040.467 I print_info: arch             = gptneox
0.00.040.468 I print_info: vocab_only       = 0
0.00.040.468 I print_info: n_ctx_train      = 2048
0.00.040.469 I print_info: n_embd           = 2048
0.00.040.469 I print_info: n_layer          = 24
0.00.040.472 I print_info: n_head           = 16
0.00.040.473 I print_info: n_head_kv        = 16
0.00.040.473 I print_info: n_rot            = 32
0.00.040.473 I print_info: n_swa            = 0
0.00.040.474 I print_info: n_embd_head_k    = 128
0.00.040.474 I print_info: n_embd_head_v    = 128
0.00.040.474 I print_info: n_gqa            = 1
0.00.040.475 I print_info: n_embd_k_gqa     = 2048
0.00.040.476 I print_info: n_embd_v_gqa     = 2048
0.00.040.479 I print_info: f_norm_eps       = 1.0e-05
0.00.040.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.480 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.480 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.480 I print_info: f_logit_scale    = 0.0e+00
0.00.040.481 I print_info: n_ff             = 8192
0.00.040.481 I print_info: n_expert         = 0
0.00.040.481 I print_info: n_expert_used    = 0
0.00.040.481 I print_info: causal attn      = 1
0.00.040.481 I print_info: pooling type     = 0
0.00.040.481 I print_info: rope type        = 2
0.00.040.482 I print_info: rope scaling     = linear
0.00.040.482 I print_info: freq_base_train  = 10000.0
0.00.040.482 I print_info: freq_scale_train = 1
0.00.040.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.482 I print_info: rope_finetuned   = unknown
0.00.040.483 I print_info: ssm_d_conv       = 0
0.00.040.483 I print_info: ssm_d_inner      = 0
0.00.040.483 I print_info: ssm_d_state      = 0
0.00.040.483 I print_info: ssm_dt_rank      = 0
0.00.040.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.483 I print_info: model type       = 1.4B
0.00.040.483 I print_info: model params     = 1.41 B
0.00.040.484 I print_info: general.name     = 1.4B
0.00.040.484 I print_info: vocab type       = BPE
0.00.040.484 I print_info: n_vocab          = 50304
0.00.040.484 I print_info: n_merges         = 50009
0.00.040.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.485 I print_info: LF token         = 187 'Ċ'
0.00.040.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.486 I print_info: max token length = 1024
0.00.040.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.554.143 I load_tensors: offloading 24 repeating layers to GPU
0.00.554.157 I load_tensors: offloading output layer to GPU
0.00.554.157 I load_tensors: offloaded 25/25 layers to GPU
0.00.554.201 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.554.203 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.555.419 I llama_context: n_seq_max     = 1
0.00.555.421 I llama_context: n_ctx         = 128
0.00.555.422 I llama_context: n_ctx_per_seq = 128
0.00.555.422 I llama_context: n_batch       = 128
0.00.555.423 I llama_context: n_ubatch      = 128
0.00.555.423 I llama_context: flash_attn    = 0
0.00.555.426 I llama_context: freq_base     = 10000.0
0.00.555.426 I llama_context: freq_scale    = 1
0.00.555.427 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.555.429 I ggml_metal_init: allocating
0.00.555.553 I ggml_metal_init: found device: Apple M4
0.00.555.567 I ggml_metal_init: picking default device: Apple M4
0.00.557.370 I ggml_metal_init: using embedded metal library
0.00.563.206 I ggml_metal_init: GPU name:   Apple M4
0.00.563.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.563.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.563.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.563.216 I ggml_metal_init: simdgroup reduction   = true
0.00.563.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.563.217 I ggml_metal_init: has residency sets    = true
0.00.563.217 I ggml_metal_init: has bfloat            = true
0.00.563.218 I ggml_metal_init: use bfloat            = true
0.00.563.219 I ggml_metal_init: hasUnifiedMemory      = true
0.00.563.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.582.666 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.582.671 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.586.323 I init:      Metal KV buffer size =    24.00 MiB
0.00.586.327 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.589.478 I init:      Metal compute buffer size =    25.56 MiB
0.00.589.480 I init:        CPU compute buffer size =     1.06 MiB
0.00.589.480 I init: graph nodes  = 967
0.00.589.481 I init: graph splits = 2
0.00.589.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.589.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.436 I 
0.00.619.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.506 I perplexity: tokenizing the input ..
0.00.626.888 I perplexity: tokenization took 7.381 ms
0.00.626.897 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.189 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.764.539 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.764.567 I llama_perf_context_print:        load time =     609.22 ms
0.00.764.568 I llama_perf_context_print: prompt eval time =     135.37 ms /   128 tokens (    1.06 ms per token,   945.54 tokens per second)
0.00.764.569 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.569 I llama_perf_context_print:       total time =     145.13 ms /   129 tokens
0.00.765.106 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.152 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.159 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.764 I llama_model_loader: - type  f32:  194 tensors
0.00.024.764 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.764 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.765 I print_info: file format = GGUF V3 (latest)
0.00.024.770 I print_info: file type   = Q4_1
0.00.024.771 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.174 I load: special tokens cache size = 25
0.00.039.451 I load: token to piece cache size = 0.2984 MB
0.00.039.468 I print_info: arch             = gptneox
0.00.039.468 I print_info: vocab_only       = 0
0.00.039.469 I print_info: n_ctx_train      = 2048
0.00.039.469 I print_info: n_embd           = 2048
0.00.039.469 I print_info: n_layer          = 24
0.00.039.472 I print_info: n_head           = 16
0.00.039.473 I print_info: n_head_kv        = 16
0.00.039.473 I print_info: n_rot            = 32
0.00.039.473 I print_info: n_swa            = 0
0.00.039.476 I print_info: n_embd_head_k    = 128
0.00.039.476 I print_info: n_embd_head_v    = 128
0.00.039.477 I print_info: n_gqa            = 1
0.00.039.477 I print_info: n_embd_k_gqa     = 2048
0.00.039.478 I print_info: n_embd_v_gqa     = 2048
0.00.039.478 I print_info: f_norm_eps       = 1.0e-05
0.00.039.479 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.479 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.479 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.480 I print_info: f_logit_scale    = 0.0e+00
0.00.039.481 I print_info: n_ff             = 8192
0.00.039.481 I print_info: n_expert         = 0
0.00.039.481 I print_info: n_expert_used    = 0
0.00.039.481 I print_info: causal attn      = 1
0.00.039.481 I print_info: pooling type     = 0
0.00.039.481 I print_info: rope type        = 2
0.00.039.482 I print_info: rope scaling     = linear
0.00.039.482 I print_info: freq_base_train  = 10000.0
0.00.039.482 I print_info: freq_scale_train = 1
0.00.039.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.483 I print_info: rope_finetuned   = unknown
0.00.039.483 I print_info: ssm_d_conv       = 0
0.00.039.483 I print_info: ssm_d_inner      = 0
0.00.039.483 I print_info: ssm_d_state      = 0
0.00.039.483 I print_info: ssm_dt_rank      = 0
0.00.039.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.484 I print_info: vocab type       = BPE
0.00.039.485 I print_info: n_vocab          = 50304
0.00.039.485 I print_info: n_merges         = 50009
0.00.039.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: LF token         = 187 'Ċ'
0.00.039.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: max token length = 1024
0.00.039.519 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.618.973 I load_tensors: offloading 24 repeating layers to GPU
0.00.618.989 I load_tensors: offloading output layer to GPU
0.00.618.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.024 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.619.026 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.620.734 I llama_context: n_seq_max     = 1
0.00.620.737 I llama_context: n_ctx         = 128
0.00.620.738 I llama_context: n_ctx_per_seq = 128
0.00.620.738 I llama_context: n_batch       = 128
0.00.620.739 I llama_context: n_ubatch      = 128
0.00.620.739 I llama_context: flash_attn    = 0
0.00.620.741 I llama_context: freq_base     = 10000.0
0.00.620.742 I llama_context: freq_scale    = 1
0.00.620.742 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.746 I ggml_metal_init: allocating
0.00.620.825 I ggml_metal_init: found device: Apple M4
0.00.620.840 I ggml_metal_init: picking default device: Apple M4
0.00.622.658 I ggml_metal_init: using embedded metal library
0.00.629.456 I ggml_metal_init: GPU name:   Apple M4
0.00.629.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.465 I ggml_metal_init: simdgroup reduction   = true
0.00.629.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.465 I ggml_metal_init: has residency sets    = true
0.00.629.466 I ggml_metal_init: has bfloat            = true
0.00.629.466 I ggml_metal_init: use bfloat            = true
0.00.629.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.633 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.647.638 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.133 I init:      Metal KV buffer size =    24.00 MiB
0.00.651.139 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.393 I init:      Metal compute buffer size =    25.56 MiB
0.00.654.395 I init:        CPU compute buffer size =     1.06 MiB
0.00.654.395 I init: graph nodes  = 967
0.00.654.396 I init: graph splits = 2
0.00.654.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.132 I 
0.00.679.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.194 I perplexity: tokenizing the input ..
0.00.687.074 I perplexity: tokenization took 7.877 ms
0.00.687.089 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.312 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.824.645 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.824.665 I llama_perf_context_print:        load time =     670.22 ms
0.00.824.666 I llama_perf_context_print: prompt eval time =     135.31 ms /   128 tokens (    1.06 ms per token,   945.99 tokens per second)
0.00.824.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.667 I llama_perf_context_print:       total time =     145.54 ms /   129 tokens
0.00.825.207 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.081s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.645 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.533 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.533 I llama_model_loader: - type  f32:  194 tensors
0.00.024.534 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.534 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.535 I print_info: file format = GGUF V3 (latest)
0.00.024.535 I print_info: file type   = Q5_0
0.00.024.538 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.877 I load: special tokens cache size = 25
0.00.038.837 I load: token to piece cache size = 0.2984 MB
0.00.038.854 I print_info: arch             = gptneox
0.00.038.855 I print_info: vocab_only       = 0
0.00.038.855 I print_info: n_ctx_train      = 2048
0.00.038.855 I print_info: n_embd           = 2048
0.00.038.855 I print_info: n_layer          = 24
0.00.038.860 I print_info: n_head           = 16
0.00.038.860 I print_info: n_head_kv        = 16
0.00.038.861 I print_info: n_rot            = 32
0.00.038.861 I print_info: n_swa            = 0
0.00.038.861 I print_info: n_embd_head_k    = 128
0.00.038.861 I print_info: n_embd_head_v    = 128
0.00.038.862 I print_info: n_gqa            = 1
0.00.038.862 I print_info: n_embd_k_gqa     = 2048
0.00.038.864 I print_info: n_embd_v_gqa     = 2048
0.00.038.865 I print_info: f_norm_eps       = 1.0e-05
0.00.038.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.866 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.866 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.868 I print_info: f_logit_scale    = 0.0e+00
0.00.038.868 I print_info: n_ff             = 8192
0.00.038.868 I print_info: n_expert         = 0
0.00.038.869 I print_info: n_expert_used    = 0
0.00.038.869 I print_info: causal attn      = 1
0.00.038.869 I print_info: pooling type     = 0
0.00.038.869 I print_info: rope type        = 2
0.00.038.869 I print_info: rope scaling     = linear
0.00.038.869 I print_info: freq_base_train  = 10000.0
0.00.038.870 I print_info: freq_scale_train = 1
0.00.038.870 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.870 I print_info: rope_finetuned   = unknown
0.00.038.870 I print_info: ssm_d_conv       = 0
0.00.038.870 I print_info: ssm_d_inner      = 0
0.00.038.870 I print_info: ssm_d_state      = 0
0.00.038.871 I print_info: ssm_dt_rank      = 0
0.00.038.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.871 I print_info: model type       = 1.4B
0.00.038.871 I print_info: model params     = 1.41 B
0.00.038.871 I print_info: general.name     = 1.4B
0.00.038.872 I print_info: vocab type       = BPE
0.00.038.872 I print_info: n_vocab          = 50304
0.00.038.872 I print_info: n_merges         = 50009
0.00.038.891 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.892 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.892 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.893 I print_info: LF token         = 187 'Ċ'
0.00.038.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.893 I print_info: max token length = 1024
0.00.038.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.706.406 I load_tensors: offloading 24 repeating layers to GPU
0.00.706.416 I load_tensors: offloading output layer to GPU
0.00.706.417 I load_tensors: offloaded 25/25 layers to GPU
0.00.706.451 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.706.452 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.708.143 I llama_context: n_seq_max     = 1
0.00.708.149 I llama_context: n_ctx         = 128
0.00.708.149 I llama_context: n_ctx_per_seq = 128
0.00.708.149 I llama_context: n_batch       = 128
0.00.708.150 I llama_context: n_ubatch      = 128
0.00.708.150 I llama_context: flash_attn    = 0
0.00.708.152 I llama_context: freq_base     = 10000.0
0.00.708.152 I llama_context: freq_scale    = 1
0.00.708.153 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.708.155 I ggml_metal_init: allocating
0.00.708.216 I ggml_metal_init: found device: Apple M4
0.00.708.232 I ggml_metal_init: picking default device: Apple M4
0.00.710.394 I ggml_metal_init: using embedded metal library
0.00.717.101 I ggml_metal_init: GPU name:   Apple M4
0.00.717.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.111 I ggml_metal_init: simdgroup reduction   = true
0.00.717.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.112 I ggml_metal_init: has residency sets    = true
0.00.717.112 I ggml_metal_init: has bfloat            = true
0.00.717.112 I ggml_metal_init: use bfloat            = true
0.00.717.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.985 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.735.990 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.642 I init:      Metal KV buffer size =    24.00 MiB
0.00.739.646 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.743.021 I init:      Metal compute buffer size =    25.56 MiB
0.00.743.023 I init:        CPU compute buffer size =     1.06 MiB
0.00.743.024 I init: graph nodes  = 967
0.00.743.025 I init: graph splits = 2
0.00.743.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.743.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.845 I 
0.00.773.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.922 I perplexity: tokenizing the input ..
0.00.780.978 I perplexity: tokenization took 7.054 ms
0.00.780.988 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.917.065 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.918.582 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.918.612 I llama_perf_context_print:        load time =     764.90 ms
0.00.918.612 I llama_perf_context_print: prompt eval time =     135.16 ms /   128 tokens (    1.06 ms per token,   947.02 tokens per second)
0.00.918.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.918.614 I llama_perf_context_print:       total time =     144.77 ms /   129 tokens
0.00.919.192 I ggml_metal_free: deallocating

real	0m0.934s
user	0m0.079s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.078 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.110 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.111 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.111 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.113 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.113 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.114 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.114 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.838 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.839 I llama_model_loader: - type  f32:  194 tensors
0.00.025.839 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.839 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.840 I print_info: file format = GGUF V3 (latest)
0.00.025.841 I print_info: file type   = Q5_1
0.00.025.842 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.790 I load: special tokens cache size = 25
0.00.039.762 I load: token to piece cache size = 0.2984 MB
0.00.039.779 I print_info: arch             = gptneox
0.00.039.780 I print_info: vocab_only       = 0
0.00.039.780 I print_info: n_ctx_train      = 2048
0.00.039.780 I print_info: n_embd           = 2048
0.00.039.780 I print_info: n_layer          = 24
0.00.039.784 I print_info: n_head           = 16
0.00.039.785 I print_info: n_head_kv        = 16
0.00.039.785 I print_info: n_rot            = 32
0.00.039.785 I print_info: n_swa            = 0
0.00.039.786 I print_info: n_embd_head_k    = 128
0.00.039.786 I print_info: n_embd_head_v    = 128
0.00.039.786 I print_info: n_gqa            = 1
0.00.039.787 I print_info: n_embd_k_gqa     = 2048
0.00.039.788 I print_info: n_embd_v_gqa     = 2048
0.00.039.788 I print_info: f_norm_eps       = 1.0e-05
0.00.039.789 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.789 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.789 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.789 I print_info: f_logit_scale    = 0.0e+00
0.00.039.790 I print_info: n_ff             = 8192
0.00.039.790 I print_info: n_expert         = 0
0.00.039.790 I print_info: n_expert_used    = 0
0.00.039.790 I print_info: causal attn      = 1
0.00.039.790 I print_info: pooling type     = 0
0.00.039.790 I print_info: rope type        = 2
0.00.039.791 I print_info: rope scaling     = linear
0.00.039.791 I print_info: freq_base_train  = 10000.0
0.00.039.791 I print_info: freq_scale_train = 1
0.00.039.791 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.792 I print_info: rope_finetuned   = unknown
0.00.039.793 I print_info: ssm_d_conv       = 0
0.00.039.793 I print_info: ssm_d_inner      = 0
0.00.039.793 I print_info: ssm_d_state      = 0
0.00.039.793 I print_info: ssm_dt_rank      = 0
0.00.039.795 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.796 I print_info: model type       = 1.4B
0.00.039.796 I print_info: model params     = 1.41 B
0.00.039.796 I print_info: general.name     = 1.4B
0.00.039.796 I print_info: vocab type       = BPE
0.00.039.797 I print_info: n_vocab          = 50304
0.00.039.797 I print_info: n_merges         = 50009
0.00.039.797 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.798 I print_info: LF token         = 187 'Ċ'
0.00.039.798 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.798 I print_info: max token length = 1024
0.00.039.799 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.132 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.147 I load_tensors: offloading output layer to GPU
0.00.631.147 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.181 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.631.187 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.632.833 I llama_context: n_seq_max     = 1
0.00.632.836 I llama_context: n_ctx         = 128
0.00.632.837 I llama_context: n_ctx_per_seq = 128
0.00.632.837 I llama_context: n_batch       = 128
0.00.632.838 I llama_context: n_ubatch      = 128
0.00.632.838 I llama_context: flash_attn    = 0
0.00.632.841 I llama_context: freq_base     = 10000.0
0.00.632.841 I llama_context: freq_scale    = 1
0.00.632.842 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.847 I ggml_metal_init: allocating
0.00.632.923 I ggml_metal_init: found device: Apple M4
0.00.632.936 I ggml_metal_init: picking default device: Apple M4
0.00.634.817 I ggml_metal_init: using embedded metal library
0.00.641.375 I ggml_metal_init: GPU name:   Apple M4
0.00.641.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.380 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.381 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.382 I ggml_metal_init: simdgroup reduction   = true
0.00.641.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.382 I ggml_metal_init: has residency sets    = true
0.00.641.382 I ggml_metal_init: has bfloat            = true
0.00.641.383 I ggml_metal_init: use bfloat            = true
0.00.641.383 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.509 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.658.514 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.908 I init:      Metal KV buffer size =    24.00 MiB
0.00.661.911 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.083 I init:      Metal compute buffer size =    25.56 MiB
0.00.665.085 I init:        CPU compute buffer size =     1.06 MiB
0.00.665.085 I init: graph nodes  = 967
0.00.665.085 I init: graph splits = 2
0.00.665.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.189 I 
0.00.691.249 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.257 I perplexity: tokenizing the input ..
0.00.698.628 I perplexity: tokenization took 7.369 ms
0.00.698.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.399 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.835.735 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.835.761 I llama_perf_context_print:        load time =     681.10 ms
0.00.835.764 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.40 tokens per second)
0.00.835.765 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.765 I llama_perf_context_print:       total time =     144.58 ms /   129 tokens
0.00.836.347 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.514 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.516 I llama_model_loader: - type  f32:  194 tensors
0.00.024.517 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.517 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.518 I print_info: file format = GGUF V3 (latest)
0.00.024.518 I print_info: file type   = Q2_K - Medium
0.00.024.519 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.533 I load: special tokens cache size = 25
0.00.038.654 I load: token to piece cache size = 0.2984 MB
0.00.038.671 I print_info: arch             = gptneox
0.00.038.672 I print_info: vocab_only       = 0
0.00.038.672 I print_info: n_ctx_train      = 2048
0.00.038.673 I print_info: n_embd           = 2048
0.00.038.673 I print_info: n_layer          = 24
0.00.038.677 I print_info: n_head           = 16
0.00.038.677 I print_info: n_head_kv        = 16
0.00.038.677 I print_info: n_rot            = 32
0.00.038.678 I print_info: n_swa            = 0
0.00.038.678 I print_info: n_embd_head_k    = 128
0.00.038.678 I print_info: n_embd_head_v    = 128
0.00.038.678 I print_info: n_gqa            = 1
0.00.038.679 I print_info: n_embd_k_gqa     = 2048
0.00.038.680 I print_info: n_embd_v_gqa     = 2048
0.00.038.680 I print_info: f_norm_eps       = 1.0e-05
0.00.038.681 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.681 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.681 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.685 I print_info: f_logit_scale    = 0.0e+00
0.00.038.685 I print_info: n_ff             = 8192
0.00.038.685 I print_info: n_expert         = 0
0.00.038.685 I print_info: n_expert_used    = 0
0.00.038.686 I print_info: causal attn      = 1
0.00.038.686 I print_info: pooling type     = 0
0.00.038.686 I print_info: rope type        = 2
0.00.038.686 I print_info: rope scaling     = linear
0.00.038.686 I print_info: freq_base_train  = 10000.0
0.00.038.687 I print_info: freq_scale_train = 1
0.00.038.687 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.687 I print_info: rope_finetuned   = unknown
0.00.038.688 I print_info: ssm_d_conv       = 0
0.00.038.688 I print_info: ssm_d_inner      = 0
0.00.038.688 I print_info: ssm_d_state      = 0
0.00.038.688 I print_info: ssm_dt_rank      = 0
0.00.038.688 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.689 I print_info: model type       = 1.4B
0.00.038.689 I print_info: model params     = 1.41 B
0.00.038.689 I print_info: general.name     = 1.4B
0.00.038.691 I print_info: vocab type       = BPE
0.00.038.691 I print_info: n_vocab          = 50304
0.00.038.691 I print_info: n_merges         = 50009
0.00.038.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.692 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.692 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.692 I print_info: LF token         = 187 'Ċ'
0.00.038.692 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.693 I print_info: max token length = 1024
0.00.038.693 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.346.409 I load_tensors: offloading 24 repeating layers to GPU
0.00.346.426 I load_tensors: offloading output layer to GPU
0.00.346.427 I load_tensors: offloaded 25/25 layers to GPU
0.00.346.459 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.346.460 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.348.090 I llama_context: n_seq_max     = 1
0.00.348.098 I llama_context: n_ctx         = 128
0.00.348.098 I llama_context: n_ctx_per_seq = 128
0.00.348.099 I llama_context: n_batch       = 128
0.00.348.099 I llama_context: n_ubatch      = 128
0.00.348.099 I llama_context: flash_attn    = 0
0.00.348.101 I llama_context: freq_base     = 10000.0
0.00.348.102 I llama_context: freq_scale    = 1
0.00.348.102 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.348.104 I ggml_metal_init: allocating
0.00.348.194 I ggml_metal_init: found device: Apple M4
0.00.348.208 I ggml_metal_init: picking default device: Apple M4
0.00.350.071 I ggml_metal_init: using embedded metal library
0.00.355.398 I ggml_metal_init: GPU name:   Apple M4
0.00.355.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.412 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.413 I ggml_metal_init: simdgroup reduction   = true
0.00.355.413 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.413 I ggml_metal_init: has residency sets    = true
0.00.355.414 I ggml_metal_init: has bfloat            = true
0.00.355.414 I ggml_metal_init: use bfloat            = true
0.00.355.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.576 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.377.581 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.381.310 I init:      Metal KV buffer size =    24.00 MiB
0.00.381.317 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.384.650 I init:      Metal compute buffer size =    25.56 MiB
0.00.384.652 I init:        CPU compute buffer size =     1.06 MiB
0.00.384.652 I init: graph nodes  = 967
0.00.384.653 I init: graph splits = 2
0.00.384.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.384.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.541 I 
0.00.416.609 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.618 I perplexity: tokenizing the input ..
0.00.422.666 I perplexity: tokenization took 6.046 ms
0.00.422.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.557.568 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.558.915 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.558.938 I llama_perf_context_print:        load time =     407.57 ms
0.00.558.939 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.49 tokens per second)
0.00.558.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.558.940 I llama_perf_context_print:       total time =     142.40 ms /   129 tokens
0.00.559.489 I ggml_metal_free: deallocating

real	0m0.574s
user	0m0.080s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.803 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.816 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.825 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.825 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.498 I llama_model_loader: - type  f32:  194 tensors
0.00.024.499 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.499 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.499 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.499 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.500 I print_info: file format = GGUF V3 (latest)
0.00.024.503 I print_info: file type   = Q3_K - Medium
0.00.024.504 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.372 I load: special tokens cache size = 25
0.00.038.236 I load: token to piece cache size = 0.2984 MB
0.00.038.252 I print_info: arch             = gptneox
0.00.038.253 I print_info: vocab_only       = 0
0.00.038.254 I print_info: n_ctx_train      = 2048
0.00.038.254 I print_info: n_embd           = 2048
0.00.038.254 I print_info: n_layer          = 24
0.00.038.259 I print_info: n_head           = 16
0.00.038.260 I print_info: n_head_kv        = 16
0.00.038.260 I print_info: n_rot            = 32
0.00.038.260 I print_info: n_swa            = 0
0.00.038.260 I print_info: n_embd_head_k    = 128
0.00.038.260 I print_info: n_embd_head_v    = 128
0.00.038.261 I print_info: n_gqa            = 1
0.00.038.262 I print_info: n_embd_k_gqa     = 2048
0.00.038.262 I print_info: n_embd_v_gqa     = 2048
0.00.038.265 I print_info: f_norm_eps       = 1.0e-05
0.00.038.265 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.267 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.267 I print_info: f_logit_scale    = 0.0e+00
0.00.038.268 I print_info: n_ff             = 8192
0.00.038.268 I print_info: n_expert         = 0
0.00.038.268 I print_info: n_expert_used    = 0
0.00.038.268 I print_info: causal attn      = 1
0.00.038.268 I print_info: pooling type     = 0
0.00.038.268 I print_info: rope type        = 2
0.00.038.269 I print_info: rope scaling     = linear
0.00.038.270 I print_info: freq_base_train  = 10000.0
0.00.038.270 I print_info: freq_scale_train = 1
0.00.038.270 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.270 I print_info: rope_finetuned   = unknown
0.00.038.273 I print_info: ssm_d_conv       = 0
0.00.038.273 I print_info: ssm_d_inner      = 0
0.00.038.274 I print_info: ssm_d_state      = 0
0.00.038.275 I print_info: ssm_dt_rank      = 0
0.00.038.275 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.275 I print_info: model type       = 1.4B
0.00.038.276 I print_info: model params     = 1.41 B
0.00.038.276 I print_info: general.name     = 1.4B
0.00.038.276 I print_info: vocab type       = BPE
0.00.038.276 I print_info: n_vocab          = 50304
0.00.038.277 I print_info: n_merges         = 50009
0.00.038.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.278 I print_info: LF token         = 187 'Ċ'
0.00.038.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.278 I print_info: max token length = 1024
0.00.038.279 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.805 I load_tensors: offloading output layer to GPU
0.00.440.850 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.890 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.892 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.606 I llama_context: n_seq_max     = 1
0.00.442.609 I llama_context: n_ctx         = 128
0.00.442.610 I llama_context: n_ctx_per_seq = 128
0.00.442.611 I llama_context: n_batch       = 128
0.00.442.611 I llama_context: n_ubatch      = 128
0.00.442.612 I llama_context: flash_attn    = 0
0.00.442.614 I llama_context: freq_base     = 10000.0
0.00.442.614 I llama_context: freq_scale    = 1
0.00.442.615 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.617 I ggml_metal_init: allocating
0.00.442.677 I ggml_metal_init: found device: Apple M4
0.00.442.695 I ggml_metal_init: picking default device: Apple M4
0.00.444.468 I ggml_metal_init: using embedded metal library
0.00.450.164 I ggml_metal_init: GPU name:   Apple M4
0.00.450.175 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.177 I ggml_metal_init: simdgroup reduction   = true
0.00.450.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.178 I ggml_metal_init: has residency sets    = true
0.00.450.178 I ggml_metal_init: has bfloat            = true
0.00.450.178 I ggml_metal_init: use bfloat            = true
0.00.450.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.694 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.470.698 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.259 I init:      Metal KV buffer size =    24.00 MiB
0.00.474.265 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.477.497 I init:      Metal compute buffer size =    25.56 MiB
0.00.477.499 I init:        CPU compute buffer size =     1.06 MiB
0.00.477.499 I init: graph nodes  = 967
0.00.477.500 I init: graph splits = 2
0.00.477.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.477.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.790 I 
0.00.508.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.866 I perplexity: tokenizing the input ..
0.00.516.366 I perplexity: tokenization took 7.497 ms
0.00.516.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.662.139 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.663.543 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.663.566 I llama_perf_context_print:        load time =     499.98 ms
0.00.663.567 I llama_perf_context_print: prompt eval time =     145.01 ms /   128 tokens (    1.13 ms per token,   882.68 tokens per second)
0.00.663.567 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.663.568 I llama_perf_context_print:       total time =     154.78 ms /   129 tokens
0.00.664.098 I ggml_metal_free: deallocating

real	0m0.678s
user	0m0.080s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.425 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.428 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.429 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.430 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.437 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.934 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.935 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.936 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.936 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.937 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.938 I llama_model_loader: - type  f32:  194 tensors
0.00.024.938 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.938 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.938 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.939 I print_info: file format = GGUF V3 (latest)
0.00.024.940 I print_info: file type   = Q4_K - Medium
0.00.024.941 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.913 I load: special tokens cache size = 25
0.00.038.991 I load: token to piece cache size = 0.2984 MB
0.00.039.008 I print_info: arch             = gptneox
0.00.039.008 I print_info: vocab_only       = 0
0.00.039.009 I print_info: n_ctx_train      = 2048
0.00.039.009 I print_info: n_embd           = 2048
0.00.039.009 I print_info: n_layer          = 24
0.00.039.012 I print_info: n_head           = 16
0.00.039.013 I print_info: n_head_kv        = 16
0.00.039.013 I print_info: n_rot            = 32
0.00.039.013 I print_info: n_swa            = 0
0.00.039.013 I print_info: n_embd_head_k    = 128
0.00.039.013 I print_info: n_embd_head_v    = 128
0.00.039.016 I print_info: n_gqa            = 1
0.00.039.017 I print_info: n_embd_k_gqa     = 2048
0.00.039.017 I print_info: n_embd_v_gqa     = 2048
0.00.039.020 I print_info: f_norm_eps       = 1.0e-05
0.00.039.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.020 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.020 I print_info: f_logit_scale    = 0.0e+00
0.00.039.021 I print_info: n_ff             = 8192
0.00.039.022 I print_info: n_expert         = 0
0.00.039.023 I print_info: n_expert_used    = 0
0.00.039.023 I print_info: causal attn      = 1
0.00.039.023 I print_info: pooling type     = 0
0.00.039.023 I print_info: rope type        = 2
0.00.039.024 I print_info: rope scaling     = linear
0.00.039.024 I print_info: freq_base_train  = 10000.0
0.00.039.024 I print_info: freq_scale_train = 1
0.00.039.024 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.024 I print_info: rope_finetuned   = unknown
0.00.039.025 I print_info: ssm_d_conv       = 0
0.00.039.025 I print_info: ssm_d_inner      = 0
0.00.039.025 I print_info: ssm_d_state      = 0
0.00.039.025 I print_info: ssm_dt_rank      = 0
0.00.039.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.025 I print_info: model type       = 1.4B
0.00.039.026 I print_info: model params     = 1.41 B
0.00.039.026 I print_info: general.name     = 1.4B
0.00.039.027 I print_info: vocab type       = BPE
0.00.039.027 I print_info: n_vocab          = 50304
0.00.039.027 I print_info: n_merges         = 50009
0.00.039.027 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.027 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: LF token         = 187 'Ċ'
0.00.039.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: max token length = 1024
0.00.039.029 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.134 I load_tensors: offloading output layer to GPU
0.00.515.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.167 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.168 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.516.803 I llama_context: n_seq_max     = 1
0.00.516.806 I llama_context: n_ctx         = 128
0.00.516.806 I llama_context: n_ctx_per_seq = 128
0.00.516.807 I llama_context: n_batch       = 128
0.00.516.807 I llama_context: n_ubatch      = 128
0.00.516.808 I llama_context: flash_attn    = 0
0.00.516.810 I llama_context: freq_base     = 10000.0
0.00.516.811 I llama_context: freq_scale    = 1
0.00.516.811 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.516.814 I ggml_metal_init: allocating
0.00.516.892 I ggml_metal_init: found device: Apple M4
0.00.516.905 I ggml_metal_init: picking default device: Apple M4
0.00.518.817 I ggml_metal_init: using embedded metal library
0.00.525.522 I ggml_metal_init: GPU name:   Apple M4
0.00.525.527 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.529 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.529 I ggml_metal_init: simdgroup reduction   = true
0.00.525.530 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.530 I ggml_metal_init: has residency sets    = true
0.00.525.531 I ggml_metal_init: has bfloat            = true
0.00.525.531 I ggml_metal_init: use bfloat            = true
0.00.525.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.304 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.543.308 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.910 I init:      Metal KV buffer size =    24.00 MiB
0.00.546.914 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.550.025 I init:      Metal compute buffer size =    25.56 MiB
0.00.550.027 I init:        CPU compute buffer size =     1.06 MiB
0.00.550.027 I init: graph nodes  = 967
0.00.550.028 I init: graph splits = 2
0.00.550.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.550.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.579.005 I 
0.00.579.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.579.073 I perplexity: tokenizing the input ..
0.00.586.292 I perplexity: tokenization took 7.215 ms
0.00.586.304 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.025 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.727.360 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.727.384 I llama_perf_context_print:        load time =     569.02 ms
0.00.727.386 I llama_perf_context_print: prompt eval time =     138.80 ms /   128 tokens (    1.08 ms per token,   922.22 tokens per second)
0.00.727.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.387 I llama_perf_context_print:       total time =     148.38 ms /   129 tokens
0.00.727.927 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.798 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.803 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.722 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.625 I llama_model_loader: - type  f32:  194 tensors
0.00.024.625 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.625 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.626 I print_info: file format = GGUF V3 (latest)
0.00.024.626 I print_info: file type   = Q5_K - Medium
0.00.024.628 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.005 I load: special tokens cache size = 25
0.00.039.353 I load: token to piece cache size = 0.2984 MB
0.00.039.370 I print_info: arch             = gptneox
0.00.039.371 I print_info: vocab_only       = 0
0.00.039.371 I print_info: n_ctx_train      = 2048
0.00.039.371 I print_info: n_embd           = 2048
0.00.039.371 I print_info: n_layer          = 24
0.00.039.376 I print_info: n_head           = 16
0.00.039.377 I print_info: n_head_kv        = 16
0.00.039.377 I print_info: n_rot            = 32
0.00.039.377 I print_info: n_swa            = 0
0.00.039.377 I print_info: n_embd_head_k    = 128
0.00.039.377 I print_info: n_embd_head_v    = 128
0.00.039.378 I print_info: n_gqa            = 1
0.00.039.378 I print_info: n_embd_k_gqa     = 2048
0.00.039.379 I print_info: n_embd_v_gqa     = 2048
0.00.039.379 I print_info: f_norm_eps       = 1.0e-05
0.00.039.383 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.383 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.383 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.383 I print_info: f_logit_scale    = 0.0e+00
0.00.039.384 I print_info: n_ff             = 8192
0.00.039.384 I print_info: n_expert         = 0
0.00.039.384 I print_info: n_expert_used    = 0
0.00.039.385 I print_info: causal attn      = 1
0.00.039.385 I print_info: pooling type     = 0
0.00.039.385 I print_info: rope type        = 2
0.00.039.385 I print_info: rope scaling     = linear
0.00.039.385 I print_info: freq_base_train  = 10000.0
0.00.039.386 I print_info: freq_scale_train = 1
0.00.039.386 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.386 I print_info: rope_finetuned   = unknown
0.00.039.386 I print_info: ssm_d_conv       = 0
0.00.039.386 I print_info: ssm_d_inner      = 0
0.00.039.388 I print_info: ssm_d_state      = 0
0.00.039.388 I print_info: ssm_dt_rank      = 0
0.00.039.388 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.388 I print_info: model type       = 1.4B
0.00.039.389 I print_info: model params     = 1.41 B
0.00.039.389 I print_info: general.name     = 1.4B
0.00.039.389 I print_info: vocab type       = BPE
0.00.039.391 I print_info: n_vocab          = 50304
0.00.039.391 I print_info: n_merges         = 50009
0.00.039.391 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.391 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.391 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.391 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.392 I print_info: LF token         = 187 'Ċ'
0.00.039.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.392 I print_info: max token length = 1024
0.00.039.393 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.658 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.674 I load_tensors: offloading output layer to GPU
0.00.591.674 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.708 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.710 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.393 I llama_context: n_seq_max     = 1
0.00.593.396 I llama_context: n_ctx         = 128
0.00.593.397 I llama_context: n_ctx_per_seq = 128
0.00.593.397 I llama_context: n_batch       = 128
0.00.593.398 I llama_context: n_ubatch      = 128
0.00.593.398 I llama_context: flash_attn    = 0
0.00.593.399 I llama_context: freq_base     = 10000.0
0.00.593.400 I llama_context: freq_scale    = 1
0.00.593.401 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.404 I ggml_metal_init: allocating
0.00.593.449 I ggml_metal_init: found device: Apple M4
0.00.593.463 I ggml_metal_init: picking default device: Apple M4
0.00.594.872 I ggml_metal_init: using embedded metal library
0.00.601.151 I ggml_metal_init: GPU name:   Apple M4
0.00.601.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.157 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.157 I ggml_metal_init: simdgroup reduction   = true
0.00.601.158 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.158 I ggml_metal_init: has residency sets    = true
0.00.601.158 I ggml_metal_init: has bfloat            = true
0.00.601.158 I ggml_metal_init: use bfloat            = true
0.00.601.159 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.940 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.617.944 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.439 I init:      Metal KV buffer size =    24.00 MiB
0.00.621.445 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.660 I init:      Metal compute buffer size =    25.56 MiB
0.00.624.661 I init:        CPU compute buffer size =     1.06 MiB
0.00.624.662 I init: graph nodes  = 967
0.00.624.662 I init: graph splits = 2
0.00.624.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.338 I 
0.00.657.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.404 I perplexity: tokenizing the input ..
0.00.662.894 I perplexity: tokenization took 5.489 ms
0.00.662.899 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.887 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.800.224 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.800.316 I llama_perf_context_print:        load time =     648.38 ms
0.00.800.317 I llama_perf_context_print: prompt eval time =     135.76 ms /   128 tokens (    1.06 ms per token,   942.85 tokens per second)
0.00.800.318 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.318 I llama_perf_context_print:       total time =     142.98 ms /   129 tokens
0.00.800.909 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.077s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.911 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.321 I llama_model_loader: - type  f32:  194 tensors
0.00.024.321 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.322 I print_info: file format = GGUF V3 (latest)
0.00.024.322 I print_info: file type   = Q6_K
0.00.024.327 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.586 I load: special tokens cache size = 25
0.00.039.027 I load: token to piece cache size = 0.2984 MB
0.00.039.044 I print_info: arch             = gptneox
0.00.039.045 I print_info: vocab_only       = 0
0.00.039.045 I print_info: n_ctx_train      = 2048
0.00.039.045 I print_info: n_embd           = 2048
0.00.039.045 I print_info: n_layer          = 24
0.00.039.049 I print_info: n_head           = 16
0.00.039.050 I print_info: n_head_kv        = 16
0.00.039.050 I print_info: n_rot            = 32
0.00.039.051 I print_info: n_swa            = 0
0.00.039.053 I print_info: n_embd_head_k    = 128
0.00.039.053 I print_info: n_embd_head_v    = 128
0.00.039.053 I print_info: n_gqa            = 1
0.00.039.054 I print_info: n_embd_k_gqa     = 2048
0.00.039.055 I print_info: n_embd_v_gqa     = 2048
0.00.039.055 I print_info: f_norm_eps       = 1.0e-05
0.00.039.056 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.056 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.056 I print_info: f_logit_scale    = 0.0e+00
0.00.039.059 I print_info: n_ff             = 8192
0.00.039.059 I print_info: n_expert         = 0
0.00.039.059 I print_info: n_expert_used    = 0
0.00.039.059 I print_info: causal attn      = 1
0.00.039.059 I print_info: pooling type     = 0
0.00.039.059 I print_info: rope type        = 2
0.00.039.060 I print_info: rope scaling     = linear
0.00.039.060 I print_info: freq_base_train  = 10000.0
0.00.039.060 I print_info: freq_scale_train = 1
0.00.039.060 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.061 I print_info: rope_finetuned   = unknown
0.00.039.061 I print_info: ssm_d_conv       = 0
0.00.039.061 I print_info: ssm_d_inner      = 0
0.00.039.061 I print_info: ssm_d_state      = 0
0.00.039.061 I print_info: ssm_dt_rank      = 0
0.00.039.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.061 I print_info: model type       = 1.4B
0.00.039.062 I print_info: model params     = 1.41 B
0.00.039.062 I print_info: general.name     = 1.4B
0.00.039.062 I print_info: vocab type       = BPE
0.00.039.064 I print_info: n_vocab          = 50304
0.00.039.064 I print_info: n_merges         = 50009
0.00.039.064 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.064 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.064 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.064 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: LF token         = 187 'Ċ'
0.00.039.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: max token length = 1024
0.00.039.065 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.618 I load_tensors: offloading output layer to GPU
0.00.612.618 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.646 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.612.647 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.614.150 I llama_context: n_seq_max     = 1
0.00.614.155 I llama_context: n_ctx         = 128
0.00.614.156 I llama_context: n_ctx_per_seq = 128
0.00.614.157 I llama_context: n_batch       = 128
0.00.614.157 I llama_context: n_ubatch      = 128
0.00.614.157 I llama_context: flash_attn    = 0
0.00.614.158 I llama_context: freq_base     = 10000.0
0.00.614.159 I llama_context: freq_scale    = 1
0.00.614.160 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.162 I ggml_metal_init: allocating
0.00.614.214 I ggml_metal_init: found device: Apple M4
0.00.614.227 I ggml_metal_init: picking default device: Apple M4
0.00.615.948 I ggml_metal_init: using embedded metal library
0.00.622.399 I ggml_metal_init: GPU name:   Apple M4
0.00.622.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.405 I ggml_metal_init: simdgroup reduction   = true
0.00.622.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.406 I ggml_metal_init: has residency sets    = true
0.00.622.406 I ggml_metal_init: has bfloat            = true
0.00.622.406 I ggml_metal_init: use bfloat            = true
0.00.622.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.932 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.639.935 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.643.426 I init:      Metal KV buffer size =    24.00 MiB
0.00.643.433 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.637 I init:      Metal compute buffer size =    25.56 MiB
0.00.646.638 I init:        CPU compute buffer size =     1.06 MiB
0.00.646.639 I init: graph nodes  = 967
0.00.646.639 I init: graph splits = 2
0.00.646.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.327 I 
0.00.679.387 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.395 I perplexity: tokenizing the input ..
0.00.686.480 I perplexity: tokenization took 7.083 ms
0.00.686.488 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.390 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.819.790 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.819.813 I llama_perf_context_print:        load time =     670.41 ms
0.00.819.814 I llama_perf_context_print: prompt eval time =     131.49 ms /   128 tokens (    1.03 ms per token,   973.43 tokens per second)
0.00.819.815 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.815 I llama_perf_context_print:       total time =     140.49 ms /   129 tokens
0.00.820.382 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.079s
sys	0m0.145s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.222 I build: 4797 (f5cedbca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.466 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.094 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.104 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.118 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.385 I llama_model_loader: - type  f32:  194 tensors
0.00.046.385 I llama_model_loader: - type  f16:   98 tensors
0.00.046.386 I print_info: file format = GGUF V3 (latest)
0.00.046.387 I print_info: file type   = all F32 (guessed)
0.00.046.388 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.054.429 I load: special tokens cache size = 25
0.00.060.717 I load: token to piece cache size = 0.2984 MB
0.00.060.735 I print_info: arch             = gptneox
0.00.060.736 I print_info: vocab_only       = 0
0.00.060.736 I print_info: n_ctx_train      = 2048
0.00.060.736 I print_info: n_embd           = 2048
0.00.060.736 I print_info: n_layer          = 24
0.00.060.740 I print_info: n_head           = 16
0.00.060.741 I print_info: n_head_kv        = 16
0.00.060.741 I print_info: n_rot            = 32
0.00.060.741 I print_info: n_swa            = 0
0.00.060.742 I print_info: n_embd_head_k    = 128
0.00.060.742 I print_info: n_embd_head_v    = 128
0.00.060.742 I print_info: n_gqa            = 1
0.00.060.746 I print_info: n_embd_k_gqa     = 2048
0.00.060.746 I print_info: n_embd_v_gqa     = 2048
0.00.060.747 I print_info: f_norm_eps       = 1.0e-05
0.00.060.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.747 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.747 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.747 I print_info: f_logit_scale    = 0.0e+00
0.00.060.748 I print_info: n_ff             = 8192
0.00.060.748 I print_info: n_expert         = 0
0.00.060.748 I print_info: n_expert_used    = 0
0.00.060.748 I print_info: causal attn      = 1
0.00.060.749 I print_info: pooling type     = 0
0.00.060.749 I print_info: rope type        = 2
0.00.060.749 I print_info: rope scaling     = linear
0.00.060.749 I print_info: freq_base_train  = 10000.0
0.00.060.750 I print_info: freq_scale_train = 1
0.00.060.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.752 I print_info: rope_finetuned   = unknown
0.00.060.752 I print_info: ssm_d_conv       = 0
0.00.060.752 I print_info: ssm_d_inner      = 0
0.00.060.753 I print_info: ssm_d_state      = 0
0.00.060.753 I print_info: ssm_dt_rank      = 0
0.00.060.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.753 I print_info: model type       = 1.4B
0.00.060.753 I print_info: model params     = 1.41 B
0.00.060.753 I print_info: general.name     = 1.4B
0.00.060.754 I print_info: vocab type       = BPE
0.00.060.755 I print_info: n_vocab          = 50304
0.00.060.756 I print_info: n_merges         = 50009
0.00.060.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.757 I print_info: LF token         = 187 'Ċ'
0.00.060.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.757 I print_info: max token length = 1024
0.00.060.758 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.328.045 I load_tensors: offloading 24 repeating layers to GPU
0.01.328.048 I load_tensors: offloading output layer to GPU
0.01.328.048 I load_tensors: offloaded 25/25 layers to GPU
0.01.328.066 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.328.067 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.328.677 I llama_context: n_seq_max     = 1
0.01.328.678 I llama_context: n_ctx         = 128
0.01.328.678 I llama_context: n_ctx_per_seq = 128
0.01.328.678 I llama_context: n_batch       = 128
0.01.328.679 I llama_context: n_ubatch      = 128
0.01.328.679 I llama_context: flash_attn    = 0
0.01.328.679 I llama_context: freq_base     = 10000.0
0.01.328.680 I llama_context: freq_scale    = 1
0.01.328.680 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.328.681 I ggml_metal_init: allocating
0.01.328.715 I ggml_metal_init: found device: Apple M4
0.01.328.721 I ggml_metal_init: picking default device: Apple M4
0.01.329.418 I ggml_metal_init: using embedded metal library
0.01.331.897 I ggml_metal_init: GPU name:   Apple M4
0.01.331.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.331.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.331.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.331.900 I ggml_metal_init: simdgroup reduction   = true
0.01.331.900 I ggml_metal_init: simdgroup matrix mul. = true
0.01.331.901 I ggml_metal_init: has residency sets    = true
0.01.331.901 I ggml_metal_init: has bfloat            = true
0.01.331.901 I ggml_metal_init: use bfloat            = true
0.01.331.901 I ggml_metal_init: hasUnifiedMemory      = true
0.01.331.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.341.925 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.341.928 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.343.597 I init:      Metal KV buffer size =    24.00 MiB
0.01.343.600 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.345.220 I init:      Metal compute buffer size =    25.56 MiB
0.01.345.221 I init:        CPU compute buffer size =     1.06 MiB
0.01.345.221 I init: graph nodes  = 967
0.01.345.222 I init: graph splits = 2
0.01.345.223 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.345.224 I 
0.01.345.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.345.254 I compute_imatrix: tokenizing the input ..
0.01.349.328 I compute_imatrix: tokenization took 4.073 ms
0.01.349.330 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.622.573 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.625.487 I llama_perf_context_print:        load time =    1603.11 ms
0.01.625.488 I llama_perf_context_print: prompt eval time =     271.31 ms /   128 tokens (    2.12 ms per token,   471.79 tokens per second)
0.01.625.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.625.489 I llama_perf_context_print:       total time =    1606.02 ms /   129 tokens
0.01.626.312 I ggml_metal_free: deallocating

real	0m1.807s
user	0m0.123s
sys	0m0.201s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4797 (f5cedbca)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115604d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115605470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115605a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115605fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115606b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1156070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115607690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115607c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115608140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115608640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115608b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115609660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115609e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11560a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11560ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11560b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11560bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11560c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11560ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11560d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11560d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11560dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11560e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11560ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11560f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11560f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1156104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115610a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115610cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115611170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115611430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115611cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115612200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1156124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115612960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115612e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1156132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115613740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115613be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115614080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115614520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1156149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115614e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115615730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115615d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115616660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115616c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115617280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115617890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115617ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1156184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115618ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1156192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115619750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115619bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115619eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11561a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11561acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11561af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11561b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11561b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11561bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11561c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11561c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11561cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11561cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11561d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11561d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11561ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11561e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11561e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11561ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11561f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11561f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11561fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115620180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1156206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115620c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x115621170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1156216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x115621c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115622160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1156226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115622c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115623150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1156236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115623bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115624140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x115624690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x115624be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x115625130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x115625680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x115625bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x115626120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x115626670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x115616350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x115626ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x115627290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1156277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115628280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1156287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1156297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11562a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11562a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11562ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11562b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11562b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11562bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11562c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11562c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11562ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11562cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11562d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11562d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11562dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11562e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11562e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11562ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11562ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11562f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11562f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11562fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1156301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115630640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115630ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115630f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1156318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115631d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115632200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1156326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115632b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115632fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115633480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115633920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115633dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115634260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115634700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115634ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115635040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1156354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115635980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115635e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1156362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115636760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115636c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1156370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115637540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1156379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115637e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115638320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1156387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115638c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115639100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1156395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115639a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115639ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11563a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11563a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11563acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11563b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11563b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11563baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11563bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11563c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11563c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11563cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11563d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11563d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11563db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11563dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11563e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11563e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11563ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11563f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11563f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11563fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115640000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1156404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115640940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115640de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115641280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115641720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115641bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115642060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115642500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1156429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115642ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115643440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115643ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1156441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1156447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115644dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1156453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115645bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115646060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115646f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115647730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115647bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115648070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115648510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115648cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115649210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115649760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115649cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11564a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11564a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11564aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11564b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11564b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11564bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11564c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11564c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11564cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11564d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11564d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11564dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11564e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11564e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11564ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11564f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11564f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11564fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1156501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1156506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115650c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115651190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1156516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115651c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115652180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1156526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115652c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115653170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1156536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115653c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115654160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1156546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115654c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115655150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1156556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115655bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115656140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115656690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115656be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115657130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115657680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115657bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115658120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115658670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115658bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115659110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115659660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115659bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11565a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11565a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11565aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11565b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11565b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11565bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11565bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11565c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11565c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11565cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11565d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11565d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11565db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11565dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11565e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11565e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11565edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11565f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11565f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11565fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1156600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115660810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115660f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115661650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115662030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115662820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115662ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1156630f0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.688.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115662da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115644a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115644460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115618160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115617b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11561a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11560f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115616000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115616920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115616f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1156153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115604510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11561a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115626da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1156622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1156116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1156119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115645690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11560fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11560fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1156100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115663550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115663810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115663ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115663d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115664050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115664310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1156645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115664890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115664b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115664e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1156650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115665390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115665650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115665910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115665bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115665e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115666150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115666410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1156666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115666c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115666f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1156671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115667490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115667750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115667a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115668250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115668510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1156687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115668a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115668d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115669010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1156692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115669590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115669850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115669b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115669dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11566a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11566a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11566a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11566a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11566ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11566ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11566b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11566b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11566b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11566b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11566bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11566bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11566c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11566c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11566c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11566c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11566cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11566cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11566d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11566d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11566d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11566da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11566dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11566dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11566e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11566e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11566e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11566ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11566ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11566f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11566f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11566f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11566f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11566fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11566fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1156700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115670390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115670650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x115670910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115670bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115670e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x115671150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115671410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1156716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x115671990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x115671c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115671f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1156721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115672490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115672750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115672a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115672cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115672f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115673250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115673510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1156737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115673a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115673d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115674010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1156742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115674590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115674850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115674b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115674dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115675090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115675350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115675610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1156758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115675b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115675e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115676110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1156763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115676690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115676950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115676c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115676ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115677190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115677450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115677710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1156779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115677c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115677f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115678210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1156784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115678790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115678a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115678d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115678fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115679290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115679550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115679810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115679ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115679d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11567a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11567a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11567a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11567a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11567ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11567ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11567b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11567b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11567b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11567b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11567bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11567be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11567c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11567c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11567c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11567c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11567cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11567cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11567d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11567d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11567d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11567da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11567dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11567df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11567e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11567e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11567e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11567ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11567ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11567f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11567f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11567f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11567f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11567fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11567fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115680090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115680350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115680610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1156808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115680b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115680e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115681110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1156813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115681690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115681950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115681c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115681ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115682190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115682450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115682710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1156829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115682c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115683260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115683520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1156837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115683aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115683d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115684020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1156842e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1156845a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115684860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115684b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115684de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1156850a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115685360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115685620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1156858e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115685ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115685e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115686120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1156863e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1156866a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115686960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115686c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115686ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1156871a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115687460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115687720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1156879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115687ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115687f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115688220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115688770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115688cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115689210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115689760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115689cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11568a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11568a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11568aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11568b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11568b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11568bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11568c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11568c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11568cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11568d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11568d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11568dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11568e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11568e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11568ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11568f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11568f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11568fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1156901a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1156906f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115690c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115691190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115691450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115691710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115691c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115692110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115692610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115692b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115693010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115693510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115693a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115693f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115694410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115694910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115694e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115695310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115695810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115695d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115696720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115696e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115697560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115697c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115697f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115698730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1156989f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115699000 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1147044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1147056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1147063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1147078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1147083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x114709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11470a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11470a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11470b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11470b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11470bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11470c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11470cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11470d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11470db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11470de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11470e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11470e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11470e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11470ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11470f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11470f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11470fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11470ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1147107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1147110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1147119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1147138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1147141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x114714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x114715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1147157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1147160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x114717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x114718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1147185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x114718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x114718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x114719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x114719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11471a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11471a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11471a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11471adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11471b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11471b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11471bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11471bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11471c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11471c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11471ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11471d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11471d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11471da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11471de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11471e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11471e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11471ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11471f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11471f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11471f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11471fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1147213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1147229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1147232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1147240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x114724370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1147247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1147250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1147259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114725e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114726280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1147266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114726b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114726fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114727440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1147278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114728190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114728a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114728ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114729350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1147297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114729c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11472a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11472a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11472a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11472adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11472b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11472b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11472bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11472bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11472c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11472c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11472cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11472d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11472d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11472da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11472dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11472e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11472e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11472ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11472f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11472f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11472f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11472fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114730240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1147306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114730b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114730f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x114731400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114731870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114731ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x114732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1147325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x114732ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114733310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114733780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114733bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114734060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1147344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114734940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114734db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114735220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114735b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114735f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1147363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114736850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114736cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114737130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1147375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114737a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114737e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1147382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114738760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114738bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114739040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1147394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114739920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114739d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11473a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11473a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11473aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11473af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11473b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11473b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11473bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11473c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11473c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11473c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11473ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11473d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11473d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11473dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11473e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11473e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11473e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11473ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11473f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11473f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11473fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11473ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1147403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114740930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114741210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114741d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114742020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1147422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114742750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114743030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1147434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114743910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114743d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1147441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114744ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114744f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1147453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114745820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114745c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114746100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114746570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1147469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114746e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1147472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114747730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114747ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114748010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114748480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1147488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1147491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114749640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114749ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114749f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11474a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11474a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11474ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11474b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11474b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11474b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11474be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11474c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11474c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11474cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11474cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11474d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11474d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11474dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11474e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11474e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11474ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11474ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11474f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11474f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11474fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1147500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1147509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114750e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114751280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1147516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114751b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114751fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1147528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114752d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114753190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114753600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114753ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114754350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1147547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114754c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1147550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114755510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114755980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1147563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114756b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114757230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114757950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114758680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114758c90 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.745s
user	0m0.278s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4797 (f5cedbca)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12df105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12df10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12df11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12df11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12df11dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12df12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12df12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12df12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12df13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12df13990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12df13e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12df14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12df14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12df15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12df15e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12df16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12df16cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12df173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12df17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12df182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12df189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12df19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12df19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12df1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12df1a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12df1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12df1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12df1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12df1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12df1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12df1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12df1cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12df1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12df1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12df1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12df1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12df1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12df1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12df1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12df1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12df1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12df1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12df20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12df206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12df20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12df20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12df21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12df21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12df224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12df22ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12df230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12df236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12df23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12df24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12df24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12df24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12df25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12df25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12df25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12df26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12df267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12df26c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12df27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12df275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12df27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12df27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12df28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12df28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12df28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12df29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12df29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12df29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12df29f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12df2a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12df2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12df2af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12df2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12df2b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12df2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12df2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12df2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12df2cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12df2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12df2d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12df2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12df2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12df2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12df2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12df2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12df2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12df2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12df30430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12df30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12df30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12df31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12df31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12df31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12df21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12df32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12df32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12df33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12df33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12df33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12df34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12df34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12df34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12df35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12df35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12df35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12df36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12df36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12df36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12df36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12df37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12df37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12df37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12df38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12df38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12df38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12df39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12df394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12df39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12df39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12df3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12df3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12df3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12df3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12df3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12df3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12df3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12df3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12df3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12df3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12df3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12df3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12df3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12df3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12df3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12df3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12df3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12df3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12df3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12df3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12df3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12df403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12df40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12df40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12df411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12df41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12df41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12df41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12df42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12df428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12df42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12df43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12df436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12df43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12df44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12df444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12df44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12df44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12df45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12df45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12df45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12df46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12df46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12df469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12df46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12df472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12df47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12df47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12df480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12df48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12df48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12df48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12df49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12df497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12df49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12df4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12df4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12df4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12df4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12df4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12df4b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12df4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12df4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12df4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12df4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12df4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12df4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12df4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12df4dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12df4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12df4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12df4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12df4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12df4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12df4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12df50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12df50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12df50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12df51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12df518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12df51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12df52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12df52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12df52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12df53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12df538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12df53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12df54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12df54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12df54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12df55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12df55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12df55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12df564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12df56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12df56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12df574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12df57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12df57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12df584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12df58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12df58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12df594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12df59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12df59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12df5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12df5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12df5af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12df5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12df5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12df5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12df5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12df5c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12df5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12df5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12df5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12df5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12df5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12df5e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12df5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12df5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12df5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12df5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12df60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12df609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12df60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12df61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12df61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12df61ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12df62430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12df62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12df62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12df63420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12df63970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12df63ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12df64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12df64960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12df64eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12df65400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12df65950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12df65ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12df663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12df66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12df66e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12df67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12df677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12df67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12df68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12df685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12df68a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12df68ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12df69390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12df69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12df69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12df6a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12df6a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12df6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12df6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12df6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12df6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12df6c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12df6c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12df6cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12df6d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12df6d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12df6e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12df6e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12df6e940 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.102.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12df6e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12df502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12df4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12df508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12df239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12df233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12df259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12df52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12df1ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12df21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12df22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12df22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12df20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12df22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12df19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12df25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12df325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12df6db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12df1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12df1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12df52a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12df50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12df1b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12df1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12df1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12df6eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12df6f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12df6f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12df6f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12df6f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12df6fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12df6fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12df700e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12df703a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12df70660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12df70920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12df70be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12df70ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12df71160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12df71420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12df716e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12df719a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12df71c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12df71f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12df721e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12df724a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12df72760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12df72a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12df72ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12df72fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12df73260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12df73520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12df737e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12df73aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12df73d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12df74020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12df742e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12df745a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12df74860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12df74b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12df74de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12df750a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12df75360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12df75620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12df758e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12df75ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12df75e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12df76120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12df763e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12df766a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12df76960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12df76c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12df76ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12df771a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12df77460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12df77720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12df779e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12df77ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12df77f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12df78220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12df784e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12df787a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12df78a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12df78d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12df78fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12df792a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12df79560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12df79820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12df79ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12df79da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12df7a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12df7a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12df7a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12df7a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12df7ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12df7ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12df7b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12df7b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12df7b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12df7b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12df7bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12df7bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12df7c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12df7c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12df7c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12df7c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12df7cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12df7cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12df7d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12df7d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12df7d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12df7da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12df7dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12df7dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12df7e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12df7e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12df7e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12df7eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12df7ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12df7f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12df7f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12df7f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12df7f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12df7fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12df7fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12df800a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12df80360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12df80620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12df808e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12df80ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12df80e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12df81120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12df813e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12df816a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12df81960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12df81c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12df81ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12df821a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12df82460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12df82720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12df829e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12df82ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12df82f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12df83220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12df834e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12df837a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12df83a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12df83d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12df83fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12df842a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12df84560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12df84820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12df84ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12df84da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12df85060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12df85320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12df855e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12df858a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12df85b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12df85e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12df860e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12df863a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12df86660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12df86920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12df86be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12df86ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12df87160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12df87420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12df876e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12df879a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12df87c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12df87f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12df881e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12df884a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12df88760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12df88a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12df88ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12df88fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12df89260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12df89520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12df897e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12df89aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12df89d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12df8a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12df8a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12df8a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12df8a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12df8ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12df8ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12df8b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12df8b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12df8b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12df8b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12df8bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12df8be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12df8c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12df8c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12df8c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12df8c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12df8cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12df8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12df8d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12df8d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12df8d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12df8d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12df8df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12df8e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12df8e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12df8eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12df8efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12df8f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12df8fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12df8fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12df90160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12df905d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12df90a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12df90eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12df91320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12df91790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12df91c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12df92070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12df924e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12df92950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12df92dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12df93230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12df936a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12df93b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12df93f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12df943f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12df94860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12df94cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12df95140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12df955b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12df95a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12df95e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12df96300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12df96770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12df96be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12df97050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12df974c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12df97930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12df97da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12df98210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12df98680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12df98af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12df98f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12df993d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12df99840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12df99cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12df9a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12df9a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12df9aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12df9ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12df9b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12df9b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12df9bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12df9c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12df9c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12df9c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12df9cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12df9d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12df9d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12df9dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12df9df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12df9e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12df9e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12df9ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12df9f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12df9f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12df9f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12df9fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dfa02c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dfa0730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dfa0ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dfa1010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dfa1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dfa18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dfa1d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dfa21d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dfa2640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dfa2ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dfa2f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dfa3390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dfa3e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dfa4520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dfa4c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dfa5360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dfa5620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12dfa5e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dfa60d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dfa66e0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12de04c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12de050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12de05540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12de059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12de05e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12de06290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12de06700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12de06b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12de06fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12de07450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12de078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12de07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12de08b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12de092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12de09ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12de0a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12de0a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12de0b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12de0b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12de0be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12de0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12de0ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12de0d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12de0db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12de0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12de0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12de0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12de0ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12de0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12de0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12de0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12de0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12de10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12de105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12de10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12de10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12de11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12de11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12de12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12de124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12de12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12de13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12de13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12de13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12de14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12de14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12de15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12de15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12de15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12de162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12de16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12de16cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12de171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12de17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12de17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12de18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12de18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12de18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12de190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12de19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12de199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12de19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12de1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12de1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12de1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12de1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12de1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12de1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12de1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12de1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12de1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12de1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12de1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12de1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12de1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12de1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12de1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12de1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12de1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12de1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12de1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12de1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12de1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12de1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12de20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12de208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12de20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12de21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12de21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12de21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12de21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12de22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12de227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12de22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12de230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12de23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12de23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12de23df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12de24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12de24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12de24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12de25300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12de25770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12de25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12de26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12de264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12de26930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12de26da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12de27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12de27680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12de27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12de27f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12de283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12de28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12de28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12de29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12de29590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12de29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12de29e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12de2a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12de2a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12de2abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12de2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12de2b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12de2b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12de2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12de2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12de2c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12de2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12de2cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12de2d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12de2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12de2dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12de2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12de2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12de2e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12de2ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12de2f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12de2f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12de2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12de30010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12de30480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12de308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12de30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12de311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12de31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12de31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12de31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12de32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12de32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12de32c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12de330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12de33550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12de339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12de33e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12de342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12de34710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12de34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12de34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12de35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12de358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12de35d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12de361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12de36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12de36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12de36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12de37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12de377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12de37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12de380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12de38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12de389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12de38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12de39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12de396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12de39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12de39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12de3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12de3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12de3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12de3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12de3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12de3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12de3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12de3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12de3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12de3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12de3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12de3d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12de3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12de3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12de3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12de3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12de3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12de3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12de3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12de3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12de3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12de40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12de405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12de40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12de40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12de41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12de418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12de42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12de426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12de42990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12de42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12de43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12de436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12de43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12de43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12de44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12de448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12de44d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12de45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12de455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12de45a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12de45ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12de46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12de467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12de46c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12de47090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12de47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12de47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12de47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12de48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12de486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12de48b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12de48fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12de49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12de49880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12de49cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12de4a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12de4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12de4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12de4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12de4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12de4b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12de4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12de4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12de4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12de4c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12de4cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12de4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12de4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12de4db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12de4df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12de4e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12de4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12de4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12de4f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12de4f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12de4fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12de4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12de50300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12de50770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12de50be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12de51050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12de514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12de51930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12de51da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12de52210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12de52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12de52af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12de52f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12de533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12de53840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12de53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12de54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12de54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12de54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12de54e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12de552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12de55750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12de55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12de56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12de56aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12de571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12de578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12de58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12de582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12de58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12de58d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12de59340 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.964s
user	0m0.230s
sys	0m0.193s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
