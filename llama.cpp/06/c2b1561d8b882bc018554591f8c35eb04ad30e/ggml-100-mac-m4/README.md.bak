### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.22 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.63 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.32 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.91 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.06 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.27 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.89 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.27 sec*proc (29 tests)

Total Test time (real) = 164.28 sec

real	2m44.291s
user	4m37.463s
sys	0m5.634s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.25 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.88 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.83 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.40 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.33 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.37 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.04 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.16 sec*proc (29 tests)

Total Test time (real) =  48.17 sec

real	0m48.183s
user	0m54.588s
sys	0m5.199s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.103 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.249 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.696 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.017.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.701 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.017.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.702 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.017.703 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.017.703 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.017.704 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.017.704 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.017.705 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.017.706 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.017.706 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.017.709 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.017.709 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.017.710 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.017.710 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.017.711 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.017.711 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.017.711 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.019.974 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.020.564 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.020.565 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.020.565 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.020.565 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.020.566 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.020.566 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.020.566 I llama_model_loader: - type  f32:  124 tensors
0.00.020.567 I llama_model_loader: - type  f16:   73 tensors
0.00.020.567 I print_info: file format = GGUF V3 (latest)
0.00.020.568 I print_info: file type   = F16
0.00.020.569 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.022.919 I load: special tokens cache size = 5
0.00.024.133 I load: token to piece cache size = 0.2032 MB
0.00.024.136 I print_info: arch             = bert
0.00.024.136 I print_info: vocab_only       = 0
0.00.024.136 I print_info: n_ctx_train      = 512
0.00.024.136 I print_info: n_embd           = 384
0.00.024.137 I print_info: n_layer          = 12
0.00.024.139 I print_info: n_head           = 12
0.00.024.140 I print_info: n_head_kv        = 12
0.00.024.140 I print_info: n_rot            = 32
0.00.024.140 I print_info: n_swa            = 0
0.00.024.140 I print_info: n_embd_head_k    = 32
0.00.024.140 I print_info: n_embd_head_v    = 32
0.00.024.141 I print_info: n_gqa            = 1
0.00.024.141 I print_info: n_embd_k_gqa     = 384
0.00.024.142 I print_info: n_embd_v_gqa     = 384
0.00.024.143 I print_info: f_norm_eps       = 1.0e-12
0.00.024.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.024.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.024.145 I print_info: f_max_alibi_bias = 0.0e+00
0.00.024.145 I print_info: f_logit_scale    = 0.0e+00
0.00.024.146 I print_info: n_ff             = 1536
0.00.024.146 I print_info: n_expert         = 0
0.00.024.146 I print_info: n_expert_used    = 0
0.00.024.146 I print_info: causal attn      = 0
0.00.024.146 I print_info: pooling type     = 2
0.00.024.147 I print_info: rope type        = 2
0.00.024.147 I print_info: rope scaling     = linear
0.00.024.147 I print_info: freq_base_train  = 10000.0
0.00.024.147 I print_info: freq_scale_train = 1
0.00.024.147 I print_info: n_ctx_orig_yarn  = 512
0.00.024.148 I print_info: rope_finetuned   = unknown
0.00.024.148 I print_info: ssm_d_conv       = 0
0.00.024.148 I print_info: ssm_d_inner      = 0
0.00.024.148 I print_info: ssm_d_state      = 0
0.00.024.148 I print_info: ssm_dt_rank      = 0
0.00.024.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.024.148 I print_info: model type       = 33M
0.00.024.148 I print_info: model params     = 33.21 M
0.00.024.149 I print_info: general.name     = Bge Small
0.00.024.149 I print_info: vocab type       = WPM
0.00.024.149 I print_info: n_vocab          = 30522
0.00.024.149 I print_info: n_merges         = 0
0.00.024.150 I print_info: BOS token        = 101 '[CLS]'
0.00.024.154 I print_info: UNK token        = 100 '[UNK]'
0.00.024.154 I print_info: SEP token        = 102 '[SEP]'
0.00.024.154 I print_info: PAD token        = 0 '[PAD]'
0.00.024.155 I print_info: MASK token       = 103 '[MASK]'
0.00.024.155 I print_info: LF token         = 0 '[PAD]'
0.00.024.155 I print_info: max token length = 21
0.00.024.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.026.165 I load_tensors: offloading 12 repeating layers to GPU
0.00.026.167 I load_tensors: offloading output layer to GPU
0.00.026.167 I load_tensors: offloaded 13/13 layers to GPU
0.00.026.188 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.026.189 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.026.374 I llama_init_from_model: n_seq_max     = 1
0.00.026.375 I llama_init_from_model: n_ctx         = 512
0.00.026.375 I llama_init_from_model: n_ctx_per_seq = 512
0.00.026.375 I llama_init_from_model: n_batch       = 2048
0.00.026.376 I llama_init_from_model: n_ubatch      = 2048
0.00.026.376 I llama_init_from_model: flash_attn    = 0
0.00.026.376 I llama_init_from_model: freq_base     = 10000.0
0.00.026.376 I llama_init_from_model: freq_scale    = 1
0.00.026.377 I ggml_metal_init: allocating
0.00.026.381 I ggml_metal_init: found device: Apple M4
0.00.026.384 I ggml_metal_init: picking default device: Apple M4
0.00.026.940 I ggml_metal_init: using embedded metal library
0.00.029.505 I ggml_metal_init: GPU name:   Apple M4
0.00.029.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.029.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.029.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.029.508 I ggml_metal_init: simdgroup reduction   = true
0.00.029.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.029.508 I ggml_metal_init: has residency sets    = true
0.00.029.509 I ggml_metal_init: has bfloat            = true
0.00.029.509 I ggml_metal_init: use bfloat            = true
0.00.029.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.029.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.040.124 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.040.702 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.040.704 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.040.726 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.041.703 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.041.704 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.041.704 I llama_init_from_model: graph nodes  = 429
0.00.041.705 I llama_init_from_model: graph splits = 2
0.00.041.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.041.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.046.039 I 
0.00.046.069 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.046.589 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.050.883 I llama_perf_context_print:        load time =      30.79 ms
0.00.050.884 I llama_perf_context_print: prompt eval time =       4.16 ms /     9 tokens (    0.46 ms per token,  2162.94 tokens per second)
0.00.050.884 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.050.885 I llama_perf_context_print:       total time =       4.84 ms /    10 tokens
0.00.051.080 I ggml_metal_free: deallocating

real	0m0.225s
user	0m0.036s
sys	0m0.026s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.290 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.295 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.296 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.296 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.297 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.298 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.298 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.298 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.299 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.299 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.301 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.302 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.302 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.303 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.303 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.303 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.530 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.154 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.155 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.155 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.156 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.156 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.156 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.157 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.157 I llama_model_loader: - type  f32:  124 tensors
0.00.014.157 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.158 I print_info: file format = GGUF V3 (latest)
0.00.014.158 I print_info: file type   = Q8_0
0.00.014.159 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.459 I load: special tokens cache size = 5
0.00.017.700 I load: token to piece cache size = 0.2032 MB
0.00.017.702 I print_info: arch             = bert
0.00.017.702 I print_info: vocab_only       = 0
0.00.017.703 I print_info: n_ctx_train      = 512
0.00.017.703 I print_info: n_embd           = 384
0.00.017.703 I print_info: n_layer          = 12
0.00.017.705 I print_info: n_head           = 12
0.00.017.706 I print_info: n_head_kv        = 12
0.00.017.706 I print_info: n_rot            = 32
0.00.017.706 I print_info: n_swa            = 0
0.00.017.707 I print_info: n_embd_head_k    = 32
0.00.017.707 I print_info: n_embd_head_v    = 32
0.00.017.707 I print_info: n_gqa            = 1
0.00.017.708 I print_info: n_embd_k_gqa     = 384
0.00.017.709 I print_info: n_embd_v_gqa     = 384
0.00.017.709 I print_info: f_norm_eps       = 1.0e-12
0.00.017.710 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.710 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.710 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.711 I print_info: f_logit_scale    = 0.0e+00
0.00.017.711 I print_info: n_ff             = 1536
0.00.017.712 I print_info: n_expert         = 0
0.00.017.713 I print_info: n_expert_used    = 0
0.00.017.713 I print_info: causal attn      = 0
0.00.017.714 I print_info: pooling type     = 2
0.00.017.714 I print_info: rope type        = 2
0.00.017.714 I print_info: rope scaling     = linear
0.00.017.715 I print_info: freq_base_train  = 10000.0
0.00.017.715 I print_info: freq_scale_train = 1
0.00.017.716 I print_info: n_ctx_orig_yarn  = 512
0.00.017.716 I print_info: rope_finetuned   = unknown
0.00.017.716 I print_info: ssm_d_conv       = 0
0.00.017.716 I print_info: ssm_d_inner      = 0
0.00.017.717 I print_info: ssm_d_state      = 0
0.00.017.717 I print_info: ssm_dt_rank      = 0
0.00.017.717 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.717 I print_info: model type       = 33M
0.00.017.718 I print_info: model params     = 33.21 M
0.00.017.718 I print_info: general.name     = Bge Small
0.00.017.719 I print_info: vocab type       = WPM
0.00.017.719 I print_info: n_vocab          = 30522
0.00.017.719 I print_info: n_merges         = 0
0.00.017.719 I print_info: BOS token        = 101 '[CLS]'
0.00.017.719 I print_info: UNK token        = 100 '[UNK]'
0.00.017.720 I print_info: SEP token        = 102 '[SEP]'
0.00.017.720 I print_info: PAD token        = 0 '[PAD]'
0.00.017.720 I print_info: MASK token       = 103 '[MASK]'
0.00.017.721 I print_info: LF token         = 0 '[PAD]'
0.00.017.721 I print_info: max token length = 21
0.00.017.721 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.258 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.259 I load_tensors: offloading output layer to GPU
0.00.019.259 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.265 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.266 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.429 I llama_init_from_model: n_seq_max     = 1
0.00.019.430 I llama_init_from_model: n_ctx         = 512
0.00.019.430 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.430 I llama_init_from_model: n_batch       = 2048
0.00.019.431 I llama_init_from_model: n_ubatch      = 2048
0.00.019.431 I llama_init_from_model: flash_attn    = 0
0.00.019.431 I llama_init_from_model: freq_base     = 10000.0
0.00.019.431 I llama_init_from_model: freq_scale    = 1
0.00.019.432 I ggml_metal_init: allocating
0.00.019.435 I ggml_metal_init: found device: Apple M4
0.00.019.439 I ggml_metal_init: picking default device: Apple M4
0.00.019.931 I ggml_metal_init: using embedded metal library
0.00.022.457 I ggml_metal_init: GPU name:   Apple M4
0.00.022.459 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.459 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.460 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.460 I ggml_metal_init: simdgroup reduction   = true
0.00.022.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.460 I ggml_metal_init: has residency sets    = true
0.00.022.460 I ggml_metal_init: has bfloat            = true
0.00.022.460 I ggml_metal_init: use bfloat            = true
0.00.022.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.169 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.792 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.794 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.808 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.826 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.828 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.828 I llama_init_from_model: graph nodes  = 429
0.00.034.828 I llama_init_from_model: graph splits = 2
0.00.034.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.830 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.969 I 
0.00.038.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.532 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.943 I llama_perf_context_print:        load time =      30.07 ms
0.00.043.944 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2105.76 tokens per second)
0.00.043.945 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.949 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.044.085 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.245 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.093 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.718 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.725 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.727 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.728 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.728 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.730 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.731 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.731 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.732 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.732 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.736 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.736 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.737 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.371 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.372 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.372 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.373 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.373 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.373 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.374 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.374 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.375 I llama_model_loader: - type  f32:   40 tensors
0.00.047.375 I llama_model_loader: - type  f16:   30 tensors
0.00.047.376 I print_info: file format = GGUF V3 (latest)
0.00.047.376 I print_info: file type   = F16
0.00.047.378 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.051.442 W load: empty token at index 5
0.00.056.322 W load: model vocab missing newline token, using special_pad_id instead
0.00.057.735 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.057.770 I load: special tokens cache size = 5
0.00.316.733 I load: token to piece cache size = 1.5060 MB
0.00.316.739 I print_info: arch             = jina-bert-v2
0.00.316.740 I print_info: vocab_only       = 0
0.00.316.740 I print_info: n_ctx_train      = 8192
0.00.316.740 I print_info: n_embd           = 384
0.00.316.740 I print_info: n_layer          = 4
0.00.316.747 I print_info: n_head           = 12
0.00.316.748 I print_info: n_head_kv        = 12
0.00.316.748 I print_info: n_rot            = 32
0.00.316.748 I print_info: n_swa            = 0
0.00.316.748 I print_info: n_embd_head_k    = 32
0.00.316.748 I print_info: n_embd_head_v    = 32
0.00.316.749 I print_info: n_gqa            = 1
0.00.316.749 I print_info: n_embd_k_gqa     = 384
0.00.316.750 I print_info: n_embd_v_gqa     = 384
0.00.316.751 I print_info: f_norm_eps       = 1.0e-12
0.00.316.754 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.316.756 I print_info: f_clamp_kqv      = 0.0e+00
0.00.316.756 I print_info: f_max_alibi_bias = 8.0e+00
0.00.316.756 I print_info: f_logit_scale    = 0.0e+00
0.00.316.757 I print_info: n_ff             = 1536
0.00.316.757 I print_info: n_expert         = 0
0.00.316.757 I print_info: n_expert_used    = 0
0.00.316.757 I print_info: causal attn      = 0
0.00.316.758 I print_info: pooling type     = -1
0.00.316.758 I print_info: rope type        = -1
0.00.316.758 I print_info: rope scaling     = linear
0.00.316.758 I print_info: freq_base_train  = 10000.0
0.00.316.759 I print_info: freq_scale_train = 1
0.00.316.759 I print_info: n_ctx_orig_yarn  = 8192
0.00.316.759 I print_info: rope_finetuned   = unknown
0.00.316.759 I print_info: ssm_d_conv       = 0
0.00.316.759 I print_info: ssm_d_inner      = 0
0.00.316.759 I print_info: ssm_d_state      = 0
0.00.316.759 I print_info: ssm_dt_rank      = 0
0.00.316.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.316.760 I print_info: model type       = 33M
0.00.316.760 I print_info: model params     = 32.90 M
0.00.316.760 I print_info: general.name     = Jina Bert Implementation
0.00.316.762 I print_info: vocab type       = BPE
0.00.316.762 I print_info: n_vocab          = 61056
0.00.316.762 I print_info: n_merges         = 39382
0.00.316.762 I print_info: BOS token        = 0 '<s>'
0.00.316.762 I print_info: EOS token        = 2 '</s>'
0.00.316.763 I print_info: UNK token        = 3 '<unk>'
0.00.316.763 I print_info: SEP token        = 2 '</s>'
0.00.316.763 I print_info: PAD token        = 1 '<pad>'
0.00.316.763 I print_info: MASK token       = 4 '<mask>'
0.00.316.764 I print_info: EOG token        = 2 '</s>'
0.00.316.764 I print_info: max token length = 45
0.00.316.764 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.318.945 I load_tensors: offloading 4 repeating layers to GPU
0.00.318.946 I load_tensors: offloading output layer to GPU
0.00.318.946 I load_tensors: offloaded 5/5 layers to GPU
0.00.318.971 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.318.972 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.319.423 I llama_init_from_model: n_seq_max     = 1
0.00.319.424 I llama_init_from_model: n_ctx         = 8192
0.00.319.424 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.319.424 I llama_init_from_model: n_batch       = 2048
0.00.319.424 I llama_init_from_model: n_ubatch      = 2048
0.00.319.424 I llama_init_from_model: flash_attn    = 0
0.00.319.425 I llama_init_from_model: freq_base     = 10000.0
0.00.319.425 I llama_init_from_model: freq_scale    = 1
0.00.319.426 I ggml_metal_init: allocating
0.00.319.433 I ggml_metal_init: found device: Apple M4
0.00.319.436 I ggml_metal_init: picking default device: Apple M4
0.00.320.325 I ggml_metal_init: using embedded metal library
0.00.322.959 I ggml_metal_init: GPU name:   Apple M4
0.00.322.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.322.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.322.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.322.961 I ggml_metal_init: simdgroup reduction   = true
0.00.322.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.322.962 I ggml_metal_init: has residency sets    = true
0.00.322.962 I ggml_metal_init: has bfloat            = true
0.00.322.962 I ggml_metal_init: use bfloat            = true
0.00.322.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.322.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.332.754 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.335.824 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.335.826 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.335.848 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.342.086 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.342.088 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.342.088 I llama_init_from_model: graph nodes  = 154
0.00.342.088 I llama_init_from_model: graph splits = 2
0.00.342.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.342.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.349.497 I 
0.00.349.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.349.781 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.349.782 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.349.793 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.349.794 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.349.800 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.349.800 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.350.311 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.353.885 I llama_perf_context_print:        load time =     328.40 ms
0.00.353.886 I llama_perf_context_print: prompt eval time =       3.57 ms /    62 tokens (    0.06 ms per token, 17381.55 tokens per second)
0.00.353.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.353.887 I llama_perf_context_print:       total time =       4.39 ms /    63 tokens
0.00.354.069 I ggml_metal_free: deallocating

real	0m1.049s
user	0m0.325s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.137 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.320 I main: llama backend init
0.00.000.327 I main: load the model and apply lora adapter, if any
0.00.071.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.085.340 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.085.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.085.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.085.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.085.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.085.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.085.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.085.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.085.383 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.085.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.085.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.085.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.085.386 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.085.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.085.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.085.394 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.085.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.092.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.094.647 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.102.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.102.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.102.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.102.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.102.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.102.779 I llama_model_loader: - type  f32:  194 tensors
0.00.102.780 I llama_model_loader: - type  f16:   98 tensors
0.00.102.791 I print_info: file format = GGUF V3 (latest)
0.00.102.793 I print_info: file type   = all F32 (guessed)
0.00.102.795 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.119.867 I load: special tokens cache size = 25
0.00.129.801 I load: token to piece cache size = 0.2984 MB
0.00.129.807 I print_info: arch             = gptneox
0.00.129.808 I print_info: vocab_only       = 0
0.00.129.809 I print_info: n_ctx_train      = 2048
0.00.129.810 I print_info: n_embd           = 2048
0.00.129.810 I print_info: n_layer          = 24
0.00.129.816 I print_info: n_head           = 16
0.00.129.817 I print_info: n_head_kv        = 16
0.00.129.817 I print_info: n_rot            = 32
0.00.129.817 I print_info: n_swa            = 0
0.00.129.818 I print_info: n_embd_head_k    = 128
0.00.129.818 I print_info: n_embd_head_v    = 128
0.00.129.819 I print_info: n_gqa            = 1
0.00.129.820 I print_info: n_embd_k_gqa     = 2048
0.00.129.821 I print_info: n_embd_v_gqa     = 2048
0.00.129.821 I print_info: f_norm_eps       = 1.0e-05
0.00.129.822 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.129.822 I print_info: f_clamp_kqv      = 0.0e+00
0.00.129.822 I print_info: f_max_alibi_bias = 0.0e+00
0.00.129.822 I print_info: f_logit_scale    = 0.0e+00
0.00.129.826 I print_info: n_ff             = 8192
0.00.129.826 I print_info: n_expert         = 0
0.00.129.826 I print_info: n_expert_used    = 0
0.00.129.826 I print_info: causal attn      = 1
0.00.129.827 I print_info: pooling type     = 0
0.00.129.827 I print_info: rope type        = 2
0.00.129.827 I print_info: rope scaling     = linear
0.00.129.828 I print_info: freq_base_train  = 10000.0
0.00.129.828 I print_info: freq_scale_train = 1
0.00.129.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.129.830 I print_info: rope_finetuned   = unknown
0.00.129.830 I print_info: ssm_d_conv       = 0
0.00.129.830 I print_info: ssm_d_inner      = 0
0.00.129.831 I print_info: ssm_d_state      = 0
0.00.129.831 I print_info: ssm_dt_rank      = 0
0.00.129.831 I print_info: ssm_dt_b_c_rms   = 0
0.00.129.831 I print_info: model type       = 1.4B
0.00.129.832 I print_info: model params     = 1.41 B
0.00.129.832 I print_info: general.name     = 1.4B
0.00.129.833 I print_info: vocab type       = BPE
0.00.129.833 I print_info: n_vocab          = 50304
0.00.129.833 I print_info: n_merges         = 50009
0.00.129.833 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.129.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.129.835 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.129.835 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.129.836 I print_info: LF token         = 187 ''
0.00.129.836 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.129.836 I print_info: max token length = 1024
0.00.129.837 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.228.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.228.873 I load_tensors: offloading output layer to GPU
0.00.228.873 I load_tensors: offloaded 25/25 layers to GPU
0.00.228.905 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.228.906 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.229.484 I llama_init_from_model: n_seq_max     = 1
0.00.229.485 I llama_init_from_model: n_ctx         = 2048
0.00.229.485 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.229.485 I llama_init_from_model: n_batch       = 2048
0.00.229.485 I llama_init_from_model: n_ubatch      = 512
0.00.229.485 I llama_init_from_model: flash_attn    = 0
0.00.229.486 I llama_init_from_model: freq_base     = 10000.0
0.00.229.486 I llama_init_from_model: freq_scale    = 1
0.00.229.488 I ggml_metal_init: allocating
0.00.229.540 I ggml_metal_init: found device: Apple M4
0.00.229.547 I ggml_metal_init: picking default device: Apple M4
0.00.230.400 I ggml_metal_init: using embedded metal library
0.00.319.811 I ggml_metal_init: GPU name:   Apple M4
0.00.319.817 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.319.817 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.319.818 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.319.818 I ggml_metal_init: simdgroup reduction   = true
0.00.319.818 I ggml_metal_init: simdgroup matrix mul. = true
0.00.319.818 I ggml_metal_init: has residency sets    = true
0.00.319.818 I ggml_metal_init: has bfloat            = true
0.00.319.818 I ggml_metal_init: use bfloat            = true
0.00.319.819 I ggml_metal_init: hasUnifiedMemory      = true
0.00.319.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.887 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.385.170 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.385.179 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.385.239 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.391.692 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.391.695 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.391.695 I llama_init_from_model: graph nodes  = 967
0.00.391.696 I llama_init_from_model: graph splits = 2
0.00.391.703 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.391.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.391.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.461.606 I main: llama threadpool init, n_threads = 4
0.00.461.668 I 
0.00.461.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.461.710 I 
0.00.461.875 I sampler seed: 1234
0.00.461.881 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.461.906 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.461.907 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.461.907 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.402.559 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.402.559 I llama_perf_context_print:        load time =     388.94 ms
0.02.402.560 I llama_perf_context_print: prompt eval time =      44.25 ms /     7 tokens (    6.32 ms per token,   158.18 tokens per second)
0.02.402.561 I llama_perf_context_print:        eval time =    1893.50 ms /    63 runs   (   30.06 ms per token,    33.27 tokens per second)
0.02.402.562 I llama_perf_context_print:       total time =    1941.84 ms /    70 tokens
0.02.402.847 I ggml_metal_free: deallocating

real	0m2.732s
user	0m0.137s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.599 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.248 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.021 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.038 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.039 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.050 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.001 I llama_model_loader: - type  f32:  194 tensors
0.00.054.001 I llama_model_loader: - type  f16:   98 tensors
0.00.054.002 I print_info: file format = GGUF V3 (latest)
0.00.054.003 I print_info: file type   = all F32 (guessed)
0.00.054.005 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.157 I load: special tokens cache size = 25
0.00.073.633 I load: token to piece cache size = 0.2984 MB
0.00.073.636 I print_info: arch             = gptneox
0.00.073.637 I print_info: vocab_only       = 0
0.00.073.637 I print_info: n_ctx_train      = 2048
0.00.073.637 I print_info: n_embd           = 2048
0.00.073.637 I print_info: n_layer          = 24
0.00.073.641 I print_info: n_head           = 16
0.00.073.642 I print_info: n_head_kv        = 16
0.00.073.642 I print_info: n_rot            = 32
0.00.073.642 I print_info: n_swa            = 0
0.00.073.643 I print_info: n_embd_head_k    = 128
0.00.073.643 I print_info: n_embd_head_v    = 128
0.00.073.644 I print_info: n_gqa            = 1
0.00.073.645 I print_info: n_embd_k_gqa     = 2048
0.00.073.646 I print_info: n_embd_v_gqa     = 2048
0.00.073.646 I print_info: f_norm_eps       = 1.0e-05
0.00.073.647 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.647 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.647 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.647 I print_info: f_logit_scale    = 0.0e+00
0.00.073.648 I print_info: n_ff             = 8192
0.00.073.648 I print_info: n_expert         = 0
0.00.073.648 I print_info: n_expert_used    = 0
0.00.073.648 I print_info: causal attn      = 1
0.00.073.649 I print_info: pooling type     = 0
0.00.073.649 I print_info: rope type        = 2
0.00.073.649 I print_info: rope scaling     = linear
0.00.073.649 I print_info: freq_base_train  = 10000.0
0.00.073.650 I print_info: freq_scale_train = 1
0.00.073.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.650 I print_info: rope_finetuned   = unknown
0.00.073.650 I print_info: ssm_d_conv       = 0
0.00.073.650 I print_info: ssm_d_inner      = 0
0.00.073.651 I print_info: ssm_d_state      = 0
0.00.073.651 I print_info: ssm_dt_rank      = 0
0.00.073.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.651 I print_info: model type       = 1.4B
0.00.073.652 I print_info: model params     = 1.41 B
0.00.073.652 I print_info: general.name     = 1.4B
0.00.073.652 I print_info: vocab type       = BPE
0.00.073.652 I print_info: n_vocab          = 50304
0.00.073.653 I print_info: n_merges         = 50009
0.00.073.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.654 I print_info: LF token         = 187 ''
0.00.073.654 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.654 I print_info: max token length = 1024
0.00.073.655 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.433.345 I load_tensors: offloading 24 repeating layers to GPU
0.01.433.350 I load_tensors: offloading output layer to GPU
0.01.433.350 I load_tensors: offloaded 25/25 layers to GPU
0.01.433.374 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.433.376 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.434.223 I llama_init_from_model: n_seq_max     = 1
0.01.434.224 I llama_init_from_model: n_ctx         = 128
0.01.434.224 I llama_init_from_model: n_ctx_per_seq = 128
0.01.434.224 I llama_init_from_model: n_batch       = 128
0.01.434.225 I llama_init_from_model: n_ubatch      = 128
0.01.434.225 I llama_init_from_model: flash_attn    = 0
0.01.434.226 I llama_init_from_model: freq_base     = 10000.0
0.01.434.226 I llama_init_from_model: freq_scale    = 1
0.01.434.226 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.434.231 I ggml_metal_init: allocating
0.01.434.314 I ggml_metal_init: found device: Apple M4
0.01.434.321 I ggml_metal_init: picking default device: Apple M4
0.01.435.501 I ggml_metal_init: using embedded metal library
0.01.439.336 I ggml_metal_init: GPU name:   Apple M4
0.01.439.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.439.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.439.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.439.339 I ggml_metal_init: simdgroup reduction   = true
0.01.439.339 I ggml_metal_init: simdgroup matrix mul. = true
0.01.439.339 I ggml_metal_init: has residency sets    = true
0.01.439.340 I ggml_metal_init: has bfloat            = true
0.01.439.340 I ggml_metal_init: use bfloat            = true
0.01.439.340 I ggml_metal_init: hasUnifiedMemory      = true
0.01.439.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.450.218 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.451.901 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.451.904 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.451.929 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.453.598 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.453.599 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.453.600 I llama_init_from_model: graph nodes  = 967
0.01.453.600 I llama_init_from_model: graph splits = 2
0.01.453.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.453.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.488.330 I 
0.01.488.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.488.373 I perplexity: tokenizing the input ..
0.01.493.452 I perplexity: tokenization took 5.077 ms
0.01.493.456 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.612.512 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.613.865 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.613.933 I llama_perf_context_print:        load time =    1465.07 ms
0.01.613.934 I llama_perf_context_print: prompt eval time =     118.79 ms /   128 tokens (    0.93 ms per token,  1077.57 tokens per second)
0.01.613.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.613.935 I llama_perf_context_print:       total time =     125.60 ms /   129 tokens
0.01.614.298 I ggml_metal_free: deallocating

real	0m1.798s
user	0m0.096s
sys	0m0.268s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.010.577 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.267 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.033.275 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.278 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.278 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.279 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.280 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.281 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.281 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.282 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.282 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.285 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.162 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.042.162 I llama_model_loader: - type  f32:  194 tensors
0.00.042.163 I llama_model_loader: - type q8_0:   98 tensors
0.00.042.164 I print_info: file format = GGUF V3 (latest)
0.00.042.164 I print_info: file type   = Q8_0
0.00.042.166 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.701 I load: special tokens cache size = 25
0.00.058.939 I load: token to piece cache size = 0.2984 MB
0.00.058.944 I print_info: arch             = gptneox
0.00.058.945 I print_info: vocab_only       = 0
0.00.058.945 I print_info: n_ctx_train      = 2048
0.00.058.947 I print_info: n_embd           = 2048
0.00.058.947 I print_info: n_layer          = 24
0.00.058.955 I print_info: n_head           = 16
0.00.058.956 I print_info: n_head_kv        = 16
0.00.058.956 I print_info: n_rot            = 32
0.00.058.956 I print_info: n_swa            = 0
0.00.058.956 I print_info: n_embd_head_k    = 128
0.00.058.957 I print_info: n_embd_head_v    = 128
0.00.058.958 I print_info: n_gqa            = 1
0.00.058.959 I print_info: n_embd_k_gqa     = 2048
0.00.058.959 I print_info: n_embd_v_gqa     = 2048
0.00.058.960 I print_info: f_norm_eps       = 1.0e-05
0.00.058.961 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.961 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.963 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.963 I print_info: f_logit_scale    = 0.0e+00
0.00.058.964 I print_info: n_ff             = 8192
0.00.058.964 I print_info: n_expert         = 0
0.00.058.964 I print_info: n_expert_used    = 0
0.00.058.964 I print_info: causal attn      = 1
0.00.058.965 I print_info: pooling type     = 0
0.00.058.967 I print_info: rope type        = 2
0.00.058.967 I print_info: rope scaling     = linear
0.00.058.967 I print_info: freq_base_train  = 10000.0
0.00.058.968 I print_info: freq_scale_train = 1
0.00.058.968 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.968 I print_info: rope_finetuned   = unknown
0.00.058.968 I print_info: ssm_d_conv       = 0
0.00.058.968 I print_info: ssm_d_inner      = 0
0.00.058.969 I print_info: ssm_d_state      = 0
0.00.058.969 I print_info: ssm_dt_rank      = 0
0.00.058.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.969 I print_info: model type       = 1.4B
0.00.058.970 I print_info: model params     = 1.41 B
0.00.058.970 I print_info: general.name     = 1.4B
0.00.058.971 I print_info: vocab type       = BPE
0.00.058.971 I print_info: n_vocab          = 50304
0.00.058.971 I print_info: n_merges         = 50009
0.00.058.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.972 I print_info: LF token         = 187 ''
0.00.058.972 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.973 I print_info: max token length = 1024
0.00.058.974 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.087.381 I load_tensors: offloading 24 repeating layers to GPU
0.01.087.386 I load_tensors: offloading output layer to GPU
0.01.087.387 I load_tensors: offloaded 25/25 layers to GPU
0.01.087.411 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.087.415 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.088.607 I llama_init_from_model: n_seq_max     = 1
0.01.088.609 I llama_init_from_model: n_ctx         = 2048
0.01.088.609 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.088.610 I llama_init_from_model: n_batch       = 2048
0.01.088.610 I llama_init_from_model: n_ubatch      = 512
0.01.088.610 I llama_init_from_model: flash_attn    = 0
0.01.088.611 I llama_init_from_model: freq_base     = 10000.0
0.01.088.612 I llama_init_from_model: freq_scale    = 1
0.01.088.613 I ggml_metal_init: allocating
0.01.088.630 I ggml_metal_init: found device: Apple M4
0.01.088.638 I ggml_metal_init: picking default device: Apple M4
0.01.089.935 I ggml_metal_init: using embedded metal library
0.01.095.730 I ggml_metal_init: GPU name:   Apple M4
0.01.095.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.095.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.095.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.095.735 I ggml_metal_init: simdgroup reduction   = true
0.01.095.735 I ggml_metal_init: simdgroup matrix mul. = true
0.01.095.736 I ggml_metal_init: has residency sets    = true
0.01.095.736 I ggml_metal_init: has bfloat            = true
0.01.095.736 I ggml_metal_init: use bfloat            = true
0.01.095.737 I ggml_metal_init: hasUnifiedMemory      = true
0.01.095.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.113.215 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.162.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.162.144 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.162.179 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.166.940 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.166.942 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.166.942 I llama_init_from_model: graph nodes  = 967
0.01.166.942 I llama_init_from_model: graph splits = 2
0.01.166.947 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.167.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.167.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.214.635 I main: llama threadpool init, n_threads = 4
0.01.214.677 I 
0.01.214.699 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.214.699 I 
0.01.214.819 I sampler seed: 1234
0.01.214.823 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.214.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.214.833 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.214.834 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.370.004 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.02.370.004 I llama_perf_context_print:        load time =    1203.35 ms
0.02.370.005 I llama_perf_context_print: prompt eval time =      47.80 ms /     7 tokens (    6.83 ms per token,   146.44 tokens per second)
0.02.370.006 I llama_perf_context_print:        eval time =    1104.45 ms /    63 runs   (   17.53 ms per token,    57.04 tokens per second)
0.02.370.007 I llama_perf_context_print:       total time =    1156.08 ms /    70 tokens
0.02.370.256 I ggml_metal_free: deallocating

real	0m2.389s
user	0m0.114s
sys	0m0.280s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.400 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.619 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.624 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.626 I llama_model_loader: - type  f32:  194 tensors
0.00.027.627 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.627 I print_info: file format = GGUF V3 (latest)
0.00.027.628 I print_info: file type   = Q8_0
0.00.027.629 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.424 I load: special tokens cache size = 25
0.00.042.536 I load: token to piece cache size = 0.2984 MB
0.00.042.540 I print_info: arch             = gptneox
0.00.042.541 I print_info: vocab_only       = 0
0.00.042.541 I print_info: n_ctx_train      = 2048
0.00.042.541 I print_info: n_embd           = 2048
0.00.042.541 I print_info: n_layer          = 24
0.00.042.545 I print_info: n_head           = 16
0.00.042.546 I print_info: n_head_kv        = 16
0.00.042.546 I print_info: n_rot            = 32
0.00.042.547 I print_info: n_swa            = 0
0.00.042.547 I print_info: n_embd_head_k    = 128
0.00.042.547 I print_info: n_embd_head_v    = 128
0.00.042.548 I print_info: n_gqa            = 1
0.00.042.549 I print_info: n_embd_k_gqa     = 2048
0.00.042.549 I print_info: n_embd_v_gqa     = 2048
0.00.042.550 I print_info: f_norm_eps       = 1.0e-05
0.00.042.550 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.551 I print_info: f_logit_scale    = 0.0e+00
0.00.042.551 I print_info: n_ff             = 8192
0.00.042.551 I print_info: n_expert         = 0
0.00.042.551 I print_info: n_expert_used    = 0
0.00.042.552 I print_info: causal attn      = 1
0.00.042.552 I print_info: pooling type     = 0
0.00.042.552 I print_info: rope type        = 2
0.00.042.552 I print_info: rope scaling     = linear
0.00.042.552 I print_info: freq_base_train  = 10000.0
0.00.042.552 I print_info: freq_scale_train = 1
0.00.042.555 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.555 I print_info: rope_finetuned   = unknown
0.00.042.555 I print_info: ssm_d_conv       = 0
0.00.042.556 I print_info: ssm_d_inner      = 0
0.00.042.556 I print_info: ssm_d_state      = 0
0.00.042.556 I print_info: ssm_dt_rank      = 0
0.00.042.556 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.557 I print_info: model type       = 1.4B
0.00.042.558 I print_info: model params     = 1.41 B
0.00.042.558 I print_info: general.name     = 1.4B
0.00.042.558 I print_info: vocab type       = BPE
0.00.042.558 I print_info: n_vocab          = 50304
0.00.042.558 I print_info: n_merges         = 50009
0.00.042.559 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.559 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.559 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.559 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.559 I print_info: LF token         = 187 ''
0.00.042.560 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.560 I print_info: max token length = 1024
0.00.042.560 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.420 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.427 I load_tensors: offloading output layer to GPU
0.00.666.427 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.457 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.666.459 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.667.973 I llama_init_from_model: n_seq_max     = 1
0.00.667.975 I llama_init_from_model: n_ctx         = 128
0.00.667.975 I llama_init_from_model: n_ctx_per_seq = 128
0.00.667.976 I llama_init_from_model: n_batch       = 128
0.00.667.976 I llama_init_from_model: n_ubatch      = 128
0.00.667.976 I llama_init_from_model: flash_attn    = 0
0.00.667.977 I llama_init_from_model: freq_base     = 10000.0
0.00.667.977 I llama_init_from_model: freq_scale    = 1
0.00.667.978 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.667.979 I ggml_metal_init: allocating
0.00.668.044 I ggml_metal_init: found device: Apple M4
0.00.668.052 I ggml_metal_init: picking default device: Apple M4
0.00.669.388 I ggml_metal_init: using embedded metal library
0.00.674.535 I ggml_metal_init: GPU name:   Apple M4
0.00.674.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.540 I ggml_metal_init: simdgroup reduction   = true
0.00.674.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.540 I ggml_metal_init: has residency sets    = true
0.00.674.540 I ggml_metal_init: has bfloat            = true
0.00.674.541 I ggml_metal_init: use bfloat            = true
0.00.674.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.602 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.874 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.692.882 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.692.937 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.081 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.696.082 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.696.082 I llama_init_from_model: graph nodes  = 967
0.00.696.083 I llama_init_from_model: graph splits = 2
0.00.696.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.696.086 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.560 I 
0.00.721.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.602 I perplexity: tokenizing the input ..
0.00.727.124 I perplexity: tokenization took 5.52 ms
0.00.727.129 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.688 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.866.027 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.866.055 I llama_perf_context_print:        load time =     709.94 ms
0.00.866.056 I llama_perf_context_print: prompt eval time =     137.33 ms /   128 tokens (    1.07 ms per token,   932.08 tokens per second)
0.00.866.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.057 I llama_perf_context_print:       total time =     144.50 ms /   129 tokens
0.00.866.467 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.075s
sys	0m0.141s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.018.520 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.962 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.964 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.965 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.965 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.966 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.966 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.050.060 I llama_model_loader: - type  f32:  194 tensors
0.00.050.060 I llama_model_loader: - type q4_0:   97 tensors
0.00.050.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.067 I print_info: file format = GGUF V3 (latest)
0.00.050.067 I print_info: file type   = Q4_0
0.00.050.069 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.178 I load: special tokens cache size = 25
0.00.072.536 I load: token to piece cache size = 0.2984 MB
0.00.072.540 I print_info: arch             = gptneox
0.00.072.541 I print_info: vocab_only       = 0
0.00.072.541 I print_info: n_ctx_train      = 2048
0.00.072.541 I print_info: n_embd           = 2048
0.00.072.542 I print_info: n_layer          = 24
0.00.072.546 I print_info: n_head           = 16
0.00.072.547 I print_info: n_head_kv        = 16
0.00.072.548 I print_info: n_rot            = 32
0.00.072.548 I print_info: n_swa            = 0
0.00.072.548 I print_info: n_embd_head_k    = 128
0.00.072.548 I print_info: n_embd_head_v    = 128
0.00.072.549 I print_info: n_gqa            = 1
0.00.072.550 I print_info: n_embd_k_gqa     = 2048
0.00.072.551 I print_info: n_embd_v_gqa     = 2048
0.00.072.552 I print_info: f_norm_eps       = 1.0e-05
0.00.072.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.553 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.553 I print_info: f_logit_scale    = 0.0e+00
0.00.072.554 I print_info: n_ff             = 8192
0.00.072.555 I print_info: n_expert         = 0
0.00.072.555 I print_info: n_expert_used    = 0
0.00.072.556 I print_info: causal attn      = 1
0.00.072.556 I print_info: pooling type     = 0
0.00.072.556 I print_info: rope type        = 2
0.00.072.557 I print_info: rope scaling     = linear
0.00.072.557 I print_info: freq_base_train  = 10000.0
0.00.072.558 I print_info: freq_scale_train = 1
0.00.072.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.558 I print_info: rope_finetuned   = unknown
0.00.072.558 I print_info: ssm_d_conv       = 0
0.00.072.558 I print_info: ssm_d_inner      = 0
0.00.072.559 I print_info: ssm_d_state      = 0
0.00.072.559 I print_info: ssm_dt_rank      = 0
0.00.072.559 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.559 I print_info: model type       = 1.4B
0.00.072.560 I print_info: model params     = 1.41 B
0.00.072.560 I print_info: general.name     = 1.4B
0.00.072.561 I print_info: vocab type       = BPE
0.00.072.564 I print_info: n_vocab          = 50304
0.00.072.564 I print_info: n_merges         = 50009
0.00.072.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.565 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.565 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.567 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.567 I print_info: LF token         = 187 ''
0.00.072.567 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.568 I print_info: max token length = 1024
0.00.072.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.328 I load_tensors: offloading output layer to GPU
0.00.614.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.362 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.614.363 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.616.019 I llama_init_from_model: n_seq_max     = 1
0.00.616.022 I llama_init_from_model: n_ctx         = 2048
0.00.616.023 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.023 I llama_init_from_model: n_batch       = 2048
0.00.616.024 I llama_init_from_model: n_ubatch      = 512
0.00.616.024 I llama_init_from_model: flash_attn    = 0
0.00.616.027 I llama_init_from_model: freq_base     = 10000.0
0.00.616.031 I llama_init_from_model: freq_scale    = 1
0.00.616.034 I ggml_metal_init: allocating
0.00.616.106 I ggml_metal_init: found device: Apple M4
0.00.616.119 I ggml_metal_init: picking default device: Apple M4
0.00.617.996 I ggml_metal_init: using embedded metal library
0.00.623.373 I ggml_metal_init: GPU name:   Apple M4
0.00.623.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.380 I ggml_metal_init: simdgroup reduction   = true
0.00.623.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.381 I ggml_metal_init: has residency sets    = true
0.00.623.381 I ggml_metal_init: has bfloat            = true
0.00.623.381 I ggml_metal_init: use bfloat            = true
0.00.623.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.384 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.834 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.736 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.747 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.793 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.005 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.006 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.006 I llama_init_from_model: graph nodes  = 967
0.00.698.007 I llama_init_from_model: graph splits = 2
0.00.698.012 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.994 I main: llama threadpool init, n_threads = 4
0.00.751.031 I 
0.00.751.052 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.055 I 
0.00.751.214 I sampler seed: 1234
0.00.751.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.229 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.230 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.442.036 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50000.00 tokens per second)
0.01.442.037 I llama_perf_context_print:        load time =     731.76 ms
0.01.442.038 I llama_perf_context_print: prompt eval time =      49.77 ms /     7 tokens (    7.11 ms per token,   140.65 tokens per second)
0.01.442.039 I llama_perf_context_print:        eval time =     638.15 ms /    63 runs   (   10.13 ms per token,    98.72 tokens per second)
0.01.442.039 I llama_perf_context_print:       total time =     691.76 ms /    70 tokens
0.01.442.264 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.127s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.287 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.844 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.828 I llama_model_loader: - type  f32:  194 tensors
0.00.025.829 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.829 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.830 I print_info: file format = GGUF V3 (latest)
0.00.025.830 I print_info: file type   = Q4_0
0.00.025.831 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.969 I load: special tokens cache size = 25
0.00.040.096 I load: token to piece cache size = 0.2984 MB
0.00.040.100 I print_info: arch             = gptneox
0.00.040.100 I print_info: vocab_only       = 0
0.00.040.101 I print_info: n_ctx_train      = 2048
0.00.040.101 I print_info: n_embd           = 2048
0.00.040.101 I print_info: n_layer          = 24
0.00.040.105 I print_info: n_head           = 16
0.00.040.106 I print_info: n_head_kv        = 16
0.00.040.106 I print_info: n_rot            = 32
0.00.040.107 I print_info: n_swa            = 0
0.00.040.107 I print_info: n_embd_head_k    = 128
0.00.040.107 I print_info: n_embd_head_v    = 128
0.00.040.108 I print_info: n_gqa            = 1
0.00.040.108 I print_info: n_embd_k_gqa     = 2048
0.00.040.109 I print_info: n_embd_v_gqa     = 2048
0.00.040.110 I print_info: f_norm_eps       = 1.0e-05
0.00.040.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.110 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.111 I print_info: f_logit_scale    = 0.0e+00
0.00.040.111 I print_info: n_ff             = 8192
0.00.040.112 I print_info: n_expert         = 0
0.00.040.114 I print_info: n_expert_used    = 0
0.00.040.114 I print_info: causal attn      = 1
0.00.040.115 I print_info: pooling type     = 0
0.00.040.115 I print_info: rope type        = 2
0.00.040.115 I print_info: rope scaling     = linear
0.00.040.115 I print_info: freq_base_train  = 10000.0
0.00.040.116 I print_info: freq_scale_train = 1
0.00.040.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.116 I print_info: rope_finetuned   = unknown
0.00.040.116 I print_info: ssm_d_conv       = 0
0.00.040.116 I print_info: ssm_d_inner      = 0
0.00.040.116 I print_info: ssm_d_state      = 0
0.00.040.116 I print_info: ssm_dt_rank      = 0
0.00.040.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.117 I print_info: model type       = 1.4B
0.00.040.117 I print_info: model params     = 1.41 B
0.00.040.117 I print_info: general.name     = 1.4B
0.00.040.117 I print_info: vocab type       = BPE
0.00.040.118 I print_info: n_vocab          = 50304
0.00.040.118 I print_info: n_merges         = 50009
0.00.040.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.119 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: LF token         = 187 ''
0.00.040.120 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.120 I print_info: max token length = 1024
0.00.040.121 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.541.450 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.465 I load_tensors: offloading output layer to GPU
0.00.541.466 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.500 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.541.501 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.543.216 I llama_init_from_model: n_seq_max     = 1
0.00.543.219 I llama_init_from_model: n_ctx         = 128
0.00.543.219 I llama_init_from_model: n_ctx_per_seq = 128
0.00.543.220 I llama_init_from_model: n_batch       = 128
0.00.543.220 I llama_init_from_model: n_ubatch      = 128
0.00.543.221 I llama_init_from_model: flash_attn    = 0
0.00.543.223 I llama_init_from_model: freq_base     = 10000.0
0.00.543.223 I llama_init_from_model: freq_scale    = 1
0.00.543.224 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.543.227 I ggml_metal_init: allocating
0.00.543.313 I ggml_metal_init: found device: Apple M4
0.00.543.330 I ggml_metal_init: picking default device: Apple M4
0.00.545.110 I ggml_metal_init: using embedded metal library
0.00.551.891 I ggml_metal_init: GPU name:   Apple M4
0.00.551.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.551.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.551.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.551.901 I ggml_metal_init: simdgroup reduction   = true
0.00.551.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.551.902 I ggml_metal_init: has residency sets    = true
0.00.551.902 I ggml_metal_init: has bfloat            = true
0.00.551.902 I ggml_metal_init: use bfloat            = true
0.00.551.903 I ggml_metal_init: hasUnifiedMemory      = true
0.00.551.906 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.312 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.573.814 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.573.822 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.573.871 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.143 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.577.145 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.577.146 I llama_init_from_model: graph nodes  = 967
0.00.577.146 I llama_init_from_model: graph splits = 2
0.00.577.150 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.577.153 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.063 I 
0.00.603.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.131 I perplexity: tokenizing the input ..
0.00.609.018 I perplexity: tokenization took 5.888 ms
0.00.609.023 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.731 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.742.508 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.742.532 I llama_perf_context_print:        load time =     593.21 ms
0.00.742.534 I llama_perf_context_print: prompt eval time =     131.47 ms /   128 tokens (    1.03 ms per token,   973.57 tokens per second)
0.00.742.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.742.535 I llama_perf_context_print:       total time =     139.47 ms /   129 tokens
0.00.742.918 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.076s
sys	0m0.121s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.810 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.288 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.908 I llama_model_loader: - type  f32:  194 tensors
0.00.030.908 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.908 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.909 I print_info: file format = GGUF V3 (latest)
0.00.030.909 I print_info: file type   = Q4_1
0.00.030.910 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.039.202 I load: special tokens cache size = 25
0.00.045.408 I load: token to piece cache size = 0.2984 MB
0.00.045.411 I print_info: arch             = gptneox
0.00.045.411 I print_info: vocab_only       = 0
0.00.045.411 I print_info: n_ctx_train      = 2048
0.00.045.411 I print_info: n_embd           = 2048
0.00.045.411 I print_info: n_layer          = 24
0.00.045.414 I print_info: n_head           = 16
0.00.045.415 I print_info: n_head_kv        = 16
0.00.045.415 I print_info: n_rot            = 32
0.00.045.415 I print_info: n_swa            = 0
0.00.045.415 I print_info: n_embd_head_k    = 128
0.00.045.415 I print_info: n_embd_head_v    = 128
0.00.045.416 I print_info: n_gqa            = 1
0.00.045.417 I print_info: n_embd_k_gqa     = 2048
0.00.045.417 I print_info: n_embd_v_gqa     = 2048
0.00.045.418 I print_info: f_norm_eps       = 1.0e-05
0.00.045.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.418 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.419 I print_info: f_logit_scale    = 0.0e+00
0.00.045.420 I print_info: n_ff             = 8192
0.00.045.420 I print_info: n_expert         = 0
0.00.045.420 I print_info: n_expert_used    = 0
0.00.045.420 I print_info: causal attn      = 1
0.00.045.420 I print_info: pooling type     = 0
0.00.045.420 I print_info: rope type        = 2
0.00.045.421 I print_info: rope scaling     = linear
0.00.045.421 I print_info: freq_base_train  = 10000.0
0.00.045.421 I print_info: freq_scale_train = 1
0.00.045.421 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.422 I print_info: rope_finetuned   = unknown
0.00.045.422 I print_info: ssm_d_conv       = 0
0.00.045.422 I print_info: ssm_d_inner      = 0
0.00.045.422 I print_info: ssm_d_state      = 0
0.00.045.422 I print_info: ssm_dt_rank      = 0
0.00.045.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.423 I print_info: model type       = 1.4B
0.00.045.423 I print_info: model params     = 1.41 B
0.00.045.423 I print_info: general.name     = 1.4B
0.00.045.426 I print_info: vocab type       = BPE
0.00.045.426 I print_info: n_vocab          = 50304
0.00.045.427 I print_info: n_merges         = 50009
0.00.045.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.428 I print_info: LF token         = 187 ''
0.00.045.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.428 I print_info: max token length = 1024
0.00.045.428 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.215 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.227 I load_tensors: offloading output layer to GPU
0.00.602.228 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.259 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.602.260 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.603.742 I llama_init_from_model: n_seq_max     = 1
0.00.603.745 I llama_init_from_model: n_ctx         = 2048
0.00.603.746 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.747 I llama_init_from_model: n_batch       = 2048
0.00.603.748 I llama_init_from_model: n_ubatch      = 512
0.00.603.748 I llama_init_from_model: flash_attn    = 0
0.00.603.750 I llama_init_from_model: freq_base     = 10000.0
0.00.603.751 I llama_init_from_model: freq_scale    = 1
0.00.603.754 I ggml_metal_init: allocating
0.00.603.822 I ggml_metal_init: found device: Apple M4
0.00.603.835 I ggml_metal_init: picking default device: Apple M4
0.00.605.824 I ggml_metal_init: using embedded metal library
0.00.612.554 I ggml_metal_init: GPU name:   Apple M4
0.00.612.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.560 I ggml_metal_init: simdgroup reduction   = true
0.00.612.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.561 I ggml_metal_init: has residency sets    = true
0.00.612.561 I ggml_metal_init: has bfloat            = true
0.00.612.561 I ggml_metal_init: use bfloat            = true
0.00.612.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.609 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.687.249 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.687.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.782 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.691.784 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.691.785 I llama_init_from_model: graph nodes  = 967
0.00.691.785 I llama_init_from_model: graph splits = 2
0.00.691.791 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.920 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.290 I main: llama threadpool init, n_threads = 4
0.00.747.329 I 
0.00.747.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.351 I 
0.00.747.500 I sampler seed: 1234
0.00.747.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.543 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.546 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.546 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.486.855 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.486.855 I llama_perf_context_print:        load time =     737.77 ms
0.01.486.856 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.35 tokens per second)
0.01.486.858 I llama_perf_context_print:        eval time =     687.45 ms /    63 runs   (   10.91 ms per token,    91.64 tokens per second)
0.01.486.858 I llama_perf_context_print:       total time =     740.27 ms /    70 tokens
0.01.487.107 I ggml_metal_free: deallocating

real	0m1.505s
user	0m0.111s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.417 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.423 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.429 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.430 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.432 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.218 I llama_model_loader: - type  f32:  194 tensors
0.00.025.218 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.218 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.219 I print_info: file format = GGUF V3 (latest)
0.00.025.219 I print_info: file type   = Q4_1
0.00.025.222 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.282 I load: special tokens cache size = 25
0.00.039.293 I load: token to piece cache size = 0.2984 MB
0.00.039.300 I print_info: arch             = gptneox
0.00.039.302 I print_info: vocab_only       = 0
0.00.039.302 I print_info: n_ctx_train      = 2048
0.00.039.302 I print_info: n_embd           = 2048
0.00.039.302 I print_info: n_layer          = 24
0.00.039.306 I print_info: n_head           = 16
0.00.039.309 I print_info: n_head_kv        = 16
0.00.039.309 I print_info: n_rot            = 32
0.00.039.309 I print_info: n_swa            = 0
0.00.039.309 I print_info: n_embd_head_k    = 128
0.00.039.309 I print_info: n_embd_head_v    = 128
0.00.039.310 I print_info: n_gqa            = 1
0.00.039.310 I print_info: n_embd_k_gqa     = 2048
0.00.039.311 I print_info: n_embd_v_gqa     = 2048
0.00.039.312 I print_info: f_norm_eps       = 1.0e-05
0.00.039.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.312 I print_info: f_logit_scale    = 0.0e+00
0.00.039.313 I print_info: n_ff             = 8192
0.00.039.313 I print_info: n_expert         = 0
0.00.039.313 I print_info: n_expert_used    = 0
0.00.039.313 I print_info: causal attn      = 1
0.00.039.313 I print_info: pooling type     = 0
0.00.039.313 I print_info: rope type        = 2
0.00.039.314 I print_info: rope scaling     = linear
0.00.039.314 I print_info: freq_base_train  = 10000.0
0.00.039.314 I print_info: freq_scale_train = 1
0.00.039.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.315 I print_info: rope_finetuned   = unknown
0.00.039.315 I print_info: ssm_d_conv       = 0
0.00.039.316 I print_info: ssm_d_inner      = 0
0.00.039.316 I print_info: ssm_d_state      = 0
0.00.039.316 I print_info: ssm_dt_rank      = 0
0.00.039.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.317 I print_info: model type       = 1.4B
0.00.039.317 I print_info: model params     = 1.41 B
0.00.039.317 I print_info: general.name     = 1.4B
0.00.039.318 I print_info: vocab type       = BPE
0.00.039.318 I print_info: n_vocab          = 50304
0.00.039.318 I print_info: n_merges         = 50009
0.00.039.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.318 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.318 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: LF token         = 187 ''
0.00.039.319 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: max token length = 1024
0.00.039.320 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.222 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.231 I load_tensors: offloading output layer to GPU
0.00.628.232 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.267 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.269 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.629.608 I llama_init_from_model: n_seq_max     = 1
0.00.629.615 I llama_init_from_model: n_ctx         = 128
0.00.629.616 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.617 I llama_init_from_model: n_batch       = 128
0.00.629.617 I llama_init_from_model: n_ubatch      = 128
0.00.629.617 I llama_init_from_model: flash_attn    = 0
0.00.629.619 I llama_init_from_model: freq_base     = 10000.0
0.00.629.620 I llama_init_from_model: freq_scale    = 1
0.00.629.620 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.623 I ggml_metal_init: allocating
0.00.629.699 I ggml_metal_init: found device: Apple M4
0.00.629.712 I ggml_metal_init: picking default device: Apple M4
0.00.631.572 I ggml_metal_init: using embedded metal library
0.00.638.431 I ggml_metal_init: GPU name:   Apple M4
0.00.638.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.441 I ggml_metal_init: simdgroup reduction   = true
0.00.638.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.441 I ggml_metal_init: has residency sets    = true
0.00.638.442 I ggml_metal_init: has bfloat            = true
0.00.638.442 I ggml_metal_init: use bfloat            = true
0.00.638.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.425 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.871 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.875 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.961 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.099 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.101 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.102 I llama_init_from_model: graph nodes  = 967
0.00.663.102 I llama_init_from_model: graph splits = 2
0.00.663.106 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.108 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.067 I 
0.00.693.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.156 I perplexity: tokenizing the input ..
0.00.700.495 I perplexity: tokenization took 7.335 ms
0.00.700.504 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.106 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.838.438 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.838.468 I llama_perf_context_print:        load time =     684.00 ms
0.00.838.473 I llama_perf_context_print: prompt eval time =     135.68 ms /   128 tokens (    1.06 ms per token,   943.43 tokens per second)
0.00.838.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.475 I llama_perf_context_print:       total time =     145.40 ms /   129 tokens
0.00.838.901 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.080s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.021.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.343 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.344 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.346 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.349 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.046.256 I llama_model_loader: - type  f32:  194 tensors
0.00.046.256 I llama_model_loader: - type q5_0:   97 tensors
0.00.046.257 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.257 I print_info: file format = GGUF V3 (latest)
0.00.046.259 I print_info: file type   = Q5_0
0.00.046.263 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.057.172 I load: special tokens cache size = 25
0.00.064.297 I load: token to piece cache size = 0.2984 MB
0.00.064.302 I print_info: arch             = gptneox
0.00.064.302 I print_info: vocab_only       = 0
0.00.064.302 I print_info: n_ctx_train      = 2048
0.00.064.302 I print_info: n_embd           = 2048
0.00.064.303 I print_info: n_layer          = 24
0.00.064.307 I print_info: n_head           = 16
0.00.064.307 I print_info: n_head_kv        = 16
0.00.064.308 I print_info: n_rot            = 32
0.00.064.308 I print_info: n_swa            = 0
0.00.064.308 I print_info: n_embd_head_k    = 128
0.00.064.308 I print_info: n_embd_head_v    = 128
0.00.064.309 I print_info: n_gqa            = 1
0.00.064.309 I print_info: n_embd_k_gqa     = 2048
0.00.064.310 I print_info: n_embd_v_gqa     = 2048
0.00.064.311 I print_info: f_norm_eps       = 1.0e-05
0.00.064.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.311 I print_info: f_logit_scale    = 0.0e+00
0.00.064.312 I print_info: n_ff             = 8192
0.00.064.312 I print_info: n_expert         = 0
0.00.064.312 I print_info: n_expert_used    = 0
0.00.064.312 I print_info: causal attn      = 1
0.00.064.313 I print_info: pooling type     = 0
0.00.064.314 I print_info: rope type        = 2
0.00.064.318 I print_info: rope scaling     = linear
0.00.064.319 I print_info: freq_base_train  = 10000.0
0.00.064.319 I print_info: freq_scale_train = 1
0.00.064.319 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.319 I print_info: rope_finetuned   = unknown
0.00.064.319 I print_info: ssm_d_conv       = 0
0.00.064.320 I print_info: ssm_d_inner      = 0
0.00.064.320 I print_info: ssm_d_state      = 0
0.00.064.320 I print_info: ssm_dt_rank      = 0
0.00.064.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.320 I print_info: model type       = 1.4B
0.00.064.321 I print_info: model params     = 1.41 B
0.00.064.321 I print_info: general.name     = 1.4B
0.00.064.321 I print_info: vocab type       = BPE
0.00.064.322 I print_info: n_vocab          = 50304
0.00.064.322 I print_info: n_merges         = 50009
0.00.064.322 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.323 I print_info: LF token         = 187 ''
0.00.064.323 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.323 I print_info: max token length = 1024
0.00.064.324 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.293.907 I load_tensors: offloading 24 repeating layers to GPU
0.01.293.911 I load_tensors: offloading output layer to GPU
0.01.293.911 I load_tensors: offloaded 25/25 layers to GPU
0.01.293.929 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.01.293.930 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.01.294.893 I llama_init_from_model: n_seq_max     = 1
0.01.294.895 I llama_init_from_model: n_ctx         = 2048
0.01.294.896 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.294.896 I llama_init_from_model: n_batch       = 2048
0.01.294.896 I llama_init_from_model: n_ubatch      = 512
0.01.294.896 I llama_init_from_model: flash_attn    = 0
0.01.294.898 I llama_init_from_model: freq_base     = 10000.0
0.01.294.898 I llama_init_from_model: freq_scale    = 1
0.01.294.906 I ggml_metal_init: allocating
0.01.294.941 I ggml_metal_init: found device: Apple M4
0.01.294.952 I ggml_metal_init: picking default device: Apple M4
0.01.296.128 I ggml_metal_init: using embedded metal library
0.01.300.507 I ggml_metal_init: GPU name:   Apple M4
0.01.300.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.300.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.300.514 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.300.514 I ggml_metal_init: simdgroup reduction   = true
0.01.300.515 I ggml_metal_init: simdgroup matrix mul. = true
0.01.300.515 I ggml_metal_init: has residency sets    = true
0.01.300.515 I ggml_metal_init: has bfloat            = true
0.01.300.515 I ggml_metal_init: use bfloat            = true
0.01.300.517 I ggml_metal_init: hasUnifiedMemory      = true
0.01.300.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.314.738 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.346.743 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.346.749 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.346.785 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.351.292 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.351.295 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.351.295 I llama_init_from_model: graph nodes  = 967
0.01.351.295 I llama_init_from_model: graph splits = 2
0.01.351.301 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.351.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.351.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.412.579 I main: llama threadpool init, n_threads = 4
0.01.412.621 I 
0.01.412.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.412.643 I 
0.01.412.801 I sampler seed: 1234
0.01.412.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.412.816 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.412.817 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.412.817 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.207.787 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.02.207.788 I llama_perf_context_print:        load time =    1389.93 ms
0.02.207.789 I llama_perf_context_print: prompt eval time =      53.24 ms /     7 tokens (    7.61 ms per token,   131.47 tokens per second)
0.02.207.790 I llama_perf_context_print:        eval time =     738.77 ms /    63 runs   (   11.73 ms per token,    85.28 tokens per second)
0.02.207.791 I llama_perf_context_print:       total time =     795.94 ms /    70 tokens
0.02.207.994 I ggml_metal_free: deallocating

real	0m2.236s
user	0m0.114s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.953 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.883 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.884 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.885 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.889 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.891 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.891 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.611 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.614 I llama_model_loader: - type  f32:  194 tensors
0.00.025.615 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.615 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.616 I print_info: file format = GGUF V3 (latest)
0.00.025.616 I print_info: file type   = Q5_0
0.00.025.617 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.810 I load: special tokens cache size = 25
0.00.039.936 I load: token to piece cache size = 0.2984 MB
0.00.039.941 I print_info: arch             = gptneox
0.00.039.941 I print_info: vocab_only       = 0
0.00.039.941 I print_info: n_ctx_train      = 2048
0.00.039.942 I print_info: n_embd           = 2048
0.00.039.942 I print_info: n_layer          = 24
0.00.039.947 I print_info: n_head           = 16
0.00.039.947 I print_info: n_head_kv        = 16
0.00.039.948 I print_info: n_rot            = 32
0.00.039.948 I print_info: n_swa            = 0
0.00.039.949 I print_info: n_embd_head_k    = 128
0.00.039.950 I print_info: n_embd_head_v    = 128
0.00.039.950 I print_info: n_gqa            = 1
0.00.039.951 I print_info: n_embd_k_gqa     = 2048
0.00.039.951 I print_info: n_embd_v_gqa     = 2048
0.00.039.952 I print_info: f_norm_eps       = 1.0e-05
0.00.039.952 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.952 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.952 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.953 I print_info: f_logit_scale    = 0.0e+00
0.00.039.953 I print_info: n_ff             = 8192
0.00.039.954 I print_info: n_expert         = 0
0.00.039.954 I print_info: n_expert_used    = 0
0.00.039.954 I print_info: causal attn      = 1
0.00.039.954 I print_info: pooling type     = 0
0.00.039.954 I print_info: rope type        = 2
0.00.039.954 I print_info: rope scaling     = linear
0.00.039.955 I print_info: freq_base_train  = 10000.0
0.00.039.955 I print_info: freq_scale_train = 1
0.00.039.957 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.957 I print_info: rope_finetuned   = unknown
0.00.039.957 I print_info: ssm_d_conv       = 0
0.00.039.957 I print_info: ssm_d_inner      = 0
0.00.039.957 I print_info: ssm_d_state      = 0
0.00.039.957 I print_info: ssm_dt_rank      = 0
0.00.039.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.958 I print_info: model type       = 1.4B
0.00.039.958 I print_info: model params     = 1.41 B
0.00.039.958 I print_info: general.name     = 1.4B
0.00.039.958 I print_info: vocab type       = BPE
0.00.039.959 I print_info: n_vocab          = 50304
0.00.039.959 I print_info: n_merges         = 50009
0.00.039.959 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.959 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.959 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.959 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.960 I print_info: LF token         = 187 ''
0.00.039.960 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.960 I print_info: max token length = 1024
0.00.039.960 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.548 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.567 I load_tensors: offloading output layer to GPU
0.00.628.567 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.605 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.628.607 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.630.246 I llama_init_from_model: n_seq_max     = 1
0.00.630.249 I llama_init_from_model: n_ctx         = 128
0.00.630.249 I llama_init_from_model: n_ctx_per_seq = 128
0.00.630.250 I llama_init_from_model: n_batch       = 128
0.00.630.250 I llama_init_from_model: n_ubatch      = 128
0.00.630.250 I llama_init_from_model: flash_attn    = 0
0.00.630.253 I llama_init_from_model: freq_base     = 10000.0
0.00.630.254 I llama_init_from_model: freq_scale    = 1
0.00.630.254 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.257 I ggml_metal_init: allocating
0.00.630.340 I ggml_metal_init: found device: Apple M4
0.00.630.354 I ggml_metal_init: picking default device: Apple M4
0.00.632.130 I ggml_metal_init: using embedded metal library
0.00.638.929 I ggml_metal_init: GPU name:   Apple M4
0.00.638.934 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.935 I ggml_metal_init: simdgroup reduction   = true
0.00.638.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.936 I ggml_metal_init: has residency sets    = true
0.00.638.936 I ggml_metal_init: has bfloat            = true
0.00.638.937 I ggml_metal_init: use bfloat            = true
0.00.638.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.153 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.772 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.813 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.143 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.146 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.146 I llama_init_from_model: graph nodes  = 967
0.00.663.146 I llama_init_from_model: graph splits = 2
0.00.663.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.209 I 
0.00.693.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.303 I perplexity: tokenizing the input ..
0.00.700.547 I perplexity: tokenization took 7.24 ms
0.00.700.556 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.483 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.848.831 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.848.856 I llama_perf_context_print:        load time =     683.25 ms
0.00.848.857 I llama_perf_context_print: prompt eval time =     146.06 ms /   128 tokens (    1.14 ms per token,   876.34 tokens per second)
0.00.848.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.858 I llama_perf_context_print:       total time =     155.65 ms /   129 tokens
0.00.849.234 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.080s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.794 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.591 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.029.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.597 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.599 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.600 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.600 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.600 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.038.429 I llama_model_loader: - type  f32:  194 tensors
0.00.038.429 I llama_model_loader: - type q5_1:   97 tensors
0.00.038.430 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.430 I print_info: file format = GGUF V3 (latest)
0.00.038.430 I print_info: file type   = Q5_1
0.00.038.432 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.047.234 I load: special tokens cache size = 25
0.00.053.589 I load: token to piece cache size = 0.2984 MB
0.00.053.592 I print_info: arch             = gptneox
0.00.053.592 I print_info: vocab_only       = 0
0.00.053.592 I print_info: n_ctx_train      = 2048
0.00.053.592 I print_info: n_embd           = 2048
0.00.053.592 I print_info: n_layer          = 24
0.00.053.595 I print_info: n_head           = 16
0.00.053.595 I print_info: n_head_kv        = 16
0.00.053.596 I print_info: n_rot            = 32
0.00.053.596 I print_info: n_swa            = 0
0.00.053.596 I print_info: n_embd_head_k    = 128
0.00.053.596 I print_info: n_embd_head_v    = 128
0.00.053.597 I print_info: n_gqa            = 1
0.00.053.598 I print_info: n_embd_k_gqa     = 2048
0.00.053.598 I print_info: n_embd_v_gqa     = 2048
0.00.053.600 I print_info: f_norm_eps       = 1.0e-05
0.00.053.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.600 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.600 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.600 I print_info: f_logit_scale    = 0.0e+00
0.00.053.601 I print_info: n_ff             = 8192
0.00.053.601 I print_info: n_expert         = 0
0.00.053.601 I print_info: n_expert_used    = 0
0.00.053.601 I print_info: causal attn      = 1
0.00.053.602 I print_info: pooling type     = 0
0.00.053.603 I print_info: rope type        = 2
0.00.053.606 I print_info: rope scaling     = linear
0.00.053.606 I print_info: freq_base_train  = 10000.0
0.00.053.606 I print_info: freq_scale_train = 1
0.00.053.607 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.607 I print_info: rope_finetuned   = unknown
0.00.053.607 I print_info: ssm_d_conv       = 0
0.00.053.607 I print_info: ssm_d_inner      = 0
0.00.053.607 I print_info: ssm_d_state      = 0
0.00.053.608 I print_info: ssm_dt_rank      = 0
0.00.053.608 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.608 I print_info: model type       = 1.4B
0.00.053.608 I print_info: model params     = 1.41 B
0.00.053.608 I print_info: general.name     = 1.4B
0.00.053.609 I print_info: vocab type       = BPE
0.00.053.609 I print_info: n_vocab          = 50304
0.00.053.609 I print_info: n_merges         = 50009
0.00.053.610 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.610 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.610 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.610 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.610 I print_info: LF token         = 187 ''
0.00.053.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.611 I print_info: max token length = 1024
0.00.053.611 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.786.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.786.644 I load_tensors: offloading output layer to GPU
0.00.786.644 I load_tensors: offloaded 25/25 layers to GPU
0.00.786.680 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.786.681 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.788.276 I llama_init_from_model: n_seq_max     = 1
0.00.788.279 I llama_init_from_model: n_ctx         = 2048
0.00.788.279 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.788.280 I llama_init_from_model: n_batch       = 2048
0.00.788.280 I llama_init_from_model: n_ubatch      = 512
0.00.788.281 I llama_init_from_model: flash_attn    = 0
0.00.788.283 I llama_init_from_model: freq_base     = 10000.0
0.00.788.283 I llama_init_from_model: freq_scale    = 1
0.00.788.286 I ggml_metal_init: allocating
0.00.788.364 I ggml_metal_init: found device: Apple M4
0.00.788.377 I ggml_metal_init: picking default device: Apple M4
0.00.789.965 I ggml_metal_init: using embedded metal library
0.00.796.417 I ggml_metal_init: GPU name:   Apple M4
0.00.796.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.796.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.796.422 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.796.423 I ggml_metal_init: simdgroup reduction   = true
0.00.796.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.796.423 I ggml_metal_init: has residency sets    = true
0.00.796.423 I ggml_metal_init: has bfloat            = true
0.00.796.424 I ggml_metal_init: use bfloat            = true
0.00.796.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.796.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.813.554 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.869.909 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.869.915 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.869.949 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.874.252 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.874.254 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.874.254 I llama_init_from_model: graph nodes  = 967
0.00.874.254 I llama_init_from_model: graph splits = 2
0.00.874.260 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.874.388 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.874.389 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.639 I main: llama threadpool init, n_threads = 4
0.00.935.681 I 
0.00.935.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.705 I 
0.00.935.847 I sampler seed: 1234
0.00.935.851 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.935.875 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.935.876 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.935.876 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.776.477 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.776.478 I llama_perf_context_print:        load time =     926.13 ms
0.01.776.478 I llama_perf_context_print: prompt eval time =      52.29 ms /     7 tokens (    7.47 ms per token,   133.87 tokens per second)
0.01.776.479 I llama_perf_context_print:        eval time =     785.34 ms /    63 runs   (   12.47 ms per token,    80.22 tokens per second)
0.01.776.479 I llama_perf_context_print:       total time =     841.55 ms /    70 tokens
0.01.776.729 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.110s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.450 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.451 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.451 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.452 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.452 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.454 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.455 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.455 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.456 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.456 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.458 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.256 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.101 I llama_model_loader: - type  f32:  194 tensors
0.00.025.102 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.102 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.103 I print_info: file format = GGUF V3 (latest)
0.00.025.107 I print_info: file type   = Q5_1
0.00.025.109 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.489 I load: special tokens cache size = 25
0.00.039.582 I load: token to piece cache size = 0.2984 MB
0.00.039.587 I print_info: arch             = gptneox
0.00.039.587 I print_info: vocab_only       = 0
0.00.039.587 I print_info: n_ctx_train      = 2048
0.00.039.588 I print_info: n_embd           = 2048
0.00.039.588 I print_info: n_layer          = 24
0.00.039.592 I print_info: n_head           = 16
0.00.039.596 I print_info: n_head_kv        = 16
0.00.039.596 I print_info: n_rot            = 32
0.00.039.596 I print_info: n_swa            = 0
0.00.039.596 I print_info: n_embd_head_k    = 128
0.00.039.596 I print_info: n_embd_head_v    = 128
0.00.039.597 I print_info: n_gqa            = 1
0.00.039.598 I print_info: n_embd_k_gqa     = 2048
0.00.039.598 I print_info: n_embd_v_gqa     = 2048
0.00.039.599 I print_info: f_norm_eps       = 1.0e-05
0.00.039.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.599 I print_info: f_logit_scale    = 0.0e+00
0.00.039.600 I print_info: n_ff             = 8192
0.00.039.600 I print_info: n_expert         = 0
0.00.039.600 I print_info: n_expert_used    = 0
0.00.039.601 I print_info: causal attn      = 1
0.00.039.601 I print_info: pooling type     = 0
0.00.039.601 I print_info: rope type        = 2
0.00.039.601 I print_info: rope scaling     = linear
0.00.039.603 I print_info: freq_base_train  = 10000.0
0.00.039.603 I print_info: freq_scale_train = 1
0.00.039.603 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.603 I print_info: rope_finetuned   = unknown
0.00.039.603 I print_info: ssm_d_conv       = 0
0.00.039.604 I print_info: ssm_d_inner      = 0
0.00.039.604 I print_info: ssm_d_state      = 0
0.00.039.604 I print_info: ssm_dt_rank      = 0
0.00.039.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.604 I print_info: model type       = 1.4B
0.00.039.605 I print_info: model params     = 1.41 B
0.00.039.605 I print_info: general.name     = 1.4B
0.00.039.605 I print_info: vocab type       = BPE
0.00.039.605 I print_info: n_vocab          = 50304
0.00.039.606 I print_info: n_merges         = 50009
0.00.039.606 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.606 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.608 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.608 I print_info: LF token         = 187 ''
0.00.039.608 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.608 I print_info: max token length = 1024
0.00.039.609 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.528 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.537 I load_tensors: offloading output layer to GPU
0.00.637.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.559 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.637.560 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.638.923 I llama_init_from_model: n_seq_max     = 1
0.00.638.926 I llama_init_from_model: n_ctx         = 128
0.00.638.927 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.927 I llama_init_from_model: n_batch       = 128
0.00.638.928 I llama_init_from_model: n_ubatch      = 128
0.00.638.928 I llama_init_from_model: flash_attn    = 0
0.00.638.931 I llama_init_from_model: freq_base     = 10000.0
0.00.638.931 I llama_init_from_model: freq_scale    = 1
0.00.638.932 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.934 I ggml_metal_init: allocating
0.00.639.023 I ggml_metal_init: found device: Apple M4
0.00.639.037 I ggml_metal_init: picking default device: Apple M4
0.00.640.838 I ggml_metal_init: using embedded metal library
0.00.647.660 I ggml_metal_init: GPU name:   Apple M4
0.00.647.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.668 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.669 I ggml_metal_init: simdgroup reduction   = true
0.00.647.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.669 I ggml_metal_init: has residency sets    = true
0.00.647.669 I ggml_metal_init: has bfloat            = true
0.00.647.670 I ggml_metal_init: use bfloat            = true
0.00.647.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.669.326 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.669.363 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.697 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.699 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.699 I llama_init_from_model: graph nodes  = 967
0.00.672.700 I llama_init_from_model: graph splits = 2
0.00.672.703 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.051 I 
0.00.701.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.141 I perplexity: tokenizing the input ..
0.00.708.364 I perplexity: tokenization took 7.221 ms
0.00.708.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.348 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.845.834 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.845.856 I llama_perf_context_print:        load time =     692.01 ms
0.00.845.856 I llama_perf_context_print: prompt eval time =     134.98 ms /   128 tokens (    1.05 ms per token,   948.30 tokens per second)
0.00.845.857 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.858 I llama_perf_context_print:       total time =     144.81 ms /   129 tokens
0.00.846.249 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.080s
sys	0m0.146s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.989 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.275 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.275 I llama_model_loader: - type  f32:  194 tensors
0.00.025.276 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.276 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.277 I print_info: file format = GGUF V3 (latest)
0.00.025.277 I print_info: file type   = Q2_K - Medium
0.00.025.278 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.154 I load: special tokens cache size = 25
0.00.038.934 I load: token to piece cache size = 0.2984 MB
0.00.038.937 I print_info: arch             = gptneox
0.00.038.937 I print_info: vocab_only       = 0
0.00.038.937 I print_info: n_ctx_train      = 2048
0.00.038.938 I print_info: n_embd           = 2048
0.00.038.938 I print_info: n_layer          = 24
0.00.038.940 I print_info: n_head           = 16
0.00.038.941 I print_info: n_head_kv        = 16
0.00.038.941 I print_info: n_rot            = 32
0.00.038.942 I print_info: n_swa            = 0
0.00.038.942 I print_info: n_embd_head_k    = 128
0.00.038.942 I print_info: n_embd_head_v    = 128
0.00.038.943 I print_info: n_gqa            = 1
0.00.038.943 I print_info: n_embd_k_gqa     = 2048
0.00.038.944 I print_info: n_embd_v_gqa     = 2048
0.00.038.945 I print_info: f_norm_eps       = 1.0e-05
0.00.038.945 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.945 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.945 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.947 I print_info: f_logit_scale    = 0.0e+00
0.00.038.948 I print_info: n_ff             = 8192
0.00.038.948 I print_info: n_expert         = 0
0.00.038.948 I print_info: n_expert_used    = 0
0.00.038.948 I print_info: causal attn      = 1
0.00.038.948 I print_info: pooling type     = 0
0.00.038.949 I print_info: rope type        = 2
0.00.038.949 I print_info: rope scaling     = linear
0.00.038.949 I print_info: freq_base_train  = 10000.0
0.00.038.950 I print_info: freq_scale_train = 1
0.00.038.950 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.950 I print_info: rope_finetuned   = unknown
0.00.038.950 I print_info: ssm_d_conv       = 0
0.00.038.950 I print_info: ssm_d_inner      = 0
0.00.038.951 I print_info: ssm_d_state      = 0
0.00.038.951 I print_info: ssm_dt_rank      = 0
0.00.038.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.951 I print_info: model type       = 1.4B
0.00.038.952 I print_info: model params     = 1.41 B
0.00.038.952 I print_info: general.name     = 1.4B
0.00.038.952 I print_info: vocab type       = BPE
0.00.038.952 I print_info: n_vocab          = 50304
0.00.038.953 I print_info: n_merges         = 50009
0.00.038.953 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: LF token         = 187 ''
0.00.038.954 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: max token length = 1024
0.00.038.955 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.370.287 I load_tensors: offloading 24 repeating layers to GPU
0.00.370.299 I load_tensors: offloading output layer to GPU
0.00.370.300 I load_tensors: offloaded 25/25 layers to GPU
0.00.370.334 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.370.335 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.371.975 I llama_init_from_model: n_seq_max     = 1
0.00.371.977 I llama_init_from_model: n_ctx         = 2048
0.00.371.977 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.371.978 I llama_init_from_model: n_batch       = 2048
0.00.371.978 I llama_init_from_model: n_ubatch      = 512
0.00.371.979 I llama_init_from_model: flash_attn    = 0
0.00.371.981 I llama_init_from_model: freq_base     = 10000.0
0.00.371.981 I llama_init_from_model: freq_scale    = 1
0.00.371.996 I ggml_metal_init: allocating
0.00.372.099 I ggml_metal_init: found device: Apple M4
0.00.372.140 I ggml_metal_init: picking default device: Apple M4
0.00.373.996 I ggml_metal_init: using embedded metal library
0.00.379.668 I ggml_metal_init: GPU name:   Apple M4
0.00.379.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.379.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.379.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.379.684 I ggml_metal_init: simdgroup reduction   = true
0.00.379.685 I ggml_metal_init: simdgroup matrix mul. = true
0.00.379.685 I ggml_metal_init: has residency sets    = true
0.00.379.685 I ggml_metal_init: has bfloat            = true
0.00.379.686 I ggml_metal_init: use bfloat            = true
0.00.379.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.379.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.401.594 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.461.091 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.461.097 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.461.178 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.465.758 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.465.760 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.465.760 I llama_init_from_model: graph nodes  = 967
0.00.465.761 I llama_init_from_model: graph splits = 2
0.00.465.767 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.465.897 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.465.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.108 I main: llama threadpool init, n_threads = 4
0.00.524.147 I 
0.00.524.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.169 I 
0.00.524.299 I sampler seed: 1234
0.00.524.304 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.524.347 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.524.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.524.348 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.211.477 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.211.478 I llama_perf_context_print:        load time =     513.42 ms
0.01.211.479 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.55 tokens per second)
0.01.211.479 I llama_perf_context_print:        eval time =     640.17 ms /    63 runs   (   10.16 ms per token,    98.41 tokens per second)
0.01.211.480 I llama_perf_context_print:       total time =     688.06 ms /    70 tokens
0.01.211.711 I ggml_metal_free: deallocating

real	0m1.230s
user	0m0.112s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.160 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.905 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.743 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.744 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.744 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.745 I llama_model_loader: - type  f32:  194 tensors
0.00.025.745 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.746 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.746 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.746 I print_info: file format = GGUF V3 (latest)
0.00.025.747 I print_info: file type   = Q2_K - Medium
0.00.025.750 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.207 I load: special tokens cache size = 25
0.00.040.419 I load: token to piece cache size = 0.2984 MB
0.00.040.423 I print_info: arch             = gptneox
0.00.040.423 I print_info: vocab_only       = 0
0.00.040.424 I print_info: n_ctx_train      = 2048
0.00.040.424 I print_info: n_embd           = 2048
0.00.040.424 I print_info: n_layer          = 24
0.00.040.428 I print_info: n_head           = 16
0.00.040.429 I print_info: n_head_kv        = 16
0.00.040.429 I print_info: n_rot            = 32
0.00.040.431 I print_info: n_swa            = 0
0.00.040.432 I print_info: n_embd_head_k    = 128
0.00.040.432 I print_info: n_embd_head_v    = 128
0.00.040.432 I print_info: n_gqa            = 1
0.00.040.433 I print_info: n_embd_k_gqa     = 2048
0.00.040.434 I print_info: n_embd_v_gqa     = 2048
0.00.040.434 I print_info: f_norm_eps       = 1.0e-05
0.00.040.438 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.438 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.439 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.439 I print_info: f_logit_scale    = 0.0e+00
0.00.040.439 I print_info: n_ff             = 8192
0.00.040.440 I print_info: n_expert         = 0
0.00.040.440 I print_info: n_expert_used    = 0
0.00.040.440 I print_info: causal attn      = 1
0.00.040.440 I print_info: pooling type     = 0
0.00.040.440 I print_info: rope type        = 2
0.00.040.440 I print_info: rope scaling     = linear
0.00.040.441 I print_info: freq_base_train  = 10000.0
0.00.040.443 I print_info: freq_scale_train = 1
0.00.040.444 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.444 I print_info: rope_finetuned   = unknown
0.00.040.444 I print_info: ssm_d_conv       = 0
0.00.040.444 I print_info: ssm_d_inner      = 0
0.00.040.444 I print_info: ssm_d_state      = 0
0.00.040.444 I print_info: ssm_dt_rank      = 0
0.00.040.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.445 I print_info: model type       = 1.4B
0.00.040.445 I print_info: model params     = 1.41 B
0.00.040.445 I print_info: general.name     = 1.4B
0.00.040.446 I print_info: vocab type       = BPE
0.00.040.446 I print_info: n_vocab          = 50304
0.00.040.446 I print_info: n_merges         = 50009
0.00.040.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.447 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.447 I print_info: LF token         = 187 ''
0.00.040.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.447 I print_info: max token length = 1024
0.00.040.448 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.366.395 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.411 I load_tensors: offloading output layer to GPU
0.00.366.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.444 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.366.445 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.368.166 I llama_init_from_model: n_seq_max     = 1
0.00.368.169 I llama_init_from_model: n_ctx         = 128
0.00.368.169 I llama_init_from_model: n_ctx_per_seq = 128
0.00.368.170 I llama_init_from_model: n_batch       = 128
0.00.368.170 I llama_init_from_model: n_ubatch      = 128
0.00.368.171 I llama_init_from_model: flash_attn    = 0
0.00.368.173 I llama_init_from_model: freq_base     = 10000.0
0.00.368.173 I llama_init_from_model: freq_scale    = 1
0.00.368.174 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.368.177 I ggml_metal_init: allocating
0.00.368.269 I ggml_metal_init: found device: Apple M4
0.00.368.283 I ggml_metal_init: picking default device: Apple M4
0.00.370.071 I ggml_metal_init: using embedded metal library
0.00.375.538 I ggml_metal_init: GPU name:   Apple M4
0.00.375.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.375.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.375.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.375.551 I ggml_metal_init: simdgroup reduction   = true
0.00.375.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.375.552 I ggml_metal_init: has residency sets    = true
0.00.375.552 I ggml_metal_init: has bfloat            = true
0.00.375.553 I ggml_metal_init: use bfloat            = true
0.00.375.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.375.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.970 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.400.573 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.400.580 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.400.639 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.403.885 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.403.887 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.403.888 I llama_init_from_model: graph nodes  = 967
0.00.403.888 I llama_init_from_model: graph splits = 2
0.00.403.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.403.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.256 I 
0.00.434.342 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.352 I perplexity: tokenizing the input ..
0.00.441.404 I perplexity: tokenization took 7.049 ms
0.00.441.412 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.583.669 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.585.040 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.585.065 I llama_perf_context_print:        load time =     424.09 ms
0.00.585.067 I llama_perf_context_print: prompt eval time =     141.35 ms /   128 tokens (    1.10 ms per token,   905.52 tokens per second)
0.00.585.069 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.585.071 I llama_perf_context_print:       total time =     150.81 ms /   129 tokens
0.00.585.451 I ggml_metal_free: deallocating

real	0m0.601s
user	0m0.082s
sys	0m0.090s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.918 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.434 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.324 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.256 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.257 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.258 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.258 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.259 I llama_model_loader: - type  f32:  194 tensors
0.00.025.259 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.259 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.260 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.260 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.260 I print_info: file format = GGUF V3 (latest)
0.00.025.261 I print_info: file type   = Q3_K - Medium
0.00.025.262 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.096 I load: special tokens cache size = 25
0.00.038.824 I load: token to piece cache size = 0.2984 MB
0.00.038.827 I print_info: arch             = gptneox
0.00.038.827 I print_info: vocab_only       = 0
0.00.038.827 I print_info: n_ctx_train      = 2048
0.00.038.827 I print_info: n_embd           = 2048
0.00.038.827 I print_info: n_layer          = 24
0.00.038.830 I print_info: n_head           = 16
0.00.038.831 I print_info: n_head_kv        = 16
0.00.038.831 I print_info: n_rot            = 32
0.00.038.831 I print_info: n_swa            = 0
0.00.038.832 I print_info: n_embd_head_k    = 128
0.00.038.832 I print_info: n_embd_head_v    = 128
0.00.038.832 I print_info: n_gqa            = 1
0.00.038.833 I print_info: n_embd_k_gqa     = 2048
0.00.038.834 I print_info: n_embd_v_gqa     = 2048
0.00.038.837 I print_info: f_norm_eps       = 1.0e-05
0.00.038.837 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.837 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.838 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.838 I print_info: f_logit_scale    = 0.0e+00
0.00.038.838 I print_info: n_ff             = 8192
0.00.038.839 I print_info: n_expert         = 0
0.00.038.839 I print_info: n_expert_used    = 0
0.00.038.841 I print_info: causal attn      = 1
0.00.038.842 I print_info: pooling type     = 0
0.00.038.842 I print_info: rope type        = 2
0.00.038.842 I print_info: rope scaling     = linear
0.00.038.843 I print_info: freq_base_train  = 10000.0
0.00.038.843 I print_info: freq_scale_train = 1
0.00.038.845 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.845 I print_info: rope_finetuned   = unknown
0.00.038.845 I print_info: ssm_d_conv       = 0
0.00.038.845 I print_info: ssm_d_inner      = 0
0.00.038.845 I print_info: ssm_d_state      = 0
0.00.038.845 I print_info: ssm_dt_rank      = 0
0.00.038.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.846 I print_info: model type       = 1.4B
0.00.038.846 I print_info: model params     = 1.41 B
0.00.038.847 I print_info: general.name     = 1.4B
0.00.038.847 I print_info: vocab type       = BPE
0.00.038.847 I print_info: n_vocab          = 50304
0.00.038.849 I print_info: n_merges         = 50009
0.00.038.849 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.849 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.849 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.849 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.850 I print_info: LF token         = 187 ''
0.00.038.850 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.850 I print_info: max token length = 1024
0.00.038.850 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.969 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.982 I load_tensors: offloading output layer to GPU
0.00.445.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.016 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.026 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.447.670 I llama_init_from_model: n_seq_max     = 1
0.00.447.673 I llama_init_from_model: n_ctx         = 2048
0.00.447.674 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.447.675 I llama_init_from_model: n_batch       = 2048
0.00.447.675 I llama_init_from_model: n_ubatch      = 512
0.00.447.675 I llama_init_from_model: flash_attn    = 0
0.00.447.677 I llama_init_from_model: freq_base     = 10000.0
0.00.447.678 I llama_init_from_model: freq_scale    = 1
0.00.447.685 I ggml_metal_init: allocating
0.00.447.761 I ggml_metal_init: found device: Apple M4
0.00.447.776 I ggml_metal_init: picking default device: Apple M4
0.00.449.644 I ggml_metal_init: using embedded metal library
0.00.455.636 I ggml_metal_init: GPU name:   Apple M4
0.00.455.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.645 I ggml_metal_init: simdgroup reduction   = true
0.00.455.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.646 I ggml_metal_init: has residency sets    = true
0.00.455.646 I ggml_metal_init: has bfloat            = true
0.00.455.646 I ggml_metal_init: use bfloat            = true
0.00.455.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.378 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.531.512 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.531.520 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.531.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.130 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.536.132 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.536.133 I llama_init_from_model: graph nodes  = 967
0.00.536.133 I llama_init_from_model: graph splits = 2
0.00.536.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.536.267 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.536.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.099 I main: llama threadpool init, n_threads = 4
0.00.594.146 I 
0.00.594.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.171 I 
0.00.594.338 I sampler seed: 1234
0.00.594.342 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.594.363 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.594.364 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.594.364 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.345.688 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.345.689 I llama_perf_context_print:        load time =     584.46 ms
0.01.345.690 I llama_perf_context_print: prompt eval time =      49.79 ms /     7 tokens (    7.11 ms per token,   140.60 tokens per second)
0.01.345.691 I llama_perf_context_print:        eval time =     698.67 ms /    63 runs   (   11.09 ms per token,    90.17 tokens per second)
0.01.345.691 I llama_perf_context_print:       total time =     752.31 ms /    70 tokens
0.01.345.976 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.110s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.637 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.952 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.868 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.802 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.804 I llama_model_loader: - type  f32:  194 tensors
0.00.024.805 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.805 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.805 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.806 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.808 I print_info: file format = GGUF V3 (latest)
0.00.024.808 I print_info: file type   = Q3_K - Medium
0.00.024.810 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.420 I load: special tokens cache size = 25
0.00.039.446 I load: token to piece cache size = 0.2984 MB
0.00.039.450 I print_info: arch             = gptneox
0.00.039.451 I print_info: vocab_only       = 0
0.00.039.451 I print_info: n_ctx_train      = 2048
0.00.039.451 I print_info: n_embd           = 2048
0.00.039.451 I print_info: n_layer          = 24
0.00.039.455 I print_info: n_head           = 16
0.00.039.456 I print_info: n_head_kv        = 16
0.00.039.456 I print_info: n_rot            = 32
0.00.039.457 I print_info: n_swa            = 0
0.00.039.457 I print_info: n_embd_head_k    = 128
0.00.039.457 I print_info: n_embd_head_v    = 128
0.00.039.458 I print_info: n_gqa            = 1
0.00.039.458 I print_info: n_embd_k_gqa     = 2048
0.00.039.459 I print_info: n_embd_v_gqa     = 2048
0.00.039.460 I print_info: f_norm_eps       = 1.0e-05
0.00.039.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.461 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.461 I print_info: f_logit_scale    = 0.0e+00
0.00.039.461 I print_info: n_ff             = 8192
0.00.039.461 I print_info: n_expert         = 0
0.00.039.462 I print_info: n_expert_used    = 0
0.00.039.462 I print_info: causal attn      = 1
0.00.039.462 I print_info: pooling type     = 0
0.00.039.462 I print_info: rope type        = 2
0.00.039.462 I print_info: rope scaling     = linear
0.00.039.463 I print_info: freq_base_train  = 10000.0
0.00.039.463 I print_info: freq_scale_train = 1
0.00.039.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.463 I print_info: rope_finetuned   = unknown
0.00.039.463 I print_info: ssm_d_conv       = 0
0.00.039.463 I print_info: ssm_d_inner      = 0
0.00.039.464 I print_info: ssm_d_state      = 0
0.00.039.464 I print_info: ssm_dt_rank      = 0
0.00.039.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.464 I print_info: model type       = 1.4B
0.00.039.464 I print_info: model params     = 1.41 B
0.00.039.465 I print_info: general.name     = 1.4B
0.00.039.465 I print_info: vocab type       = BPE
0.00.039.465 I print_info: n_vocab          = 50304
0.00.039.465 I print_info: n_merges         = 50009
0.00.039.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.466 I print_info: LF token         = 187 ''
0.00.039.469 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.469 I print_info: max token length = 1024
0.00.039.470 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.642 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.657 I load_tensors: offloading output layer to GPU
0.00.444.658 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.693 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.694 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.446.352 I llama_init_from_model: n_seq_max     = 1
0.00.446.355 I llama_init_from_model: n_ctx         = 128
0.00.446.355 I llama_init_from_model: n_ctx_per_seq = 128
0.00.446.356 I llama_init_from_model: n_batch       = 128
0.00.446.356 I llama_init_from_model: n_ubatch      = 128
0.00.446.357 I llama_init_from_model: flash_attn    = 0
0.00.446.359 I llama_init_from_model: freq_base     = 10000.0
0.00.446.360 I llama_init_from_model: freq_scale    = 1
0.00.446.360 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.446.363 I ggml_metal_init: allocating
0.00.446.464 I ggml_metal_init: found device: Apple M4
0.00.446.478 I ggml_metal_init: picking default device: Apple M4
0.00.448.320 I ggml_metal_init: using embedded metal library
0.00.454.595 I ggml_metal_init: GPU name:   Apple M4
0.00.454.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.604 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.605 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.606 I ggml_metal_init: simdgroup reduction   = true
0.00.454.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.606 I ggml_metal_init: has residency sets    = true
0.00.454.607 I ggml_metal_init: has bfloat            = true
0.00.454.607 I ggml_metal_init: use bfloat            = true
0.00.454.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.664 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.477.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.477.316 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.477.393 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.480.694 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.480.695 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.480.696 I llama_init_from_model: graph nodes  = 967
0.00.480.696 I llama_init_from_model: graph splits = 2
0.00.480.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.480.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.511.107 I 
0.00.511.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.511.198 I perplexity: tokenizing the input ..
0.00.518.228 I perplexity: tokenization took 7.027 ms
0.00.518.238 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.661.979 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.663.311 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.663.333 I llama_perf_context_print:        load time =     502.46 ms
0.00.663.334 I llama_perf_context_print: prompt eval time =     142.79 ms /   128 tokens (    1.12 ms per token,   896.40 tokens per second)
0.00.663.334 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.663.335 I llama_perf_context_print:       total time =     152.23 ms /   129 tokens
0.00.663.717 I ggml_metal_free: deallocating

real	0m0.677s
user	0m0.081s
sys	0m0.115s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.815 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.310 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.310 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.310 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.943 I llama_model_loader: - type  f32:  194 tensors
0.00.025.944 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.944 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.944 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.945 I print_info: file format = GGUF V3 (latest)
0.00.025.945 I print_info: file type   = Q4_K - Medium
0.00.025.946 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.728 I load: special tokens cache size = 25
0.00.039.732 I load: token to piece cache size = 0.2984 MB
0.00.039.735 I print_info: arch             = gptneox
0.00.039.735 I print_info: vocab_only       = 0
0.00.039.736 I print_info: n_ctx_train      = 2048
0.00.039.736 I print_info: n_embd           = 2048
0.00.039.736 I print_info: n_layer          = 24
0.00.039.739 I print_info: n_head           = 16
0.00.039.739 I print_info: n_head_kv        = 16
0.00.039.742 I print_info: n_rot            = 32
0.00.039.742 I print_info: n_swa            = 0
0.00.039.742 I print_info: n_embd_head_k    = 128
0.00.039.742 I print_info: n_embd_head_v    = 128
0.00.039.743 I print_info: n_gqa            = 1
0.00.039.744 I print_info: n_embd_k_gqa     = 2048
0.00.039.744 I print_info: n_embd_v_gqa     = 2048
0.00.039.745 I print_info: f_norm_eps       = 1.0e-05
0.00.039.745 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.745 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.745 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.746 I print_info: f_logit_scale    = 0.0e+00
0.00.039.746 I print_info: n_ff             = 8192
0.00.039.746 I print_info: n_expert         = 0
0.00.039.746 I print_info: n_expert_used    = 0
0.00.039.747 I print_info: causal attn      = 1
0.00.039.747 I print_info: pooling type     = 0
0.00.039.748 I print_info: rope type        = 2
0.00.039.748 I print_info: rope scaling     = linear
0.00.039.748 I print_info: freq_base_train  = 10000.0
0.00.039.749 I print_info: freq_scale_train = 1
0.00.039.749 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.749 I print_info: rope_finetuned   = unknown
0.00.039.749 I print_info: ssm_d_conv       = 0
0.00.039.749 I print_info: ssm_d_inner      = 0
0.00.039.749 I print_info: ssm_d_state      = 0
0.00.039.751 I print_info: ssm_dt_rank      = 0
0.00.039.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.752 I print_info: model type       = 1.4B
0.00.039.752 I print_info: model params     = 1.41 B
0.00.039.752 I print_info: general.name     = 1.4B
0.00.039.753 I print_info: vocab type       = BPE
0.00.039.753 I print_info: n_vocab          = 50304
0.00.039.753 I print_info: n_merges         = 50009
0.00.039.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.754 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.754 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.754 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.754 I print_info: LF token         = 187 ''
0.00.039.755 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.755 I print_info: max token length = 1024
0.00.039.755 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.514.463 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.478 I load_tensors: offloading output layer to GPU
0.00.514.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.514 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.516 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.516.017 I llama_init_from_model: n_seq_max     = 1
0.00.516.020 I llama_init_from_model: n_ctx         = 2048
0.00.516.020 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.516.021 I llama_init_from_model: n_batch       = 2048
0.00.516.021 I llama_init_from_model: n_ubatch      = 512
0.00.516.021 I llama_init_from_model: flash_attn    = 0
0.00.516.024 I llama_init_from_model: freq_base     = 10000.0
0.00.516.025 I llama_init_from_model: freq_scale    = 1
0.00.516.040 I ggml_metal_init: allocating
0.00.516.112 I ggml_metal_init: found device: Apple M4
0.00.516.126 I ggml_metal_init: picking default device: Apple M4
0.00.518.019 I ggml_metal_init: using embedded metal library
0.00.523.930 I ggml_metal_init: GPU name:   Apple M4
0.00.523.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.936 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.937 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.937 I ggml_metal_init: simdgroup reduction   = true
0.00.523.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.938 I ggml_metal_init: has residency sets    = true
0.00.523.938 I ggml_metal_init: has bfloat            = true
0.00.523.938 I ggml_metal_init: use bfloat            = true
0.00.523.939 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.594 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.600.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.600.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.600.637 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.605.059 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.605.061 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.605.062 I llama_init_from_model: graph nodes  = 967
0.00.605.062 I llama_init_from_model: graph splits = 2
0.00.605.068 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.605.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.605.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.491 I main: llama threadpool init, n_threads = 4
0.00.663.547 I 
0.00.663.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.574 I 
0.00.663.726 I sampler seed: 1234
0.00.663.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.775 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.791 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.793 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.425.941 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49374.13 tokens per second)
0.01.425.942 I llama_perf_context_print:        load time =     652.95 ms
0.01.425.943 I llama_perf_context_print: prompt eval time =      57.83 ms /     7 tokens (    8.26 ms per token,   121.05 tokens per second)
0.01.425.943 I llama_perf_context_print:        eval time =     701.28 ms /    63 runs   (   11.13 ms per token,    89.84 tokens per second)
0.01.425.944 I llama_perf_context_print:       total time =     763.17 ms /    70 tokens
0.01.426.203 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.111s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.968 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.968 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.840 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.683 I llama_model_loader: - type  f32:  194 tensors
0.00.024.684 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.684 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.684 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.685 I print_info: file format = GGUF V3 (latest)
0.00.024.685 I print_info: file type   = Q4_K - Medium
0.00.024.687 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.700 I load: special tokens cache size = 25
0.00.038.854 I load: token to piece cache size = 0.2984 MB
0.00.038.858 I print_info: arch             = gptneox
0.00.038.858 I print_info: vocab_only       = 0
0.00.038.858 I print_info: n_ctx_train      = 2048
0.00.038.858 I print_info: n_embd           = 2048
0.00.038.859 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.864 I print_info: n_head_kv        = 16
0.00.038.864 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.864 I print_info: n_embd_head_k    = 128
0.00.038.865 I print_info: n_embd_head_v    = 128
0.00.038.865 I print_info: n_gqa            = 1
0.00.038.866 I print_info: n_embd_k_gqa     = 2048
0.00.038.867 I print_info: n_embd_v_gqa     = 2048
0.00.038.867 I print_info: f_norm_eps       = 1.0e-05
0.00.038.868 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.868 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.868 I print_info: f_logit_scale    = 0.0e+00
0.00.038.869 I print_info: n_ff             = 8192
0.00.038.869 I print_info: n_expert         = 0
0.00.038.869 I print_info: n_expert_used    = 0
0.00.038.869 I print_info: causal attn      = 1
0.00.038.870 I print_info: pooling type     = 0
0.00.038.870 I print_info: rope type        = 2
0.00.038.870 I print_info: rope scaling     = linear
0.00.038.870 I print_info: freq_base_train  = 10000.0
0.00.038.871 I print_info: freq_scale_train = 1
0.00.038.871 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.871 I print_info: rope_finetuned   = unknown
0.00.038.871 I print_info: ssm_d_conv       = 0
0.00.038.871 I print_info: ssm_d_inner      = 0
0.00.038.871 I print_info: ssm_d_state      = 0
0.00.038.871 I print_info: ssm_dt_rank      = 0
0.00.038.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.872 I print_info: model type       = 1.4B
0.00.038.872 I print_info: model params     = 1.41 B
0.00.038.872 I print_info: general.name     = 1.4B
0.00.038.873 I print_info: vocab type       = BPE
0.00.038.874 I print_info: n_vocab          = 50304
0.00.038.875 I print_info: n_merges         = 50009
0.00.038.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: LF token         = 187 ''
0.00.038.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: max token length = 1024
0.00.038.878 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.510.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.510.655 I load_tensors: offloading output layer to GPU
0.00.510.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.510.692 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.510.694 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.512.349 I llama_init_from_model: n_seq_max     = 1
0.00.512.353 I llama_init_from_model: n_ctx         = 128
0.00.512.353 I llama_init_from_model: n_ctx_per_seq = 128
0.00.512.354 I llama_init_from_model: n_batch       = 128
0.00.512.354 I llama_init_from_model: n_ubatch      = 128
0.00.512.354 I llama_init_from_model: flash_attn    = 0
0.00.512.357 I llama_init_from_model: freq_base     = 10000.0
0.00.512.358 I llama_init_from_model: freq_scale    = 1
0.00.512.359 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.512.361 I ggml_metal_init: allocating
0.00.512.436 I ggml_metal_init: found device: Apple M4
0.00.512.451 I ggml_metal_init: picking default device: Apple M4
0.00.514.238 I ggml_metal_init: using embedded metal library
0.00.521.006 I ggml_metal_init: GPU name:   Apple M4
0.00.521.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.016 I ggml_metal_init: simdgroup reduction   = true
0.00.521.016 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.016 I ggml_metal_init: has residency sets    = true
0.00.521.017 I ggml_metal_init: has bfloat            = true
0.00.521.017 I ggml_metal_init: use bfloat            = true
0.00.521.018 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.596 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.542.602 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.542.645 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.748 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.545.750 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.545.750 I llama_init_from_model: graph nodes  = 967
0.00.545.751 I llama_init_from_model: graph splits = 2
0.00.545.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.545.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.006 I 
0.00.573.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.573.089 I perplexity: tokenizing the input ..
0.00.580.644 I perplexity: tokenization took 7.552 ms
0.00.580.652 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.574 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.727.991 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.728.025 I llama_perf_context_print:        load time =     564.26 ms
0.00.728.027 I llama_perf_context_print: prompt eval time =     144.96 ms /   128 tokens (    1.13 ms per token,   882.98 tokens per second)
0.00.728.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.029 I llama_perf_context_print:       total time =     155.02 ms /   129 tokens
0.00.728.440 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.080s
sys	0m0.121s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.087 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.101 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.102 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.106 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.799 I llama_model_loader: - type  f32:  194 tensors
0.00.025.800 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.800 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.801 I print_info: file format = GGUF V3 (latest)
0.00.025.801 I print_info: file type   = Q5_K - Medium
0.00.025.802 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.585 I load: special tokens cache size = 25
0.00.039.601 I load: token to piece cache size = 0.2984 MB
0.00.039.604 I print_info: arch             = gptneox
0.00.039.604 I print_info: vocab_only       = 0
0.00.039.605 I print_info: n_ctx_train      = 2048
0.00.039.605 I print_info: n_embd           = 2048
0.00.039.605 I print_info: n_layer          = 24
0.00.039.607 I print_info: n_head           = 16
0.00.039.608 I print_info: n_head_kv        = 16
0.00.039.608 I print_info: n_rot            = 32
0.00.039.609 I print_info: n_swa            = 0
0.00.039.609 I print_info: n_embd_head_k    = 128
0.00.039.609 I print_info: n_embd_head_v    = 128
0.00.039.610 I print_info: n_gqa            = 1
0.00.039.613 I print_info: n_embd_k_gqa     = 2048
0.00.039.614 I print_info: n_embd_v_gqa     = 2048
0.00.039.614 I print_info: f_norm_eps       = 1.0e-05
0.00.039.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.615 I print_info: f_logit_scale    = 0.0e+00
0.00.039.616 I print_info: n_ff             = 8192
0.00.039.616 I print_info: n_expert         = 0
0.00.039.616 I print_info: n_expert_used    = 0
0.00.039.616 I print_info: causal attn      = 1
0.00.039.617 I print_info: pooling type     = 0
0.00.039.618 I print_info: rope type        = 2
0.00.039.619 I print_info: rope scaling     = linear
0.00.039.619 I print_info: freq_base_train  = 10000.0
0.00.039.619 I print_info: freq_scale_train = 1
0.00.039.619 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.620 I print_info: rope_finetuned   = unknown
0.00.039.620 I print_info: ssm_d_conv       = 0
0.00.039.620 I print_info: ssm_d_inner      = 0
0.00.039.620 I print_info: ssm_d_state      = 0
0.00.039.620 I print_info: ssm_dt_rank      = 0
0.00.039.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.620 I print_info: model type       = 1.4B
0.00.039.621 I print_info: model params     = 1.41 B
0.00.039.621 I print_info: general.name     = 1.4B
0.00.039.621 I print_info: vocab type       = BPE
0.00.039.622 I print_info: n_vocab          = 50304
0.00.039.622 I print_info: n_merges         = 50009
0.00.039.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.622 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.625 I print_info: LF token         = 187 ''
0.00.039.625 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.625 I print_info: max token length = 1024
0.00.039.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.972 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.976 I load_tensors: offloading output layer to GPU
0.00.608.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.002 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.609.003 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.610.357 I llama_init_from_model: n_seq_max     = 1
0.00.610.359 I llama_init_from_model: n_ctx         = 2048
0.00.610.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.360 I llama_init_from_model: n_batch       = 2048
0.00.610.360 I llama_init_from_model: n_ubatch      = 512
0.00.610.361 I llama_init_from_model: flash_attn    = 0
0.00.610.362 I llama_init_from_model: freq_base     = 10000.0
0.00.610.362 I llama_init_from_model: freq_scale    = 1
0.00.610.364 I ggml_metal_init: allocating
0.00.610.382 I ggml_metal_init: found device: Apple M4
0.00.610.391 I ggml_metal_init: picking default device: Apple M4
0.00.611.868 I ggml_metal_init: using embedded metal library
0.00.618.074 I ggml_metal_init: GPU name:   Apple M4
0.00.618.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.079 I ggml_metal_init: simdgroup reduction   = true
0.00.618.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.080 I ggml_metal_init: has residency sets    = true
0.00.618.080 I ggml_metal_init: has bfloat            = true
0.00.618.081 I ggml_metal_init: use bfloat            = true
0.00.618.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.091 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.902 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.313 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.320 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.353 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.711 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.714 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.714 I llama_init_from_model: graph nodes  = 967
0.00.696.714 I llama_init_from_model: graph splits = 2
0.00.696.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.754 I main: llama threadpool init, n_threads = 4
0.00.760.804 I 
0.00.760.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.829 I 
0.00.760.986 I sampler seed: 1234
0.00.760.992 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.039 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.039 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.798 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.602.799 I llama_perf_context_print:        load time =     751.26 ms
0.01.602.799 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.04 tokens per second)
0.01.602.800 I llama_perf_context_print:        eval time =     786.13 ms /    63 runs   (   12.48 ms per token,    80.14 tokens per second)
0.01.602.801 I llama_perf_context_print:       total time =     842.76 ms /    70 tokens
0.01.603.024 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.108s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.150 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.153 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.156 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.161 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.060 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.846 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.847 I llama_model_loader: - type  f32:  194 tensors
0.00.025.848 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.848 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.849 I print_info: file format = GGUF V3 (latest)
0.00.025.849 I print_info: file type   = Q5_K - Medium
0.00.025.850 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.227 I load: special tokens cache size = 25
0.00.040.336 I load: token to piece cache size = 0.2984 MB
0.00.040.340 I print_info: arch             = gptneox
0.00.040.340 I print_info: vocab_only       = 0
0.00.040.341 I print_info: n_ctx_train      = 2048
0.00.040.341 I print_info: n_embd           = 2048
0.00.040.341 I print_info: n_layer          = 24
0.00.040.345 I print_info: n_head           = 16
0.00.040.346 I print_info: n_head_kv        = 16
0.00.040.346 I print_info: n_rot            = 32
0.00.040.347 I print_info: n_swa            = 0
0.00.040.349 I print_info: n_embd_head_k    = 128
0.00.040.350 I print_info: n_embd_head_v    = 128
0.00.040.350 I print_info: n_gqa            = 1
0.00.040.351 I print_info: n_embd_k_gqa     = 2048
0.00.040.351 I print_info: n_embd_v_gqa     = 2048
0.00.040.352 I print_info: f_norm_eps       = 1.0e-05
0.00.040.352 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.353 I print_info: f_logit_scale    = 0.0e+00
0.00.040.353 I print_info: n_ff             = 8192
0.00.040.353 I print_info: n_expert         = 0
0.00.040.353 I print_info: n_expert_used    = 0
0.00.040.354 I print_info: causal attn      = 1
0.00.040.354 I print_info: pooling type     = 0
0.00.040.354 I print_info: rope type        = 2
0.00.040.354 I print_info: rope scaling     = linear
0.00.040.355 I print_info: freq_base_train  = 10000.0
0.00.040.355 I print_info: freq_scale_train = 1
0.00.040.356 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.356 I print_info: rope_finetuned   = unknown
0.00.040.356 I print_info: ssm_d_conv       = 0
0.00.040.357 I print_info: ssm_d_inner      = 0
0.00.040.357 I print_info: ssm_d_state      = 0
0.00.040.357 I print_info: ssm_dt_rank      = 0
0.00.040.357 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.358 I print_info: model type       = 1.4B
0.00.040.359 I print_info: model params     = 1.41 B
0.00.040.359 I print_info: general.name     = 1.4B
0.00.040.359 I print_info: vocab type       = BPE
0.00.040.359 I print_info: n_vocab          = 50304
0.00.040.360 I print_info: n_merges         = 50009
0.00.040.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.360 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.361 I print_info: LF token         = 187 ''
0.00.040.361 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.361 I print_info: max token length = 1024
0.00.040.367 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.571 I load_tensors: offloading output layer to GPU
0.00.595.572 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.606 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.595.607 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.597.327 I llama_init_from_model: n_seq_max     = 1
0.00.597.330 I llama_init_from_model: n_ctx         = 128
0.00.597.331 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.331 I llama_init_from_model: n_batch       = 128
0.00.597.331 I llama_init_from_model: n_ubatch      = 128
0.00.597.332 I llama_init_from_model: flash_attn    = 0
0.00.597.334 I llama_init_from_model: freq_base     = 10000.0
0.00.597.334 I llama_init_from_model: freq_scale    = 1
0.00.597.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.337 I ggml_metal_init: allocating
0.00.597.421 I ggml_metal_init: found device: Apple M4
0.00.597.435 I ggml_metal_init: picking default device: Apple M4
0.00.599.257 I ggml_metal_init: using embedded metal library
0.00.606.058 I ggml_metal_init: GPU name:   Apple M4
0.00.606.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.069 I ggml_metal_init: simdgroup reduction   = true
0.00.606.069 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.069 I ggml_metal_init: has residency sets    = true
0.00.606.069 I ggml_metal_init: has bfloat            = true
0.00.606.070 I ggml_metal_init: use bfloat            = true
0.00.606.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.699 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.217 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.220 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.264 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.435 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.630.436 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.630.437 I llama_init_from_model: graph nodes  = 967
0.00.630.437 I llama_init_from_model: graph splits = 2
0.00.630.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.392 I 
0.00.662.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.480 I perplexity: tokenizing the input ..
0.00.669.708 I perplexity: tokenization took 7.224 ms
0.00.669.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.922 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.278 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.300 I llama_perf_context_print:        load time =     652.46 ms
0.00.809.301 I llama_perf_context_print: prompt eval time =     137.28 ms /   128 tokens (    1.07 ms per token,   932.41 tokens per second)
0.00.809.302 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.302 I llama_perf_context_print:       total time =     146.91 ms /   129 tokens
0.00.809.656 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.080s
sys	0m0.144s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.412 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.414 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.415 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.415 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.416 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.416 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.298 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.004 I llama_model_loader: - type  f32:  194 tensors
0.00.026.004 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.005 I print_info: file format = GGUF V3 (latest)
0.00.026.005 I print_info: file type   = Q6_K
0.00.026.006 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.849 I load: special tokens cache size = 25
0.00.039.825 I load: token to piece cache size = 0.2984 MB
0.00.039.828 I print_info: arch             = gptneox
0.00.039.828 I print_info: vocab_only       = 0
0.00.039.829 I print_info: n_ctx_train      = 2048
0.00.039.829 I print_info: n_embd           = 2048
0.00.039.829 I print_info: n_layer          = 24
0.00.039.832 I print_info: n_head           = 16
0.00.039.833 I print_info: n_head_kv        = 16
0.00.039.833 I print_info: n_rot            = 32
0.00.039.833 I print_info: n_swa            = 0
0.00.039.833 I print_info: n_embd_head_k    = 128
0.00.039.834 I print_info: n_embd_head_v    = 128
0.00.039.835 I print_info: n_gqa            = 1
0.00.039.835 I print_info: n_embd_k_gqa     = 2048
0.00.039.836 I print_info: n_embd_v_gqa     = 2048
0.00.039.837 I print_info: f_norm_eps       = 1.0e-05
0.00.039.837 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.837 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.837 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.838 I print_info: f_logit_scale    = 0.0e+00
0.00.039.838 I print_info: n_ff             = 8192
0.00.039.838 I print_info: n_expert         = 0
0.00.039.838 I print_info: n_expert_used    = 0
0.00.039.839 I print_info: causal attn      = 1
0.00.039.839 I print_info: pooling type     = 0
0.00.039.839 I print_info: rope type        = 2
0.00.039.841 I print_info: rope scaling     = linear
0.00.039.843 I print_info: freq_base_train  = 10000.0
0.00.039.843 I print_info: freq_scale_train = 1
0.00.039.843 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.843 I print_info: rope_finetuned   = unknown
0.00.039.844 I print_info: ssm_d_conv       = 0
0.00.039.844 I print_info: ssm_d_inner      = 0
0.00.039.844 I print_info: ssm_d_state      = 0
0.00.039.844 I print_info: ssm_dt_rank      = 0
0.00.039.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.844 I print_info: model type       = 1.4B
0.00.039.845 I print_info: model params     = 1.41 B
0.00.039.845 I print_info: general.name     = 1.4B
0.00.039.845 I print_info: vocab type       = BPE
0.00.039.845 I print_info: n_vocab          = 50304
0.00.039.846 I print_info: n_merges         = 50009
0.00.039.846 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.846 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.846 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.846 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: LF token         = 187 ''
0.00.039.847 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.847 I print_info: max token length = 1024
0.00.039.851 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.083 I load_tensors: offloading output layer to GPU
0.00.650.084 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.112 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.114 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.651.656 I llama_init_from_model: n_seq_max     = 1
0.00.651.659 I llama_init_from_model: n_ctx         = 2048
0.00.651.660 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.660 I llama_init_from_model: n_batch       = 2048
0.00.651.660 I llama_init_from_model: n_ubatch      = 512
0.00.651.661 I llama_init_from_model: flash_attn    = 0
0.00.651.662 I llama_init_from_model: freq_base     = 10000.0
0.00.651.662 I llama_init_from_model: freq_scale    = 1
0.00.651.663 I ggml_metal_init: allocating
0.00.651.676 I ggml_metal_init: found device: Apple M4
0.00.651.687 I ggml_metal_init: picking default device: Apple M4
0.00.653.158 I ggml_metal_init: using embedded metal library
0.00.659.434 I ggml_metal_init: GPU name:   Apple M4
0.00.659.437 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.439 I ggml_metal_init: simdgroup reduction   = true
0.00.659.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.440 I ggml_metal_init: has residency sets    = true
0.00.659.440 I ggml_metal_init: has bfloat            = true
0.00.659.440 I ggml_metal_init: use bfloat            = true
0.00.659.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.464 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.752 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.734.759 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.792 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.865 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.868 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.868 I llama_init_from_model: graph nodes  = 967
0.00.738.868 I llama_init_from_model: graph splits = 2
0.00.738.875 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.739.003 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.004 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.566 I main: llama threadpool init, n_threads = 4
0.00.807.609 I 
0.00.807.631 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.633 I 
0.00.807.782 I sampler seed: 1234
0.00.807.786 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.827 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.827 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.692.548 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.01.692.549 I llama_perf_context_print:        load time =     796.92 ms
0.01.692.550 I llama_perf_context_print: prompt eval time =      57.98 ms /     7 tokens (    8.28 ms per token,   120.73 tokens per second)
0.01.692.551 I llama_perf_context_print:        eval time =     824.15 ms /    63 runs   (   13.08 ms per token,    76.44 tokens per second)
0.01.692.551 I llama_perf_context_print:       total time =     885.70 ms /    70 tokens
0.01.692.817 I ggml_metal_free: deallocating

real	0m1.711s
user	0m0.111s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4794 (06c2b156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.049 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.174 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.060 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.854 I llama_model_loader: - type  f32:  194 tensors
0.00.024.854 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.855 I print_info: file format = GGUF V3 (latest)
0.00.024.855 I print_info: file type   = Q6_K
0.00.024.857 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.260 I load: special tokens cache size = 25
0.00.039.425 I load: token to piece cache size = 0.2984 MB
0.00.039.429 I print_info: arch             = gptneox
0.00.039.429 I print_info: vocab_only       = 0
0.00.039.429 I print_info: n_ctx_train      = 2048
0.00.039.430 I print_info: n_embd           = 2048
0.00.039.430 I print_info: n_layer          = 24
0.00.039.434 I print_info: n_head           = 16
0.00.039.435 I print_info: n_head_kv        = 16
0.00.039.435 I print_info: n_rot            = 32
0.00.039.435 I print_info: n_swa            = 0
0.00.039.435 I print_info: n_embd_head_k    = 128
0.00.039.435 I print_info: n_embd_head_v    = 128
0.00.039.436 I print_info: n_gqa            = 1
0.00.039.437 I print_info: n_embd_k_gqa     = 2048
0.00.039.437 I print_info: n_embd_v_gqa     = 2048
0.00.039.438 I print_info: f_norm_eps       = 1.0e-05
0.00.039.438 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.439 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.439 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.439 I print_info: f_logit_scale    = 0.0e+00
0.00.039.440 I print_info: n_ff             = 8192
0.00.039.440 I print_info: n_expert         = 0
0.00.039.440 I print_info: n_expert_used    = 0
0.00.039.440 I print_info: causal attn      = 1
0.00.039.440 I print_info: pooling type     = 0
0.00.039.440 I print_info: rope type        = 2
0.00.039.441 I print_info: rope scaling     = linear
0.00.039.441 I print_info: freq_base_train  = 10000.0
0.00.039.441 I print_info: freq_scale_train = 1
0.00.039.441 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.441 I print_info: rope_finetuned   = unknown
0.00.039.442 I print_info: ssm_d_conv       = 0
0.00.039.442 I print_info: ssm_d_inner      = 0
0.00.039.442 I print_info: ssm_d_state      = 0
0.00.039.442 I print_info: ssm_dt_rank      = 0
0.00.039.442 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.442 I print_info: model type       = 1.4B
0.00.039.442 I print_info: model params     = 1.41 B
0.00.039.443 I print_info: general.name     = 1.4B
0.00.039.443 I print_info: vocab type       = BPE
0.00.039.446 I print_info: n_vocab          = 50304
0.00.039.446 I print_info: n_merges         = 50009
0.00.039.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.446 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: LF token         = 187 ''
0.00.039.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: max token length = 1024
0.00.039.449 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.136 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.144 I load_tensors: offloading output layer to GPU
0.00.601.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.174 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.601.177 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.602.758 I llama_init_from_model: n_seq_max     = 1
0.00.602.760 I llama_init_from_model: n_ctx         = 128
0.00.602.761 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.761 I llama_init_from_model: n_batch       = 128
0.00.602.762 I llama_init_from_model: n_ubatch      = 128
0.00.602.762 I llama_init_from_model: flash_attn    = 0
0.00.602.764 I llama_init_from_model: freq_base     = 10000.0
0.00.602.764 I llama_init_from_model: freq_scale    = 1
0.00.602.765 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.767 I ggml_metal_init: allocating
0.00.602.818 I ggml_metal_init: found device: Apple M4
0.00.602.831 I ggml_metal_init: picking default device: Apple M4
0.00.604.293 I ggml_metal_init: using embedded metal library
0.00.610.397 I ggml_metal_init: GPU name:   Apple M4
0.00.610.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.402 I ggml_metal_init: simdgroup reduction   = true
0.00.610.402 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.403 I ggml_metal_init: has residency sets    = true
0.00.610.403 I ggml_metal_init: has bfloat            = true
0.00.610.403 I ggml_metal_init: use bfloat            = true
0.00.610.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.159 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.620 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.629 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.683 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.869 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.871 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.871 I llama_init_from_model: graph nodes  = 967
0.00.633.872 I llama_init_from_model: graph splits = 2
0.00.633.875 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.200 I 
0.00.669.286 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.294 I perplexity: tokenizing the input ..
0.00.677.017 I perplexity: tokenization took 7.721 ms
0.00.677.032 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.807 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.811.143 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.811.166 I llama_perf_context_print:        load time =     660.14 ms
0.00.811.167 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.36 tokens per second)
0.00.811.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.168 I llama_perf_context_print:       total time =     141.97 ms /   129 tokens
0.00.811.535 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.079s
sys	0m0.132s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4794 (06c2b156)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11a6045f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11a604c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11a6050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11a605680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11a605c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11a6061e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11a606790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11a606d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11a6072f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11a6077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11a607cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11a6081f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11a608d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11a6094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11a609cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11a60a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11a60ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11a60b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11a60b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11a60c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11a60c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11a60cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11a60d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11a60df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11a60e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11a60e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11a60ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11a60fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11a6100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11a610380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11a610820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11a610ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11a611370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11a6118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11a611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11a612010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11a6124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11a612950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11a612df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11a613290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11a613730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11a613bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11a614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11a614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11a6147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11a614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11a6153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11a615d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11a616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11a616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11a616f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11a617550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11a617b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11a618170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11a618960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11a618e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11a6192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11a619560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11a619b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11a61a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11a61a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11a61aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11a61af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11a61b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11a61b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11a61bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11a61c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11a61c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11a61cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11a61cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11a61d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11a61d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11a61dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11a61e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11a61e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11a61ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11a61f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11a61f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11a61fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11a6202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11a620820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11a620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11a6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11a621810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11a621d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11a6222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11a622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11a622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11a6232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11a6237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11a623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11a624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11a6247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11a624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11a625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11a6257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11a625d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11a615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11a626190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11a626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11a626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11a6273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11a627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11a627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11a6283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11a628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11a628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11a6293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11a629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11a629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11a62a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11a62a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11a62ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11a62b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11a62b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11a62bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11a62c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11a62c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11a62ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11a62ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11a62d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11a62d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11a62dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11a62e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11a62e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11a62ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11a62ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11a62f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11a62f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11a62fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11a630190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11a630630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11a630ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11a630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11a631410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11a6318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11a631d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11a6321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11a632690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11a632b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11a632fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11a633470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11a633910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11a633db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11a634250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11a6346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11a634b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11a635030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11a6354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11a635970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11a635e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11a6362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11a636750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11a636bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11a637090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11a637530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11a6379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11a637e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11a638310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11a6387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11a638c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11a6390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11a639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11a639a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11a639ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11a63a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11a63a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11a63acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11a63b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11a63b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11a63ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11a63bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11a63c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11a63c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11a63cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11a63d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11a63d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11a63daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11a63df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11a63e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11a63e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11a63ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11a63f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11a63f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11a63fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11a63fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11a640490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11a640930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11a640dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11a641270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11a641710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11a641bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11a642050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11a6425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11a642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11a643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11a643590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11a643850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11a643e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11a644470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11a644a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11a645270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11a645710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a6459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a645fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11a6465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a646de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a647280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a647720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a647bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a648370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a6488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a649360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a6498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a649e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a64a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a64a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a64adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a64b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a64b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a64bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a64c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a64c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a64cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a64d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a64d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a64ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a64e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a64e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a64edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a64f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a64f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a64fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a6502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a650d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a6512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a651d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a6522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a652d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a6532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a6542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a6552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a6557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a656290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a6567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a656d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a657280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a6577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a658270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a6587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a6597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a659d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a65a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a65a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a65acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11a65b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11a65b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a65bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a65bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a65c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a65c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a65cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a65d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a65d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a65db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a65dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a65e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a65e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a65edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a65f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11a65f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11a65fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11a660030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11a6604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11a660970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11a660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11a6612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11a661750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11a661bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11a662090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a6625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a662d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a663420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a663b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a664260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a664520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11a664d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a664fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a6655e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.693.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.693.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x109a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x109a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x109a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x109a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x109a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x109a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x109a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x109a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x109a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x109a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x109a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x109a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x109a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x109a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x109a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x109a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x109a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x109a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x109a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x109a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x109a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x109a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x109a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x109a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x109a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x109a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x109a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x109a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x109a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x109a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x109a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x109a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x109a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x109a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x109a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x109a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x109a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x109a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x109a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x109a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x109a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x109a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x109a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x109a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x109a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x109a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109a35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x109a36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109a363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109a36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x109a36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109a37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x109a375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109a37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x109a37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109a38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109a38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109a38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109a39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109a394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109a39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x109a39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109a3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109a3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109a3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109a3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x109a3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109a3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x109a3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x109a3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x109a3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109a3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x109a3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x109a3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x109a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x109a3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x109a3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109a3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109a3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109a3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109a3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x109a3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x109a400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109a40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109a409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x109a40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109a41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109a417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109a41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109a42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109a42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109a430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x109a43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109a43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109a441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x109a447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109a44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109a458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109a45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109a46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109a46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109a475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109a47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109a48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109a486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109a48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109a49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109a49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109a49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109a4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109a4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109a4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109a4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109a4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109a4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109a4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109a4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x109a4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109a4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109a4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x109a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x109a4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109a4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x109a4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x109a50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x109a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109a510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109a516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x109a51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x109a52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x109a527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x109a52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x109a53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x109a53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x109a53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x109a544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x109a54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x109a55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109a555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109a55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109a56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109a56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109a56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109a571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x109a576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109a57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109a580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109a585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109a58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109a58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109a594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109a599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109a59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109a5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109a5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109a5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109a5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x109a5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x109a5bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x109a5c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x109a5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x109a5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x109a5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x109a5d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x109a5daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x109a5dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x109a5e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109a5e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109a5f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109a5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109a60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109a60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109a60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109a61410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109a616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109a61ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10a6086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10a608b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10a608f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10a6093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10a609860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10a609cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10a60a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10a60a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10a60aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10a60ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10a60b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10a60b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10a60c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10a60cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10a60d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10a60dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10a60e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10a60ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10a60f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10a60f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10a610020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10a610740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10a610e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10a611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10a611ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10a611f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10a612220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10a612690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10a612b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10a612f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10a6133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10a613910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10a613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10a614040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10a6144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10a614920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10a614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10a615200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10a615670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10a615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10a615f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10a6163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10a616830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10a616ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10a617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10a617580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10a6179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10a617e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10a6182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10a618740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10a618bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10a619020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10a619490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10a619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10a619d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10a61a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10a61a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10a61ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10a61b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10a61b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10a61b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10a61be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10a61c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10a61c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10a61cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10a61cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10a61d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10a61d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10a61dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10a61e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10a61e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10a61ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10a61eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10a61f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10a61f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10a61fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10a6200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10a620510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10a620980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10a620df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10a621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10a6216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10a621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10a621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10a622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10a622890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10a622d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10a623170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10a6235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10a623a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10a623ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10a624330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10a6247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10a624c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10a625080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10a6254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10a625960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10a625dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10a626630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10a626b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10a627100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10a6276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10a627c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10a628210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10a6287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10a628d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10a629320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10a6298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10a629e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10a62a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10a62a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10a62af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10a62b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10a62baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10a62bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10a62c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10a62c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10a62cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10a62d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10a62d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10a62ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10a62e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10a62e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10a62ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10a62f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10a62f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10a62fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10a6300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10a6305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10a630af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10a630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10a6314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10a6319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10a631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10a6323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10a6328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10a632df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10a6332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10a6337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10a633cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10a6341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10a6346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10a634bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10a6350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10a6355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10a635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10a635ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10a6364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10a6369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10a636ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10a6373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10a6378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10a637df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10a6382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10a6387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10a638cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10a6391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10a6396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10a639bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10a63a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10a63a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10a63aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10a63aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10a63b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10a63b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10a63bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10a63c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10a63c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10a63cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10a63d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10a63d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10a63dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10a63e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10a63e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10a63ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10a63f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10a63f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10a63faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10a63fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10a6404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10a6409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10a640ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10a6413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10a6418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10a641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10a6422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10a6427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10a642cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10a6431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10a6436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10a643bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10a6440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10a6445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10a644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10a6450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10a645650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10a645c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10a6461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10a6467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10a646dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10a6473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10a647bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10a648070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10a648330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10a648940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10a648f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10a649740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10a649be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10a64a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10a64a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10a64acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10a64b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10a64b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10a64bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10a64c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10a64c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10a64ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10a64d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10a64d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10a64dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10a64e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10a64e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10a64ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10a64f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10a64f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10a64fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10a6501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10a650720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10a650c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10a6511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10a651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10a651c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10a6521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10a652700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10a652c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10a6531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10a6536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10a653c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10a654190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10a6546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10a654c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10a655180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10a6556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10a655c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10a656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10a6566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10a656c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10a657160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10a6576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10a657c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10a658150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10a6586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10a658bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10a659140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10a659690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10a659be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10a65a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10a65a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10a65abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10a65b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10a65b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10a65bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10a65c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10a65c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10a65cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10a65d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10a65d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10a65daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10a65df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10a65e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10a65e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10a65ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10a65f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10a65f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10a65fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10a65fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10a660490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10a660930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10a660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10a661270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10a661710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10a661bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10a662050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10a6624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10a662990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10a662e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10a6632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10a663770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10a663c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10a6640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10a664550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10a6649f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10a664f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10a665660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10a665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10a6664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10a666bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10a666e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10a667670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10a667930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10a667f40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.753s
user	0m0.282s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4794 (06c2b156)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147e0a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147e0ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147e0b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147e0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147e0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147e0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147e0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147e0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147e0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147e0d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147e0deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147e0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147e0eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147e0f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147e0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147e105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147e10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147e113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147e11b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147e122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147e12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147e13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147e13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147e140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147e14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147e14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147e150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147e15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147e16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147e16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147e169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147e16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147e17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147e17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147e181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147e18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147e18b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147e18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147e19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147e198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147e19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147e1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147e1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147e1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147e1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147e1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147e1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147e1c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147e1caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147e1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147e1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147e1dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147e1e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147e1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147e1efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147e1f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147e1f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147e1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147e20520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147e207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147e20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147e21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147e215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147e21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147e21f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147e223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147e22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147e22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147e23180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147e23620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147e23ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147e23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147e244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147e24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147e24f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147e254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147e259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147e25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147e26490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147e269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147e26f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147e27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147e279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147e27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147e28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147e289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147e28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147e29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147e299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147e29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147e2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147e2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147e2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147e2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147e2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147e2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147e1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147e2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147e2cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147e2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147e2d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147e2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147e2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147e2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147e2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147e2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147e2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147e2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147e30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147e30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147e30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147e31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147e314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147e31950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147e31df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147e32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147e32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147e32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147e33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147e33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147e339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147e342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147e34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147e34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147e350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147e35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147e35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147e35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147e36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147e367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147e36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147e37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147e375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147e37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147e383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147e38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147e39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147e39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147e39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147e39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147e3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147e3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147e3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147e3b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147e3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147e3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147e3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147e3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147e3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147e3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147e3d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147e3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147e3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147e3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147e3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147e3e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147e3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147e3f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147e3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147e3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147e40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147e40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147e409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147e40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147e41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147e417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147e41c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147e420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147e42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147e42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147e42ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147e43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147e43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147e43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147e44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147e445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147e44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147e44f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147e453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147e45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147e461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147e46650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147e46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147e46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147e47430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147e478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147e47d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147e48210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147e48760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147e48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147e49750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147e49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147e4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147e4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147e4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147e4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147e4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147e4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147e4c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147e4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147e4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147e4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147e4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147e4dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147e4ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147e4f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147e4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147e4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147e50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147e50a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147e50fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147e51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147e51a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147e51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147e524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147e52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147e52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147e534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147e53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147e53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147e544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147e54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147e54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147e554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147e55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147e55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147e564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147e56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147e56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147e574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147e579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147e57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147e58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147e589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147e58f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147e59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147e599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147e59f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147e5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147e5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147e5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147e5b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147e5b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147e5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147e5c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147e5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147e5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147e5d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147e5d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147e5dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147e5e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147e5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147e5eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147e5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147e5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147e5fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147e60410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147e60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147e60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147e61350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147e617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147e61c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147e62130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147e625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147e62a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147e62f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147e633b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147e63850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147e63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147e64190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147e64630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147e64ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147e64f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147e65410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x147e658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x147e65d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x147e661f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x147e66690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x147e66b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x147e66fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x147e67470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x147e67910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x147e67db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x147e68250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147e687a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147e68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147e695e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147e69d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147e6a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147e6a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147e6aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147e6b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147e6b7a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.739 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157e05460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157e058d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157e05d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157e061b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157e06620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157e06a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157e06f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157e07370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157e077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157e07c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157e080c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157e08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157e092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157e09a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157e0a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157e0a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157e0b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157e0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157e0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157e0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157e0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157e0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157e0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157e0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157e0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157e0ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157e0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157e0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157e0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157e0fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157e10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157e10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157e10ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157e11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157e121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157e126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157e12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157e130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157e135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157e13aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157e13fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157e144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157e14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157e14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157e151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157e15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157e15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157e15f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157e163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157e16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157e16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157e17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157e178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157e17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157e18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157e18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157e18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157e192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157e19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157e19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157e1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157e1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157e1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157e1ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157e1b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157e1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157e1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157e1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157e1cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157e1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157e1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157e1df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157e1e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157e1ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157e1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157e1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157e1fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157e1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157e204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157e20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157e20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157e214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157e21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157e21f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157e224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157e22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157e23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157e239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157e23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157e24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157e249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157e24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157e25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157e259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157e25f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157e26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157e269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157e26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157e27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157e279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157e27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157e28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157e28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157e28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157e29430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157e29980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157e29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157e2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157e2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157e2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157e2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157e2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157e2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157e2bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157e2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157e2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157e2cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157e2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157e2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157e2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157e2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157e2e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157e2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157e2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157e2f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157e2f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157e2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157e2fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157e30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157e30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157e30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157e31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157e31710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157e31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157e32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157e324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157e32990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157e32e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157e332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157e33770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157e33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157e340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157e34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157e349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157e34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157e35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157e357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157e35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157e36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157e365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157e36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157e36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157e37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157e37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157e37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157e38170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157e38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157e38ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157e38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157e393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157e39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157e39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157e3a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157e3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157e3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157e3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157e3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157e3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157e3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157e3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157e3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157e3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157e3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157e3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157e3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157e3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157e3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157e3e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157e3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157e3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157e3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157e3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157e3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157e402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157e40790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157e40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157e410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157e41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157e41b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157e420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157e42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157e428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157e42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157e434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157e43b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157e442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157e44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157e44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157e45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157e45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157e45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157e46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157e467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157e46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157e473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157e47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157e47e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157e483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157e48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157e48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157e493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157e49920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157e49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157e4a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157e4ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157e4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157e4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157e4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157e4c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157e4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157e4ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157e4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157e4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157e4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157e4e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157e4e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157e4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157e4f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157e4f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157e4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157e50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157e508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157e50e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157e51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157e518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157e51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157e52340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157e52890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157e52de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157e53330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157e53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157e53dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157e54320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157e54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157e54dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157e55310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157e55860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157e55db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157e56300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157e56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157e56da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157e572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157e57840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157e57d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157e582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157e58830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157e58d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157e592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157e59820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157e59d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157e5a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157e5a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157e5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157e5aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157e5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157e5b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157e5bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157e5c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157e5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157e5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157e5d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157e5d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157e5d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157e5de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157e5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x157e5e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x157e5ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x157e5f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x157e5f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x157e5f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x157e5fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x157e60330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x157e607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x157e60c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x157e61110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157e61660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157e61d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157e624a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157e62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157e632e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157e635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157e63d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157e64050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157e64660 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1588080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158808560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1588089d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158808e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1588092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158809720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158809b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15880a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15880a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15880a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15880ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15880b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15880bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15880c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15880cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15880d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15880dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15880e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15880ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15880f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15880fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1588101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1588108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158811010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158811730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1588119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158811cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158812120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158812590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158812a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158812e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1588133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158813810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158813ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158813f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1588143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158814820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158814c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158815100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158815570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1588159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158815e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1588162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158816730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158817010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158817480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1588178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158817d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1588181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158818640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158818ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158818f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158819390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158819800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158819c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15881a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15881a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15881ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15881afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15881b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15881b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15881bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15881c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15881c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15881ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15881ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15881d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15881d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15881dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15881e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15881e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15881e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15881ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15881f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15881f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15881fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15881ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158820410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158820cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158821160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1588215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158821a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158821eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158822320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158822790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158822c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158823070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1588234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158823950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158823dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158824230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1588246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158824b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158824f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1588253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158825860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1588260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1588265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158826b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158827140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1588276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158827ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158828250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158828800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158828db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158829360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158829910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158829ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15882a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15882aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15882afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15882b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15882ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15882bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15882c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15882c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15882ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15882d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15882d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15882dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15882e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15882e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15882ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15882f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15882f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15882fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158830080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158830580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158830a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158830f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158831480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158831980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158831e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158832880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158832d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158833280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158833780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158833c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158834180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158834680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158834b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158835080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158835580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158835a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158835f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158836480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158836980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158836e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158837380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158837880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158837d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158838280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158838780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158838c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158839180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158839680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158839b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15883a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15883a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15883aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15883af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15883b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15883b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15883be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15883c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15883c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15883cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15883d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15883d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15883dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15883e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15883e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15883eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15883f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15883f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15883fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15883ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158840480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158840980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158840e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158841380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158841880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158841d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158842280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158842780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158842c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158843180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158843680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158843b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158844080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158844580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1588450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158845c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158846860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158846e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158847660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158847b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158847dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1588483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1588489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1588491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158849670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158849b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158849fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15884a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15884acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15884b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15884b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15884bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15884c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15884c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15884cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15884d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15884d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15884dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15884e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15884e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15884ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15884f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15884f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15884fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1588501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158850700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158850c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1588511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1588516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158851c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158852190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1588526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158852c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158853180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1588536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158853c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158854170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1588546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158854c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158855160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1588556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158855c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158856150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1588566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158856bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158857140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158857690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158857be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158858130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158858680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158858bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158859120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158859670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158859bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15885a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15885a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15885abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15885b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15885b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15885bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15885c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15885c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15885cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15885d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15885d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15885da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15885dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15885e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15885e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15885eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15885f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15885f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15885fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15885ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1588603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158860860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158860d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1588611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158861640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x158861ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x158861f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x158862420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1588628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x158862d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x158863200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1588636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x158863b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x158863fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x158864480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1588649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1588650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158865810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158865f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158866650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158866910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158867100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1588673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1588679d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.950s
user	0m0.231s
sys	0m0.187s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.49 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.66 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.15 sec*proc (2 tests)

Total Test time (real) =   2.16 sec
        2.18 real         0.53 user         0.28 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
