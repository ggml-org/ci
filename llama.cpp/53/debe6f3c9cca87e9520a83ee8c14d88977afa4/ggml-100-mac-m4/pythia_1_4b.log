Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.599s
user	0m0.908s
sys	0m1.245s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target build_info
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target xxhash
[  6%] Built target sha1
[  6%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library ../../bin/libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 33%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 33%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 34%] Linking C executable ../bin/test-c
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 36%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Built target test-sampling
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-chat
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Built target test-log
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 63%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 63%] Built target test-arg-parser
[ 63%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Built target test-gguf
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-backend-ops
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Built target test-barrier
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched-bench
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Built target llama-infill
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Built target llama-lookup
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-create
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-passkey
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-retrieval
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-export-lora
[ 99%] Built target llama-gen-docs
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-export-lora
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.019s
user	0m6.438s
sys	0m9.611s

main: quantize time =  5288.63 ms
main:    total time =  5288.63 ms

main: quantize time =  3678.92 ms
main:    total time =  3678.92 ms

main: quantize time =  4175.36 ms
main:    total time =  4175.36 ms

main: quantize time =  2123.28 ms
main:    total time =  2123.28 ms

main: quantize time =  2055.85 ms
main:    total time =  2055.85 ms

main: quantize time =  5171.13 ms
main:    total time =  5171.13 ms

main: quantize time =  6014.77 ms
main:    total time =  6014.77 ms

main: quantize time =  6992.92 ms
main:    total time =  6992.92 ms

main: quantize time =  6401.81 ms
main:    total time =  6401.81 ms

main: quantize time =  4596.92 ms
main:    total time =  4596.92 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.152 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.300 I main: llama backend init
0.00.000.307 I main: load the model and apply lora adapter, if any
0.00.060.219 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.073.010 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.073.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.073.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.073.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.073.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.073.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.073.039 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.073.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.073.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.073.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.073.044 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.073.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.073.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.073.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.073.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.073.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.073.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.080.175 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.091.290 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.091.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.091.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.091.300 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.091.300 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.091.301 I llama_model_loader: - type  f32:  194 tensors
0.00.091.302 I llama_model_loader: - type  f16:   98 tensors
0.00.091.303 I print_info: file format = GGUF V3 (latest)
0.00.091.308 I print_info: file type   = all F32 (guessed)
0.00.091.317 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.108.072 I load: special tokens cache size = 25
0.00.117.814 I load: token to piece cache size = 0.2984 MB
0.00.117.817 I print_info: arch             = gptneox
0.00.117.817 I print_info: vocab_only       = 0
0.00.117.818 I print_info: n_ctx_train      = 2048
0.00.117.818 I print_info: n_embd           = 2048
0.00.117.818 I print_info: n_layer          = 24
0.00.117.822 I print_info: n_head           = 16
0.00.117.823 I print_info: n_head_kv        = 16
0.00.117.823 I print_info: n_rot            = 32
0.00.117.823 I print_info: n_swa            = 0
0.00.117.823 I print_info: n_embd_head_k    = 128
0.00.117.824 I print_info: n_embd_head_v    = 128
0.00.117.824 I print_info: n_gqa            = 1
0.00.117.825 I print_info: n_embd_k_gqa     = 2048
0.00.117.828 I print_info: n_embd_v_gqa     = 2048
0.00.117.829 I print_info: f_norm_eps       = 1.0e-05
0.00.117.830 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.117.830 I print_info: f_clamp_kqv      = 0.0e+00
0.00.117.830 I print_info: f_max_alibi_bias = 0.0e+00
0.00.117.830 I print_info: f_logit_scale    = 0.0e+00
0.00.117.831 I print_info: n_ff             = 8192
0.00.117.831 I print_info: n_expert         = 0
0.00.117.832 I print_info: n_expert_used    = 0
0.00.117.832 I print_info: causal attn      = 1
0.00.117.832 I print_info: pooling type     = 0
0.00.117.832 I print_info: rope type        = 2
0.00.117.833 I print_info: rope scaling     = linear
0.00.117.833 I print_info: freq_base_train  = 10000.0
0.00.117.833 I print_info: freq_scale_train = 1
0.00.117.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.117.834 I print_info: rope_finetuned   = unknown
0.00.117.834 I print_info: ssm_d_conv       = 0
0.00.117.834 I print_info: ssm_d_inner      = 0
0.00.117.835 I print_info: ssm_d_state      = 0
0.00.117.835 I print_info: ssm_dt_rank      = 0
0.00.117.835 I print_info: ssm_dt_b_c_rms   = 0
0.00.117.835 I print_info: model type       = 1.4B
0.00.117.836 I print_info: model params     = 1.41 B
0.00.117.836 I print_info: general.name     = 1.4B
0.00.117.837 I print_info: vocab type       = BPE
0.00.117.838 I print_info: n_vocab          = 50304
0.00.117.838 I print_info: n_merges         = 50009
0.00.117.839 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.117.839 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.117.839 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.117.839 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.117.839 I print_info: LF token         = 187 'Ċ'
0.00.117.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.117.840 I print_info: max token length = 1024
0.00.165.487 I load_tensors: offloading 24 repeating layers to GPU
0.00.165.491 I load_tensors: offloading output layer to GPU
0.00.165.492 I load_tensors: offloaded 25/25 layers to GPU
0.00.165.519 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.165.520 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.166.002 I llama_init_from_model: n_seq_max     = 1
0.00.166.003 I llama_init_from_model: n_ctx         = 2048
0.00.166.003 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.166.003 I llama_init_from_model: n_batch       = 2048
0.00.166.003 I llama_init_from_model: n_ubatch      = 512
0.00.166.003 I llama_init_from_model: flash_attn    = 0
0.00.166.004 I llama_init_from_model: freq_base     = 10000.0
0.00.166.004 I llama_init_from_model: freq_scale    = 1
0.00.166.005 I ggml_metal_init: allocating
0.00.166.036 I ggml_metal_init: found device: Apple M4
0.00.166.042 I ggml_metal_init: picking default device: Apple M4
0.00.166.677 I ggml_metal_init: using embedded metal library
0.00.182.035 I ggml_metal_init: GPU name:   Apple M4
0.00.182.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.182.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.182.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.182.037 I ggml_metal_init: simdgroup reduction   = true
0.00.182.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.182.038 I ggml_metal_init: has residency sets    = true
0.00.182.038 I ggml_metal_init: has bfloat            = true
0.00.182.038 I ggml_metal_init: use bfloat            = true
0.00.182.038 I ggml_metal_init: hasUnifiedMemory      = true
0.00.182.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.336.261 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.365.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.365.397 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.365.419 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.369.465 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.369.468 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.369.468 I llama_init_from_model: graph nodes  = 967
0.00.369.468 I llama_init_from_model: graph splits = 2
0.00.369.475 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.369.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.369.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.436.919 I main: llama threadpool init, n_threads = 4
0.00.436.965 I 
0.00.437.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.003 I 
0.00.437.175 I sampler seed: 1234
0.00.437.180 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.204 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.206 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.206 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.277.279 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.277.280 I llama_perf_context_print:        load time =     375.60 ms
0.02.277.282 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.32 tokens per second)
0.02.277.282 I llama_perf_context_print:        eval time =    1793.48 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.277.283 I llama_perf_context_print:       total time =    1841.45 ms /    70 tokens
0.02.277.492 I ggml_metal_free: deallocating

real	0m2.588s
user	0m0.136s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.942 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.089 I llama_model_loader: - type  f32:  194 tensors
0.00.034.089 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.090 I print_info: file format = GGUF V3 (latest)
0.00.034.091 I print_info: file type   = Q8_0
0.00.034.095 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.841 I load: special tokens cache size = 25
0.00.048.991 I load: token to piece cache size = 0.2984 MB
0.00.048.995 I print_info: arch             = gptneox
0.00.048.995 I print_info: vocab_only       = 0
0.00.048.996 I print_info: n_ctx_train      = 2048
0.00.048.999 I print_info: n_embd           = 2048
0.00.048.999 I print_info: n_layer          = 24
0.00.049.005 I print_info: n_head           = 16
0.00.049.006 I print_info: n_head_kv        = 16
0.00.049.006 I print_info: n_rot            = 32
0.00.049.006 I print_info: n_swa            = 0
0.00.049.006 I print_info: n_embd_head_k    = 128
0.00.049.006 I print_info: n_embd_head_v    = 128
0.00.049.007 I print_info: n_gqa            = 1
0.00.049.008 I print_info: n_embd_k_gqa     = 2048
0.00.049.008 I print_info: n_embd_v_gqa     = 2048
0.00.049.009 I print_info: f_norm_eps       = 1.0e-05
0.00.049.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.010 I print_info: f_logit_scale    = 0.0e+00
0.00.049.011 I print_info: n_ff             = 8192
0.00.049.011 I print_info: n_expert         = 0
0.00.049.011 I print_info: n_expert_used    = 0
0.00.049.011 I print_info: causal attn      = 1
0.00.049.012 I print_info: pooling type     = 0
0.00.049.012 I print_info: rope type        = 2
0.00.049.013 I print_info: rope scaling     = linear
0.00.049.013 I print_info: freq_base_train  = 10000.0
0.00.049.014 I print_info: freq_scale_train = 1
0.00.049.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.014 I print_info: rope_finetuned   = unknown
0.00.049.015 I print_info: ssm_d_conv       = 0
0.00.049.015 I print_info: ssm_d_inner      = 0
0.00.049.015 I print_info: ssm_d_state      = 0
0.00.049.015 I print_info: ssm_dt_rank      = 0
0.00.049.015 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.016 I print_info: model type       = 1.4B
0.00.049.016 I print_info: model params     = 1.41 B
0.00.049.016 I print_info: general.name     = 1.4B
0.00.049.017 I print_info: vocab type       = BPE
0.00.049.017 I print_info: n_vocab          = 50304
0.00.049.017 I print_info: n_merges         = 50009
0.00.049.017 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.018 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.027 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.029 I print_info: LF token         = 187 'Ċ'
0.00.049.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.030 I print_info: max token length = 1024
0.01.214.066 I load_tensors: offloading 24 repeating layers to GPU
0.01.214.070 I load_tensors: offloading output layer to GPU
0.01.214.072 I load_tensors: offloaded 25/25 layers to GPU
0.01.214.096 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.214.097 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.214.825 I llama_init_from_model: n_seq_max     = 1
0.01.214.827 I llama_init_from_model: n_ctx         = 2048
0.01.214.827 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.214.827 I llama_init_from_model: n_batch       = 2048
0.01.214.828 I llama_init_from_model: n_ubatch      = 512
0.01.214.828 I llama_init_from_model: flash_attn    = 0
0.01.214.829 I llama_init_from_model: freq_base     = 10000.0
0.01.214.829 I llama_init_from_model: freq_scale    = 1
0.01.214.830 I ggml_metal_init: allocating
0.01.214.846 I ggml_metal_init: found device: Apple M4
0.01.214.853 I ggml_metal_init: picking default device: Apple M4
0.01.215.998 I ggml_metal_init: using embedded metal library
0.01.221.055 I ggml_metal_init: GPU name:   Apple M4
0.01.221.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.221.058 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.221.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.221.059 I ggml_metal_init: simdgroup reduction   = true
0.01.221.060 I ggml_metal_init: simdgroup matrix mul. = true
0.01.221.060 I ggml_metal_init: has residency sets    = true
0.01.221.060 I ggml_metal_init: has bfloat            = true
0.01.221.060 I ggml_metal_init: use bfloat            = true
0.01.221.061 I ggml_metal_init: hasUnifiedMemory      = true
0.01.221.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.239.101 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.290.972 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.290.978 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.291.001 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.295.898 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.295.900 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.295.900 I llama_init_from_model: graph nodes  = 967
0.01.295.900 I llama_init_from_model: graph splits = 2
0.01.295.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.296.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.296.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.351.131 I main: llama threadpool init, n_threads = 4
0.01.351.171 I 
0.01.351.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.351.197 I 
0.01.351.345 I sampler seed: 1234
0.01.351.350 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.351.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.351.361 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.351.361 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.451.967 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.02.451.967 I llama_perf_context_print:        load time =    1340.27 ms
0.02.451.969 I llama_perf_context_print: prompt eval time =      48.86 ms /     7 tokens (    6.98 ms per token,   143.28 tokens per second)
0.02.451.970 I llama_perf_context_print:        eval time =    1049.00 ms /    63 runs   (   16.65 ms per token,    60.06 tokens per second)
0.02.451.970 I llama_perf_context_print:       total time =    1101.75 ms /    70 tokens
0.02.452.191 I ggml_metal_free: deallocating

real	0m2.471s
user	0m0.109s
sys	0m0.257s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.016.177 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.822 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.823 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.823 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.217 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.526 I llama_model_loader: - type  f32:  194 tensors
0.00.039.526 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.526 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.527 I print_info: file format = GGUF V3 (latest)
0.00.039.528 I print_info: file type   = Q4_0
0.00.039.529 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.049.177 I load: special tokens cache size = 25
0.00.056.533 I load: token to piece cache size = 0.2984 MB
0.00.056.537 I print_info: arch             = gptneox
0.00.056.537 I print_info: vocab_only       = 0
0.00.056.537 I print_info: n_ctx_train      = 2048
0.00.056.537 I print_info: n_embd           = 2048
0.00.056.538 I print_info: n_layer          = 24
0.00.056.542 I print_info: n_head           = 16
0.00.056.543 I print_info: n_head_kv        = 16
0.00.056.543 I print_info: n_rot            = 32
0.00.056.544 I print_info: n_swa            = 0
0.00.056.545 I print_info: n_embd_head_k    = 128
0.00.056.546 I print_info: n_embd_head_v    = 128
0.00.056.547 I print_info: n_gqa            = 1
0.00.056.548 I print_info: n_embd_k_gqa     = 2048
0.00.056.549 I print_info: n_embd_v_gqa     = 2048
0.00.056.549 I print_info: f_norm_eps       = 1.0e-05
0.00.056.550 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.552 I print_info: f_logit_scale    = 0.0e+00
0.00.056.552 I print_info: n_ff             = 8192
0.00.056.553 I print_info: n_expert         = 0
0.00.056.553 I print_info: n_expert_used    = 0
0.00.056.553 I print_info: causal attn      = 1
0.00.056.553 I print_info: pooling type     = 0
0.00.056.553 I print_info: rope type        = 2
0.00.056.553 I print_info: rope scaling     = linear
0.00.056.554 I print_info: freq_base_train  = 10000.0
0.00.056.554 I print_info: freq_scale_train = 1
0.00.056.554 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.554 I print_info: rope_finetuned   = unknown
0.00.056.555 I print_info: ssm_d_conv       = 0
0.00.056.555 I print_info: ssm_d_inner      = 0
0.00.056.555 I print_info: ssm_d_state      = 0
0.00.056.556 I print_info: ssm_dt_rank      = 0
0.00.056.556 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.560 I print_info: model type       = 1.4B
0.00.056.560 I print_info: model params     = 1.41 B
0.00.056.560 I print_info: general.name     = 1.4B
0.00.056.562 I print_info: vocab type       = BPE
0.00.056.563 I print_info: n_vocab          = 50304
0.00.056.563 I print_info: n_merges         = 50009
0.00.056.563 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.563 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.563 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.565 I print_info: LF token         = 187 'Ċ'
0.00.056.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.566 I print_info: max token length = 1024
0.00.620.926 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.940 I load_tensors: offloading output layer to GPU
0.00.620.941 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.975 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.620.976 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.622.197 I llama_init_from_model: n_seq_max     = 1
0.00.622.202 I llama_init_from_model: n_ctx         = 2048
0.00.622.203 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.622.203 I llama_init_from_model: n_batch       = 2048
0.00.622.204 I llama_init_from_model: n_ubatch      = 512
0.00.622.204 I llama_init_from_model: flash_attn    = 0
0.00.622.206 I llama_init_from_model: freq_base     = 10000.0
0.00.622.207 I llama_init_from_model: freq_scale    = 1
0.00.622.210 I ggml_metal_init: allocating
0.00.622.282 I ggml_metal_init: found device: Apple M4
0.00.622.297 I ggml_metal_init: picking default device: Apple M4
0.00.623.999 I ggml_metal_init: using embedded metal library
0.00.629.544 I ggml_metal_init: GPU name:   Apple M4
0.00.629.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.551 I ggml_metal_init: simdgroup reduction   = true
0.00.629.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.552 I ggml_metal_init: has residency sets    = true
0.00.629.553 I ggml_metal_init: has bfloat            = true
0.00.629.553 I ggml_metal_init: use bfloat            = true
0.00.629.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.824 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.443 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.450 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.473 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.910 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.711.912 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.711.912 I llama_init_from_model: graph nodes  = 967
0.00.711.913 I llama_init_from_model: graph splits = 2
0.00.711.919 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.826 I main: llama threadpool init, n_threads = 4
0.00.769.870 I 
0.00.769.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.895 I 
0.00.770.046 I sampler seed: 1234
0.00.770.051 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.071 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.071 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.071 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.466.335 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.466.336 I llama_perf_context_print:        load time =     752.70 ms
0.01.466.337 I llama_perf_context_print: prompt eval time =      49.35 ms /     7 tokens (    7.05 ms per token,   141.84 tokens per second)
0.01.466.338 I llama_perf_context_print:        eval time =     643.99 ms /    63 runs   (   10.22 ms per token,    97.83 tokens per second)
0.01.466.339 I llama_perf_context_print:       total time =     697.46 ms /    70 tokens
0.01.466.565 I ggml_metal_free: deallocating

real	0m1.486s
user	0m0.115s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.726 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.068 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.733 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.734 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.735 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.735 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.736 I llama_model_loader: - type  f32:  194 tensors
0.00.029.736 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.737 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.737 I print_info: file format = GGUF V3 (latest)
0.00.029.738 I print_info: file type   = Q4_1
0.00.029.738 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.038.195 I load: special tokens cache size = 25
0.00.044.495 I load: token to piece cache size = 0.2984 MB
0.00.044.498 I print_info: arch             = gptneox
0.00.044.498 I print_info: vocab_only       = 0
0.00.044.499 I print_info: n_ctx_train      = 2048
0.00.044.499 I print_info: n_embd           = 2048
0.00.044.499 I print_info: n_layer          = 24
0.00.044.504 I print_info: n_head           = 16
0.00.044.504 I print_info: n_head_kv        = 16
0.00.044.505 I print_info: n_rot            = 32
0.00.044.505 I print_info: n_swa            = 0
0.00.044.505 I print_info: n_embd_head_k    = 128
0.00.044.507 I print_info: n_embd_head_v    = 128
0.00.044.507 I print_info: n_gqa            = 1
0.00.044.508 I print_info: n_embd_k_gqa     = 2048
0.00.044.509 I print_info: n_embd_v_gqa     = 2048
0.00.044.515 I print_info: f_norm_eps       = 1.0e-05
0.00.044.515 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.517 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.517 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.517 I print_info: f_logit_scale    = 0.0e+00
0.00.044.518 I print_info: n_ff             = 8192
0.00.044.518 I print_info: n_expert         = 0
0.00.044.518 I print_info: n_expert_used    = 0
0.00.044.518 I print_info: causal attn      = 1
0.00.044.519 I print_info: pooling type     = 0
0.00.044.520 I print_info: rope type        = 2
0.00.044.520 I print_info: rope scaling     = linear
0.00.044.521 I print_info: freq_base_train  = 10000.0
0.00.044.521 I print_info: freq_scale_train = 1
0.00.044.522 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.522 I print_info: rope_finetuned   = unknown
0.00.044.522 I print_info: ssm_d_conv       = 0
0.00.044.522 I print_info: ssm_d_inner      = 0
0.00.044.522 I print_info: ssm_d_state      = 0
0.00.044.522 I print_info: ssm_dt_rank      = 0
0.00.044.523 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.523 I print_info: model type       = 1.4B
0.00.044.523 I print_info: model params     = 1.41 B
0.00.044.523 I print_info: general.name     = 1.4B
0.00.044.524 I print_info: vocab type       = BPE
0.00.044.525 I print_info: n_vocab          = 50304
0.00.044.525 I print_info: n_merges         = 50009
0.00.044.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.525 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.526 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.526 I print_info: LF token         = 187 'Ċ'
0.00.044.526 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.526 I print_info: max token length = 1024
0.00.671.049 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.068 I load_tensors: offloading output layer to GPU
0.00.671.068 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.100 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.671.101 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.672.685 I llama_init_from_model: n_seq_max     = 1
0.00.672.690 I llama_init_from_model: n_ctx         = 2048
0.00.672.691 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.672.692 I llama_init_from_model: n_batch       = 2048
0.00.672.692 I llama_init_from_model: n_ubatch      = 512
0.00.672.692 I llama_init_from_model: flash_attn    = 0
0.00.672.695 I llama_init_from_model: freq_base     = 10000.0
0.00.672.695 I llama_init_from_model: freq_scale    = 1
0.00.672.701 I ggml_metal_init: allocating
0.00.672.795 I ggml_metal_init: found device: Apple M4
0.00.672.809 I ggml_metal_init: picking default device: Apple M4
0.00.674.639 I ggml_metal_init: using embedded metal library
0.00.681.107 I ggml_metal_init: GPU name:   Apple M4
0.00.681.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.112 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.114 I ggml_metal_init: simdgroup reduction   = true
0.00.681.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.114 I ggml_metal_init: has residency sets    = true
0.00.681.114 I ggml_metal_init: has bfloat            = true
0.00.681.115 I ggml_metal_init: use bfloat            = true
0.00.681.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.755.431 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.755.438 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.755.460 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.759.936 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.759.938 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.759.939 I llama_init_from_model: graph nodes  = 967
0.00.759.939 I llama_init_from_model: graph splits = 2
0.00.759.944 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.760.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.760.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.289 I main: llama threadpool init, n_threads = 4
0.00.816.332 I 
0.00.816.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.359 I 
0.00.816.531 I sampler seed: 1234
0.00.816.536 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.581 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.582 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.584 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.556.261 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.556.262 I llama_perf_context_print:        load time =     806.63 ms
0.01.556.263 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.39 tokens per second)
0.01.556.263 I llama_perf_context_print:        eval time =     687.75 ms /    63 runs   (   10.92 ms per token,    91.60 tokens per second)
0.01.556.264 I llama_perf_context_print:       total time =     740.90 ms /    70 tokens
0.01.556.497 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.908 I llama_model_loader: - type  f32:  194 tensors
0.00.025.909 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.909 I print_info: file format = GGUF V3 (latest)
0.00.025.910 I print_info: file type   = Q5_0
0.00.025.910 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.556 I load: special tokens cache size = 25
0.00.039.611 I load: token to piece cache size = 0.2984 MB
0.00.039.614 I print_info: arch             = gptneox
0.00.039.614 I print_info: vocab_only       = 0
0.00.039.614 I print_info: n_ctx_train      = 2048
0.00.039.615 I print_info: n_embd           = 2048
0.00.039.615 I print_info: n_layer          = 24
0.00.039.617 I print_info: n_head           = 16
0.00.039.618 I print_info: n_head_kv        = 16
0.00.039.618 I print_info: n_rot            = 32
0.00.039.621 I print_info: n_swa            = 0
0.00.039.621 I print_info: n_embd_head_k    = 128
0.00.039.622 I print_info: n_embd_head_v    = 128
0.00.039.622 I print_info: n_gqa            = 1
0.00.039.623 I print_info: n_embd_k_gqa     = 2048
0.00.039.624 I print_info: n_embd_v_gqa     = 2048
0.00.039.624 I print_info: f_norm_eps       = 1.0e-05
0.00.039.625 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.625 I print_info: f_logit_scale    = 0.0e+00
0.00.039.626 I print_info: n_ff             = 8192
0.00.039.626 I print_info: n_expert         = 0
0.00.039.626 I print_info: n_expert_used    = 0
0.00.039.631 I print_info: causal attn      = 1
0.00.039.633 I print_info: pooling type     = 0
0.00.039.635 I print_info: rope type        = 2
0.00.039.636 I print_info: rope scaling     = linear
0.00.039.636 I print_info: freq_base_train  = 10000.0
0.00.039.638 I print_info: freq_scale_train = 1
0.00.039.638 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.638 I print_info: rope_finetuned   = unknown
0.00.039.638 I print_info: ssm_d_conv       = 0
0.00.039.639 I print_info: ssm_d_inner      = 0
0.00.039.639 I print_info: ssm_d_state      = 0
0.00.039.639 I print_info: ssm_dt_rank      = 0
0.00.039.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.639 I print_info: model type       = 1.4B
0.00.039.640 I print_info: model params     = 1.41 B
0.00.039.640 I print_info: general.name     = 1.4B
0.00.039.640 I print_info: vocab type       = BPE
0.00.039.640 I print_info: n_vocab          = 50304
0.00.039.641 I print_info: n_merges         = 50009
0.00.039.641 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.641 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.644 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.644 I print_info: LF token         = 187 'Ċ'
0.00.039.644 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.644 I print_info: max token length = 1024
0.00.687.082 I load_tensors: offloading 24 repeating layers to GPU
0.00.687.097 I load_tensors: offloading output layer to GPU
0.00.687.098 I load_tensors: offloaded 25/25 layers to GPU
0.00.687.130 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.687.131 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.688.704 I llama_init_from_model: n_seq_max     = 1
0.00.688.708 I llama_init_from_model: n_ctx         = 2048
0.00.688.709 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.688.709 I llama_init_from_model: n_batch       = 2048
0.00.688.709 I llama_init_from_model: n_ubatch      = 512
0.00.688.710 I llama_init_from_model: flash_attn    = 0
0.00.688.712 I llama_init_from_model: freq_base     = 10000.0
0.00.688.712 I llama_init_from_model: freq_scale    = 1
0.00.688.720 I ggml_metal_init: allocating
0.00.688.802 I ggml_metal_init: found device: Apple M4
0.00.688.816 I ggml_metal_init: picking default device: Apple M4
0.00.690.593 I ggml_metal_init: using embedded metal library
0.00.697.260 I ggml_metal_init: GPU name:   Apple M4
0.00.697.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.697.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.697.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.697.266 I ggml_metal_init: simdgroup reduction   = true
0.00.697.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.697.267 I ggml_metal_init: has residency sets    = true
0.00.697.267 I ggml_metal_init: has bfloat            = true
0.00.697.267 I ggml_metal_init: use bfloat            = true
0.00.697.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.697.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.715.003 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.770.351 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.770.360 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.770.393 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.774.760 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.774.763 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.774.763 I llama_init_from_model: graph nodes  = 967
0.00.774.763 I llama_init_from_model: graph splits = 2
0.00.774.770 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.774.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.774.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.902 I main: llama threadpool init, n_threads = 4
0.00.831.951 I 
0.00.831.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.831.978 I 
0.00.832.147 I sampler seed: 1234
0.00.832.151 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.832.162 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.832.162 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.832.162 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.621.723 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.621.724 I llama_perf_context_print:        load time =     821.19 ms
0.01.621.724 I llama_perf_context_print: prompt eval time =      43.02 ms /     7 tokens (    6.15 ms per token,   162.72 tokens per second)
0.01.621.725 I llama_perf_context_print:        eval time =     743.63 ms /    63 runs   (   11.80 ms per token,    84.72 tokens per second)
0.01.621.726 I llama_perf_context_print:       total time =     790.75 ms /    70 tokens
0.01.622.034 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.676 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.224 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.051 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.814 I llama_model_loader: - type  f32:  194 tensors
0.00.025.814 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.815 I print_info: file format = GGUF V3 (latest)
0.00.025.815 I print_info: file type   = Q5_1
0.00.025.816 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.811 I load: special tokens cache size = 25
0.00.039.842 I load: token to piece cache size = 0.2984 MB
0.00.039.845 I print_info: arch             = gptneox
0.00.039.845 I print_info: vocab_only       = 0
0.00.039.845 I print_info: n_ctx_train      = 2048
0.00.039.845 I print_info: n_embd           = 2048
0.00.039.845 I print_info: n_layer          = 24
0.00.039.848 I print_info: n_head           = 16
0.00.039.849 I print_info: n_head_kv        = 16
0.00.039.849 I print_info: n_rot            = 32
0.00.039.849 I print_info: n_swa            = 0
0.00.039.849 I print_info: n_embd_head_k    = 128
0.00.039.852 I print_info: n_embd_head_v    = 128
0.00.039.853 I print_info: n_gqa            = 1
0.00.039.853 I print_info: n_embd_k_gqa     = 2048
0.00.039.854 I print_info: n_embd_v_gqa     = 2048
0.00.039.859 I print_info: f_norm_eps       = 1.0e-05
0.00.039.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.860 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.860 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.860 I print_info: f_logit_scale    = 0.0e+00
0.00.039.861 I print_info: n_ff             = 8192
0.00.039.861 I print_info: n_expert         = 0
0.00.039.861 I print_info: n_expert_used    = 0
0.00.039.862 I print_info: causal attn      = 1
0.00.039.862 I print_info: pooling type     = 0
0.00.039.863 I print_info: rope type        = 2
0.00.039.864 I print_info: rope scaling     = linear
0.00.039.864 I print_info: freq_base_train  = 10000.0
0.00.039.864 I print_info: freq_scale_train = 1
0.00.039.865 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.865 I print_info: rope_finetuned   = unknown
0.00.039.865 I print_info: ssm_d_conv       = 0
0.00.039.865 I print_info: ssm_d_inner      = 0
0.00.039.865 I print_info: ssm_d_state      = 0
0.00.039.866 I print_info: ssm_dt_rank      = 0
0.00.039.867 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.867 I print_info: model type       = 1.4B
0.00.039.867 I print_info: model params     = 1.41 B
0.00.039.867 I print_info: general.name     = 1.4B
0.00.039.868 I print_info: vocab type       = BPE
0.00.039.868 I print_info: n_vocab          = 50304
0.00.039.868 I print_info: n_merges         = 50009
0.00.039.868 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.868 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: LF token         = 187 'Ċ'
0.00.039.869 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.869 I print_info: max token length = 1024
0.00.606.060 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.074 I load_tensors: offloading output layer to GPU
0.00.606.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.109 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.606.111 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.607.707 I llama_init_from_model: n_seq_max     = 1
0.00.607.713 I llama_init_from_model: n_ctx         = 2048
0.00.607.714 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.607.714 I llama_init_from_model: n_batch       = 2048
0.00.607.714 I llama_init_from_model: n_ubatch      = 512
0.00.607.715 I llama_init_from_model: flash_attn    = 0
0.00.607.717 I llama_init_from_model: freq_base     = 10000.0
0.00.607.717 I llama_init_from_model: freq_scale    = 1
0.00.607.724 I ggml_metal_init: allocating
0.00.607.794 I ggml_metal_init: found device: Apple M4
0.00.607.808 I ggml_metal_init: picking default device: Apple M4
0.00.609.613 I ggml_metal_init: using embedded metal library
0.00.616.191 I ggml_metal_init: GPU name:   Apple M4
0.00.616.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.198 I ggml_metal_init: simdgroup reduction   = true
0.00.616.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.198 I ggml_metal_init: has residency sets    = true
0.00.616.198 I ggml_metal_init: has bfloat            = true
0.00.616.199 I ggml_metal_init: use bfloat            = true
0.00.616.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.611 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.411 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.686.418 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.686.449 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.554 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.690.556 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.690.556 I llama_init_from_model: graph nodes  = 967
0.00.690.557 I llama_init_from_model: graph splits = 2
0.00.690.561 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.728 I main: llama threadpool init, n_threads = 4
0.00.751.773 I 
0.00.751.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.798 I 
0.00.751.965 I sampler seed: 1234
0.00.751.969 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.990 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.991 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.601.915 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.601.916 I llama_perf_context_print:        load time =     742.13 ms
0.01.601.916 I llama_perf_context_print: prompt eval time =      52.03 ms /     7 tokens (    7.43 ms per token,   134.55 tokens per second)
0.01.601.917 I llama_perf_context_print:        eval time =     794.98 ms /    63 runs   (   12.62 ms per token,    79.25 tokens per second)
0.01.601.917 I llama_perf_context_print:       total time =     851.11 ms /    70 tokens
0.01.602.196 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.110s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.752 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.299 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.706 I llama_model_loader: - type  f32:  194 tensors
0.00.025.706 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.706 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.707 I print_info: file format = GGUF V3 (latest)
0.00.025.708 I print_info: file type   = Q2_K - Medium
0.00.025.708 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.374 I load: special tokens cache size = 25
0.00.039.342 I load: token to piece cache size = 0.2984 MB
0.00.039.345 I print_info: arch             = gptneox
0.00.039.345 I print_info: vocab_only       = 0
0.00.039.345 I print_info: n_ctx_train      = 2048
0.00.039.345 I print_info: n_embd           = 2048
0.00.039.345 I print_info: n_layer          = 24
0.00.039.348 I print_info: n_head           = 16
0.00.039.349 I print_info: n_head_kv        = 16
0.00.039.349 I print_info: n_rot            = 32
0.00.039.350 I print_info: n_swa            = 0
0.00.039.350 I print_info: n_embd_head_k    = 128
0.00.039.350 I print_info: n_embd_head_v    = 128
0.00.039.351 I print_info: n_gqa            = 1
0.00.039.351 I print_info: n_embd_k_gqa     = 2048
0.00.039.352 I print_info: n_embd_v_gqa     = 2048
0.00.039.353 I print_info: f_norm_eps       = 1.0e-05
0.00.039.353 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.353 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.354 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.354 I print_info: f_logit_scale    = 0.0e+00
0.00.039.355 I print_info: n_ff             = 8192
0.00.039.355 I print_info: n_expert         = 0
0.00.039.355 I print_info: n_expert_used    = 0
0.00.039.355 I print_info: causal attn      = 1
0.00.039.355 I print_info: pooling type     = 0
0.00.039.356 I print_info: rope type        = 2
0.00.039.356 I print_info: rope scaling     = linear
0.00.039.356 I print_info: freq_base_train  = 10000.0
0.00.039.357 I print_info: freq_scale_train = 1
0.00.039.357 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.357 I print_info: rope_finetuned   = unknown
0.00.039.359 I print_info: ssm_d_conv       = 0
0.00.039.359 I print_info: ssm_d_inner      = 0
0.00.039.359 I print_info: ssm_d_state      = 0
0.00.039.360 I print_info: ssm_dt_rank      = 0
0.00.039.360 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.360 I print_info: model type       = 1.4B
0.00.039.360 I print_info: model params     = 1.41 B
0.00.039.361 I print_info: general.name     = 1.4B
0.00.039.361 I print_info: vocab type       = BPE
0.00.039.361 I print_info: n_vocab          = 50304
0.00.039.362 I print_info: n_merges         = 50009
0.00.039.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.362 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.362 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.362 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.366 I print_info: LF token         = 187 'Ċ'
0.00.039.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: max token length = 1024
0.00.344.812 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.828 I load_tensors: offloading output layer to GPU
0.00.344.829 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.862 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.864 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.346.482 I llama_init_from_model: n_seq_max     = 1
0.00.346.488 I llama_init_from_model: n_ctx         = 2048
0.00.346.489 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.346.489 I llama_init_from_model: n_batch       = 2048
0.00.346.490 I llama_init_from_model: n_ubatch      = 512
0.00.346.490 I llama_init_from_model: flash_attn    = 0
0.00.346.491 I llama_init_from_model: freq_base     = 10000.0
0.00.346.497 I llama_init_from_model: freq_scale    = 1
0.00.346.501 I ggml_metal_init: allocating
0.00.346.604 I ggml_metal_init: found device: Apple M4
0.00.346.617 I ggml_metal_init: picking default device: Apple M4
0.00.348.445 I ggml_metal_init: using embedded metal library
0.00.353.949 I ggml_metal_init: GPU name:   Apple M4
0.00.353.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.963 I ggml_metal_init: simdgroup reduction   = true
0.00.353.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.964 I ggml_metal_init: has residency sets    = true
0.00.353.964 I ggml_metal_init: has bfloat            = true
0.00.353.964 I ggml_metal_init: use bfloat            = true
0.00.353.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.376 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.435.082 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.435.088 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.435.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.447 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.439.448 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.439.449 I llama_init_from_model: graph nodes  = 967
0.00.439.449 I llama_init_from_model: graph splits = 2
0.00.439.455 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.119 I main: llama threadpool init, n_threads = 4
0.00.500.163 I 
0.00.500.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.196 I 
0.00.500.368 I sampler seed: 1234
0.00.500.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.383 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.383 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.182.938 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.182.938 I llama_perf_context_print:        load time =     488.45 ms
0.01.182.939 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.56 tokens per second)
0.01.182.940 I llama_perf_context_print:        eval time =     635.56 ms /    63 runs   (   10.09 ms per token,    99.13 tokens per second)
0.01.182.940 I llama_perf_context_print:       total time =     683.73 ms /    70 tokens
0.01.183.177 I ggml_metal_free: deallocating

real	0m1.202s
user	0m0.111s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.634 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.227 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.227 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.768 I llama_model_loader: - type  f32:  194 tensors
0.00.025.768 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.769 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.769 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.769 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.770 I print_info: file format = GGUF V3 (latest)
0.00.025.770 I print_info: file type   = Q3_K - Medium
0.00.025.771 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.885 I load: special tokens cache size = 25
0.00.039.955 I load: token to piece cache size = 0.2984 MB
0.00.039.958 I print_info: arch             = gptneox
0.00.039.959 I print_info: vocab_only       = 0
0.00.039.959 I print_info: n_ctx_train      = 2048
0.00.039.959 I print_info: n_embd           = 2048
0.00.039.959 I print_info: n_layer          = 24
0.00.039.962 I print_info: n_head           = 16
0.00.039.962 I print_info: n_head_kv        = 16
0.00.039.963 I print_info: n_rot            = 32
0.00.039.963 I print_info: n_swa            = 0
0.00.039.963 I print_info: n_embd_head_k    = 128
0.00.039.963 I print_info: n_embd_head_v    = 128
0.00.039.964 I print_info: n_gqa            = 1
0.00.039.965 I print_info: n_embd_k_gqa     = 2048
0.00.039.966 I print_info: n_embd_v_gqa     = 2048
0.00.039.966 I print_info: f_norm_eps       = 1.0e-05
0.00.039.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.969 I print_info: f_logit_scale    = 0.0e+00
0.00.039.970 I print_info: n_ff             = 8192
0.00.039.970 I print_info: n_expert         = 0
0.00.039.970 I print_info: n_expert_used    = 0
0.00.039.972 I print_info: causal attn      = 1
0.00.039.973 I print_info: pooling type     = 0
0.00.039.973 I print_info: rope type        = 2
0.00.039.973 I print_info: rope scaling     = linear
0.00.039.974 I print_info: freq_base_train  = 10000.0
0.00.039.974 I print_info: freq_scale_train = 1
0.00.039.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.974 I print_info: rope_finetuned   = unknown
0.00.039.974 I print_info: ssm_d_conv       = 0
0.00.039.974 I print_info: ssm_d_inner      = 0
0.00.039.975 I print_info: ssm_d_state      = 0
0.00.039.975 I print_info: ssm_dt_rank      = 0
0.00.039.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.975 I print_info: model type       = 1.4B
0.00.039.975 I print_info: model params     = 1.41 B
0.00.039.976 I print_info: general.name     = 1.4B
0.00.039.976 I print_info: vocab type       = BPE
0.00.039.976 I print_info: n_vocab          = 50304
0.00.039.976 I print_info: n_merges         = 50009
0.00.039.977 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.977 I print_info: LF token         = 187 'Ċ'
0.00.039.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.985 I print_info: max token length = 1024
0.00.442.884 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.898 I load_tensors: offloading output layer to GPU
0.00.442.898 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.931 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.932 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.444.322 I llama_init_from_model: n_seq_max     = 1
0.00.444.328 I llama_init_from_model: n_ctx         = 2048
0.00.444.328 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.329 I llama_init_from_model: n_batch       = 2048
0.00.444.329 I llama_init_from_model: n_ubatch      = 512
0.00.444.329 I llama_init_from_model: flash_attn    = 0
0.00.444.334 I llama_init_from_model: freq_base     = 10000.0
0.00.444.339 I llama_init_from_model: freq_scale    = 1
0.00.444.341 I ggml_metal_init: allocating
0.00.444.391 I ggml_metal_init: found device: Apple M4
0.00.444.404 I ggml_metal_init: picking default device: Apple M4
0.00.446.100 I ggml_metal_init: using embedded metal library
0.00.451.701 I ggml_metal_init: GPU name:   Apple M4
0.00.451.714 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.715 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.716 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.717 I ggml_metal_init: simdgroup reduction   = true
0.00.451.717 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.717 I ggml_metal_init: has residency sets    = true
0.00.451.717 I ggml_metal_init: has bfloat            = true
0.00.451.718 I ggml_metal_init: use bfloat            = true
0.00.451.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.968 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.530.357 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.530.363 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.530.385 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.534.939 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.534.941 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.534.941 I llama_init_from_model: graph nodes  = 967
0.00.534.942 I llama_init_from_model: graph splits = 2
0.00.534.948 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.535.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.535.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.771 I main: llama threadpool init, n_threads = 4
0.00.594.817 I 
0.00.594.839 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.839 I 
0.00.595.009 I sampler seed: 1234
0.00.595.014 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.595.039 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.595.040 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.595.040 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.337.240 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.01.337.240 I llama_perf_context_print:        load time =     584.22 ms
0.01.337.242 I llama_perf_context_print: prompt eval time =      49.00 ms /     7 tokens (    7.00 ms per token,   142.86 tokens per second)
0.01.337.243 I llama_perf_context_print:        eval time =     690.32 ms /    63 runs   (   10.96 ms per token,    91.26 tokens per second)
0.01.337.245 I llama_perf_context_print:       total time =     743.39 ms /    70 tokens
0.01.337.459 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.762 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.208 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.218 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.720 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.721 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.722 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.722 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.723 I llama_model_loader: - type  f32:  194 tensors
0.00.024.723 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.723 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.724 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.724 I print_info: file format = GGUF V3 (latest)
0.00.024.725 I print_info: file type   = Q4_K - Medium
0.00.024.726 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.389 I load: special tokens cache size = 25
0.00.038.363 I load: token to piece cache size = 0.2984 MB
0.00.038.366 I print_info: arch             = gptneox
0.00.038.366 I print_info: vocab_only       = 0
0.00.038.367 I print_info: n_ctx_train      = 2048
0.00.038.367 I print_info: n_embd           = 2048
0.00.038.367 I print_info: n_layer          = 24
0.00.038.370 I print_info: n_head           = 16
0.00.038.370 I print_info: n_head_kv        = 16
0.00.038.371 I print_info: n_rot            = 32
0.00.038.371 I print_info: n_swa            = 0
0.00.038.371 I print_info: n_embd_head_k    = 128
0.00.038.371 I print_info: n_embd_head_v    = 128
0.00.038.372 I print_info: n_gqa            = 1
0.00.038.375 I print_info: n_embd_k_gqa     = 2048
0.00.038.375 I print_info: n_embd_v_gqa     = 2048
0.00.038.376 I print_info: f_norm_eps       = 1.0e-05
0.00.038.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.377 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.377 I print_info: f_logit_scale    = 0.0e+00
0.00.038.378 I print_info: n_ff             = 8192
0.00.038.378 I print_info: n_expert         = 0
0.00.038.378 I print_info: n_expert_used    = 0
0.00.038.378 I print_info: causal attn      = 1
0.00.038.378 I print_info: pooling type     = 0
0.00.038.378 I print_info: rope type        = 2
0.00.038.379 I print_info: rope scaling     = linear
0.00.038.380 I print_info: freq_base_train  = 10000.0
0.00.038.380 I print_info: freq_scale_train = 1
0.00.038.380 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.381 I print_info: rope_finetuned   = unknown
0.00.038.382 I print_info: ssm_d_conv       = 0
0.00.038.382 I print_info: ssm_d_inner      = 0
0.00.038.382 I print_info: ssm_d_state      = 0
0.00.038.382 I print_info: ssm_dt_rank      = 0
0.00.038.382 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.382 I print_info: model type       = 1.4B
0.00.038.384 I print_info: model params     = 1.41 B
0.00.038.384 I print_info: general.name     = 1.4B
0.00.038.385 I print_info: vocab type       = BPE
0.00.038.385 I print_info: n_vocab          = 50304
0.00.038.385 I print_info: n_merges         = 50009
0.00.038.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: LF token         = 187 'Ċ'
0.00.038.388 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.388 I print_info: max token length = 1024
0.00.544.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.907 I load_tensors: offloading output layer to GPU
0.00.544.907 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.948 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.949 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.546.616 I llama_init_from_model: n_seq_max     = 1
0.00.546.620 I llama_init_from_model: n_ctx         = 2048
0.00.546.620 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.546.621 I llama_init_from_model: n_batch       = 2048
0.00.546.621 I llama_init_from_model: n_ubatch      = 512
0.00.546.621 I llama_init_from_model: flash_attn    = 0
0.00.546.624 I llama_init_from_model: freq_base     = 10000.0
0.00.546.624 I llama_init_from_model: freq_scale    = 1
0.00.546.628 I ggml_metal_init: allocating
0.00.546.740 I ggml_metal_init: found device: Apple M4
0.00.546.754 I ggml_metal_init: picking default device: Apple M4
0.00.548.600 I ggml_metal_init: using embedded metal library
0.00.555.282 I ggml_metal_init: GPU name:   Apple M4
0.00.555.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.555.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.555.288 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.555.289 I ggml_metal_init: simdgroup reduction   = true
0.00.555.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.555.289 I ggml_metal_init: has residency sets    = true
0.00.555.289 I ggml_metal_init: has bfloat            = true
0.00.555.290 I ggml_metal_init: use bfloat            = true
0.00.555.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.555.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.254 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.875 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.634.880 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.634.905 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.071 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.639.073 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.639.073 I llama_init_from_model: graph nodes  = 967
0.00.639.074 I llama_init_from_model: graph splits = 2
0.00.639.079 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.639.191 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.639.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.352 I main: llama threadpool init, n_threads = 4
0.00.697.397 I 
0.00.697.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.422 I 
0.00.697.593 I sampler seed: 1234
0.00.697.598 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.618 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.618 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.619 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.459.936 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.459.937 I llama_perf_context_print:        load time =     687.57 ms
0.01.459.937 I llama_perf_context_print: prompt eval time =      57.55 ms /     7 tokens (    8.22 ms per token,   121.63 tokens per second)
0.01.459.938 I llama_perf_context_print:        eval time =     701.92 ms /    63 runs   (   11.14 ms per token,    89.75 tokens per second)
0.01.459.938 I llama_perf_context_print:       total time =     763.60 ms /    70 tokens
0.01.460.191 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.109s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.257 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.261 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.264 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.120 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.914 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.917 I llama_model_loader: - type  f32:  194 tensors
0.00.026.917 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.917 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.918 I print_info: file format = GGUF V3 (latest)
0.00.026.919 I print_info: file type   = Q5_K - Medium
0.00.026.919 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.996 I load: special tokens cache size = 25
0.00.040.715 I load: token to piece cache size = 0.2984 MB
0.00.040.718 I print_info: arch             = gptneox
0.00.040.719 I print_info: vocab_only       = 0
0.00.040.719 I print_info: n_ctx_train      = 2048
0.00.040.719 I print_info: n_embd           = 2048
0.00.040.719 I print_info: n_layer          = 24
0.00.040.722 I print_info: n_head           = 16
0.00.040.723 I print_info: n_head_kv        = 16
0.00.040.723 I print_info: n_rot            = 32
0.00.040.724 I print_info: n_swa            = 0
0.00.040.724 I print_info: n_embd_head_k    = 128
0.00.040.724 I print_info: n_embd_head_v    = 128
0.00.040.725 I print_info: n_gqa            = 1
0.00.040.726 I print_info: n_embd_k_gqa     = 2048
0.00.040.726 I print_info: n_embd_v_gqa     = 2048
0.00.040.727 I print_info: f_norm_eps       = 1.0e-05
0.00.040.729 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.729 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.729 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.730 I print_info: f_logit_scale    = 0.0e+00
0.00.040.730 I print_info: n_ff             = 8192
0.00.040.730 I print_info: n_expert         = 0
0.00.040.731 I print_info: n_expert_used    = 0
0.00.040.731 I print_info: causal attn      = 1
0.00.040.731 I print_info: pooling type     = 0
0.00.040.731 I print_info: rope type        = 2
0.00.040.732 I print_info: rope scaling     = linear
0.00.040.734 I print_info: freq_base_train  = 10000.0
0.00.040.734 I print_info: freq_scale_train = 1
0.00.040.734 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.734 I print_info: rope_finetuned   = unknown
0.00.040.734 I print_info: ssm_d_conv       = 0
0.00.040.735 I print_info: ssm_d_inner      = 0
0.00.040.735 I print_info: ssm_d_state      = 0
0.00.040.735 I print_info: ssm_dt_rank      = 0
0.00.040.735 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.737 I print_info: model type       = 1.4B
0.00.040.737 I print_info: model params     = 1.41 B
0.00.040.737 I print_info: general.name     = 1.4B
0.00.040.738 I print_info: vocab type       = BPE
0.00.040.739 I print_info: n_vocab          = 50304
0.00.040.739 I print_info: n_merges         = 50009
0.00.040.739 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.740 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.740 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.740 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.740 I print_info: LF token         = 187 'Ċ'
0.00.040.741 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.741 I print_info: max token length = 1024
0.00.620.079 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.095 I load_tensors: offloading output layer to GPU
0.00.620.095 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.129 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.620.130 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.621.514 I llama_init_from_model: n_seq_max     = 1
0.00.621.520 I llama_init_from_model: n_ctx         = 2048
0.00.621.521 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.621.521 I llama_init_from_model: n_batch       = 2048
0.00.621.521 I llama_init_from_model: n_ubatch      = 512
0.00.621.522 I llama_init_from_model: flash_attn    = 0
0.00.621.523 I llama_init_from_model: freq_base     = 10000.0
0.00.621.524 I llama_init_from_model: freq_scale    = 1
0.00.621.526 I ggml_metal_init: allocating
0.00.621.607 I ggml_metal_init: found device: Apple M4
0.00.621.620 I ggml_metal_init: picking default device: Apple M4
0.00.623.403 I ggml_metal_init: using embedded metal library
0.00.630.121 I ggml_metal_init: GPU name:   Apple M4
0.00.630.125 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.126 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.127 I ggml_metal_init: simdgroup reduction   = true
0.00.630.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.128 I ggml_metal_init: has residency sets    = true
0.00.630.128 I ggml_metal_init: has bfloat            = true
0.00.630.128 I ggml_metal_init: use bfloat            = true
0.00.630.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.131 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.573 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.692 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.699 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.198 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.200 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.200 I llama_init_from_model: graph nodes  = 967
0.00.713.200 I llama_init_from_model: graph splits = 2
0.00.713.206 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.713.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.713.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.848 I main: llama threadpool init, n_threads = 4
0.00.775.895 I 
0.00.775.921 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.923 I 
0.00.776.074 I sampler seed: 1234
0.00.776.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.100 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.100 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.615.735 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.615.736 I llama_perf_context_print:        load time =     764.12 ms
0.01.615.737 I llama_perf_context_print: prompt eval time =      51.20 ms /     7 tokens (    7.31 ms per token,   136.72 tokens per second)
0.01.615.737 I llama_perf_context_print:        eval time =     785.61 ms /    63 runs   (   12.47 ms per token,    80.19 tokens per second)
0.01.615.738 I llama_perf_context_print:       total time =     840.82 ms /    70 tokens
0.01.615.981 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.110s
sys	0m0.236s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.641 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.604 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.606 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.606 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.609 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.610 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.003 I llama_model_loader: - type  f32:  194 tensors
0.00.025.003 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.004 I print_info: file format = GGUF V3 (latest)
0.00.025.004 I print_info: file type   = Q6_K
0.00.025.005 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.661 I load: special tokens cache size = 25
0.00.038.610 I load: token to piece cache size = 0.2984 MB
0.00.038.613 I print_info: arch             = gptneox
0.00.038.613 I print_info: vocab_only       = 0
0.00.038.613 I print_info: n_ctx_train      = 2048
0.00.038.614 I print_info: n_embd           = 2048
0.00.038.614 I print_info: n_layer          = 24
0.00.038.616 I print_info: n_head           = 16
0.00.038.617 I print_info: n_head_kv        = 16
0.00.038.617 I print_info: n_rot            = 32
0.00.038.617 I print_info: n_swa            = 0
0.00.038.618 I print_info: n_embd_head_k    = 128
0.00.038.618 I print_info: n_embd_head_v    = 128
0.00.038.619 I print_info: n_gqa            = 1
0.00.038.620 I print_info: n_embd_k_gqa     = 2048
0.00.038.620 I print_info: n_embd_v_gqa     = 2048
0.00.038.621 I print_info: f_norm_eps       = 1.0e-05
0.00.038.622 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.622 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.622 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.622 I print_info: f_logit_scale    = 0.0e+00
0.00.038.623 I print_info: n_ff             = 8192
0.00.038.623 I print_info: n_expert         = 0
0.00.038.623 I print_info: n_expert_used    = 0
0.00.038.623 I print_info: causal attn      = 1
0.00.038.623 I print_info: pooling type     = 0
0.00.038.623 I print_info: rope type        = 2
0.00.038.624 I print_info: rope scaling     = linear
0.00.038.624 I print_info: freq_base_train  = 10000.0
0.00.038.624 I print_info: freq_scale_train = 1
0.00.038.626 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.626 I print_info: rope_finetuned   = unknown
0.00.038.626 I print_info: ssm_d_conv       = 0
0.00.038.627 I print_info: ssm_d_inner      = 0
0.00.038.627 I print_info: ssm_d_state      = 0
0.00.038.627 I print_info: ssm_dt_rank      = 0
0.00.038.628 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.628 I print_info: model type       = 1.4B
0.00.038.629 I print_info: model params     = 1.41 B
0.00.038.629 I print_info: general.name     = 1.4B
0.00.038.629 I print_info: vocab type       = BPE
0.00.038.629 I print_info: n_vocab          = 50304
0.00.038.630 I print_info: n_merges         = 50009
0.00.038.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.631 I print_info: LF token         = 187 'Ċ'
0.00.038.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.631 I print_info: max token length = 1024
0.00.664.415 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.430 I load_tensors: offloading output layer to GPU
0.00.664.431 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.468 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.664.470 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.665.975 I llama_init_from_model: n_seq_max     = 1
0.00.665.977 I llama_init_from_model: n_ctx         = 2048
0.00.665.978 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.665.978 I llama_init_from_model: n_batch       = 2048
0.00.665.979 I llama_init_from_model: n_ubatch      = 512
0.00.665.979 I llama_init_from_model: flash_attn    = 0
0.00.665.980 I llama_init_from_model: freq_base     = 10000.0
0.00.665.981 I llama_init_from_model: freq_scale    = 1
0.00.665.982 I ggml_metal_init: allocating
0.00.666.033 I ggml_metal_init: found device: Apple M4
0.00.666.048 I ggml_metal_init: picking default device: Apple M4
0.00.667.522 I ggml_metal_init: using embedded metal library
0.00.673.779 I ggml_metal_init: GPU name:   Apple M4
0.00.673.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.785 I ggml_metal_init: simdgroup reduction   = true
0.00.673.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.786 I ggml_metal_init: has residency sets    = true
0.00.673.786 I ggml_metal_init: has bfloat            = true
0.00.673.786 I ggml_metal_init: use bfloat            = true
0.00.673.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.367 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.936 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.957 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.164 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.166 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.167 I llama_init_from_model: graph nodes  = 967
0.00.750.167 I llama_init_from_model: graph splits = 2
0.00.750.173 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.506 I main: llama threadpool init, n_threads = 4
0.00.813.549 I 
0.00.813.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.571 I 
0.00.813.744 I sampler seed: 1234
0.00.813.749 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.813.768 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.813.768 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.813.768 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.682.777 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.682.778 I llama_perf_context_print:        load time =     803.95 ms
0.01.682.779 I llama_perf_context_print: prompt eval time =      54.05 ms /     7 tokens (    7.72 ms per token,   129.51 tokens per second)
0.01.682.779 I llama_perf_context_print:        eval time =     812.07 ms /    63 runs   (   12.89 ms per token,    77.58 tokens per second)
0.01.682.780 I llama_perf_context_print:       total time =     870.18 ms /    70 tokens
0.01.683.037 I ggml_metal_free: deallocating

real	0m1.698s
user	0m0.107s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.636 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.345 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.837 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.176 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.308 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.310 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.311 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.312 I llama_model_loader: - type  f32:  194 tensors
0.00.056.313 I llama_model_loader: - type  f16:   98 tensors
0.00.056.313 I print_info: file format = GGUF V3 (latest)
0.00.056.314 I print_info: file type   = all F32 (guessed)
0.00.056.317 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.482 I load: special tokens cache size = 25
0.00.076.104 I load: token to piece cache size = 0.2984 MB
0.00.076.107 I print_info: arch             = gptneox
0.00.076.107 I print_info: vocab_only       = 0
0.00.076.107 I print_info: n_ctx_train      = 2048
0.00.076.108 I print_info: n_embd           = 2048
0.00.076.108 I print_info: n_layer          = 24
0.00.076.111 I print_info: n_head           = 16
0.00.076.111 I print_info: n_head_kv        = 16
0.00.076.112 I print_info: n_rot            = 32
0.00.076.114 I print_info: n_swa            = 0
0.00.076.114 I print_info: n_embd_head_k    = 128
0.00.076.114 I print_info: n_embd_head_v    = 128
0.00.076.115 I print_info: n_gqa            = 1
0.00.076.116 I print_info: n_embd_k_gqa     = 2048
0.00.076.120 I print_info: n_embd_v_gqa     = 2048
0.00.076.121 I print_info: f_norm_eps       = 1.0e-05
0.00.076.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.122 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.122 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.122 I print_info: f_logit_scale    = 0.0e+00
0.00.076.123 I print_info: n_ff             = 8192
0.00.076.123 I print_info: n_expert         = 0
0.00.076.123 I print_info: n_expert_used    = 0
0.00.076.123 I print_info: causal attn      = 1
0.00.076.124 I print_info: pooling type     = 0
0.00.076.124 I print_info: rope type        = 2
0.00.076.124 I print_info: rope scaling     = linear
0.00.076.124 I print_info: freq_base_train  = 10000.0
0.00.076.125 I print_info: freq_scale_train = 1
0.00.076.125 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.125 I print_info: rope_finetuned   = unknown
0.00.076.125 I print_info: ssm_d_conv       = 0
0.00.076.125 I print_info: ssm_d_inner      = 0
0.00.076.125 I print_info: ssm_d_state      = 0
0.00.076.125 I print_info: ssm_dt_rank      = 0
0.00.076.126 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.126 I print_info: model type       = 1.4B
0.00.076.126 I print_info: model params     = 1.41 B
0.00.076.126 I print_info: general.name     = 1.4B
0.00.076.127 I print_info: vocab type       = BPE
0.00.076.127 I print_info: n_vocab          = 50304
0.00.076.127 I print_info: n_merges         = 50009
0.00.076.127 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.128 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.128 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.129 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.131 I print_info: LF token         = 187 'Ċ'
0.00.076.132 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.132 I print_info: max token length = 1024
0.01.474.427 I load_tensors: offloading 24 repeating layers to GPU
0.01.474.432 I load_tensors: offloading output layer to GPU
0.01.474.432 I load_tensors: offloaded 25/25 layers to GPU
0.01.474.458 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.474.459 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.475.244 I llama_init_from_model: n_seq_max     = 1
0.01.475.245 I llama_init_from_model: n_ctx         = 128
0.01.475.245 I llama_init_from_model: n_ctx_per_seq = 128
0.01.475.246 I llama_init_from_model: n_batch       = 128
0.01.475.246 I llama_init_from_model: n_ubatch      = 128
0.01.475.246 I llama_init_from_model: flash_attn    = 0
0.01.475.246 I llama_init_from_model: freq_base     = 10000.0
0.01.475.247 I llama_init_from_model: freq_scale    = 1
0.01.475.247 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.475.251 I ggml_metal_init: allocating
0.01.475.361 I ggml_metal_init: found device: Apple M4
0.01.475.367 I ggml_metal_init: picking default device: Apple M4
0.01.476.455 I ggml_metal_init: using embedded metal library
0.01.480.286 I ggml_metal_init: GPU name:   Apple M4
0.01.480.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.480.288 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.480.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.480.289 I ggml_metal_init: simdgroup reduction   = true
0.01.480.289 I ggml_metal_init: simdgroup matrix mul. = true
0.01.480.290 I ggml_metal_init: has residency sets    = true
0.01.480.290 I ggml_metal_init: has bfloat            = true
0.01.480.290 I ggml_metal_init: use bfloat            = true
0.01.480.290 I ggml_metal_init: hasUnifiedMemory      = true
0.01.480.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.490.720 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.492.401 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.492.404 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.492.419 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.494.038 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.494.040 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.494.040 I llama_init_from_model: graph nodes  = 967
0.01.494.040 I llama_init_from_model: graph splits = 2
0.01.494.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.494.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.529.132 I 
0.01.529.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.529.170 I perplexity: tokenizing the input ..
0.01.534.195 I perplexity: tokenization took 5.022 ms
0.01.534.200 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.652.761 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.654.108 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.654.142 I llama_perf_context_print:        load time =    1503.78 ms
0.01.654.143 I llama_perf_context_print: prompt eval time =     118.29 ms /   128 tokens (    0.92 ms per token,  1082.06 tokens per second)
0.01.654.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.654.144 I llama_perf_context_print:       total time =     125.01 ms /   129 tokens
0.01.654.501 I ggml_metal_free: deallocating

real	0m1.841s
user	0m0.096s
sys	0m0.284s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.517 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.141 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.143 I llama_model_loader: - type  f32:  194 tensors
0.00.025.143 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.144 I print_info: file format = GGUF V3 (latest)
0.00.025.144 I print_info: file type   = Q8_0
0.00.025.146 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.491 I load: special tokens cache size = 25
0.00.039.858 I load: token to piece cache size = 0.2984 MB
0.00.039.862 I print_info: arch             = gptneox
0.00.039.862 I print_info: vocab_only       = 0
0.00.039.862 I print_info: n_ctx_train      = 2048
0.00.039.862 I print_info: n_embd           = 2048
0.00.039.862 I print_info: n_layer          = 24
0.00.039.866 I print_info: n_head           = 16
0.00.039.867 I print_info: n_head_kv        = 16
0.00.039.867 I print_info: n_rot            = 32
0.00.039.867 I print_info: n_swa            = 0
0.00.039.868 I print_info: n_embd_head_k    = 128
0.00.039.868 I print_info: n_embd_head_v    = 128
0.00.039.870 I print_info: n_gqa            = 1
0.00.039.871 I print_info: n_embd_k_gqa     = 2048
0.00.039.872 I print_info: n_embd_v_gqa     = 2048
0.00.039.872 I print_info: f_norm_eps       = 1.0e-05
0.00.039.872 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.877 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.877 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.879 I print_info: f_logit_scale    = 0.0e+00
0.00.039.879 I print_info: n_ff             = 8192
0.00.039.880 I print_info: n_expert         = 0
0.00.039.880 I print_info: n_expert_used    = 0
0.00.039.880 I print_info: causal attn      = 1
0.00.039.880 I print_info: pooling type     = 0
0.00.039.880 I print_info: rope type        = 2
0.00.039.881 I print_info: rope scaling     = linear
0.00.039.881 I print_info: freq_base_train  = 10000.0
0.00.039.881 I print_info: freq_scale_train = 1
0.00.039.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.882 I print_info: rope_finetuned   = unknown
0.00.039.882 I print_info: ssm_d_conv       = 0
0.00.039.882 I print_info: ssm_d_inner      = 0
0.00.039.882 I print_info: ssm_d_state      = 0
0.00.039.883 I print_info: ssm_dt_rank      = 0
0.00.039.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.883 I print_info: model type       = 1.4B
0.00.039.883 I print_info: model params     = 1.41 B
0.00.039.883 I print_info: general.name     = 1.4B
0.00.039.887 I print_info: vocab type       = BPE
0.00.039.887 I print_info: n_vocab          = 50304
0.00.039.887 I print_info: n_merges         = 50009
0.00.039.888 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.888 I print_info: LF token         = 187 'Ċ'
0.00.039.889 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.890 I print_info: max token length = 1024
0.00.899.761 I load_tensors: offloading 24 repeating layers to GPU
0.00.899.767 I load_tensors: offloading output layer to GPU
0.00.899.769 I load_tensors: offloaded 25/25 layers to GPU
0.00.899.792 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.899.795 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.901.013 I llama_init_from_model: n_seq_max     = 1
0.00.901.015 I llama_init_from_model: n_ctx         = 128
0.00.901.015 I llama_init_from_model: n_ctx_per_seq = 128
0.00.901.016 I llama_init_from_model: n_batch       = 128
0.00.901.016 I llama_init_from_model: n_ubatch      = 128
0.00.901.016 I llama_init_from_model: flash_attn    = 0
0.00.901.017 I llama_init_from_model: freq_base     = 10000.0
0.00.901.018 I llama_init_from_model: freq_scale    = 1
0.00.901.018 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.901.019 I ggml_metal_init: allocating
0.00.901.047 I ggml_metal_init: found device: Apple M4
0.00.901.058 I ggml_metal_init: picking default device: Apple M4
0.00.902.299 I ggml_metal_init: using embedded metal library
0.00.907.749 I ggml_metal_init: GPU name:   Apple M4
0.00.907.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.907.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.907.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.907.754 I ggml_metal_init: simdgroup reduction   = true
0.00.907.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.907.755 I ggml_metal_init: has residency sets    = true
0.00.907.755 I ggml_metal_init: has bfloat            = true
0.00.907.755 I ggml_metal_init: use bfloat            = true
0.00.907.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.907.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.923.407 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.926.777 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.926.786 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.926.822 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.929.977 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.929.979 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.929.979 I llama_init_from_model: graph nodes  = 967
0.00.929.979 I llama_init_from_model: graph splits = 2
0.00.929.982 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.929.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.958.179 I 
0.00.958.278 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.958.291 I perplexity: tokenizing the input ..
0.00.965.216 I perplexity: tokenization took 6.923 ms
0.00.965.222 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.102.898 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.104.300 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.104.325 I llama_perf_context_print:        load time =     948.35 ms
0.01.104.326 I llama_perf_context_print: prompt eval time =     137.04 ms /   128 tokens (    1.07 ms per token,   934.03 tokens per second)
0.01.104.327 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.104.327 I llama_perf_context_print:       total time =     146.15 ms /   129 tokens
0.01.104.729 I ggml_metal_free: deallocating

real	0m1.121s
user	0m0.076s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.113 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.243 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.245 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.245 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.849 I llama_model_loader: - type  f32:  194 tensors
0.00.025.850 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.850 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.851 I print_info: file format = GGUF V3 (latest)
0.00.025.851 I print_info: file type   = Q4_0
0.00.025.852 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.731 I load: special tokens cache size = 25
0.00.039.527 I load: token to piece cache size = 0.2984 MB
0.00.039.530 I print_info: arch             = gptneox
0.00.039.531 I print_info: vocab_only       = 0
0.00.039.531 I print_info: n_ctx_train      = 2048
0.00.039.531 I print_info: n_embd           = 2048
0.00.039.531 I print_info: n_layer          = 24
0.00.039.534 I print_info: n_head           = 16
0.00.039.535 I print_info: n_head_kv        = 16
0.00.039.535 I print_info: n_rot            = 32
0.00.039.535 I print_info: n_swa            = 0
0.00.039.535 I print_info: n_embd_head_k    = 128
0.00.039.536 I print_info: n_embd_head_v    = 128
0.00.039.536 I print_info: n_gqa            = 1
0.00.039.537 I print_info: n_embd_k_gqa     = 2048
0.00.039.538 I print_info: n_embd_v_gqa     = 2048
0.00.039.538 I print_info: f_norm_eps       = 1.0e-05
0.00.039.539 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.539 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.539 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.539 I print_info: f_logit_scale    = 0.0e+00
0.00.039.540 I print_info: n_ff             = 8192
0.00.039.540 I print_info: n_expert         = 0
0.00.039.540 I print_info: n_expert_used    = 0
0.00.039.540 I print_info: causal attn      = 1
0.00.039.540 I print_info: pooling type     = 0
0.00.039.541 I print_info: rope type        = 2
0.00.039.541 I print_info: rope scaling     = linear
0.00.039.541 I print_info: freq_base_train  = 10000.0
0.00.039.549 I print_info: freq_scale_train = 1
0.00.039.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.551 I print_info: rope_finetuned   = unknown
0.00.039.552 I print_info: ssm_d_conv       = 0
0.00.039.552 I print_info: ssm_d_inner      = 0
0.00.039.552 I print_info: ssm_d_state      = 0
0.00.039.552 I print_info: ssm_dt_rank      = 0
0.00.039.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.552 I print_info: model type       = 1.4B
0.00.039.553 I print_info: model params     = 1.41 B
0.00.039.554 I print_info: general.name     = 1.4B
0.00.039.555 I print_info: vocab type       = BPE
0.00.039.555 I print_info: n_vocab          = 50304
0.00.039.555 I print_info: n_merges         = 50009
0.00.039.555 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: LF token         = 187 'Ċ'
0.00.039.556 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.557 I print_info: max token length = 1024
0.00.567.192 I load_tensors: offloading 24 repeating layers to GPU
0.00.567.209 I load_tensors: offloading output layer to GPU
0.00.567.209 I load_tensors: offloaded 25/25 layers to GPU
0.00.567.243 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.567.244 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.568.864 I llama_init_from_model: n_seq_max     = 1
0.00.568.870 I llama_init_from_model: n_ctx         = 128
0.00.568.871 I llama_init_from_model: n_ctx_per_seq = 128
0.00.568.876 I llama_init_from_model: n_batch       = 128
0.00.568.876 I llama_init_from_model: n_ubatch      = 128
0.00.568.877 I llama_init_from_model: flash_attn    = 0
0.00.568.892 I llama_init_from_model: freq_base     = 10000.0
0.00.568.893 I llama_init_from_model: freq_scale    = 1
0.00.568.893 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.568.895 I ggml_metal_init: allocating
0.00.568.986 I ggml_metal_init: found device: Apple M4
0.00.568.999 I ggml_metal_init: picking default device: Apple M4
0.00.570.865 I ggml_metal_init: using embedded metal library
0.00.576.503 I ggml_metal_init: GPU name:   Apple M4
0.00.576.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.576.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.576.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.576.512 I ggml_metal_init: simdgroup reduction   = true
0.00.576.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.576.512 I ggml_metal_init: has residency sets    = true
0.00.576.513 I ggml_metal_init: has bfloat            = true
0.00.576.513 I ggml_metal_init: use bfloat            = true
0.00.576.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.576.516 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.595.300 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.598.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.598.772 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.598.802 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.601.999 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.602.001 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.602.002 I llama_init_from_model: graph nodes  = 967
0.00.602.002 I llama_init_from_model: graph splits = 2
0.00.602.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.602.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.512 I 
0.00.631.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.602 I perplexity: tokenizing the input ..
0.00.639.150 I perplexity: tokenization took 7.544 ms
0.00.639.158 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.586 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.777.934 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.777.956 I llama_perf_context_print:        load time =     621.39 ms
0.00.777.956 I llama_perf_context_print: prompt eval time =     136.53 ms /   128 tokens (    1.07 ms per token,   937.50 tokens per second)
0.00.777.957 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.957 I llama_perf_context_print:       total time =     146.45 ms /   129 tokens
0.00.778.335 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.778 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.216 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.216 I llama_model_loader: - type  f32:  194 tensors
0.00.024.216 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.217 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.217 I print_info: file format = GGUF V3 (latest)
0.00.024.218 I print_info: file type   = Q4_1
0.00.024.219 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.186 I load: special tokens cache size = 25
0.00.038.177 I load: token to piece cache size = 0.2984 MB
0.00.038.180 I print_info: arch             = gptneox
0.00.038.180 I print_info: vocab_only       = 0
0.00.038.180 I print_info: n_ctx_train      = 2048
0.00.038.180 I print_info: n_embd           = 2048
0.00.038.181 I print_info: n_layer          = 24
0.00.038.183 I print_info: n_head           = 16
0.00.038.184 I print_info: n_head_kv        = 16
0.00.038.184 I print_info: n_rot            = 32
0.00.038.185 I print_info: n_swa            = 0
0.00.038.185 I print_info: n_embd_head_k    = 128
0.00.038.185 I print_info: n_embd_head_v    = 128
0.00.038.186 I print_info: n_gqa            = 1
0.00.038.186 I print_info: n_embd_k_gqa     = 2048
0.00.038.187 I print_info: n_embd_v_gqa     = 2048
0.00.038.188 I print_info: f_norm_eps       = 1.0e-05
0.00.038.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.188 I print_info: f_logit_scale    = 0.0e+00
0.00.038.190 I print_info: n_ff             = 8192
0.00.038.190 I print_info: n_expert         = 0
0.00.038.190 I print_info: n_expert_used    = 0
0.00.038.190 I print_info: causal attn      = 1
0.00.038.190 I print_info: pooling type     = 0
0.00.038.190 I print_info: rope type        = 2
0.00.038.191 I print_info: rope scaling     = linear
0.00.038.193 I print_info: freq_base_train  = 10000.0
0.00.038.193 I print_info: freq_scale_train = 1
0.00.038.193 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.194 I print_info: rope_finetuned   = unknown
0.00.038.194 I print_info: ssm_d_conv       = 0
0.00.038.194 I print_info: ssm_d_inner      = 0
0.00.038.194 I print_info: ssm_d_state      = 0
0.00.038.194 I print_info: ssm_dt_rank      = 0
0.00.038.194 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.195 I print_info: model type       = 1.4B
0.00.038.195 I print_info: model params     = 1.41 B
0.00.038.195 I print_info: general.name     = 1.4B
0.00.038.195 I print_info: vocab type       = BPE
0.00.038.196 I print_info: n_vocab          = 50304
0.00.038.197 I print_info: n_merges         = 50009
0.00.038.198 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.198 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.198 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.198 I print_info: LF token         = 187 'Ċ'
0.00.038.200 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.200 I print_info: max token length = 1024
0.00.633.198 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.212 I load_tensors: offloading output layer to GPU
0.00.633.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.248 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.250 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.634.802 I llama_init_from_model: n_seq_max     = 1
0.00.634.807 I llama_init_from_model: n_ctx         = 128
0.00.634.807 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.808 I llama_init_from_model: n_batch       = 128
0.00.634.809 I llama_init_from_model: n_ubatch      = 128
0.00.634.809 I llama_init_from_model: flash_attn    = 0
0.00.634.811 I llama_init_from_model: freq_base     = 10000.0
0.00.634.812 I llama_init_from_model: freq_scale    = 1
0.00.634.812 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.815 I ggml_metal_init: allocating
0.00.634.891 I ggml_metal_init: found device: Apple M4
0.00.634.905 I ggml_metal_init: picking default device: Apple M4
0.00.636.601 I ggml_metal_init: using embedded metal library
0.00.642.741 I ggml_metal_init: GPU name:   Apple M4
0.00.642.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.749 I ggml_metal_init: simdgroup reduction   = true
0.00.642.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.750 I ggml_metal_init: has residency sets    = true
0.00.642.750 I ggml_metal_init: has bfloat            = true
0.00.642.750 I ggml_metal_init: use bfloat            = true
0.00.642.751 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.856 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.441 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.665.447 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.793 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.795 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.795 I llama_init_from_model: graph nodes  = 967
0.00.668.795 I llama_init_from_model: graph splits = 2
0.00.668.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.931 I 
0.00.695.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.022 I perplexity: tokenizing the input ..
0.00.702.484 I perplexity: tokenization took 7.459 ms
0.00.702.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.058 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.839.439 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.839.462 I llama_perf_context_print:        load time =     686.15 ms
0.00.839.463 I llama_perf_context_print: prompt eval time =     134.65 ms /   128 tokens (    1.05 ms per token,   950.61 tokens per second)
0.00.839.464 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.464 I llama_perf_context_print:       total time =     144.53 ms /   129 tokens
0.00.839.820 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.812 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.826 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.827 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.830 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.831 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.831 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.834 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.678 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.533 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.533 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.533 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.534 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.534 I llama_model_loader: - type  f32:  194 tensors
0.00.025.535 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.535 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.535 I print_info: file format = GGUF V3 (latest)
0.00.025.536 I print_info: file type   = Q5_0
0.00.025.537 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.623 I load: special tokens cache size = 25
0.00.039.619 I load: token to piece cache size = 0.2984 MB
0.00.039.621 I print_info: arch             = gptneox
0.00.039.621 I print_info: vocab_only       = 0
0.00.039.622 I print_info: n_ctx_train      = 2048
0.00.039.622 I print_info: n_embd           = 2048
0.00.039.622 I print_info: n_layer          = 24
0.00.039.625 I print_info: n_head           = 16
0.00.039.626 I print_info: n_head_kv        = 16
0.00.039.626 I print_info: n_rot            = 32
0.00.039.626 I print_info: n_swa            = 0
0.00.039.626 I print_info: n_embd_head_k    = 128
0.00.039.626 I print_info: n_embd_head_v    = 128
0.00.039.627 I print_info: n_gqa            = 1
0.00.039.630 I print_info: n_embd_k_gqa     = 2048
0.00.039.631 I print_info: n_embd_v_gqa     = 2048
0.00.039.632 I print_info: f_norm_eps       = 1.0e-05
0.00.039.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.632 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.632 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.633 I print_info: f_logit_scale    = 0.0e+00
0.00.039.633 I print_info: n_ff             = 8192
0.00.039.633 I print_info: n_expert         = 0
0.00.039.634 I print_info: n_expert_used    = 0
0.00.039.634 I print_info: causal attn      = 1
0.00.039.634 I print_info: pooling type     = 0
0.00.039.634 I print_info: rope type        = 2
0.00.039.634 I print_info: rope scaling     = linear
0.00.039.635 I print_info: freq_base_train  = 10000.0
0.00.039.635 I print_info: freq_scale_train = 1
0.00.039.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.635 I print_info: rope_finetuned   = unknown
0.00.039.635 I print_info: ssm_d_conv       = 0
0.00.039.636 I print_info: ssm_d_inner      = 0
0.00.039.636 I print_info: ssm_d_state      = 0
0.00.039.637 I print_info: ssm_dt_rank      = 0
0.00.039.637 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.637 I print_info: model type       = 1.4B
0.00.039.638 I print_info: model params     = 1.41 B
0.00.039.638 I print_info: general.name     = 1.4B
0.00.039.638 I print_info: vocab type       = BPE
0.00.039.639 I print_info: n_vocab          = 50304
0.00.039.639 I print_info: n_merges         = 50009
0.00.039.639 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.639 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.639 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.640 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.641 I print_info: LF token         = 187 'Ċ'
0.00.039.641 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.641 I print_info: max token length = 1024
0.00.714.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.714.506 I load_tensors: offloading output layer to GPU
0.00.714.507 I load_tensors: offloaded 25/25 layers to GPU
0.00.714.553 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.714.558 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.716.344 I llama_init_from_model: n_seq_max     = 1
0.00.716.352 I llama_init_from_model: n_ctx         = 128
0.00.716.352 I llama_init_from_model: n_ctx_per_seq = 128
0.00.716.353 I llama_init_from_model: n_batch       = 128
0.00.716.353 I llama_init_from_model: n_ubatch      = 128
0.00.716.354 I llama_init_from_model: flash_attn    = 0
0.00.716.356 I llama_init_from_model: freq_base     = 10000.0
0.00.716.357 I llama_init_from_model: freq_scale    = 1
0.00.716.357 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.716.366 I ggml_metal_init: allocating
0.00.716.452 I ggml_metal_init: found device: Apple M4
0.00.716.465 I ggml_metal_init: picking default device: Apple M4
0.00.718.237 I ggml_metal_init: using embedded metal library
0.00.724.941 I ggml_metal_init: GPU name:   Apple M4
0.00.724.946 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.724.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.724.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.724.948 I ggml_metal_init: simdgroup reduction   = true
0.00.724.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.724.949 I ggml_metal_init: has residency sets    = true
0.00.724.949 I ggml_metal_init: has bfloat            = true
0.00.724.949 I ggml_metal_init: use bfloat            = true
0.00.724.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.724.952 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.567 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.208 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.747.212 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.747.243 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.449 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.750.451 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.750.451 I llama_init_from_model: graph nodes  = 967
0.00.750.452 I llama_init_from_model: graph splits = 2
0.00.750.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.750.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.633 I 
0.00.778.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.730 I perplexity: tokenizing the input ..
0.00.785.639 I perplexity: tokenization took 6.907 ms
0.00.785.644 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.920.198 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.921.563 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.921.589 I llama_perf_context_print:        load time =     768.75 ms
0.00.921.590 I llama_perf_context_print: prompt eval time =     134.23 ms /   128 tokens (    1.05 ms per token,   953.59 tokens per second)
0.00.921.591 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.921.591 I llama_perf_context_print:       total time =     142.96 ms /   129 tokens
0.00.921.973 I ggml_metal_free: deallocating

real	0m0.937s
user	0m0.079s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.120 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.878 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.880 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.882 I llama_model_loader: - type  f32:  194 tensors
0.00.024.882 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.883 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.883 I print_info: file format = GGUF V3 (latest)
0.00.024.884 I print_info: file type   = Q5_1
0.00.024.885 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.039 I load: special tokens cache size = 25
0.00.039.093 I load: token to piece cache size = 0.2984 MB
0.00.039.101 I print_info: arch             = gptneox
0.00.039.101 I print_info: vocab_only       = 0
0.00.039.101 I print_info: n_ctx_train      = 2048
0.00.039.101 I print_info: n_embd           = 2048
0.00.039.102 I print_info: n_layer          = 24
0.00.039.106 I print_info: n_head           = 16
0.00.039.106 I print_info: n_head_kv        = 16
0.00.039.106 I print_info: n_rot            = 32
0.00.039.107 I print_info: n_swa            = 0
0.00.039.107 I print_info: n_embd_head_k    = 128
0.00.039.107 I print_info: n_embd_head_v    = 128
0.00.039.107 I print_info: n_gqa            = 1
0.00.039.108 I print_info: n_embd_k_gqa     = 2048
0.00.039.109 I print_info: n_embd_v_gqa     = 2048
0.00.039.109 I print_info: f_norm_eps       = 1.0e-05
0.00.039.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.110 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.110 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.110 I print_info: f_logit_scale    = 0.0e+00
0.00.039.110 I print_info: n_ff             = 8192
0.00.039.111 I print_info: n_expert         = 0
0.00.039.111 I print_info: n_expert_used    = 0
0.00.039.111 I print_info: causal attn      = 1
0.00.039.111 I print_info: pooling type     = 0
0.00.039.111 I print_info: rope type        = 2
0.00.039.111 I print_info: rope scaling     = linear
0.00.039.112 I print_info: freq_base_train  = 10000.0
0.00.039.112 I print_info: freq_scale_train = 1
0.00.039.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.112 I print_info: rope_finetuned   = unknown
0.00.039.113 I print_info: ssm_d_conv       = 0
0.00.039.113 I print_info: ssm_d_inner      = 0
0.00.039.113 I print_info: ssm_d_state      = 0
0.00.039.113 I print_info: ssm_dt_rank      = 0
0.00.039.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.113 I print_info: model type       = 1.4B
0.00.039.114 I print_info: model params     = 1.41 B
0.00.039.114 I print_info: general.name     = 1.4B
0.00.039.114 I print_info: vocab type       = BPE
0.00.039.114 I print_info: n_vocab          = 50304
0.00.039.114 I print_info: n_merges         = 50009
0.00.039.114 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.115 I print_info: LF token         = 187 'Ċ'
0.00.039.115 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.116 I print_info: max token length = 1024
0.00.604.089 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.093 I load_tensors: offloading output layer to GPU
0.00.604.094 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.114 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.115 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.605.092 I llama_init_from_model: n_seq_max     = 1
0.00.605.099 I llama_init_from_model: n_ctx         = 128
0.00.605.100 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.100 I llama_init_from_model: n_batch       = 128
0.00.605.100 I llama_init_from_model: n_ubatch      = 128
0.00.605.101 I llama_init_from_model: flash_attn    = 0
0.00.605.102 I llama_init_from_model: freq_base     = 10000.0
0.00.605.103 I llama_init_from_model: freq_scale    = 1
0.00.605.103 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.104 I ggml_metal_init: allocating
0.00.605.148 I ggml_metal_init: found device: Apple M4
0.00.605.159 I ggml_metal_init: picking default device: Apple M4
0.00.606.219 I ggml_metal_init: using embedded metal library
0.00.610.515 I ggml_metal_init: GPU name:   Apple M4
0.00.610.523 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.524 I ggml_metal_init: simdgroup reduction   = true
0.00.610.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.525 I ggml_metal_init: has residency sets    = true
0.00.610.525 I ggml_metal_init: has bfloat            = true
0.00.610.525 I ggml_metal_init: use bfloat            = true
0.00.610.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.004 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.693 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.697 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.717 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.316 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.317 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.317 I llama_init_from_model: graph nodes  = 967
0.00.626.318 I llama_init_from_model: graph splits = 2
0.00.626.319 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.884 I 
0.00.652.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.919 I perplexity: tokenizing the input ..
0.00.656.471 I perplexity: tokenization took 3.552 ms
0.00.656.475 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.470 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.799.001 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.799.026 I llama_perf_context_print:        load time =     643.83 ms
0.00.799.027 I llama_perf_context_print: prompt eval time =     140.76 ms /   128 tokens (    1.10 ms per token,   909.35 tokens per second)
0.00.799.027 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.028 I llama_perf_context_print:       total time =     146.14 ms /   129 tokens
0.00.799.353 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.066s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.974 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.540 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.541 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.542 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.543 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.543 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.545 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.546 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.352 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.365 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.172 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.173 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.174 I llama_model_loader: - type  f32:  194 tensors
0.00.027.174 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.175 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.175 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.175 I print_info: file format = GGUF V3 (latest)
0.00.027.176 I print_info: file type   = Q2_K - Medium
0.00.027.177 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.454 I load: special tokens cache size = 25
0.00.041.525 I load: token to piece cache size = 0.2984 MB
0.00.041.529 I print_info: arch             = gptneox
0.00.041.529 I print_info: vocab_only       = 0
0.00.041.529 I print_info: n_ctx_train      = 2048
0.00.041.529 I print_info: n_embd           = 2048
0.00.041.529 I print_info: n_layer          = 24
0.00.041.533 I print_info: n_head           = 16
0.00.041.534 I print_info: n_head_kv        = 16
0.00.041.534 I print_info: n_rot            = 32
0.00.041.534 I print_info: n_swa            = 0
0.00.041.534 I print_info: n_embd_head_k    = 128
0.00.041.534 I print_info: n_embd_head_v    = 128
0.00.041.535 I print_info: n_gqa            = 1
0.00.041.536 I print_info: n_embd_k_gqa     = 2048
0.00.041.537 I print_info: n_embd_v_gqa     = 2048
0.00.041.537 I print_info: f_norm_eps       = 1.0e-05
0.00.041.538 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.538 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.538 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.538 I print_info: f_logit_scale    = 0.0e+00
0.00.041.539 I print_info: n_ff             = 8192
0.00.041.539 I print_info: n_expert         = 0
0.00.041.539 I print_info: n_expert_used    = 0
0.00.041.539 I print_info: causal attn      = 1
0.00.041.539 I print_info: pooling type     = 0
0.00.041.541 I print_info: rope type        = 2
0.00.041.567 I print_info: rope scaling     = linear
0.00.041.569 I print_info: freq_base_train  = 10000.0
0.00.041.569 I print_info: freq_scale_train = 1
0.00.041.569 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.570 I print_info: rope_finetuned   = unknown
0.00.041.570 I print_info: ssm_d_conv       = 0
0.00.041.570 I print_info: ssm_d_inner      = 0
0.00.041.570 I print_info: ssm_d_state      = 0
0.00.041.573 I print_info: ssm_dt_rank      = 0
0.00.041.573 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.573 I print_info: model type       = 1.4B
0.00.041.574 I print_info: model params     = 1.41 B
0.00.041.574 I print_info: general.name     = 1.4B
0.00.041.575 I print_info: vocab type       = BPE
0.00.041.575 I print_info: n_vocab          = 50304
0.00.041.575 I print_info: n_merges         = 50009
0.00.041.575 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.576 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.576 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.576 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.576 I print_info: LF token         = 187 'Ċ'
0.00.041.576 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.577 I print_info: max token length = 1024
0.00.348.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.998 I load_tensors: offloading output layer to GPU
0.00.348.998 I load_tensors: offloaded 25/25 layers to GPU
0.00.349.015 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.349.016 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.349.871 I llama_init_from_model: n_seq_max     = 1
0.00.349.874 I llama_init_from_model: n_ctx         = 128
0.00.349.874 I llama_init_from_model: n_ctx_per_seq = 128
0.00.349.874 I llama_init_from_model: n_batch       = 128
0.00.349.875 I llama_init_from_model: n_ubatch      = 128
0.00.349.875 I llama_init_from_model: flash_attn    = 0
0.00.349.876 I llama_init_from_model: freq_base     = 10000.0
0.00.349.876 I llama_init_from_model: freq_scale    = 1
0.00.349.877 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.349.878 I ggml_metal_init: allocating
0.00.349.920 I ggml_metal_init: found device: Apple M4
0.00.349.930 I ggml_metal_init: picking default device: Apple M4
0.00.350.948 I ggml_metal_init: using embedded metal library
0.00.355.353 I ggml_metal_init: GPU name:   Apple M4
0.00.355.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.359 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.360 I ggml_metal_init: simdgroup reduction   = true
0.00.355.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.361 I ggml_metal_init: has residency sets    = true
0.00.355.361 I ggml_metal_init: has bfloat            = true
0.00.355.361 I ggml_metal_init: use bfloat            = true
0.00.355.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.632 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.375.264 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.375.267 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.375.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.376.964 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.376.965 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.376.965 I llama_init_from_model: graph nodes  = 967
0.00.376.965 I llama_init_from_model: graph splits = 2
0.00.376.967 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.376.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.863 I 
0.00.404.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.904 I perplexity: tokenizing the input ..
0.00.408.693 I perplexity: tokenization took 3.787 ms
0.00.408.698 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.541.018 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.542.449 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.542.475 I llama_perf_context_print:        load time =     392.88 ms
0.00.542.476 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.01 tokens per second)
0.00.542.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.542.478 I llama_perf_context_print:       total time =     137.61 ms /   129 tokens
0.00.542.843 I ggml_metal_free: deallocating

real	0m0.558s
user	0m0.073s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.934 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.935 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.305 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.309 I llama_model_loader: - type  f32:  194 tensors
0.00.024.309 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.309 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.310 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.310 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.311 I print_info: file format = GGUF V3 (latest)
0.00.024.311 I print_info: file type   = Q3_K - Medium
0.00.024.312 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.786 I load: special tokens cache size = 25
0.00.038.764 I load: token to piece cache size = 0.2984 MB
0.00.038.769 I print_info: arch             = gptneox
0.00.038.769 I print_info: vocab_only       = 0
0.00.038.770 I print_info: n_ctx_train      = 2048
0.00.038.770 I print_info: n_embd           = 2048
0.00.038.770 I print_info: n_layer          = 24
0.00.038.776 I print_info: n_head           = 16
0.00.038.777 I print_info: n_head_kv        = 16
0.00.038.777 I print_info: n_rot            = 32
0.00.038.777 I print_info: n_swa            = 0
0.00.038.778 I print_info: n_embd_head_k    = 128
0.00.038.778 I print_info: n_embd_head_v    = 128
0.00.038.778 I print_info: n_gqa            = 1
0.00.038.779 I print_info: n_embd_k_gqa     = 2048
0.00.038.780 I print_info: n_embd_v_gqa     = 2048
0.00.038.781 I print_info: f_norm_eps       = 1.0e-05
0.00.038.781 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.781 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.781 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.781 I print_info: f_logit_scale    = 0.0e+00
0.00.038.782 I print_info: n_ff             = 8192
0.00.038.782 I print_info: n_expert         = 0
0.00.038.783 I print_info: n_expert_used    = 0
0.00.038.783 I print_info: causal attn      = 1
0.00.038.783 I print_info: pooling type     = 0
0.00.038.783 I print_info: rope type        = 2
0.00.038.783 I print_info: rope scaling     = linear
0.00.038.784 I print_info: freq_base_train  = 10000.0
0.00.038.784 I print_info: freq_scale_train = 1
0.00.038.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.784 I print_info: rope_finetuned   = unknown
0.00.038.784 I print_info: ssm_d_conv       = 0
0.00.038.785 I print_info: ssm_d_inner      = 0
0.00.038.785 I print_info: ssm_d_state      = 0
0.00.038.785 I print_info: ssm_dt_rank      = 0
0.00.038.785 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.785 I print_info: model type       = 1.4B
0.00.038.786 I print_info: model params     = 1.41 B
0.00.038.786 I print_info: general.name     = 1.4B
0.00.038.786 I print_info: vocab type       = BPE
0.00.038.788 I print_info: n_vocab          = 50304
0.00.038.788 I print_info: n_merges         = 50009
0.00.038.788 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.788 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: LF token         = 187 'Ċ'
0.00.038.790 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.790 I print_info: max token length = 1024
0.00.413.189 I load_tensors: offloading 24 repeating layers to GPU
0.00.413.205 I load_tensors: offloading output layer to GPU
0.00.413.206 I load_tensors: offloaded 25/25 layers to GPU
0.00.413.245 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.413.246 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.414.591 I llama_init_from_model: n_seq_max     = 1
0.00.414.600 I llama_init_from_model: n_ctx         = 128
0.00.414.600 I llama_init_from_model: n_ctx_per_seq = 128
0.00.414.601 I llama_init_from_model: n_batch       = 128
0.00.414.601 I llama_init_from_model: n_ubatch      = 128
0.00.414.601 I llama_init_from_model: flash_attn    = 0
0.00.414.603 I llama_init_from_model: freq_base     = 10000.0
0.00.414.604 I llama_init_from_model: freq_scale    = 1
0.00.414.605 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.414.607 I ggml_metal_init: allocating
0.00.414.712 I ggml_metal_init: found device: Apple M4
0.00.414.728 I ggml_metal_init: picking default device: Apple M4
0.00.416.465 I ggml_metal_init: using embedded metal library
0.00.421.986 I ggml_metal_init: GPU name:   Apple M4
0.00.422.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.422.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.422.010 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.422.011 I ggml_metal_init: simdgroup reduction   = true
0.00.422.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.422.011 I ggml_metal_init: has residency sets    = true
0.00.422.011 I ggml_metal_init: has bfloat            = true
0.00.422.012 I ggml_metal_init: use bfloat            = true
0.00.422.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.422.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.441.922 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.445.521 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.445.528 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.445.573 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.448.982 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.448.984 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.448.984 I llama_init_from_model: graph nodes  = 967
0.00.448.985 I llama_init_from_model: graph splits = 2
0.00.448.988 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.448.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.877 I 
0.00.479.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.975 I perplexity: tokenizing the input ..
0.00.486.854 I perplexity: tokenization took 6.876 ms
0.00.486.867 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.624 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.977 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.998 I llama_perf_context_print:        load time =     471.05 ms
0.00.634.999 I llama_perf_context_print: prompt eval time =     145.78 ms /   128 tokens (    1.14 ms per token,   878.06 tokens per second)
0.00.634.999 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.000 I llama_perf_context_print:       total time =     155.12 ms /   129 tokens
0.00.635.377 I ggml_metal_free: deallocating

real	0m0.649s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.920 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.915 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.916 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.918 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.919 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.919 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.920 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.921 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.922 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.463 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.464 I llama_model_loader: - type  f32:  194 tensors
0.00.024.464 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.464 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.465 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.465 I print_info: file format = GGUF V3 (latest)
0.00.024.466 I print_info: file type   = Q4_K - Medium
0.00.024.467 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.499 I load: special tokens cache size = 25
0.00.038.607 I load: token to piece cache size = 0.2984 MB
0.00.038.610 I print_info: arch             = gptneox
0.00.038.610 I print_info: vocab_only       = 0
0.00.038.611 I print_info: n_ctx_train      = 2048
0.00.038.611 I print_info: n_embd           = 2048
0.00.038.611 I print_info: n_layer          = 24
0.00.038.614 I print_info: n_head           = 16
0.00.038.614 I print_info: n_head_kv        = 16
0.00.038.615 I print_info: n_rot            = 32
0.00.038.615 I print_info: n_swa            = 0
0.00.038.615 I print_info: n_embd_head_k    = 128
0.00.038.615 I print_info: n_embd_head_v    = 128
0.00.038.616 I print_info: n_gqa            = 1
0.00.038.617 I print_info: n_embd_k_gqa     = 2048
0.00.038.617 I print_info: n_embd_v_gqa     = 2048
0.00.038.618 I print_info: f_norm_eps       = 1.0e-05
0.00.038.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.620 I print_info: f_logit_scale    = 0.0e+00
0.00.038.621 I print_info: n_ff             = 8192
0.00.038.621 I print_info: n_expert         = 0
0.00.038.621 I print_info: n_expert_used    = 0
0.00.038.621 I print_info: causal attn      = 1
0.00.038.621 I print_info: pooling type     = 0
0.00.038.622 I print_info: rope type        = 2
0.00.038.624 I print_info: rope scaling     = linear
0.00.038.624 I print_info: freq_base_train  = 10000.0
0.00.038.624 I print_info: freq_scale_train = 1
0.00.038.625 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.625 I print_info: rope_finetuned   = unknown
0.00.038.625 I print_info: ssm_d_conv       = 0
0.00.038.625 I print_info: ssm_d_inner      = 0
0.00.038.625 I print_info: ssm_d_state      = 0
0.00.038.625 I print_info: ssm_dt_rank      = 0
0.00.038.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.626 I print_info: model type       = 1.4B
0.00.038.626 I print_info: model params     = 1.41 B
0.00.038.626 I print_info: general.name     = 1.4B
0.00.038.627 I print_info: vocab type       = BPE
0.00.038.627 I print_info: n_vocab          = 50304
0.00.038.627 I print_info: n_merges         = 50009
0.00.038.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: LF token         = 187 'Ċ'
0.00.038.629 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.629 I print_info: max token length = 1024
0.00.548.511 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.522 I load_tensors: offloading output layer to GPU
0.00.548.523 I load_tensors: offloaded 25/25 layers to GPU
0.00.548.555 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.548.561 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.550.067 I llama_init_from_model: n_seq_max     = 1
0.00.550.071 I llama_init_from_model: n_ctx         = 128
0.00.550.072 I llama_init_from_model: n_ctx_per_seq = 128
0.00.550.072 I llama_init_from_model: n_batch       = 128
0.00.550.073 I llama_init_from_model: n_ubatch      = 128
0.00.550.073 I llama_init_from_model: flash_attn    = 0
0.00.550.075 I llama_init_from_model: freq_base     = 10000.0
0.00.550.075 I llama_init_from_model: freq_scale    = 1
0.00.550.076 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.550.078 I ggml_metal_init: allocating
0.00.550.140 I ggml_metal_init: found device: Apple M4
0.00.550.153 I ggml_metal_init: picking default device: Apple M4
0.00.551.809 I ggml_metal_init: using embedded metal library
0.00.557.604 I ggml_metal_init: GPU name:   Apple M4
0.00.557.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.612 I ggml_metal_init: simdgroup reduction   = true
0.00.557.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.613 I ggml_metal_init: has residency sets    = true
0.00.557.613 I ggml_metal_init: has bfloat            = true
0.00.557.613 I ggml_metal_init: use bfloat            = true
0.00.557.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.602 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.580.222 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.580.233 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.580.268 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.583.380 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.583.381 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.583.382 I llama_init_from_model: graph nodes  = 967
0.00.583.382 I llama_init_from_model: graph splits = 2
0.00.583.385 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.583.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.722 I 
0.00.614.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.812 I perplexity: tokenizing the input ..
0.00.622.362 I perplexity: tokenization took 7.547 ms
0.00.622.377 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.540 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.771.875 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.771.903 I llama_perf_context_print:        load time =     605.79 ms
0.00.771.904 I llama_perf_context_print: prompt eval time =     147.25 ms /   128 tokens (    1.15 ms per token,   869.29 tokens per second)
0.00.771.904 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.905 I llama_perf_context_print:       total time =     157.18 ms /   129 tokens
0.00.772.276 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.080s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.167 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.839 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.534 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.262 I llama_model_loader: - type  f32:  194 tensors
0.00.025.263 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.263 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.263 I print_info: file format = GGUF V3 (latest)
0.00.025.264 I print_info: file type   = Q5_K - Medium
0.00.025.265 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.312 I load: special tokens cache size = 25
0.00.039.351 I load: token to piece cache size = 0.2984 MB
0.00.039.353 I print_info: arch             = gptneox
0.00.039.354 I print_info: vocab_only       = 0
0.00.039.354 I print_info: n_ctx_train      = 2048
0.00.039.354 I print_info: n_embd           = 2048
0.00.039.354 I print_info: n_layer          = 24
0.00.039.357 I print_info: n_head           = 16
0.00.039.358 I print_info: n_head_kv        = 16
0.00.039.358 I print_info: n_rot            = 32
0.00.039.358 I print_info: n_swa            = 0
0.00.039.359 I print_info: n_embd_head_k    = 128
0.00.039.359 I print_info: n_embd_head_v    = 128
0.00.039.359 I print_info: n_gqa            = 1
0.00.039.360 I print_info: n_embd_k_gqa     = 2048
0.00.039.364 I print_info: n_embd_v_gqa     = 2048
0.00.039.364 I print_info: f_norm_eps       = 1.0e-05
0.00.039.365 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.365 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.365 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.365 I print_info: f_logit_scale    = 0.0e+00
0.00.039.372 I print_info: n_ff             = 8192
0.00.039.375 I print_info: n_expert         = 0
0.00.039.375 I print_info: n_expert_used    = 0
0.00.039.375 I print_info: causal attn      = 1
0.00.039.375 I print_info: pooling type     = 0
0.00.039.376 I print_info: rope type        = 2
0.00.039.376 I print_info: rope scaling     = linear
0.00.039.377 I print_info: freq_base_train  = 10000.0
0.00.039.377 I print_info: freq_scale_train = 1
0.00.039.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.378 I print_info: rope_finetuned   = unknown
0.00.039.378 I print_info: ssm_d_conv       = 0
0.00.039.378 I print_info: ssm_d_inner      = 0
0.00.039.378 I print_info: ssm_d_state      = 0
0.00.039.378 I print_info: ssm_dt_rank      = 0
0.00.039.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.379 I print_info: model type       = 1.4B
0.00.039.379 I print_info: model params     = 1.41 B
0.00.039.379 I print_info: general.name     = 1.4B
0.00.039.380 I print_info: vocab type       = BPE
0.00.039.380 I print_info: n_vocab          = 50304
0.00.039.380 I print_info: n_merges         = 50009
0.00.039.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: LF token         = 187 'Ċ'
0.00.039.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.383 I print_info: max token length = 1024
0.00.591.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.828 I load_tensors: offloading output layer to GPU
0.00.591.828 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.861 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.862 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.593.374 I llama_init_from_model: n_seq_max     = 1
0.00.593.380 I llama_init_from_model: n_ctx         = 128
0.00.593.380 I llama_init_from_model: n_ctx_per_seq = 128
0.00.593.381 I llama_init_from_model: n_batch       = 128
0.00.593.382 I llama_init_from_model: n_ubatch      = 128
0.00.593.382 I llama_init_from_model: flash_attn    = 0
0.00.593.384 I llama_init_from_model: freq_base     = 10000.0
0.00.593.385 I llama_init_from_model: freq_scale    = 1
0.00.593.386 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.388 I ggml_metal_init: allocating
0.00.593.482 I ggml_metal_init: found device: Apple M4
0.00.593.496 I ggml_metal_init: picking default device: Apple M4
0.00.595.237 I ggml_metal_init: using embedded metal library
0.00.601.751 I ggml_metal_init: GPU name:   Apple M4
0.00.601.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.756 I ggml_metal_init: simdgroup reduction   = true
0.00.601.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.757 I ggml_metal_init: has residency sets    = true
0.00.601.757 I ggml_metal_init: has bfloat            = true
0.00.601.757 I ggml_metal_init: use bfloat            = true
0.00.601.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.564 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.994 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.997 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.622.041 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.237 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.239 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.239 I llama_init_from_model: graph nodes  = 967
0.00.625.240 I llama_init_from_model: graph splits = 2
0.00.625.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.061 I 
0.00.661.143 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.151 I perplexity: tokenizing the input ..
0.00.667.207 I perplexity: tokenization took 6.054 ms
0.00.667.211 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.718 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.049 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.075 I llama_perf_context_print:        load time =     650.88 ms
0.00.814.075 I llama_perf_context_print: prompt eval time =     145.11 ms /   128 tokens (    1.13 ms per token,   882.11 tokens per second)
0.00.814.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.077 I llama_perf_context_print:       total time =     153.02 ms /   129 tokens
0.00.814.463 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.077s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.563 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.943 I llama_model_loader: - type  f32:  194 tensors
0.00.023.943 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.944 I print_info: file format = GGUF V3 (latest)
0.00.023.944 I print_info: file type   = Q6_K
0.00.023.945 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.514 I load: special tokens cache size = 25
0.00.037.487 I load: token to piece cache size = 0.2984 MB
0.00.037.489 I print_info: arch             = gptneox
0.00.037.489 I print_info: vocab_only       = 0
0.00.037.490 I print_info: n_ctx_train      = 2048
0.00.037.490 I print_info: n_embd           = 2048
0.00.037.490 I print_info: n_layer          = 24
0.00.037.493 I print_info: n_head           = 16
0.00.037.494 I print_info: n_head_kv        = 16
0.00.037.494 I print_info: n_rot            = 32
0.00.037.495 I print_info: n_swa            = 0
0.00.037.495 I print_info: n_embd_head_k    = 128
0.00.037.495 I print_info: n_embd_head_v    = 128
0.00.037.496 I print_info: n_gqa            = 1
0.00.037.496 I print_info: n_embd_k_gqa     = 2048
0.00.037.497 I print_info: n_embd_v_gqa     = 2048
0.00.037.498 I print_info: f_norm_eps       = 1.0e-05
0.00.037.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.499 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.499 I print_info: f_logit_scale    = 0.0e+00
0.00.037.499 I print_info: n_ff             = 8192
0.00.037.500 I print_info: n_expert         = 0
0.00.037.500 I print_info: n_expert_used    = 0
0.00.037.500 I print_info: causal attn      = 1
0.00.037.500 I print_info: pooling type     = 0
0.00.037.500 I print_info: rope type        = 2
0.00.037.500 I print_info: rope scaling     = linear
0.00.037.501 I print_info: freq_base_train  = 10000.0
0.00.037.501 I print_info: freq_scale_train = 1
0.00.037.501 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.501 I print_info: rope_finetuned   = unknown
0.00.037.502 I print_info: ssm_d_conv       = 0
0.00.037.502 I print_info: ssm_d_inner      = 0
0.00.037.502 I print_info: ssm_d_state      = 0
0.00.037.502 I print_info: ssm_dt_rank      = 0
0.00.037.502 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.503 I print_info: model type       = 1.4B
0.00.037.503 I print_info: model params     = 1.41 B
0.00.037.504 I print_info: general.name     = 1.4B
0.00.037.505 I print_info: vocab type       = BPE
0.00.037.505 I print_info: n_vocab          = 50304
0.00.037.505 I print_info: n_merges         = 50009
0.00.037.506 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.506 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.508 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.508 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.508 I print_info: LF token         = 187 'Ċ'
0.00.037.508 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.509 I print_info: max token length = 1024
0.00.597.036 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.039 I load_tensors: offloading output layer to GPU
0.00.597.040 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.064 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.597.067 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.598.527 I llama_init_from_model: n_seq_max     = 1
0.00.598.529 I llama_init_from_model: n_ctx         = 128
0.00.598.529 I llama_init_from_model: n_ctx_per_seq = 128
0.00.598.530 I llama_init_from_model: n_batch       = 128
0.00.598.533 I llama_init_from_model: n_ubatch      = 128
0.00.598.534 I llama_init_from_model: flash_attn    = 0
0.00.598.535 I llama_init_from_model: freq_base     = 10000.0
0.00.598.535 I llama_init_from_model: freq_scale    = 1
0.00.598.540 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.598.542 I ggml_metal_init: allocating
0.00.598.589 I ggml_metal_init: found device: Apple M4
0.00.598.601 I ggml_metal_init: picking default device: Apple M4
0.00.599.959 I ggml_metal_init: using embedded metal library
0.00.605.748 I ggml_metal_init: GPU name:   Apple M4
0.00.605.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.753 I ggml_metal_init: simdgroup reduction   = true
0.00.605.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.753 I ggml_metal_init: has residency sets    = true
0.00.605.754 I ggml_metal_init: has bfloat            = true
0.00.605.754 I ggml_metal_init: use bfloat            = true
0.00.605.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.280 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.625.288 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.625.324 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.465 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.467 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.467 I llama_init_from_model: graph nodes  = 967
0.00.628.467 I llama_init_from_model: graph splits = 2
0.00.628.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.186 I 
0.00.665.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.282 I perplexity: tokenizing the input ..
0.00.671.110 I perplexity: tokenization took 5.827 ms
0.00.671.116 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.775 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.823.266 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.823.293 I llama_perf_context_print:        load time =     656.34 ms
0.00.823.294 I llama_perf_context_print: prompt eval time =     150.43 ms /   128 tokens (    1.18 ms per token,   850.87 tokens per second)
0.00.823.294 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.295 I llama_perf_context_print:       total time =     158.11 ms /   129 tokens
0.00.823.691 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.074s
sys	0m0.132s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.329 I build: 4611 (53debe6f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.462 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.576 I llama_model_loader: - type  f32:  194 tensors
0.00.051.577 I llama_model_loader: - type  f16:   98 tensors
0.00.051.578 I print_info: file format = GGUF V3 (latest)
0.00.051.578 I print_info: file type   = all F32 (guessed)
0.00.051.579 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.327 I load: special tokens cache size = 25
0.00.071.130 I load: token to piece cache size = 0.2984 MB
0.00.071.133 I print_info: arch             = gptneox
0.00.071.133 I print_info: vocab_only       = 0
0.00.071.134 I print_info: n_ctx_train      = 2048
0.00.071.134 I print_info: n_embd           = 2048
0.00.071.134 I print_info: n_layer          = 24
0.00.071.137 I print_info: n_head           = 16
0.00.071.138 I print_info: n_head_kv        = 16
0.00.071.140 I print_info: n_rot            = 32
0.00.071.140 I print_info: n_swa            = 0
0.00.071.140 I print_info: n_embd_head_k    = 128
0.00.071.140 I print_info: n_embd_head_v    = 128
0.00.071.141 I print_info: n_gqa            = 1
0.00.071.142 I print_info: n_embd_k_gqa     = 2048
0.00.071.143 I print_info: n_embd_v_gqa     = 2048
0.00.071.143 I print_info: f_norm_eps       = 1.0e-05
0.00.071.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.144 I print_info: f_logit_scale    = 0.0e+00
0.00.071.145 I print_info: n_ff             = 8192
0.00.071.145 I print_info: n_expert         = 0
0.00.071.145 I print_info: n_expert_used    = 0
0.00.071.145 I print_info: causal attn      = 1
0.00.071.145 I print_info: pooling type     = 0
0.00.071.145 I print_info: rope type        = 2
0.00.071.151 I print_info: rope scaling     = linear
0.00.071.153 I print_info: freq_base_train  = 10000.0
0.00.071.154 I print_info: freq_scale_train = 1
0.00.071.154 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.154 I print_info: rope_finetuned   = unknown
0.00.071.155 I print_info: ssm_d_conv       = 0
0.00.071.155 I print_info: ssm_d_inner      = 0
0.00.071.155 I print_info: ssm_d_state      = 0
0.00.071.155 I print_info: ssm_dt_rank      = 0
0.00.071.155 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.155 I print_info: model type       = 1.4B
0.00.071.156 I print_info: model params     = 1.41 B
0.00.071.156 I print_info: general.name     = 1.4B
0.00.071.157 I print_info: vocab type       = BPE
0.00.071.157 I print_info: n_vocab          = 50304
0.00.071.157 I print_info: n_merges         = 50009
0.00.071.159 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.159 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.159 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.159 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.160 I print_info: LF token         = 187 'Ċ'
0.00.071.160 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.160 I print_info: max token length = 1024
0.01.362.383 I load_tensors: offloading 24 repeating layers to GPU
0.01.362.388 I load_tensors: offloading output layer to GPU
0.01.362.389 I load_tensors: offloaded 25/25 layers to GPU
0.01.362.412 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.362.414 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.363.412 I llama_init_from_model: n_seq_max     = 1
0.01.363.413 I llama_init_from_model: n_ctx         = 128
0.01.363.413 I llama_init_from_model: n_ctx_per_seq = 128
0.01.363.414 I llama_init_from_model: n_batch       = 128
0.01.363.414 I llama_init_from_model: n_ubatch      = 128
0.01.363.414 I llama_init_from_model: flash_attn    = 0
0.01.363.414 I llama_init_from_model: freq_base     = 10000.0
0.01.363.415 I llama_init_from_model: freq_scale    = 1
0.01.363.415 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.363.416 I ggml_metal_init: allocating
0.01.363.459 I ggml_metal_init: found device: Apple M4
0.01.363.466 I ggml_metal_init: picking default device: Apple M4
0.01.364.432 I ggml_metal_init: using embedded metal library
0.01.368.305 I ggml_metal_init: GPU name:   Apple M4
0.01.368.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.368.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.368.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.368.309 I ggml_metal_init: simdgroup reduction   = true
0.01.368.309 I ggml_metal_init: simdgroup matrix mul. = true
0.01.368.309 I ggml_metal_init: has residency sets    = true
0.01.368.310 I ggml_metal_init: has bfloat            = true
0.01.368.310 I ggml_metal_init: use bfloat            = true
0.01.368.310 I ggml_metal_init: hasUnifiedMemory      = true
0.01.368.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.379.002 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.380.655 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.380.657 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.380.670 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.382.270 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.382.271 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.382.272 I llama_init_from_model: graph nodes  = 967
0.01.382.272 I llama_init_from_model: graph splits = 2
0.01.382.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.382.273 I 
0.01.382.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.382.310 I compute_imatrix: tokenizing the input ..
0.01.386.265 I compute_imatrix: tokenization took 3.955 ms
0.01.386.267 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.652.582 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.654.994 I llama_perf_context_print:        load time =    1631.85 ms
0.01.654.995 I llama_perf_context_print: prompt eval time =     264.59 ms /   128 tokens (    2.07 ms per token,   483.77 tokens per second)
0.01.654.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.654.996 I llama_perf_context_print:       total time =    1634.26 ms /   129 tokens
0.01.655.562 I ggml_metal_free: deallocating

real	0m1.846s
user	0m0.123s
sys	0m0.274s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4611 (53debe6f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105904a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105905160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105905710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105905cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105906270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105906820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105906dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105907380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105907930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105907e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105908330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105908830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105909350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105909b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10590a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10590aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10590b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10590b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10590bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10590c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10590ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10590d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10590dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10590e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10590ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10590ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10590f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1059101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105910700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1059109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105910e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105911120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1059119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105911ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1059121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105912650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105912af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105912f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105913430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1059138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105913d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105914210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1059146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105914b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105914e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105915420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105915a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105916350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105916960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105916f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105917580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105917b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1059181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1059187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105918fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105919440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1059198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105919ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10591a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10591a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10591ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10591b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10591b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10591ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10591bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10591c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10591c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10591ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10591d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10591d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10591daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10591df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10591e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10591e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10591ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10591f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10591f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10591fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1059203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105920910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105920e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1059213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105921900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105921e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1059223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1059228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105922e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105923390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1059238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105923e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105924380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1059248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105924e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105925370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1059258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105925e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105926360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105916040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1059267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105926f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1059274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105927a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105927f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1059284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105928a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105928f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1059294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105929a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105929f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10592a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10592a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10592af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10592b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10592b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10592bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10592c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10592c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10592cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10592d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10592d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10592d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10592de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10592e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10592e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10592ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10592f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10592f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10592f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10592fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105930330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1059307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105930c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105931110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1059315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105931a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105931ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105932390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105932830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105932cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105933170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105933610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105933ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105933f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1059343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105934890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105934d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1059351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105935670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105935b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105935fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105936450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1059368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105936d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ee04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ee046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ee04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ee04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ee053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ee05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ee05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ee06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ee065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ee06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ee06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ee07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ee07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ee07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ee08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ee084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ee08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ee08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ee09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ee09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ee09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ee09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ee0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ee0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ee0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ee0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ee0b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ee0ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ee0be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ee0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ee0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ee0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ee0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ee0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ee0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ee0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ee0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ee0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ee0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ee0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ee0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ee0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ee0fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ee10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ee10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ee109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ee10e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ee112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ee11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ee11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ee12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ee12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ee128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ee12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ee132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ee13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ee13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ee14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ee149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ee15080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ee15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ee15bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ee161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ee16750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ee16d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ee172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ee17860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ee17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ee183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ee18970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ee18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ee194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ee19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ee1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ee1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ee1ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ee1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ee1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ee1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ee1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ee1c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ee1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ee1d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ee1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ee1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ee1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ee1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ee1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ee1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ee1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ee200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ee20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ee20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ee211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ee217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ee21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ee22300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ee228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ee22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ee23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ee239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ee23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ee24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ee24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ee25080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ee25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ee25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ee26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ee26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ee26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ee272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ee27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ee27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ee283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ee28960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ee28e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ee29360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ee29860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ee29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ee2a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ee2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ee2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ee2b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ee2b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ee2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ee2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ee2c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ee2ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ee2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ee2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ee2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ee2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ee2ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ee2f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ee2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ee2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ee30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ee30750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.722.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.944 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ee1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ee1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ee1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ee17b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ee26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ee247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ee225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ee203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ee18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ee15eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ee1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ee1bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ee214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ee1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ee25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ee19d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ee23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ee1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ee1ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ee1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ee24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ee1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ee17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ee1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ee25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ee1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ee1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ee21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ee191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ee236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ee180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ee26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ee1f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ee23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ee1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ee28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ee16fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ee280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ee16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ee26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ee20950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ee22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ee258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ee24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ee1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ee15340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ee30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ee30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ee30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ee31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ee31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ee317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ee31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ee31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ee322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ee32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ee32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ee32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ee32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ee33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ee33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ee335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ee338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ee33b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ee33e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ee340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ee343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ee34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ee34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ee34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ee34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ee35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ee358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ee35d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ee361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ee36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ee36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ee36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ee37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ee377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ee37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ee380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ee38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ee389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ee38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ee39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ee396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ee39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ee39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ee3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ee3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ee3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ee3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ee3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ee3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ee3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ee3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ee3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ee3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ee3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ee3d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ee3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ee3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ee3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ee3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ee3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ee3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ee3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ee3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ee3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ee40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ee405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ee40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ee40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ee41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ee417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ee41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ee42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ee424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ee42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ee42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ee43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ee436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ee43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ee43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ee44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ee44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ee44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ee45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ee455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ee45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ee45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ee46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ee46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ee46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ee47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ee474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ee47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ee47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ee48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ee48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ee48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ee48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ee493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ee49850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ee49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ee4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ee4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ee4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ee4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ee4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ee4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ee4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ee4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ee4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ee4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ee4cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ee4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ee4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ee4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ee4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ee4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ee4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ee4eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ee4f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ee4f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ee4fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ee501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ee506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ee50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ee510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ee515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ee51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ee51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ee524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ee529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ee52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ee533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ee538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ee53db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ee542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ee547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ee54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ee551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ee556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ee55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ee560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ee565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ee56ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ee56fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ee574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ee579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ee57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ee583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ee58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ee58f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ee594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ee59a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ed04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ed044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ed04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ed04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ed05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ed056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ed05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ed05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ed06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ed06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ed06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ed07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ed07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ed07fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ed082a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ed08710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ed08b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ed08ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ed09460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ed098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ed09d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ed0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ed0a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ed0aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ed0af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ed0b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ed0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ed0bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ed0c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ed0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ed0c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ed0ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ed0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ed0d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ed0db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ed0dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ed0e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ed0e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ed0ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ed0f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ed0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ed0fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ed0fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ed10350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ed107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ed10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ed110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ed11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ed11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ed11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ed12260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ed126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ed12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ed12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ed13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ed13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ed13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ed14170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ed145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ed14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ed14ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ed15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ed157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ed15c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ed16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ed164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ed16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ed16dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ed17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ed176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ed17b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ed17f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ed18400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ed18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ed18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ed19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ed195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ed19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ed19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ed1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ed1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ed1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ed1b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ed1b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ed1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ed1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ed1cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ed1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ed1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ed1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ed1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ed1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ed1ec50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105917840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105917e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105919e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10590f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105915cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105916610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105916c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1059156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1059150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105918460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105917230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10590e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105904080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10591a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1059113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1059116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10590f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10590fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10590fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105937050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105937310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1059375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105937890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105937b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105937e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1059380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105938390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105938650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105938910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105938bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105938e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105939150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105939410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1059396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105939990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105939c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105939f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10593a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10593a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10593a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10593aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10593acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10593af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10593b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10593b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10593b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10593ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10593bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10593c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10593c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10593c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10593c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10593cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10593cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10593d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10593d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10593d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10593d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10593db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10593de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10593e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10593e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10593e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10593e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10593ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10593eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10593f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10593f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10593f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10593f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10593fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10593ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105940210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1059404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105940790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105940a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105940d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105940fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105941290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105941550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105941810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105941ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105941d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105942050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105942310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1059425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105942890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105942b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105942e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1059430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105943390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105943650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105943910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105943bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105943e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105944150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105944410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1059446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105944990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105944c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105944f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1059451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105945490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105945750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105945a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105945cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105945f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105946250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105946510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1059467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105946a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105946d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105947010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1059472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105947590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105947850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105947b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105947fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105948450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1059488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105948d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105949230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1059496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105949b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10594a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10594a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10594a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10594adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10594b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10594b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10594bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10594c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10594c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10594c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10594ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10594d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10594d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10594dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10594e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10594e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10594ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10594eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10594f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10594f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10594fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105950130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1059505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105950a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105950f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1059513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105951850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105951cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105952190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105952630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105952ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105952f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105953410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1059538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105953d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1059541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105954690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105954b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105954fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105955470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105955910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105955db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105956250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1059566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105956b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105957030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1059574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105957970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105957e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1059582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105958750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105958bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105959090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105959530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1059599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105959e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10595a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10595a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10595ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10595b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10595b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10595ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10595bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10595c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10595c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10595ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10595d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10595d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10595da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10595dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10595e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10595ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10595efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10595f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10595f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10595feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1059604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105960cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105961150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105961410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105961a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105962030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105962820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105962cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105963160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105963600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105963db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105964300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105964850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105964da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1059652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105965840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105965d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1059662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105966830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105966d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1059672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105967820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105967d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1059682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105968810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105968d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1059692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105969800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105969d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10596a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10596a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10596ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10596b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10596b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10596bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10596c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10596c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10596cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10596d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10596d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10596dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10596e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10596e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10596ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10596f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10596f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10596fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105970240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105970790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105970ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105971230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105971780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105971cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105972220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105972770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105972cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105973210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105973760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105973cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105974200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105974750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105974ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1059751f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105975740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105975c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1059761e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105976730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105976bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105977070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105977510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1059779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105977e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1059782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105978790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105978c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1059790d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105979570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105979a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105979eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10597a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10597a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10597ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10597b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10597b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10597c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10597c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10597ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10597d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10597d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10597dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10597e1e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.772s
user	0m0.282s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4611 (53debe6f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15600a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15600ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15600b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15600b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15600bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15600c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15600c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15600cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15600d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15600d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15600dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15600e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15600ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15600f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15600fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1560103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156011220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156011940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156012110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156012830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156013670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156013f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156014630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1560148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156014f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156015b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1560160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156016370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156016810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156016ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1560178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156017b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156018000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1560184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156018940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156018de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156019720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156019bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15601a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15601a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15601a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15601add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15601b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15601bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15601c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15601c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15601cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15601d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15601db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15601e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15601e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15601edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15601f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15601f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15601fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156020350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156020610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156020ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156020f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1560213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156021890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156021d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1560221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156022670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156022b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156022fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156023450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1560238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1560242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156024830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156024d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1560252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156025820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156025d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1560262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156026810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156026d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1560272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156027800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156027d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1560282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1560287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156028d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156029290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1560297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156029d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15602a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15602a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15602ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15602b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15602b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15602bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15601b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15602c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15602c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15602ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15602d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15602d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15602de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15602e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15602e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15602ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15602f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15602f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15602fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1560303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1560308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156030e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1560312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156031780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156031c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1560320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156032560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156032a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156032ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156033340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1560337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156033c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156034120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1560345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156034a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156034f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1560353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156035840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156035ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156036180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156036620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156036ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156036f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156037400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1560378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156037d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1560381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156038680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156038b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156038fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156039460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156039900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156039da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15603a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15603a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15603ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15603b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15603b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15603b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15603be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15603c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15603c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15603cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15603d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15603d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15603d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15603de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15603e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15603e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15603ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15603f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15603f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15603fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15603fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156040360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156040800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156040ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156041140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1560415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156041a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156041f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1560423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156042860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156042d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1560431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156043640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156043ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156043f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156044420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1560448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156044d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156045200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1560456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156045b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156045fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156046480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156046920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156046dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156047700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156047ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156048040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156048590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156048ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156049030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156049580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156049840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156049e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15604a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15604aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15604b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15604b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15604b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15604bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15604c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15604cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15604d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15604d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15604dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15604e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15604e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15604ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15604f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15604f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15604fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156050340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156050890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156050de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156051330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156051880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156051dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156052320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156052870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156052dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156053310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156053860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156053db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156054300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156054850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156054da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1560552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156055840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156055d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1560562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156056830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156056d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1560572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156057820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156057d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1560582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156058810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156058d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1560592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156059800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156059d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15605a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15605a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15605ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15605b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15605b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15605bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15605c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15605c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15605cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15605d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15605d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15605dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15605e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15605e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15605ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15605f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15605f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15605fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156060240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156060790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156060ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156061180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156061620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156061ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156061f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156062400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1560628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156062d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1560631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156063680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156063b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156063fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156064460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156064900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156064da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156065240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156065790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156065eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1560665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156066cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156067410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1560676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156067ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156068180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156068790 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.553 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154e059f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154e05e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154e062d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154e06740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154e06bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154e07020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154e07490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154e07900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154e07d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154e081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154e08650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154e08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154e09860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154e0a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154e0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154e0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154e0b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154e0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154e0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154e0cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154e0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154e0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154e0e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154e0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154e0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154e0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154e0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154e0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154e0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154e10240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154e106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154e10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154e11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154e11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154e11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154e11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154e12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154e124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154e12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154e12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154e13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154e13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154e13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154e13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154e143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154e14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154e14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154e15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154e155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154e15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154e15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154e162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154e16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154e16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154e17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154e174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154e17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154e17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154e18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x154e18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154e18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154e190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154e19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154e199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154e19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154e1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154e1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154e1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154e1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154e1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154e1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154e1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154e1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154e1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154e1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154e1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154e1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154e1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154e1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154e1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154e1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154e1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154e1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154e1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154e1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154e1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154e1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154e20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154e208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154e20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154e21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154e21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154e21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154e21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154e22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154e227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154e22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154e230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154e23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154e23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154e23df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154e24260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154e246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154e24b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154e24fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154e25420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154e25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154e25d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154e26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154e265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154e26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154e26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154e27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154e277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154e27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154e28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154e284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154e28960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154e28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154e29240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154e296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154e29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154e29f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154e2a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154e2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154e2ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154e2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154e2b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154e2ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154e2c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154e2c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154e2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154e2d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154e2d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154e2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154e2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154e2e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154e2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154e2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154e2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154e2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154e2f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154e2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154e30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154e305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154e30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154e30e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154e312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154e31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154e31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154e32040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154e324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154e32920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154e32d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154e33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154e33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154e33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154e33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154e343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154e34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154e34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154e35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154e35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154e359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154e35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154e36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154e368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x154e36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154e371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154e37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154e37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154e37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154e38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154e387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154e38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154e390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154e39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154e399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154e39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154e3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154e3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154e3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154e3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154e3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154e3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154e3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154e3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154e3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154e3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154e3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154e3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154e3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154e3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154e3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154e3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154e3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154e3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154e3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154e3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154e3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154e3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154e40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154e40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154e40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154e41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154e415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154e41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154e41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154e42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154e42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154e431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154e43460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154e438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154e43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154e441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154e44620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154e44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154e44f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154e45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154e457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154e45c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154e460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154e46530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154e469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154e46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154e47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154e476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154e47b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154e47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154e48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154e488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154e48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154e49190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154e49600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154e49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154e49ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154e4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154e4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154e4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154e4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154e4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154e4b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154e4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154e4c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154e4c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154e4cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154e4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154e4d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154e4d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154e4dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154e4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154e4e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154e4ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154e4eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154e4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154e4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154e4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154e50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154e504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154e50960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154e50dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154e51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154e516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154e51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154e51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154e52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154e52870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154e52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154e53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154e535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154e53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154e53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154e54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154e54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154e54bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154e55060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154e554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154e55940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154e55db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154e56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154e56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154e56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154e57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154e57c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154e583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154e58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154e58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154e59200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154e59800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154e59e10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156068440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15604a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156049b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15604a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15601d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15601d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15601f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15604c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156014bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15601b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15601bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15601c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15601aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15601cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156013bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15601fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15602c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156067990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156016d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156017050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15604c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15604ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1560151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156015480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156068bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156068eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156069170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156069430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1560696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1560699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156069c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156069f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15606a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15606a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15606a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15606aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15606acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15606afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15606b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15606b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15606b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15606bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15606bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15606c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15606c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15606c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15606c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15606cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15606cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15606d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15606d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15606d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15606d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15606dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15606de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15606e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15606e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15606e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15606e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15606ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15606eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15606f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15606f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15606f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15606f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15606fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15606ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156070230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1560704f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1560707b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156070a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156070d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156070ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1560712b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156071570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156071830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156071af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156071db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156072070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156072330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1560725f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1560728b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156072b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156072e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1560730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1560733b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156073670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156073930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156073bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156073eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156074170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156074430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1560746f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1560749b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156074c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156074f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1560751f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1560754b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156075770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156075a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156075cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156075fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156076270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156076530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1560767f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156076ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156076d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156077030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1560772f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1560775b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156077870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156077b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156077df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1560780b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156078370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156078630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1560788f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156078bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156078e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156079130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1560793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1560796b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156079970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156079c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156079ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15607a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15607a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15607a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15607a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15607acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15607af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15607b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15607b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15607b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15607ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15607bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15607bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15607c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15607c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15607c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15607caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15607cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15607d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15607d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15607d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15607d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15607db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15607de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15607e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15607e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15607e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15607e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15607ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15607eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15607f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15607f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15607f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15607f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15607fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15607ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1560801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1560804b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156080770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156080a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156080cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156080fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156081270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156081530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1560817f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156081ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156081d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156082030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1560822f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1560825b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156082870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156082b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156082df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1560830b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156083370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156083630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1560838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156083bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156083e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156084130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1560843f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1560846b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156084970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156084c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156084ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1560851b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156085470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156085730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1560859f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156085cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156085f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156086230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1560864f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1560867b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156086a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156086d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156086ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1560872b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156087570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156087830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156087af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156087db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156088070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156088470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156088910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1560890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156089380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156089640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156089ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156089f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15608a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15608a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15608ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15608b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15608b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15608b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15608be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15608c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15608c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15608cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15608cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15608d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15608d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15608dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15608e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15608e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15608ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15608ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15608f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15608f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15608fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1560900c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156090530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1560909a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156090e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156091280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1560916f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156091b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156091fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156092440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1560928b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156092d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156093190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156093600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156093a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156093ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156094350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1560947c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156094c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1560950a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156095510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156095980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156095df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156096260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1560966d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156096b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156096fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156097420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156097890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156097d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156098170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1560985e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156098a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156098ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156099330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1560997a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156099c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15609a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15609a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15609a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15609add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15609b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15609b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15609bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15609bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15609c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15609c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15609cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15609d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15609de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15609e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15609ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15609ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15609f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15609fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1560a0030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.962s
user	0m0.237s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
